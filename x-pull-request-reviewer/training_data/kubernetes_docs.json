{
  "technology": "kubernetes",
  "base_url": "https://kubernetes.io/docs/",
  "pages": [
    {
      "url": "https://kubernetes.io/docs/",
      "title": "Kubernetes Documentation | Kubernetes",
      "content": "Kubernetes Documentation\nDocumentation\nKubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerized applications. The open source project is hosted by the Cloud Native Computing Foundation (\nCNCF\n).\nUnderstand Kubernetes\nLearn about Kubernetes and its fundamental concepts.\nWhy Kubernetes?\nComponents of a cluster\nThe Kubernetes API\nObjects In Kubernetes\nContainers\nWorkloads and Pods\nView Concepts\nTry Kubernetes\nFollow tutorials to learn how to deploy applications in Kubernetes.\nHello Minikube\nWalkthrough the basics\nStateless Example: PHP Guestbook with Redis\nStateful Example: Wordpress with Persistent Volumes\nView Tutorials\nSet up a K8s cluster\nGet Kubernetes running based on your resources and needs.\nLearning environment\nProduction environment\nInstall the kubeadm setup tool\nSecuring a cluster\nkubeadm command reference\nSet up Kubernetes\nLearn how to use Kubernetes\nLook up common tasks and how to perform them using a short sequence of steps.\nkubectl Quick Reference\nInstall kubectl\nConfigure access to clusters\nUse the Web UI Dashboard\nConfigure a Pod to Use a ConfigMap\nGetting help\nView Tasks\nLook up reference information\nBrowse terminology, command line syntax, API resource types, and setup tool documentation.\nGlossary\nkubectl command line tool\nLabels, annotations and taints\nKubernetes API reference\nOverview of API\nFeature Gates\nView Reference\nContribute to Kubernetes\nFind out how you can help make Kubernetes better.\nContribute to Kubernetes\nContribute to documentation\nSuggest content improvements\nOpening a pull request\nDocumenting a feature for a release\nLocalizing the docs\nParticipating in SIG Docs\nViewing Site Analytics\nSee Ways to Contribute\nTraining\nGet certified in Kubernetes and make your cloud native projects successful!\nView training\nDownload Kubernetes\nInstall Kubernetes or upgrade to the newest version.\nDownload Kubernetes\nAbout the documentation\nThis website contains documentation for the current and previous 4 versions of Kubernetes.\nSee available versions\nLast modified April 20, 2024 at 8:53 PM PST:\nReady docs landing page for vanilla Docsy (4da04a797b)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Kubernetes Documentation",
          "id": ""
        },
        {
          "level": 2,
          "text": "Understand Kubernetes",
          "id": "understand-kubernetes"
        },
        {
          "level": 2,
          "text": "Try Kubernetes",
          "id": "try-kubernetes"
        },
        {
          "level": 2,
          "text": "Set up a K8s cluster",
          "id": "set-up-a-k8s-cluster"
        },
        {
          "level": 2,
          "text": "Learn how to use Kubernetes",
          "id": "learn-how-to-use-kubernetes"
        },
        {
          "level": 2,
          "text": "Look up reference information",
          "id": "look-up-reference-information"
        },
        {
          "level": 2,
          "text": "Contribute to Kubernetes",
          "id": "contribute-to-kubernetes"
        },
        {
          "level": 2,
          "text": "Training",
          "id": "training"
        },
        {
          "level": 2,
          "text": "Download Kubernetes",
          "id": "download-kubernetes"
        },
        {
          "level": 2,
          "text": "About the documentation",
          "id": "about-the-documentation"
        }
      ],
      "timestamp": 1750731633.696502
    },
    {
      "url": "https://kubernetes.io/docs/home/",
      "title": "Kubernetes Documentation | Kubernetes",
      "content": "Kubernetes Documentation\nDocumentation\nKubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerized applications. The open source project is hosted by the Cloud Native Computing Foundation (\nCNCF\n).\nUnderstand Kubernetes\nLearn about Kubernetes and its fundamental concepts.\nWhy Kubernetes?\nComponents of a cluster\nThe Kubernetes API\nObjects In Kubernetes\nContainers\nWorkloads and Pods\nView Concepts\nTry Kubernetes\nFollow tutorials to learn how to deploy applications in Kubernetes.\nHello Minikube\nWalkthrough the basics\nStateless Example: PHP Guestbook with Redis\nStateful Example: Wordpress with Persistent Volumes\nView Tutorials\nSet up a K8s cluster\nGet Kubernetes running based on your resources and needs.\nLearning environment\nProduction environment\nInstall the kubeadm setup tool\nSecuring a cluster\nkubeadm command reference\nSet up Kubernetes\nLearn how to use Kubernetes\nLook up common tasks and how to perform them using a short sequence of steps.\nkubectl Quick Reference\nInstall kubectl\nConfigure access to clusters\nUse the Web UI Dashboard\nConfigure a Pod to Use a ConfigMap\nGetting help\nView Tasks\nLook up reference information\nBrowse terminology, command line syntax, API resource types, and setup tool documentation.\nGlossary\nkubectl command line tool\nLabels, annotations and taints\nKubernetes API reference\nOverview of API\nFeature Gates\nView Reference\nContribute to Kubernetes\nFind out how you can help make Kubernetes better.\nContribute to Kubernetes\nContribute to documentation\nSuggest content improvements\nOpening a pull request\nDocumenting a feature for a release\nLocalizing the docs\nParticipating in SIG Docs\nViewing Site Analytics\nSee Ways to Contribute\nTraining\nGet certified in Kubernetes and make your cloud native projects successful!\nView training\nDownload Kubernetes\nInstall Kubernetes or upgrade to the newest version.\nDownload Kubernetes\nAbout the documentation\nThis website contains documentation for the current and previous 4 versions of Kubernetes.\nSee available versions\nLast modified April 20, 2024 at 8:53 PM PST:\nReady docs landing page for vanilla Docsy (4da04a797b)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Kubernetes Documentation",
          "id": ""
        },
        {
          "level": 2,
          "text": "Understand Kubernetes",
          "id": "understand-kubernetes"
        },
        {
          "level": 2,
          "text": "Try Kubernetes",
          "id": "try-kubernetes"
        },
        {
          "level": 2,
          "text": "Set up a K8s cluster",
          "id": "set-up-a-k8s-cluster"
        },
        {
          "level": 2,
          "text": "Learn how to use Kubernetes",
          "id": "learn-how-to-use-kubernetes"
        },
        {
          "level": 2,
          "text": "Look up reference information",
          "id": "look-up-reference-information"
        },
        {
          "level": 2,
          "text": "Contribute to Kubernetes",
          "id": "contribute-to-kubernetes"
        },
        {
          "level": 2,
          "text": "Training",
          "id": "training"
        },
        {
          "level": 2,
          "text": "Download Kubernetes",
          "id": "download-kubernetes"
        },
        {
          "level": 2,
          "text": "About the documentation",
          "id": "about-the-documentation"
        }
      ],
      "timestamp": 1750731636.7839859
    },
    {
      "url": "https://kubernetes.io/docs/home/supported-doc-versions/",
      "title": "Available Documentation Versions | Kubernetes",
      "content": "Kubernetes Documentation\nDocumentation\nAvailable Documentation Versions\nAvailable Documentation Versions\nThis website contains documentation for the current version of Kubernetes\nand the four previous versions of Kubernetes.\nThe availability of documentation for a Kubernetes version is separate from whether\nthat release is currently supported.\nRead\nSupport period\nto learn about\nwhich versions of Kubernetes are officially supported, and for how long.\nLatest version\nv1.33\n(this documentation)\nOlder versions\nv1.32\nv1.31\nv1.30\nv1.29\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified April 12, 2024 at 10:34 AM PST:\nUpdate button label to 'See available versions' in docs homepage (#45571) (84cb81aca1)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Available Documentation Versions",
          "id": ""
        },
        {
          "level": 2,
          "text": "Latest version",
          "id": "version-latest"
        },
        {
          "level": 2,
          "text": "Older versions",
          "id": "versions-older"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        }
      ],
      "timestamp": 1750731639.948523
    },
    {
      "url": "https://kubernetes.io/docs/setup/",
      "title": "Getting started | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nThis section lists the different ways to set up and run Kubernetes.\nWhen you install Kubernetes, choose an installation type based on: ease of maintenance, security,\ncontrol, available resources, and expertise required to operate and manage a cluster.\nYou can\ndownload Kubernetes\nto deploy a Kubernetes cluster\non a local machine, into the cloud, or for your own datacenter.\nSeveral\nKubernetes components\nsuch as\nkube-apiserver\nor\nkube-proxy\ncan also be\ndeployed as\ncontainer images\nwithin the cluster.\nIt is\nrecommended\nto run Kubernetes components as container images wherever\nthat is possible, and to have Kubernetes manage those components.\nComponents that run containers - notably, the kubelet - can't be included in this category.\nIf you don't want to manage a Kubernetes cluster yourself, you could pick a managed service, including\ncertified platforms\n.\nThere are also other standardized and custom solutions across a wide range of cloud and\nbare metal environments.\nLearning environment\nIf you're learning Kubernetes, use the tools supported by the Kubernetes community,\nor tools in the ecosystem to set up a Kubernetes cluster on a local machine.\nSee\nInstall tools\n.\nProduction environment\nWhen evaluating a solution for a\nproduction environment\n, consider which aspects of\noperating a Kubernetes cluster (or\nabstractions\n) you want to manage yourself and which you\nprefer to hand off to a provider.\nFor a cluster you're managing yourself, the officially supported tool\nfor deploying Kubernetes is\nkubeadm\n.\nWhat's next\nDownload Kubernetes\nDownload and\ninstall tools\nincluding\nkubectl\nSelect a\ncontainer runtime\nfor your new cluster\nLearn about\nbest practices\nfor cluster setup\nKubernetes is designed for its\ncontrol plane\nto\nrun on Linux. Within your cluster you can run applications on Linux or other operating systems, including\nWindows.\nLearn to\nset up clusters with Windows nodes\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified November 23, 2022 at 3:06 PM PST:\nAdded glossary tooltip to kube-apiserver and kube-prox (c430d9a72c)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Getting started",
          "id": ""
        },
        {
          "level": 2,
          "text": "Learning environment",
          "id": "learning-environment"
        },
        {
          "level": 2,
          "text": "Production environment",
          "id": "production-environment"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        }
      ],
      "timestamp": 1750731643.119694
    },
    {
      "url": "https://kubernetes.io/docs/setup/learning-environment/",
      "title": "Install Tools | Kubernetes",
      "content": "Kubernetes Documentation\nTasks\nInstall Tools\nInstall Tools\nSet up Kubernetes tools on your computer.\nkubectl\nThe Kubernetes command-line tool,\nkubectl\n, allows\nyou to run commands against Kubernetes clusters.\nYou can use kubectl to deploy applications, inspect and manage cluster resources,\nand view logs. For more information including a complete list of kubectl operations, see the\nkubectl\nreference documentation\n.\nkubectl is installable on a variety of Linux platforms, macOS and Windows.\nFind your preferred operating system below.\nInstall kubectl on Linux\nInstall kubectl on macOS\nInstall kubectl on Windows\nkind\nkind\nlets you run Kubernetes on\nyour local computer. This tool requires that you have either\nDocker\nor\nPodman\ninstalled.\nThe kind\nQuick Start\npage\nshows you what you need to do to get up and running with kind.\nView kind Quick Start Guide\nminikube\nLike\nkind\n,\nminikube\nis a tool that lets you run Kubernetes\nlocally.\nminikube\nruns an all-in-one or a multi-node local Kubernetes cluster on your personal\ncomputer (including Windows, macOS and Linux PCs) so that you can try out\nKubernetes, or for daily development work.\nYou can follow the official\nGet Started!\nguide if your focus is\non getting the tool installed.\nView minikube Get Started! Guide\nOnce you have\nminikube\nworking, you can use it to\nrun a sample application\n.\nkubeadm\nYou can use the\nkubeadm\ntool to create and manage Kubernetes clusters.\nIt performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way.\nInstalling kubeadm\nshows you how to install kubeadm.\nOnce installed, you can use it to\ncreate a cluster\n.\nView kubeadm Install Guide\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified July 12, 2023 at 1:25 AM PST:\nRevise docs home page (9520b96a61)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Install Tools",
          "id": ""
        },
        {
          "level": 2,
          "text": "kubectl",
          "id": "kubectl"
        },
        {
          "level": 2,
          "text": "kind",
          "id": "kind"
        },
        {
          "level": 2,
          "text": "minikube",
          "id": "minikube"
        },
        {
          "level": 2,
          "text": "kubeadm",
          "id": "kubeadm"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        }
      ],
      "timestamp": 1750731646.737599
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/",
      "title": "Production environment | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nProduction environment\nCreate a production-quality Kubernetes cluster\nA production-quality Kubernetes cluster requires planning and preparation.\nIf your Kubernetes cluster is to run critical workloads, it must be configured to be resilient.\nThis page explains steps you can take to set up a production-ready cluster,\nor to promote an existing cluster for production use.\nIf you're already familiar with production setup and want the links, skip to\nWhat's next\n.\nProduction considerations\nTypically, a production Kubernetes cluster environment has more requirements than a\npersonal learning, development, or test environment Kubernetes. A production environment may require\nsecure access by many users, consistent availability, and the resources to adapt\nto changing demands.\nAs you decide where you want your production Kubernetes environment to live\n(on premises or in a cloud) and the amount of management you want to take\non or hand to others, consider how your requirements for a Kubernetes cluster\nare influenced by the following issues:\nAvailability\n: A single-machine Kubernetes\nlearning environment\nhas a single point of failure. Creating a highly available cluster means considering:\nSeparating the control plane from the worker nodes.\nReplicating the control plane components on multiple nodes.\nLoad balancing traffic to the cluster’s\nAPI server\n.\nHaving enough worker nodes available, or able to quickly become available, as changing workloads warrant it.\nScale\n: If you expect your production Kubernetes environment to receive a stable amount of\ndemand, you might be able to set up for the capacity you need and be done. However,\nif you expect demand to grow over time or change dramatically based on things like\nseason or special events, you need to plan how to scale to relieve increased\npressure from more requests to the control plane and worker nodes or scale down to reduce unused\nresources.\nSecurity and access management\n: You have full admin privileges on your own\nKubernetes learning cluster. But shared clusters with important workloads, and\nmore than one or two users, require a more refined approach to who and what can\naccess cluster resources. You can use role-based access control\n(\nRBAC\n) and other\nsecurity mechanisms to make sure that users and workloads can get access to the\nresources they need, while keeping workloads, and the cluster itself, secure.\nYou can set limits on the resources that users and workloads can access\nby managing\npolicies\nand\ncontainer resources\n.\nBefore building a Kubernetes production environment on your own, consider\nhanding off some or all of this job to\nTurnkey Cloud Solutions\nproviders or other\nKubernetes Partners\n.\nOptions include:\nServerless\n: Just run workloads on third-party equipment without managing\na cluster at all. You will be charged for things like CPU usage, memory, and\ndisk requests.\nManaged control plane\n: Let the provider manage the scale and availability\nof the cluster's control plane, as well as handle patches and upgrades.\nManaged worker nodes\n: Configure pools of nodes to meet your needs,\nthen the provider makes sure those nodes are available and ready to implement\nupgrades when needed.\nIntegration\n: There are providers that integrate Kubernetes with other\nservices you may need, such as storage, container registries, authentication\nmethods, and development tools.\nWhether you build a production Kubernetes cluster yourself or work with\npartners, review the following sections to evaluate your needs as they relate\nto your cluster’s\ncontrol plane\n,\nworker nodes\n,\nuser access\n, and\nworkload resources\n.\nProduction cluster setup\nIn a production-quality Kubernetes cluster, the control plane manages the\ncluster from services that can be spread across multiple computers\nin different ways. Each worker node, however, represents a single entity that\nis configured to run Kubernetes pods.\nProduction control plane\nThe simplest Kubernetes cluster has the entire control plane and worker node\nservices running on the same machine. You can grow that environment by adding\nworker nodes, as reflected in the diagram illustrated in\nKubernetes Components\n.\nIf the cluster is meant to be available for a short period of time, or can be\ndiscarded if something goes seriously wrong, this might meet your needs.\nIf you need a more permanent, highly available cluster, however, you should\nconsider ways of extending the control plane. By design, one-machine control\nplane services running on a single machine are not highly available.\nIf keeping the cluster up and running\nand ensuring that it can be repaired if something goes wrong is important,\nconsider these steps:\nChoose deployment tools\n: You can deploy a control plane using tools such\nas kubeadm, kops, and kubespray. See\nInstalling Kubernetes with deployment tools\nto learn tips for production-quality deployments using each of those deployment\nmethods. Different\nContainer Runtimes\nare available to use with your deployments.\nManage certificates\n: Secure communications between control plane services\nare implemented using certificates. Certificates are automatically generated\nduring deployment or you can generate them using your own certificate authority.\nSee\nPKI certificates and requirements\nfor details.\nConfigure load balancer for apiserver\n: Configure a load balancer\nto distribute external API requests to the apiserver service instances running on different nodes. See\nCreate an External Load Balancer\nfor details.\nSeparate and backup etcd service\n: The etcd services can either run on the\nsame machines as other control plane services or run on separate machines, for\nextra security and availability. Because etcd stores cluster configuration data,\nbacking up the etcd database should be done regularly to ensure that you can\nrepair that database if needed.\nSee the\netcd FAQ\nfor details on configuring and using etcd.\nSee\nOperating etcd clusters for Kubernetes\nand\nSet up a High Availability etcd cluster with kubeadm\nfor details.\nCreate multiple control plane systems\n: For high availability, the\ncontrol plane should not be limited to a single machine. If the control plane\nservices are run by an init service (such as systemd), each service should run on at\nleast three machines. However, running control plane services as pods in\nKubernetes ensures that the replicated number of services that you request\nwill always be available.\nThe scheduler should be fault tolerant,\nbut not highly available. Some deployment tools set up\nRaft\nconsensus algorithm to do leader election of Kubernetes services. If the\nprimary goes away, another service elects itself and take over.\nSpan multiple zones\n: If keeping your cluster available at all times is\ncritical, consider creating a cluster that runs across multiple data centers,\nreferred to as zones in cloud environments. Groups of zones are referred to as regions.\nBy spreading a cluster across\nmultiple zones in the same region, it can improve the chances that your\ncluster will continue to function even if one zone becomes unavailable.\nSee\nRunning in multiple zones\nfor details.\nManage on-going features\n: If you plan to keep your cluster over time,\nthere are tasks you need to do to maintain its health and security. For example,\nif you installed with kubeadm, there are instructions to help you with\nCertificate Management\nand\nUpgrading kubeadm clusters\n.\nSee\nAdminister a Cluster\nfor a longer list of Kubernetes administrative tasks.\nTo learn about available options when you run control plane services, see\nkube-apiserver\n,\nkube-controller-manager\n,\nand\nkube-scheduler\ncomponent pages. For highly available control plane examples, see\nOptions for Highly Available topology\n,\nCreating Highly Available clusters with kubeadm\n,\nand\nOperating etcd clusters for Kubernetes\n.\nSee\nBacking up an etcd cluster\nfor information on making an etcd backup plan.\nProduction worker nodes\nProduction-quality workloads need to be resilient and anything they rely\non needs to be resilient (such as CoreDNS). Whether you manage your own\ncontrol plane or have a cloud provider do it for you, you still need to\nconsider how you want to manage your worker nodes (also referred to\nsimply as\nnodes\n).\nConfigure nodes\n: Nodes can be physical or virtual machines. If you want to\ncreate and manage your own nodes, you can install a supported operating system,\nthen add and run the appropriate\nNode services\n. Consider:\nThe demands of your workloads when you set up nodes by having appropriate memory, CPU, and disk speed and storage capacity available.\nWhether generic computer systems will do or you have workloads that need GPU processors, Windows nodes, or VM isolation.\nValidate nodes\n: See\nValid node setup\nfor information on how to ensure that a node meets the requirements to join\na Kubernetes cluster.\nAdd nodes to the cluster\n: If you are managing your own cluster you can\nadd nodes by setting up your own machines and either adding them manually or\nhaving them register themselves to the cluster’s apiserver. See the\nNodes\nsection for information on how to set up Kubernetes to add nodes in these ways.\nScale nodes\n: Have a plan for expanding the capacity your cluster will\neventually need. See\nConsiderations for large clusters\nto help determine how many nodes you need, based on the number of pods and\ncontainers you need to run. If you are managing nodes yourself, this can mean\npurchasing and installing your own physical equipment.\nAutoscale nodes\n: Read\nNode Autoscaling\nto learn about the\ntools available to automatically manage your nodes and the capacity they\nprovide.\nSet up node health checks\n: For important workloads, you want to make sure\nthat the nodes and pods running on those nodes are healthy. Using the\nNode Problem Detector\ndaemon, you can ensure your nodes are healthy.\nProduction user management\nIn production, you may be moving from a model where you or a small group of\npeople are accessing the cluster to where there may potentially be dozens or\nhundreds of people. In a learning environment or platform prototype, you might have a single\nadministrative account for everything you do. In production, you will want\nmore accounts with different levels of access to different namespaces.\nTaking on a production-quality cluster means deciding how you\nwant to selectively allow access by other users. In particular, you need to\nselect strategies for validating the identities of those who try to access your\ncluster (authentication) and deciding if they have permissions to do what they\nare asking (authorization):\nAuthentication\n: The apiserver can authenticate users using client\ncertificates, bearer tokens, an authenticating proxy, or HTTP basic auth.\nYou can choose which authentication methods you want to use.\nUsing plugins, the apiserver can leverage your organization’s existing\nauthentication methods, such as LDAP or Kerberos. See\nAuthentication\nfor a description of these different methods of authenticating Kubernetes users.\nAuthorization\n: When you set out to authorize your regular users, you will probably choose\nbetween RBAC and ABAC authorization. See\nAuthorization Overview\nto review different modes for authorizing user accounts (as well as service account access to\nyour cluster):\nRole-based access control\n(\nRBAC\n): Lets you\nassign access to your cluster by allowing specific sets of permissions to authenticated users.\nPermissions can be assigned for a specific namespace (Role) or across the entire cluster\n(ClusterRole). Then using RoleBindings and ClusterRoleBindings, those permissions can be attached\nto particular users.\nAttribute-based access control\n(\nABAC\n): Lets you\ncreate policies based on resource attributes in the cluster and will allow or deny access\nbased on those attributes. Each line of a policy file identifies versioning properties (apiVersion\nand kind) and a map of spec properties to match the subject (user or group), resource property,\nnon-resource property (/version or /apis), and readonly. See\nExamples\nfor details.\nAs someone setting up authentication and authorization on your production Kubernetes cluster, here are some things to consider:\nSet the authorization mode\n: When the Kubernetes API server\n(\nkube-apiserver\n)\nstarts, supported authorization modes must be set using an\n--authorization-config\nfile or the\n--authorization-mode\nflag. For example, that flag in the\nkube-adminserver.yaml\nfile (in\n/etc/kubernetes/manifests\n)\ncould be set to Node,RBAC. This would allow Node and RBAC authorization for authenticated requests.\nCreate user certificates and role bindings (RBAC)\n: If you are using RBAC\nauthorization, users can create a CertificateSigningRequest (CSR) that can be\nsigned by the cluster CA. Then you can bind Roles and ClusterRoles to each user.\nSee\nCertificate Signing Requests\nfor details.\nCreate policies that combine attributes (ABAC)\n: If you are using ABAC\nauthorization, you can assign combinations of attributes to form policies to\nauthorize selected users or groups to access particular resources (such as a\npod), namespace, or apiGroup. For more information, see\nExamples\n.\nConsider Admission Controllers\n: Additional forms of authorization for\nrequests that can come in through the API server include\nWebhook Token Authentication\n.\nWebhooks and other special authorization types need to be enabled by adding\nAdmission Controllers\nto the API server.\nSet limits on workload resources\nDemands from production workloads can cause pressure both inside and outside\nof the Kubernetes control plane. Consider these items when setting up for the\nneeds of your cluster's workloads:\nSet namespace limits\n: Set per-namespace quotas on things like memory and CPU. See\nManage Memory, CPU, and API Resources\nfor details.\nPrepare for DNS demand\n: If you expect workloads to massively scale up,\nyour DNS service must be ready to scale up as well. See\nAutoscale the DNS service in a Cluster\n.\nCreate additional service accounts\n: User accounts determine what users can\ndo on a cluster, while a service account defines pod access within a particular\nnamespace. By default, a pod takes on the default service account from its namespace.\nSee\nManaging Service Accounts\nfor information on creating a new service account. For example, you might want to:\nAdd secrets that a pod could use to pull images from a particular container registry. See\nConfigure Service Accounts for Pods\nfor an example.\nAssign RBAC permissions to a service account. See\nServiceAccount permissions\nfor details.\nWhat's next\nDecide if you want to build your own production Kubernetes or obtain one from\navailable\nTurnkey Cloud Solutions\nor\nKubernetes Partners\n.\nIf you choose to build your own cluster, plan how you want to\nhandle\ncertificates\nand set up high availability for features such as\netcd\nand the\nAPI server\n.\nChoose from\nkubeadm\n,\nkops\nor\nKubespray\ndeployment methods.\nConfigure user management by determining your\nAuthentication\nand\nAuthorization\nmethods.\nPrepare for application workloads by setting up\nresource limits\n,\nDNS autoscaling\nand\nservice accounts\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified April 18, 2025 at 3:26 PM PST:\nRemove references to archived HNC project (c5441a5da7)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Production environment",
          "id": ""
        },
        {
          "level": 2,
          "text": "Production considerations",
          "id": "production-considerations"
        },
        {
          "level": 2,
          "text": "Production cluster setup",
          "id": "production-cluster-setup"
        },
        {
          "level": 2,
          "text": "Production user management",
          "id": "production-user-management"
        },
        {
          "level": 2,
          "text": "Set limits on workload resources",
          "id": "set-limits-on-workload-resources"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Production control plane",
          "id": "production-control-plane"
        },
        {
          "level": 3,
          "text": "Production worker nodes",
          "id": "production-worker-nodes"
        }
      ],
      "timestamp": 1750731649.832912
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/container-runtimes/",
      "title": "Container Runtimes | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nContainer Runtimes\nContainer Runtimes\nNote:\nDockershim has been removed from the Kubernetes project as of release 1.24. Read the\nDockershim Removal FAQ\nfor further details.\nYou need to install a\ncontainer runtime\ninto each node in the cluster so that Pods can run there. This page outlines\nwhat is involved and describes related tasks for setting up nodes.\nKubernetes 1.33 requires that you use a runtime that\nconforms with the\nContainer Runtime Interface\n(CRI).\nSee\nCRI version support\nfor more information.\nThis page provides an outline of how to use several common container runtimes with\nKubernetes.\ncontainerd\nCRI-O\nDocker Engine\nMirantis Container Runtime\nNote:\nKubernetes releases before v1.24 included a direct integration with Docker Engine,\nusing a component named\ndockershim\n. That special direct integration is no longer\npart of Kubernetes (this removal was\nannounced\nas part of the v1.20 release).\nYou can read\nCheck whether Dockershim removal affects you\nto understand how this removal might affect you. To learn about migrating from using dockershim, see\nMigrating from dockershim\n.\nIf you are running a version of Kubernetes other than v1.33,\ncheck the documentation for that version.\nInstall and configure prerequisites\nNetwork configuration\nBy default, the Linux kernel does not allow IPv4 packets to be routed\nbetween interfaces. Most Kubernetes cluster networking implementations\nwill change this setting (if needed), but some might expect the\nadministrator to do it for them. (Some might also expect other sysctl\nparameters to be set, kernel modules to be loaded, etc; consult the\ndocumentation for your specific network implementation.)\nEnable IPv4 packet forwarding\nTo manually enable IPv4 packet forwarding:\n# sysctl params required by setup, params persist across reboots\ncat\n<<EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nEOF\n# Apply sysctl params without reboot\nsudo sysctl --system\nVerify that\nnet.ipv4.ip_forward\nis set to 1 with:\nsysctl net.ipv4.ip_forward\ncgroup drivers\nOn Linux,\ncontrol groups\nare used to constrain resources that are allocated to processes.\nBoth the\nkubelet\nand the\nunderlying container runtime need to interface with control groups to enforce\nresource management for pods and containers\nand set resources such as cpu/memory requests and limits. To interface with control\ngroups, the kubelet and the container runtime need to use a\ncgroup driver\n.\nIt's critical that the kubelet and the container runtime use the same cgroup\ndriver and are configured the same.\nThere are two cgroup drivers available:\ncgroupfs\nsystemd\ncgroupfs driver\nThe\ncgroupfs\ndriver is the\ndefault cgroup driver in the kubelet\n.\nWhen the\ncgroupfs\ndriver is used, the kubelet and the container runtime directly interface with\nthe cgroup filesystem to configure cgroups.\nThe\ncgroupfs\ndriver is\nnot\nrecommended when\nsystemd\nis the\ninit system because systemd expects a single cgroup manager on\nthe system. Additionally, if you use\ncgroup v2\n, use the\nsystemd\ncgroup driver instead of\ncgroupfs\n.\nsystemd cgroup driver\nWhen\nsystemd\nis chosen as the init\nsystem for a Linux distribution, the init process generates and consumes a root control group\n(\ncgroup\n) and acts as a cgroup manager.\nsystemd has a tight integration with cgroups and allocates a cgroup per systemd\nunit. As a result, if you use\nsystemd\nas the init system with the\ncgroupfs\ndriver, the system gets two different cgroup managers.\nTwo cgroup managers result in two views of the available and in-use resources in\nthe system. In some cases, nodes that are configured to use\ncgroupfs\nfor the\nkubelet and container runtime, but use\nsystemd\nfor the rest of the processes become\nunstable under resource pressure.\nThe approach to mitigate this instability is to use\nsystemd\nas the cgroup driver for\nthe kubelet and the container runtime when systemd is the selected init system.\nTo set\nsystemd\nas the cgroup driver, edit the\nKubeletConfiguration\noption of\ncgroupDriver\nand set it to\nsystemd\n. For example:\napiVersion\n:\nkubelet.config.k8s.io/v1beta1\nkind\n:\nKubeletConfiguration\n...\ncgroupDriver\n:\nsystemd\nNote:\nStarting with v1.22 and later, when creating a cluster with kubeadm, if the user does not set\nthe\ncgroupDriver\nfield under\nKubeletConfiguration\n, kubeadm defaults it to\nsystemd\n.\nIf you configure\nsystemd\nas the cgroup driver for the kubelet, you must also\nconfigure\nsystemd\nas the cgroup driver for the container runtime. Refer to\nthe documentation for your container runtime for instructions. For example:\ncontainerd\nCRI-O\nIn Kubernetes 1.33, with the\nKubeletCgroupDriverFromCRI\nfeature gate\nenabled and a container runtime that supports the\nRuntimeConfig\nCRI RPC,\nthe kubelet automatically detects the appropriate cgroup driver from the runtime,\nand ignores the\ncgroupDriver\nsetting within the kubelet configuration.\nCaution:\nChanging the cgroup driver of a Node that has joined a cluster is a sensitive operation.\nIf the kubelet has created Pods using the semantics of one cgroup driver, changing the container\nruntime to another cgroup driver can cause errors when trying to re-create the Pod sandbox\nfor such existing Pods. Restarting the kubelet may not solve such errors.\nIf you have automation that makes it feasible, replace the node with another using the updated\nconfiguration, or reinstall it using automation.\nMigrating to the\nsystemd\ndriver in kubeadm managed clusters\nIf you wish to migrate to the\nsystemd\ncgroup driver in existing kubeadm managed clusters,\nfollow\nconfiguring a cgroup driver\n.\nCRI version support\nYour container runtime must support at least v1alpha2 of the container runtime interface.\nKubernetes\nstarting v1.26\nonly works\nwith v1 of the CRI API. Earlier versions default\nto v1 version, however if a container runtime does not support the v1 API, the kubelet falls back to\nusing the (deprecated) v1alpha2 API instead.\nContainer runtimes\nNote:\nThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the\ncontent guide\nbefore submitting a change.\nMore information.\ncontainerd\nThis section outlines the necessary steps to use containerd as CRI runtime.\nTo install containerd on your system, follow the instructions on\ngetting started with containerd\n.\nReturn to this step once you've created a valid\nconfig.toml\nconfiguration file.\nLinux\nWindows\nYou can find this file under the path\n/etc/containerd/config.toml\n.\nYou can find this file under the path\nC:\\Program Files\\containerd\\config.toml\n.\nOn Linux the default CRI socket for containerd is\n/run/containerd/containerd.sock\n.\nOn Windows the default CRI endpoint is\nnpipe://./pipe/containerd-containerd\n.\nConfiguring the\nsystemd\ncgroup driver\nTo use the\nsystemd\ncgroup driver in\n/etc/containerd/config.toml\nwith\nrunc\n, set\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n  ...\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n    SystemdCgroup = true\nThe\nsystemd\ncgroup driver is recommended if you use\ncgroup v2\n.\nNote:\nIf you installed containerd from a package (for example, RPM or\n.deb\n), you may find\nthat the CRI integration plugin is disabled by default.\nYou need CRI support enabled to use containerd with Kubernetes. Make sure that\ncri\nis not included in the\ndisabled_plugins\nlist within\n/etc/containerd/config.toml\n;\nif you made changes to that file, also restart\ncontainerd\n.\nIf you experience container crash loops after the initial cluster installation or after\ninstalling a CNI, the containerd configuration provided with the package might contain\nincompatible configuration parameters. Consider resetting the containerd configuration\nwith\ncontainerd config default > /etc/containerd/config.toml\nas specified in\ngetting-started.md\nand then set the configuration parameters specified above accordingly.\nIf you apply this change, make sure to restart containerd:\nsudo systemctl restart containerd\nWhen using kubeadm, manually configure the\ncgroup driver for kubelet\n.\nIn Kubernetes v1.28, you can enable automatic detection of the\ncgroup driver as an alpha feature. See\nsystemd cgroup driver\nfor more details.\nOverriding the sandbox (pause) image\nIn your\ncontainerd config\nyou can overwrite the\nsandbox image by setting the following config:\n[plugins.\n\"io.containerd.grpc.v1.cri\"\n]\nsandbox_image =\n\"registry.k8s.io/pause:3.10\"\nYou might need to restart\ncontainerd\nas well once you've updated the config file:\nsystemctl restart containerd\n.\nCRI-O\nThis section contains the necessary steps to install CRI-O as a container runtime.\nTo install CRI-O, follow\nCRI-O Install Instructions\n.\ncgroup driver\nCRI-O uses the systemd cgroup driver per default, which is likely to work fine\nfor you. To switch to the\ncgroupfs\ncgroup driver, either edit\n/etc/crio/crio.conf\nor place a drop-in configuration in\n/etc/crio/crio.conf.d/02-cgroup-manager.conf\n, for example:\n[crio.runtime]\nconmon_cgroup =\n\"pod\"\ncgroup_manager =\n\"cgroupfs\"\nYou should also note the changed\nconmon_cgroup\n, which has to be set to the value\npod\nwhen using CRI-O with\ncgroupfs\n. It is generally necessary to keep the\ncgroup driver configuration of the kubelet (usually done via kubeadm) and CRI-O\nin sync.\nIn Kubernetes v1.28, you can enable automatic detection of the\ncgroup driver as an alpha feature. See\nsystemd cgroup driver\nfor more details.\nFor CRI-O, the CRI socket is\n/var/run/crio/crio.sock\nby default.\nOverriding the sandbox (pause) image\nIn your\nCRI-O config\nyou can set the following\nconfig value:\n[crio.image]\npause_image=\n\"registry.k8s.io/pause:3.10\"\nThis config option supports live configuration reload to apply this change:\nsystemctl reload crio\nor by sending\nSIGHUP\nto the\ncrio\nprocess.\nDocker Engine\nNote:\nThese instructions assume that you are using the\ncri-dockerd\nadapter to integrate\nDocker Engine with Kubernetes.\nOn each of your nodes, install Docker for your Linux distribution as per\nInstall Docker Engine\n.\nInstall\ncri-dockerd\n, following the directions in the install section of the documentation.\nFor\ncri-dockerd\n, the CRI socket is\n/run/cri-dockerd.sock\nby default.\nMirantis Container Runtime\nMirantis Container Runtime\n(MCR) is a commercially\navailable container runtime that was formerly known as Docker Enterprise Edition.\nYou can use Mirantis Container Runtime with Kubernetes using the open source\ncri-dockerd\ncomponent, included with MCR.\nTo learn more about how to install Mirantis Container Runtime,\nvisit\nMCR Deployment Guide\n.\nCheck the systemd unit named\ncri-docker.socket\nto find out the path to the CRI\nsocket.\nOverriding the sandbox (pause) image\nThe\ncri-dockerd\nadapter accepts a command line argument for\nspecifying which container image to use as the Pod infrastructure container (“pause image”).\nThe command line argument to use is\n--pod-infra-container-image\n.\nWhat's next\nAs well as a container runtime, your cluster will need a working\nnetwork plugin\n.\nItems on this page refer to third party products or projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. See the\nCNCF website guidelines\nfor more details.\nYou should read the\ncontent guide\nbefore proposing a change that adds an extra third-party link.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified June 11, 2025 at 10:32 AM PST:\nupdate mcr from 20.10 to 25.0 in cri en pages (1081a625a8)",
      "code_examples": [
        {
          "language": "",
          "code": "# sysctl params required by setup, params persist across rebootscat<<EOF | sudo tee /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1EOF# Apply sysctl params without rebootsudo sysctl --system",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#080;font-s..."
        },
        {
          "language": "language-bash",
          "code": "# sysctl params required by setup, params persist across rebootscat<<EOF | sudo tee /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1EOF# Apply sysctl params without rebootsudo sysctl --system",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#080;font-style:italic\"># sysctl params required by setup, params persist across reboots</span>\n</span>..."
        },
        {
          "language": "",
          "code": "net.ipv4.ip_forward",
          "element": "<code>net.ipv4.ip_forward</code>"
        },
        {
          "language": "",
          "code": "sysctl net.ipv4.ip_forward",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>sysctl net.ipv4.ip_forward\n</s..."
        },
        {
          "language": "language-bash",
          "code": "sysctl net.ipv4.ip_forward",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>sysctl net.ipv4.ip_forward\n</span></span></code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "cgroupDriver",
          "element": "<code>cgroupDriver</code>"
        },
        {
          "language": "",
          "code": "apiVersion:kubelet.config.k8s.io/v1beta1kind:KubeletConfiguration...cgroupDriver:systemd",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubelet.config.k8s.io/v1beta1kind:KubeletConfiguration...cgroupDriver:systemd",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubelet.config.k8s.io/v1beta1..."
        },
        {
          "language": "",
          "code": "cgroupDriver",
          "element": "<code>cgroupDriver</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "KubeletCgroupDriverFromCRI",
          "element": "<code>KubeletCgroupDriverFromCRI</code>"
        },
        {
          "language": "",
          "code": "RuntimeConfig",
          "element": "<code>RuntimeConfig</code>"
        },
        {
          "language": "",
          "code": "cgroupDriver",
          "element": "<code>cgroupDriver</code>"
        },
        {
          "language": "",
          "code": "config.toml",
          "element": "<code>config.toml</code>"
        },
        {
          "language": "",
          "code": "/etc/containerd/config.toml",
          "element": "<code>/etc/containerd/config.toml</code>"
        },
        {
          "language": "",
          "code": "C:\\Program Files\\containerd\\config.toml",
          "element": "<code>C:\\Program Files\\containerd\\config.toml</code>"
        },
        {
          "language": "",
          "code": "/run/containerd/containerd.sock",
          "element": "<code>/run/containerd/containerd.sock</code>"
        },
        {
          "language": "",
          "code": "npipe://./pipe/containerd-containerd",
          "element": "<code>npipe://./pipe/containerd-containerd</code>"
        },
        {
          "language": "",
          "code": "/etc/containerd/config.toml",
          "element": "<code>/etc/containerd/config.toml</code>"
        },
        {
          "language": "",
          "code": "[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n  ...\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n    SystemdCgroup = true",
          "element": "<pre tabindex=\"0\"><code>[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n  ...\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n    SystemdCgroup = true\n</code></..."
        },
        {
          "language": "",
          "code": "[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n  ...\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n    SystemdCgroup = true",
          "element": "<code>[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n  ...\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n    SystemdCgroup = true\n</code>"
        },
        {
          "language": "",
          "code": "disabled_plugins",
          "element": "<code>disabled_plugins</code>"
        },
        {
          "language": "",
          "code": "/etc/containerd/config.toml",
          "element": "<code>/etc/containerd/config.toml</code>"
        },
        {
          "language": "",
          "code": "containerd config default > /etc/containerd/config.toml",
          "element": "<code>containerd config default &gt; /etc/containerd/config.toml</code>"
        },
        {
          "language": "",
          "code": "sudo systemctl restart containerd",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo systemctl restart conta..."
        },
        {
          "language": "language-shell",
          "code": "sudo systemctl restart containerd",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo systemctl restart containerd\n</span></span></code>"
        },
        {
          "language": "",
          "code": "[plugins.\"io.containerd.grpc.v1.cri\"]sandbox_image =\"registry.k8s.io/pause:3.10\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex\"><span>[plugins.<span style=\"color:#b..."
        },
        {
          "language": "language-toml",
          "code": "[plugins.\"io.containerd.grpc.v1.cri\"]sandbox_image =\"registry.k8s.io/pause:3.10\"",
          "element": "<code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex\"><span>[plugins.<span style=\"color:#b44\">\"io.containerd.grpc.v1.cri\"</span>]\n</span></span><span style=\"display:flex\"><span>  san..."
        },
        {
          "language": "",
          "code": "systemctl restart containerd",
          "element": "<code>systemctl restart containerd</code>"
        },
        {
          "language": "",
          "code": "/etc/crio/crio.conf",
          "element": "<code>/etc/crio/crio.conf</code>"
        },
        {
          "language": "",
          "code": "/etc/crio/crio.conf.d/02-cgroup-manager.conf",
          "element": "<code>/etc/crio/crio.conf.d/02-cgroup-manager.conf</code>"
        },
        {
          "language": "",
          "code": "[crio.runtime]conmon_cgroup =\"pod\"cgroup_manager =\"cgroupfs\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex\"><span>[crio.runtime]\n</span></span><..."
        },
        {
          "language": "language-toml",
          "code": "[crio.runtime]conmon_cgroup =\"pod\"cgroup_manager =\"cgroupfs\"",
          "element": "<code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex\"><span>[crio.runtime]\n</span></span><span style=\"display:flex\"><span>conmon_cgroup = <span style=\"color:#b44\">\"pod\"</span>\n</span..."
        },
        {
          "language": "",
          "code": "conmon_cgroup",
          "element": "<code>conmon_cgroup</code>"
        },
        {
          "language": "",
          "code": "/var/run/crio/crio.sock",
          "element": "<code>/var/run/crio/crio.sock</code>"
        },
        {
          "language": "",
          "code": "[crio.image]pause_image=\"registry.k8s.io/pause:3.10\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex\"><span>[crio.image]\n</span></span><sp..."
        },
        {
          "language": "language-toml",
          "code": "[crio.image]pause_image=\"registry.k8s.io/pause:3.10\"",
          "element": "<code class=\"language-toml\" data-lang=\"toml\"><span style=\"display:flex\"><span>[crio.image]\n</span></span><span style=\"display:flex\"><span>pause_image=<span style=\"color:#b44\">\"registry.k8s.io/pause:3...."
        },
        {
          "language": "",
          "code": "systemctl reload crio",
          "element": "<code>systemctl reload crio</code>"
        },
        {
          "language": "",
          "code": "cri-dockerd",
          "element": "<code>cri-dockerd</code>"
        },
        {
          "language": "",
          "code": "cri-dockerd",
          "element": "<code>cri-dockerd</code>"
        },
        {
          "language": "",
          "code": "cri-dockerd",
          "element": "<code>cri-dockerd</code>"
        },
        {
          "language": "",
          "code": "/run/cri-dockerd.sock",
          "element": "<code>/run/cri-dockerd.sock</code>"
        },
        {
          "language": "",
          "code": "cri-dockerd",
          "element": "<code>cri-dockerd</code>"
        },
        {
          "language": "",
          "code": "cri-docker.socket",
          "element": "<code>cri-docker.socket</code>"
        },
        {
          "language": "",
          "code": "cri-dockerd",
          "element": "<code>cri-dockerd</code>"
        },
        {
          "language": "",
          "code": "--pod-infra-container-image",
          "element": "<code>--pod-infra-container-image</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Container Runtimes",
          "id": ""
        },
        {
          "level": 2,
          "text": "Install and configure prerequisites",
          "id": "install-and-configure-prerequisites"
        },
        {
          "level": 2,
          "text": "cgroup drivers",
          "id": "cgroup-drivers"
        },
        {
          "level": 2,
          "text": "CRI version support",
          "id": "cri-versions"
        },
        {
          "level": 2,
          "text": "Container runtimes",
          "id": "container-runtimes"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Network configuration",
          "id": "network-configuration"
        },
        {
          "level": 3,
          "text": "Enable IPv4 packet forwarding",
          "id": "prerequisite-ipv4-forwarding-optional"
        },
        {
          "level": 3,
          "text": "cgroupfs driver",
          "id": "cgroupfs-cgroup-driver"
        },
        {
          "level": 3,
          "text": "systemd cgroup driver",
          "id": "systemd-cgroup-driver"
        },
        {
          "level": 3,
          "text": "Migrating to thesystemddriver in kubeadm managed clusters",
          "id": "migrating-to-the-systemd-driver-in-kubeadm-managed-clusters"
        },
        {
          "level": 3,
          "text": "containerd",
          "id": "containerd"
        },
        {
          "level": 3,
          "text": "CRI-O",
          "id": "cri-o"
        },
        {
          "level": 3,
          "text": "Docker Engine",
          "id": "docker"
        },
        {
          "level": 3,
          "text": "Mirantis Container Runtime",
          "id": "mcr"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Caution:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Configuring thesystemdcgroup driver",
          "id": "containerd-systemd"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Overriding the sandbox (pause) image",
          "id": "override-pause-image-containerd"
        },
        {
          "level": 4,
          "text": "cgroup driver",
          "id": "cgroup-driver"
        },
        {
          "level": 4,
          "text": "Overriding the sandbox (pause) image",
          "id": "override-pause-image-cri-o"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Overriding the sandbox (pause) image",
          "id": "override-pause-image-cri-dockerd-mcr"
        }
      ],
      "timestamp": 1750731653.074048
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/",
      "title": "Installing Kubernetes with deployment tools | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nInstalling Kubernetes with deployment tools\nThere are many methods and tools for setting up your own production Kubernetes cluster.\nFor example:\nkubeadm\nCluster API\n: A Kubernetes sub-project focused on\nproviding declarative APIs and tooling to simplify provisioning, upgrading, and operating\nmultiple Kubernetes clusters.\nkops\n: An automated cluster provisioning tool.\nFor tutorials, best practices, configuration options and information on\nreaching out to the community, please check the\nkOps\nwebsite\nfor details.\nkubespray\n:\nA composition of\nAnsible\nplaybooks,\ninventory\n,\nprovisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration\nmanagement tasks. You can reach out to the community on Slack channel\n#kubespray\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified December 09, 2024 at 11:46 PM PST:\nUpdate Link of Inventory in Install k8s doc (7692020f50)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Installing Kubernetes with deployment tools",
          "id": ""
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        }
      ],
      "timestamp": 1750731656.435606
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/",
      "title": "Bootstrapping clusters with kubeadm | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nBootstrapping clusters with kubeadm\nInstalling kubeadm\nTroubleshooting kubeadm\nCreating a cluster with kubeadm\nCustomizing components with the kubeadm API\nOptions for Highly Available Topology\nCreating Highly Available Clusters with kubeadm\nSet up a High Availability etcd Cluster with kubeadm\nConfiguring each kubelet in your cluster using kubeadm\nDual-stack support with kubeadm\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified June 12, 2019 at 5:27 PM PST:\nRestructure the left navigation pane of setup (#14826) (55ac801bc4)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Bootstrapping clusters with kubeadm",
          "id": ""
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 5,
          "text": "Installing kubeadm",
          "id": ""
        },
        {
          "level": 5,
          "text": "Troubleshooting kubeadm",
          "id": ""
        },
        {
          "level": 5,
          "text": "Creating a cluster with kubeadm",
          "id": ""
        },
        {
          "level": 5,
          "text": "Customizing components with the kubeadm API",
          "id": ""
        },
        {
          "level": 5,
          "text": "Options for Highly Available Topology",
          "id": ""
        },
        {
          "level": 5,
          "text": "Creating Highly Available Clusters with kubeadm",
          "id": ""
        },
        {
          "level": 5,
          "text": "Set up a High Availability etcd Cluster with kubeadm",
          "id": ""
        },
        {
          "level": 5,
          "text": "Configuring each kubelet in your cluster using kubeadm",
          "id": ""
        },
        {
          "level": 5,
          "text": "Dual-stack support with kubeadm",
          "id": ""
        }
      ],
      "timestamp": 1750731659.662265
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/",
      "title": "Installing kubeadm | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nInstalling kubeadm\nInstalling kubeadm\nThis page shows how to install the\nkubeadm\ntoolbox.\nFor information on how to create a cluster with kubeadm once you have performed this installation process,\nsee the\nCreating a cluster with kubeadm\npage.\nThis installation guide is for Kubernetes v1.33. If you want to use a different Kubernetes version, please refer to the following pages instead:\nInstalling kubeadm (Kubernetes v1.32)\nInstalling kubeadm (Kubernetes v1.31)\nInstalling kubeadm (Kubernetes v1.30)\nInstalling kubeadm (Kubernetes v1.29)\nBefore you begin\nA compatible Linux host. The Kubernetes project provides generic instructions for Linux distributions\nbased on Debian and Red Hat, and those distributions without a package manager.\n2 GB or more of RAM per machine (any less will leave little room for your apps).\n2 CPUs or more for control plane machines.\nFull network connectivity between all machines in the cluster (public or private network is fine).\nUnique hostname, MAC address, and product_uuid for every node. See\nhere\nfor more details.\nCertain ports are open on your machines. See\nhere\nfor more details.\nNote:\nThe\nkubeadm\ninstallation is done via binaries that use dynamic linking and assumes that your target system provides\nglibc\n.\nThis is a reasonable assumption on many Linux distributions (including Debian, Ubuntu, Fedora, CentOS, etc.)\nbut it is not always the case with custom and lightweight distributions which don't include\nglibc\nby default, such as Alpine Linux.\nThe expectation is that the distribution either includes\nglibc\nor a\ncompatibility layer\nthat provides the expected symbols.\nCheck your OS version\nNote:\nThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the\ncontent guide\nbefore submitting a change.\nMore information.\nLinux\nWindows\nThe kubeadm project supports LTS kernels. See\nList of LTS kernels\n.\nYou can get the kernel version using the command\nuname -r\nFor more information, see\nLinux Kernel Requirements\n.\nThe kubeadm project supports recent kernel versions. For a list of recent kernels, see\nWindows Server Release Information\n.\nYou can get the kernel version (also called the OS version) using the command\nsysteminfo\nFor more information, see\nWindows OS version compatibility\n.\nA Kubernetes cluster created by kubeadm depends on software that use kernel features.\nThis software includes, but is not limited to the\ncontainer runtime\n,\nthe\nkubelet\n, and a\nContainer Network Interface\nplugin.\nTo help you avoid unexpected errors as a result of an unsupported kernel version, kubeadm runs the\nSystemVerification\npre-flight check. This check fails if the kernel version is not supported.\nYou may choose to skip the check, if you know that your kernel\nprovides the required features, even though kubeadm does not support its version.\nVerify the MAC address and product_uuid are unique for every node\nYou can get the MAC address of the network interfaces using the command\nip link\nor\nifconfig -a\nThe product_uuid can be checked by using the command\nsudo cat /sys/class/dmi/id/product_uuid\nIt is very likely that hardware devices will have unique addresses, although some virtual machines may have\nidentical values. Kubernetes uses these values to uniquely identify the nodes in the cluster.\nIf these values are not unique to each node, the installation process\nmay\nfail\n.\nCheck network adapters\nIf you have more than one network adapter, and your Kubernetes components are not reachable on the default\nroute, we recommend you add IP route(s) so Kubernetes cluster addresses go via the appropriate adapter.\nCheck required ports\nThese\nrequired ports\nneed to be open in order for Kubernetes components to communicate with each other.\nYou can use tools like\nnetcat\nto check if a port is open. For example:\nnc 127.0.0.1\n6443\n-zv -w\n2\nThe pod network plugin you use may also require certain ports to be\nopen. Since this differs with each pod network plugin, please see the\ndocumentation for the plugins about what port(s) those need.\nSwap configuration\nThe default behavior of a kubelet is to fail to start if swap memory is detected on a node.\nThis means that swap should either be disabled or tolerated by kubelet.\nTo tolerate swap, add\nfailSwapOn: false\nto kubelet configuration or as a command line argument.\nNote: even if\nfailSwapOn: false\nis provided, workloads wouldn't have swap access by default.\nThis can be changed by setting a\nswapBehavior\n, again in the kubelet configuration file. To use swap,\nset a\nswapBehavior\nother than the default\nNoSwap\nsetting.\nSee\nSwap memory management\nfor more details.\nTo disable swap,\nsudo swapoff -a\ncan be used to disable swapping temporarily.\nTo make this change persistent across reboots, make sure swap is disabled in\nconfig files like\n/etc/fstab\n,\nsystemd.swap\n, depending how it was configured on your system.\nInstalling a container runtime\nTo run containers in Pods, Kubernetes uses a\ncontainer runtime\n.\nBy default, Kubernetes uses the\nContainer Runtime Interface\n(CRI)\nto interface with your chosen container runtime.\nIf you don't specify a runtime, kubeadm automatically tries to detect an installed\ncontainer runtime by scanning through a list of known endpoints.\nIf multiple or no container runtimes are detected kubeadm will throw an error\nand will request that you specify which one you want to use.\nSee\ncontainer runtimes\nfor more information.\nNote:\nDocker Engine does not implement the\nCRI\nwhich is a requirement for a container runtime to work with Kubernetes.\nFor that reason, an additional service\ncri-dockerd\nhas to be installed. cri-dockerd is a project based on the legacy built-in\nDocker Engine support that was\nremoved\nfrom the kubelet in version 1.24.\nThe tables below include the known endpoints for supported operating systems:\nLinux\nWindows\nLinux container runtimes\nRuntime\nPath to Unix domain socket\ncontainerd\nunix:///var/run/containerd/containerd.sock\nCRI-O\nunix:///var/run/crio/crio.sock\nDocker Engine (using cri-dockerd)\nunix:///var/run/cri-dockerd.sock\nWindows container runtimes\nRuntime\nPath to Windows named pipe\ncontainerd\nnpipe:////./pipe/containerd-containerd\nDocker Engine (using cri-dockerd)\nnpipe:////./pipe/cri-dockerd\nInstalling kubeadm, kubelet and kubectl\nYou will install these packages on all of your machines:\nkubeadm\n: the command to bootstrap the cluster.\nkubelet\n: the component that runs on all of the machines in your cluster\nand does things like starting pods and containers.\nkubectl\n: the command line util to talk to your cluster.\nkubeadm\nwill not\ninstall or manage\nkubelet\nor\nkubectl\nfor you, so you will\nneed to ensure they match the version of the Kubernetes control plane you want\nkubeadm to install for you. If you do not, there is a risk of a version skew occurring that\ncan lead to unexpected, buggy behaviour. However,\none\nminor version skew between the\nkubelet and the control plane is supported, but the kubelet version may never exceed the API\nserver version. For example, the kubelet running 1.7.0 should be fully compatible with a 1.8.0 API server,\nbut not vice versa.\nFor information about installing\nkubectl\n, see\nInstall and set up kubectl\n.\nWarning:\nThese instructions exclude all Kubernetes packages from any system upgrades.\nThis is because kubeadm and Kubernetes require\nspecial attention to upgrade\n.\nFor more information on version skews, see:\nKubernetes\nversion and version-skew policy\nKubeadm-specific\nversion skew policy\nNote:\nThe legacy package repositories (\napt.kubernetes.io\nand\nyum.kubernetes.io\n) have been\ndeprecated and frozen starting from September 13, 2023\n.\nUsing the\nnew package repositories hosted at\npkgs.k8s.io\nis strongly recommended and required in order to install Kubernetes versions released after September 13, 2023.\nThe deprecated legacy repositories, and their contents, might be removed at any time in the future and without\na further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.\nNote:\nThere's a dedicated package repository for each Kubernetes minor version. If you want to install\na minor version other than v1.33, please see the installation guide for\nyour desired minor version.\nDebian-based distributions\nRed Hat-based distributions\nWithout a package manager\nThese instructions are for Kubernetes v1.33.\nUpdate the\napt\npackage index and install packages needed to use the Kubernetes\napt\nrepository:\nsudo apt-get update\n# apt-transport-https may be a dummy package; if so, you can skip that package\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\nDownload the public signing key for the Kubernetes package repositories.\nThe same signing key is used for all repositories so you can disregard the version in the URL:\n# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.\n# sudo mkdir -p -m 755 /etc/apt/keyrings\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nNote:\nIn releases older than Debian 12 and Ubuntu 22.04, directory\n/etc/apt/keyrings\ndoes not\nexist by default, and it should be created before the curl command.\nAdd the appropriate Kubernetes\napt\nrepository. Please note that this repository have packages\nonly for Kubernetes 1.33; for other Kubernetes minor versions, you need to\nchange the Kubernetes minor version in the URL to match your desired minor version\n(you should also check that you are reading the documentation for the version of Kubernetes\nthat you plan to install).\n# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list\necho\n'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /'\n| sudo tee /etc/apt/sources.list.d/kubernetes.list\nUpdate the\napt\npackage index, install kubelet, kubeadm and kubectl, and pin their version:\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n(Optional) Enable the kubelet service before running kubeadm:\nsudo systemctl\nenable\n--now kubelet\nSet SELinux to\npermissive\nmode:\nThese instructions are for Kubernetes 1.33.\n# Set SELinux in permissive mode (effectively disabling it)\nsudo setenforce\n0\nsudo sed -i\n's/^SELINUX=enforcing$/SELINUX=permissive/'\n/etc/selinux/config\nCaution:\nSetting SELinux in permissive mode by running\nsetenforce 0\nand\nsed ...\neffectively disables it. This is required to allow containers to access the host\nfilesystem; for example, some cluster network plugins require that. You have to\ndo this until SELinux support is improved in the kubelet.\nYou can leave SELinux enabled if you know how to configure it but it may require\nsettings that are not supported by kubeadm.\nAdd the Kubernetes\nyum\nrepository. The\nexclude\nparameter in the\nrepository definition ensures that the packages related to Kubernetes are\nnot upgraded upon running\nyum update\nas there's a special procedure that\nmust be followed for upgrading Kubernetes. Please note that this repository\nhave packages only for Kubernetes 1.33; for other\nKubernetes minor versions, you need to change the Kubernetes minor version\nin the URL to match your desired minor version (you should also check that\nyou are reading the documentation for the version of Kubernetes that you\nplan to install).\n# This overwrites any existing configuration in /etc/yum.repos.d/kubernetes.repo\ncat\n<<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key\nexclude=kubelet kubeadm kubectl cri-tools kubernetes-cni\nEOF\nInstall kubelet, kubeadm and kubectl:\nsudo yum install -y kubelet kubeadm kubectl --disableexcludes\n=\nkubernetes\n(Optional) Enable the kubelet service before running kubeadm:\nsudo systemctl\nenable\n--now kubelet\nInstall CNI plugins (required for most pod network):\nCNI_PLUGINS_VERSION\n=\n\"v1.3.0\"\nARCH\n=\n\"amd64\"\nDEST\n=\n\"/opt/cni/bin\"\nsudo mkdir -p\n\"\n$DEST\n\"\ncurl -L\n\"https://github.com/containernetworking/plugins/releases/download/\n${\nCNI_PLUGINS_VERSION\n}\n/cni-plugins-linux-\n${\nARCH\n}\n-\n${\nCNI_PLUGINS_VERSION\n}\n.tgz\"\n| sudo tar -C\n\"\n$DEST\n\"\n-xz\nDefine the directory to download command files:\nNote:\nThe\nDOWNLOAD_DIR\nvariable must be set to a writable directory.\nIf you are running Flatcar Container Linux, set\nDOWNLOAD_DIR=\"/opt/bin\"\n.\nDOWNLOAD_DIR\n=\n\"/usr/local/bin\"\nsudo mkdir -p\n\"\n$DOWNLOAD_DIR\n\"\nOptionally install crictl (required for interaction with the Container Runtime Interface (CRI), optional for kubeadm):\nCRICTL_VERSION\n=\n\"v1.31.0\"\nARCH\n=\n\"amd64\"\ncurl -L\n\"https://github.com/kubernetes-sigs/cri-tools/releases/download/\n${\nCRICTL_VERSION\n}\n/crictl-\n${\nCRICTL_VERSION\n}\n-linux-\n${\nARCH\n}\n.tar.gz\"\n| sudo tar -C\n$DOWNLOAD_DIR\n-xz\nInstall\nkubeadm\n,\nkubelet\nand add a\nkubelet\nsystemd service:\nRELEASE\n=\n\"\n$(\ncurl -sSL https://dl.k8s.io/release/stable.txt\n)\n\"\nARCH\n=\n\"amd64\"\ncd\n$DOWNLOAD_DIR\nsudo curl -L --remote-name-all https://dl.k8s.io/release/\n${\nRELEASE\n}\n/bin/linux/\n${\nARCH\n}\n/\n{\nkubeadm,kubelet\n}\nsudo chmod +x\n{\nkubeadm,kubelet\n}\nRELEASE_VERSION\n=\n\"v0.16.2\"\ncurl -sSL\n\"https://raw.githubusercontent.com/kubernetes/release/\n${\nRELEASE_VERSION\n}\n/cmd/krel/templates/latest/kubelet/kubelet.service\"\n| sed\n\"s:/usr/bin:\n${\nDOWNLOAD_DIR\n}\n:g\"\n| sudo tee /usr/lib/systemd/system/kubelet.service\nsudo mkdir -p /usr/lib/systemd/system/kubelet.service.d\ncurl -sSL\n\"https://raw.githubusercontent.com/kubernetes/release/\n${\nRELEASE_VERSION\n}\n/cmd/krel/templates/latest/kubeadm/10-kubeadm.conf\"\n| sed\n\"s:/usr/bin:\n${\nDOWNLOAD_DIR\n}\n:g\"\n| sudo tee /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf\nNote:\nPlease refer to the note in the\nBefore you begin\nsection for Linux distributions\nthat do not include\nglibc\nby default.\nInstall\nkubectl\nby following the instructions on\nInstall Tools page\n.\nOptionally, enable the kubelet service before running kubeadm:\nsudo systemctl\nenable\n--now kubelet\nNote:\nThe Flatcar Container Linux distribution mounts the\n/usr\ndirectory as a read-only filesystem.\nBefore bootstrapping your cluster, you need to take additional steps to configure a writable directory.\nSee the\nKubeadm Troubleshooting guide\nto learn how to set up a writable directory.\nThe kubelet is now restarting every few seconds, as it waits in a crashloop for\nkubeadm to tell it what to do.\nConfiguring a cgroup driver\nBoth the container runtime and the kubelet have a property called\n\"cgroup driver\"\n, which is important\nfor the management of cgroups on Linux machines.\nWarning:\nMatching the container runtime and kubelet cgroup drivers is required or otherwise the kubelet process will fail.\nSee\nConfiguring a cgroup driver\nfor more details.\nTroubleshooting\nIf you are running into difficulties with kubeadm, please consult our\ntroubleshooting docs\n.\nWhat's next\nUsing kubeadm to Create a Cluster\nItems on this page refer to third party products or projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. See the\nCNCF website guidelines\nfor more details.\nYou should read the\ncontent guide\nbefore proposing a change that adds an extra third-party link.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified April 09, 2025 at 9:57 AM PST:\nAdd links to Kubernetes docs about Linux and Windows requirements, and update text. (6e8297ea91)",
      "code_examples": [
        {
          "language": "",
          "code": "SystemVerification",
          "element": "<code>SystemVerification</code>"
        },
        {
          "language": "",
          "code": "ifconfig -a",
          "element": "<code>ifconfig -a</code>"
        },
        {
          "language": "",
          "code": "sudo cat /sys/class/dmi/id/product_uuid",
          "element": "<code>sudo cat /sys/class/dmi/id/product_uuid</code>"
        },
        {
          "language": "",
          "code": "nc 127.0.0.16443-zv -w2",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>nc 127.0.0.1 <span style=\"co..."
        },
        {
          "language": "language-shell",
          "code": "nc 127.0.0.16443-zv -w2",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>nc 127.0.0.1 <span style=\"color:#666\">6443</span> -zv -w <span style=\"color:#666\">2</span>\n</span></span></code>"
        },
        {
          "language": "",
          "code": "failSwapOn: false",
          "element": "<code>failSwapOn: false</code>"
        },
        {
          "language": "",
          "code": "failSwapOn: false",
          "element": "<code>failSwapOn: false</code>"
        },
        {
          "language": "",
          "code": "swapBehavior",
          "element": "<code>swapBehavior</code>"
        },
        {
          "language": "",
          "code": "swapBehavior",
          "element": "<code>swapBehavior</code>"
        },
        {
          "language": "",
          "code": "sudo swapoff -a",
          "element": "<code>sudo swapoff -a</code>"
        },
        {
          "language": "",
          "code": "systemd.swap",
          "element": "<code>systemd.swap</code>"
        },
        {
          "language": "",
          "code": "unix:///var/run/containerd/containerd.sock",
          "element": "<code>unix:///var/run/containerd/containerd.sock</code>"
        },
        {
          "language": "",
          "code": "unix:///var/run/crio/crio.sock",
          "element": "<code>unix:///var/run/crio/crio.sock</code>"
        },
        {
          "language": "",
          "code": "unix:///var/run/cri-dockerd.sock",
          "element": "<code>unix:///var/run/cri-dockerd.sock</code>"
        },
        {
          "language": "",
          "code": "npipe:////./pipe/containerd-containerd",
          "element": "<code>npipe:////./pipe/containerd-containerd</code>"
        },
        {
          "language": "",
          "code": "npipe:////./pipe/cri-dockerd",
          "element": "<code>npipe:////./pipe/cri-dockerd</code>"
        },
        {
          "language": "",
          "code": "apt.kubernetes.io",
          "element": "<code>apt.kubernetes.io</code>"
        },
        {
          "language": "",
          "code": "yum.kubernetes.io",
          "element": "<code>yum.kubernetes.io</code>"
        },
        {
          "language": "",
          "code": "pkgs.k8s.io",
          "element": "<code>pkgs.k8s.io</code>"
        },
        {
          "language": "",
          "code": "sudo apt-get update# apt-transport-https may be a dummy package; if so, you can skip that packagesudo apt-get install -y apt-transport-https ca-certificates curl gpg",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo apt-get update\n</span><..."
        },
        {
          "language": "language-shell",
          "code": "sudo apt-get update# apt-transport-https may be a dummy package; if so, you can skip that packagesudo apt-get install -y apt-transport-https ca-certificates curl gpg",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo apt-get update\n</span></span><span style=\"display:flex\"><span><span style=\"color:#080;font-style:italic\"># apt-tran..."
        },
        {
          "language": "",
          "code": "# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.# sudo mkdir -p -m 755 /etc/apt/keyringscurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font..."
        },
        {
          "language": "language-shell",
          "code": "# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.# sudo mkdir -p -m 755 /etc/apt/keyringscurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font-style:italic\"># If the directory `/etc/apt/keyrings` does not exist, it should be created b..."
        },
        {
          "language": "",
          "code": "/etc/apt/keyrings",
          "element": "<code>/etc/apt/keyrings</code>"
        },
        {
          "language": "",
          "code": "# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.listecho'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /'| sudo tee /etc/apt/sources.list.d/kubernetes.list",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font..."
        },
        {
          "language": "language-shell",
          "code": "# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.listecho'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /'| sudo tee /etc/apt/sources.list.d/kubernetes.list",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font-style:italic\"># This overwrites any existing configuration in /etc/apt/sources.list.d/kuber..."
        },
        {
          "language": "",
          "code": "sudo apt-get updatesudo apt-get install -y kubelet kubeadm kubectlsudo apt-mark hold kubelet kubeadm kubectl",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo apt-get update\n</span><..."
        },
        {
          "language": "language-shell",
          "code": "sudo apt-get updatesudo apt-get install -y kubelet kubeadm kubectlsudo apt-mark hold kubelet kubeadm kubectl",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo apt-get update\n</span></span><span style=\"display:flex\"><span>sudo apt-get install -y kubelet kubeadm kubectl\n</spa..."
        },
        {
          "language": "",
          "code": "sudo systemctlenable--now kubelet",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo systemctl <span style=\"..."
        },
        {
          "language": "language-shell",
          "code": "sudo systemctlenable--now kubelet",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo systemctl <span style=\"color:#a2f\">enable</span> --now kubelet\n</span></span></code>"
        },
        {
          "language": "",
          "code": "# Set SELinux in permissive mode (effectively disabling it)sudo setenforce0sudo sed -i's/^SELINUX=enforcing$/SELINUX=permissive/'/etc/selinux/config",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font..."
        },
        {
          "language": "language-shell",
          "code": "# Set SELinux in permissive mode (effectively disabling it)sudo setenforce0sudo sed -i's/^SELINUX=enforcing$/SELINUX=permissive/'/etc/selinux/config",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font-style:italic\"># Set SELinux in permissive mode (effectively disabling it)</span>\n</span></s..."
        },
        {
          "language": "",
          "code": "setenforce 0",
          "element": "<code>setenforce 0</code>"
        },
        {
          "language": "",
          "code": "# This overwrites any existing configuration in /etc/yum.repos.d/kubernetes.repocat<<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/enabled=1gpgcheck=1gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.keyexclude=kubelet kubeadm kubectl cri-tools kubernetes-cniEOF",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font..."
        },
        {
          "language": "language-shell",
          "code": "# This overwrites any existing configuration in /etc/yum.repos.d/kubernetes.repocat<<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/enabled=1gpgcheck=1gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.keyexclude=kubelet kubeadm kubectl cri-tools kubernetes-cniEOF",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font-style:italic\"># This overwrites any existing configuration in /etc/yum.repos.d/kubernetes.r..."
        },
        {
          "language": "",
          "code": "sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo yum install -y kubelet ..."
        },
        {
          "language": "language-shell",
          "code": "sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo yum install -y kubelet kubeadm kubectl --disableexcludes<span style=\"color:#666\">=</span>kubernetes\n</span></span><..."
        },
        {
          "language": "",
          "code": "sudo systemctlenable--now kubelet",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo systemctl <span style=\"..."
        },
        {
          "language": "language-shell",
          "code": "sudo systemctlenable--now kubelet",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo systemctl <span style=\"color:#a2f\">enable</span> --now kubelet\n</span></span></code>"
        },
        {
          "language": "",
          "code": "CNI_PLUGINS_VERSION=\"v1.3.0\"ARCH=\"amd64\"DEST=\"/opt/cni/bin\"sudo mkdir -p\"$DEST\"curl -L\"https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGINS_VERSION}/cni-plugins-linux-${ARCH}-${CNI_PLUGINS_VERSION}.tgz\"| sudo tar -C\"$DEST\"-xz",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">CN..."
        },
        {
          "language": "language-bash",
          "code": "CNI_PLUGINS_VERSION=\"v1.3.0\"ARCH=\"amd64\"DEST=\"/opt/cni/bin\"sudo mkdir -p\"$DEST\"curl -L\"https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGINS_VERSION}/cni-plugins-linux-${ARCH}-${CNI_PLUGINS_VERSION}.tgz\"| sudo tar -C\"$DEST\"-xz",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">CNI_PLUGINS_VERSION</span><span style=\"color:#666\">=</span><span style=\"color:#b44\">\"v1.3.0\"</..."
        },
        {
          "language": "",
          "code": "DOWNLOAD_DIR",
          "element": "<code>DOWNLOAD_DIR</code>"
        },
        {
          "language": "",
          "code": "DOWNLOAD_DIR=\"/opt/bin\"",
          "element": "<code>DOWNLOAD_DIR=\"/opt/bin\"</code>"
        },
        {
          "language": "",
          "code": "DOWNLOAD_DIR=\"/usr/local/bin\"sudo mkdir -p\"$DOWNLOAD_DIR\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">DO..."
        },
        {
          "language": "language-bash",
          "code": "DOWNLOAD_DIR=\"/usr/local/bin\"sudo mkdir -p\"$DOWNLOAD_DIR\"",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">DOWNLOAD_DIR</span><span style=\"color:#666\">=</span><span style=\"color:#b44\">\"/usr/local/bin\"<..."
        },
        {
          "language": "",
          "code": "CRICTL_VERSION=\"v1.31.0\"ARCH=\"amd64\"curl -L\"https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${ARCH}.tar.gz\"| sudo tar -C$DOWNLOAD_DIR-xz",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">CR..."
        },
        {
          "language": "language-bash",
          "code": "CRICTL_VERSION=\"v1.31.0\"ARCH=\"amd64\"curl -L\"https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${ARCH}.tar.gz\"| sudo tar -C$DOWNLOAD_DIR-xz",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">CRICTL_VERSION</span><span style=\"color:#666\">=</span><span style=\"color:#b44\">\"v1.31.0\"</span..."
        },
        {
          "language": "",
          "code": "RELEASE=\"$(curl -sSL https://dl.k8s.io/release/stable.txt)\"ARCH=\"amd64\"cd$DOWNLOAD_DIRsudo curl -L --remote-name-all https://dl.k8s.io/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet}sudo chmod +x{kubeadm,kubelet}RELEASE_VERSION=\"v0.16.2\"curl -sSL\"https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/krel/templates/latest/kubelet/kubelet.service\"| sed\"s:/usr/bin:${DOWNLOAD_DIR}:g\"| sudo tee /usr/lib/systemd/system/kubelet.servicesudo mkdir -p /usr/lib/systemd/system/kubelet.service.dcurl -sSL\"https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/krel/templates/latest/kubeadm/10-kubeadm.conf\"| sed\"s:/usr/bin:${DOWNLOAD_DIR}:g\"| sudo tee /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">RE..."
        },
        {
          "language": "language-bash",
          "code": "RELEASE=\"$(curl -sSL https://dl.k8s.io/release/stable.txt)\"ARCH=\"amd64\"cd$DOWNLOAD_DIRsudo curl -L --remote-name-all https://dl.k8s.io/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet}sudo chmod +x{kubeadm,kubelet}RELEASE_VERSION=\"v0.16.2\"curl -sSL\"https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/krel/templates/latest/kubelet/kubelet.service\"| sed\"s:/usr/bin:${DOWNLOAD_DIR}:g\"| sudo tee /usr/lib/systemd/system/kubelet.servicesudo mkdir -p /usr/lib/systemd/system/kubelet.service.dcurl -sSL\"https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/krel/templates/latest/kubeadm/10-kubeadm.conf\"| sed\"s:/usr/bin:${DOWNLOAD_DIR}:g\"| sudo tee /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">RELEASE</span><span style=\"color:#666\">=</span><span style=\"color:#b44\">\"</span><span style=\"c..."
        },
        {
          "language": "",
          "code": "sudo systemctlenable--now kubelet",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>sudo systemctl <span style=\"co..."
        },
        {
          "language": "language-bash",
          "code": "sudo systemctlenable--now kubelet",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>sudo systemctl <span style=\"color:#a2f\">enable</span> --now kubelet\n</span></span></code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Installing kubeadm",
          "id": ""
        },
        {
          "level": 2,
          "text": "Before you begin",
          "id": "before-you-begin"
        },
        {
          "level": 2,
          "text": "Check your OS version",
          "id": "check-your-os-version"
        },
        {
          "level": 2,
          "text": "Verify the MAC address and product_uuid are unique for every node",
          "id": "verify-mac-address"
        },
        {
          "level": 2,
          "text": "Check network adapters",
          "id": "check-network-adapters"
        },
        {
          "level": 2,
          "text": "Check required ports",
          "id": "check-required-ports"
        },
        {
          "level": 2,
          "text": "Swap configuration",
          "id": "swap-configuration"
        },
        {
          "level": 2,
          "text": "Installing a container runtime",
          "id": "installing-runtime"
        },
        {
          "level": 2,
          "text": "Installing kubeadm, kubelet and kubectl",
          "id": "installing-kubeadm-kubelet-and-kubectl"
        },
        {
          "level": 2,
          "text": "Configuring a cgroup driver",
          "id": "configuring-a-cgroup-driver"
        },
        {
          "level": 2,
          "text": "Troubleshooting",
          "id": "troubleshooting"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Warning:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Caution:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Warning:",
          "id": ""
        }
      ],
      "timestamp": 1750731663.010921
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/",
      "title": "Troubleshooting kubeadm | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nTroubleshooting kubeadm\nTroubleshooting kubeadm\nAs with any program, you might run into an error installing or running kubeadm.\nThis page lists some common failure scenarios and have provided steps that can help you understand and fix the problem.\nIf your problem is not listed below, please follow the following steps:\nIf you think your problem is a bug with kubeadm:\nGo to\ngithub.com/kubernetes/kubeadm\nand search for existing issues.\nIf no issue exists, please\nopen one\nand follow the issue template.\nIf you are unsure about how kubeadm works, you can ask on\nSlack\nin\n#kubeadm\n,\nor open a question on\nStackOverflow\n. Please include\nrelevant tags like\n#kubernetes\nand\n#kubeadm\nso folks can help you.\nNot possible to join a v1.18 Node to a v1.17 cluster due to missing RBAC\nIn v1.18 kubeadm added prevention for joining a Node in the cluster if a Node with the same name already exists.\nThis required adding RBAC for the bootstrap-token user to be able to GET a Node object.\nHowever this causes an issue where\nkubeadm join\nfrom v1.18 cannot join a cluster created by kubeadm v1.17.\nTo workaround the issue you have two options:\nExecute\nkubeadm init phase bootstrap-token\non a control-plane node using kubeadm v1.18.\nNote that this enables the rest of the bootstrap-token permissions as well.\nor\nApply the following RBAC manually using\nkubectl apply -f ...\n:\napiVersion\n:\nrbac.authorization.k8s.io/v1\nkind\n:\nClusterRole\nmetadata\n:\nname\n:\nkubeadm:get-nodes\nrules\n:\n-\napiGroups\n:\n-\n\"\"\nresources\n:\n- nodes\nverbs\n:\n- get\n---\napiVersion\n:\nrbac.authorization.k8s.io/v1\nkind\n:\nClusterRoleBinding\nmetadata\n:\nname\n:\nkubeadm:get-nodes\nroleRef\n:\napiGroup\n:\nrbac.authorization.k8s.io\nkind\n:\nClusterRole\nname\n:\nkubeadm:get-nodes\nsubjects\n:\n-\napiGroup\n:\nrbac.authorization.k8s.io\nkind\n:\nGroup\nname\n:\nsystem:bootstrappers:kubeadm:default-node-token\nebtables\nor some similar executable not found during installation\nIf you see the following warnings while running\nkubeadm init\n[preflight] WARNING: ebtables not found in system path\n[preflight] WARNING: ethtool not found in system path\nThen you may be missing\nebtables\n,\nethtool\nor a similar executable on your node.\nYou can install them with the following commands:\nFor Ubuntu/Debian users, run\napt install ebtables ethtool\n.\nFor CentOS/Fedora users, run\nyum install ebtables ethtool\n.\nkubeadm blocks waiting for control plane during installation\nIf you notice that\nkubeadm init\nhangs after printing out the following line:\n[apiclient] Created API client, waiting for the control plane to become ready\nThis may be caused by a number of problems. The most common are:\nnetwork connection problems. Check that your machine has full network connectivity before continuing.\nthe cgroup driver of the container runtime differs from that of the kubelet. To understand how to\nconfigure it properly, see\nConfiguring a cgroup driver\n.\ncontrol plane containers are crashlooping or hanging. You can check this by running\ndocker ps\nand investigating each container by running\ndocker logs\n. For other container runtime, see\nDebugging Kubernetes nodes with crictl\n.\nkubeadm blocks when removing managed containers\nThe following could happen if the container runtime halts and does not remove\nany Kubernetes-managed containers:\nsudo kubeadm reset\n[preflight] Running pre-flight checks\n[reset] Stopping the kubelet service\n[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n[reset] Removing kubernetes-managed containers\n(block)\nA possible solution is to restart the container runtime and then re-run\nkubeadm reset\n.\nYou can also use\ncrictl\nto debug the state of the container runtime. See\nDebugging Kubernetes nodes with crictl\n.\nPods in\nRunContainerError\n,\nCrashLoopBackOff\nor\nError\nstate\nRight after\nkubeadm init\nthere should not be any pods in these states.\nIf there are pods in one of these states\nright after\nkubeadm init\n, please open an\nissue in the kubeadm repo.\ncoredns\n(or\nkube-dns\n) should be in the\nPending\nstate\nuntil you have deployed the network add-on.\nIf you see Pods in the\nRunContainerError\n,\nCrashLoopBackOff\nor\nError\nstate\nafter deploying the network add-on and nothing happens to\ncoredns\n(or\nkube-dns\n),\nit's very likely that the Pod Network add-on that you installed is somehow broken.\nYou might have to grant it more RBAC privileges or use a newer version. Please file\nan issue in the Pod Network providers' issue tracker and get the issue triaged there.\ncoredns\nis stuck in the\nPending\nstate\nThis is\nexpected\nand part of the design. kubeadm is network provider-agnostic, so the admin\nshould\ninstall the pod network add-on\nof choice. You have to install a Pod Network\nbefore CoreDNS may be deployed fully. Hence the\nPending\nstate before the network is set up.\nHostPort\nservices do not work\nThe\nHostPort\nand\nHostIP\nfunctionality is available depending on your Pod Network\nprovider. Please contact the author of the Pod Network add-on to find out whether\nHostPort\nand\nHostIP\nfunctionality are available.\nCalico, Canal, and Flannel CNI providers are verified to support HostPort.\nFor more information, see the\nCNI portmap documentation\n.\nIf your network provider does not support the portmap CNI plugin, you may need to use the\nNodePort feature of services\nor use\nHostNetwork=true\n.\nPods are not accessible via their Service IP\nMany network add-ons do not yet enable\nhairpin mode\nwhich allows pods to access themselves via their Service IP. This is an issue related to\nCNI\n. Please contact the network\nadd-on provider to get the latest status of their support for hairpin mode.\nIf you are using VirtualBox (directly or via Vagrant), you will need to\nensure that\nhostname -i\nreturns a routable IP address. By default, the first\ninterface is connected to a non-routable host-only network. A work around\nis to modify\n/etc/hosts\n, see this\nVagrantfile\nfor an example.\nTLS certificate errors\nThe following error indicates a possible certificate mismatch.\n# kubectl get pods\nUnable to connect to the server: x509: certificate signed by unknown authority (possibly because of \"crypto/rsa: verification error\" while trying to verify candidate authority certificate \"kubernetes\")\nVerify that the\n$HOME/.kube/config\nfile contains a valid certificate, and\nregenerate a certificate if necessary. The certificates in a kubeconfig file\nare base64 encoded. The\nbase64 --decode\ncommand can be used to decode the certificate\nand\nopenssl x509 -text -noout\ncan be used for viewing the certificate information.\nUnset the\nKUBECONFIG\nenvironment variable using:\nunset\nKUBECONFIG\nOr set it to the default\nKUBECONFIG\nlocation:\nexport\nKUBECONFIG\n=\n/etc/kubernetes/admin.conf\nAnother workaround is to overwrite the existing\nkubeconfig\nfor the \"admin\" user:\nmv\n$HOME\n/.kube\n$HOME\n/.kube.bak\nmkdir\n$HOME\n/.kube\nsudo cp -i /etc/kubernetes/admin.conf\n$HOME\n/.kube/config\nsudo chown\n$(\nid -u\n)\n:\n$(\nid -g\n)\n$HOME\n/.kube/config\nKubelet client certificate rotation fails\nBy default, kubeadm configures a kubelet with automatic rotation of client certificates by using the\n/var/lib/kubelet/pki/kubelet-client-current.pem\nsymlink specified in\n/etc/kubernetes/kubelet.conf\n.\nIf this rotation process fails you might see errors such as\nx509: certificate has expired or is not yet valid\nin kube-apiserver logs. To fix the issue you must follow these steps:\nBackup and delete\n/etc/kubernetes/kubelet.conf\nand\n/var/lib/kubelet/pki/kubelet-client*\nfrom the failed node.\nFrom a working control plane node in the cluster that has\n/etc/kubernetes/pki/ca.key\nexecute\nkubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE > kubelet.conf\n.\n$NODE\nmust be set to the name of the existing failed node in the cluster.\nModify the resulted\nkubelet.conf\nmanually to adjust the cluster name and server endpoint,\nor pass\nkubeconfig user --config\n(see\nGenerating kubeconfig files for additional users\n). If your cluster does not have\nthe\nca.key\nyou must sign the embedded certificates in the\nkubelet.conf\nexternally.\nCopy this resulted\nkubelet.conf\nto\n/etc/kubernetes/kubelet.conf\non the failed node.\nRestart the kubelet (\nsystemctl restart kubelet\n) on the failed node and wait for\n/var/lib/kubelet/pki/kubelet-client-current.pem\nto be recreated.\nManually edit the\nkubelet.conf\nto point to the rotated kubelet client certificates, by replacing\nclient-certificate-data\nand\nclient-key-data\nwith:\nclient-certificate\n:\n/var/lib/kubelet/pki/kubelet-client-current.pem\nclient-key\n:\n/var/lib/kubelet/pki/kubelet-client-current.pem\nRestart the kubelet.\nMake sure the node becomes\nReady\n.\nDefault NIC When using flannel as the pod network in Vagrant\nThe following error might indicate that something was wrong in the pod network:\nError from server\n(\nNotFound\n)\n: the server could not find the requested resource\nIf you're using flannel as the pod network inside Vagrant, then you will have to\nspecify the default interface name for flannel.\nVagrant typically assigns two interfaces to all VMs. The first, for which all hosts\nare assigned the IP address\n10.0.2.15\n, is for external traffic that gets NATed.\nThis may lead to problems with flannel, which defaults to the first interface on a host.\nThis leads to all hosts thinking they have the same public IP address. To prevent this,\npass the\n--iface eth1\nflag to flannel so that the second interface is chosen.\nNon-public IP used for containers\nIn some situations\nkubectl logs\nand\nkubectl run\ncommands may return with the\nfollowing errors in an otherwise functional cluster:\nError from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host\nThis may be due to Kubernetes using an IP that can not communicate with other IPs on\nthe seemingly same subnet, possibly by policy of the machine provider.\nDigitalOcean assigns a public IP to\neth0\nas well as a private one to be used internally\nas anchor for their floating IP feature, yet\nkubelet\nwill pick the latter as the node's\nInternalIP\ninstead of the public one.\nUse\nip addr show\nto check for this scenario instead of\nifconfig\nbecause\nifconfig\nwill\nnot display the offending alias IP address. Alternatively an API endpoint specific to\nDigitalOcean allows to query for the anchor IP from the droplet:\ncurl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address\nThe workaround is to tell\nkubelet\nwhich IP to use using\n--node-ip\n.\nWhen using DigitalOcean, it can be the public one (assigned to\neth0\n) or\nthe private one (assigned to\neth1\n) should you want to use the optional\nprivate network. The\nkubeletExtraArgs\nsection of the kubeadm\nNodeRegistrationOptions\nstructure\ncan be used for this.\nThen restart\nkubelet\n:\nsystemctl daemon-reload\nsystemctl restart kubelet\ncoredns\npods have\nCrashLoopBackOff\nor\nError\nstate\nIf you have nodes that are running SELinux with an older version of Docker, you might experience a scenario\nwhere the\ncoredns\npods are not starting. To solve that, you can try one of the following options:\nUpgrade to a\nnewer version of Docker\n.\nDisable SELinux\n.\nModify the\ncoredns\ndeployment to set\nallowPrivilegeEscalation\nto\ntrue\n:\nkubectl -n kube-system get deployment coredns -o yaml |\n\\\nsed\n's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g'\n|\n\\\nkubectl apply -f -\nAnother cause for CoreDNS to have\nCrashLoopBackOff\nis when a CoreDNS Pod deployed in Kubernetes detects a loop.\nA number of workarounds\nare available to avoid Kubernetes trying to restart the CoreDNS Pod every time CoreDNS detects the loop and exits.\nWarning:\nDisabling SELinux or setting\nallowPrivilegeEscalation\nto\ntrue\ncan compromise\nthe security of your cluster.\netcd pods restart continually\nIf you encounter the following error:\nrpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused \"process_linux.go:110: decoding init error from pipe caused \\\"read parent: connection reset by peer\\\"\"\nThis issue appears if you run CentOS 7 with Docker 1.13.1.84.\nThis version of Docker can prevent the kubelet from executing into the etcd container.\nTo work around the issue, choose one of these options:\nRoll back to an earlier version of Docker, such as 1.13.1-75\nyum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64\nInstall one of the more recent recommended versions, such as 18.06:\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nyum install docker-ce-18.06.1.ce-3.el7.x86_64\nNot possible to pass a comma separated list of values to arguments inside a\n--component-extra-args\nflag\nkubeadm init\nflags such as\n--component-extra-args\nallow you to pass custom arguments to a control-plane\ncomponent like the kube-apiserver. However, this mechanism is limited due to the underlying type used for parsing\nthe values (\nmapStringString\n).\nIf you decide to pass an argument that supports multiple, comma-separated values such as\n--apiserver-extra-args \"enable-admission-plugins=LimitRanger,NamespaceExists\"\nthis flag will fail with\nflag: malformed pair, expect string=string\n. This happens because the list of arguments for\n--apiserver-extra-args\nexpects\nkey=value\npairs and in this case\nNamespacesExists\nis considered\nas a key that is missing a value.\nAlternatively, you can try separating the\nkey=value\npairs like so:\n--apiserver-extra-args \"enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists\"\nbut this will result in the key\nenable-admission-plugins\nonly having the value of\nNamespaceExists\n.\nA known workaround is to use the kubeadm\nconfiguration file\n.\nkube-proxy scheduled before node is initialized by cloud-controller-manager\nIn cloud provider scenarios, kube-proxy can end up being scheduled on new worker nodes before\nthe cloud-controller-manager has initialized the node addresses. This causes kube-proxy to fail\nto pick up the node's IP address properly and has knock-on effects to the proxy function managing\nload balancers.\nThe following error can be seen in kube-proxy Pods:\nserver.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []\nproxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP\nA known solution is to patch the kube-proxy DaemonSet to allow scheduling it on control-plane\nnodes regardless of their conditions, keeping it off of other nodes until their initial guarding\nconditions abate:\nkubectl -n kube-system patch ds kube-proxy -p='{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"tolerations\": [\n          {\n            \"key\": \"CriticalAddonsOnly\",\n            \"operator\": \"Exists\"\n          },\n          {\n            \"effect\": \"NoSchedule\",\n            \"key\": \"node-role.kubernetes.io/control-plane\"\n          }\n        ]\n      }\n    }\n  }\n}'\nThe tracking issue for this problem is\nhere\n.\n/usr\nis mounted read-only on nodes\nOn Linux distributions such as Fedora CoreOS or Flatcar Container Linux, the directory\n/usr\nis mounted as a read-only filesystem.\nFor\nflex-volume support\n,\nKubernetes components like the kubelet and kube-controller-manager use the default path of\n/usr/libexec/kubernetes/kubelet-plugins/volume/exec/\n, yet the flex-volume directory\nmust be writeable\nfor the feature to work.\nNote:\nFlexVolume was deprecated in the Kubernetes v1.23 release.\nTo workaround this issue, you can configure the flex-volume directory using the kubeadm\nconfiguration file\n.\nOn the primary control-plane Node (created using\nkubeadm init\n), pass the following\nfile using\n--config\n:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nInitConfiguration\nnodeRegistration\n:\nkubeletExtraArgs\n:\n-\nname\n:\n\"volume-plugin-dir\"\nvalue\n:\n\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"\n---\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\ncontrollerManager\n:\nextraArgs\n:\n-\nname\n:\n\"flex-volume-plugin-dir\"\nvalue\n:\n\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"\nOn joining Nodes:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nJoinConfiguration\nnodeRegistration\n:\nkubeletExtraArgs\n:\n-\nname\n:\n\"volume-plugin-dir\"\nvalue\n:\n\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"\nAlternatively, you can modify\n/etc/fstab\nto make the\n/usr\nmount writeable, but please\nbe advised that this is modifying a design principle of the Linux distribution.\nkubeadm upgrade plan\nprints out\ncontext deadline exceeded\nerror message\nThis error message is shown when upgrading a Kubernetes cluster with\nkubeadm\nin\nthe case of running an external etcd. This is not a critical bug and happens because\nolder versions of kubeadm perform a version check on the external etcd cluster.\nYou can proceed with\nkubeadm upgrade apply ...\n.\nThis issue is fixed as of version 1.19.\nkubeadm reset\nunmounts\n/var/lib/kubelet\nIf\n/var/lib/kubelet\nis being mounted, performing a\nkubeadm reset\nwill effectively unmount it.\nTo workaround the issue, re-mount the\n/var/lib/kubelet\ndirectory after performing the\nkubeadm reset\noperation.\nThis is a regression introduced in kubeadm 1.15. The issue is fixed in 1.20.\nCannot use the metrics-server securely in a kubeadm cluster\nIn a kubeadm cluster, the\nmetrics-server\ncan be used insecurely by passing the\n--kubelet-insecure-tls\nto it. This is not recommended for production clusters.\nIf you want to use TLS between the metrics-server and the kubelet there is a problem,\nsince kubeadm deploys a self-signed serving certificate for the kubelet. This can cause the following errors\non the side of the metrics-server:\nx509: certificate signed by unknown authority\nx509: certificate is valid for IP-foo not IP-bar\nSee\nEnabling signed kubelet serving certificates\nto understand how to configure the kubelets in a kubeadm cluster to have properly signed serving certificates.\nAlso see\nHow to run the metrics-server securely\n.\nUpgrade fails due to etcd hash not changing\nOnly applicable to upgrading a control plane node with a kubeadm binary v1.28.3 or later,\nwhere the node is currently managed by kubeadm versions v1.28.0, v1.28.1 or v1.28.2.\nHere is the error message you may encounter:\n[upgrade/etcd] Failed to upgrade etcd: couldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced: static Pod hash for component etcd on Node kinder-upgrade-control-plane-1 did not change after 5m0s: timed out waiting for the condition\n[upgrade/etcd] Waiting for previous etcd to become available\nI0907 10:10:09.109104    3704 etcd.go:588] [etcd] attempting to see if all cluster endpoints ([https://172.17.0.6:2379/ https://172.17.0.4:2379/ https://172.17.0.3:2379/]) are available 1/10\n[upgrade/etcd] Etcd was rolled back and is now available\nstatic Pod hash for component etcd on Node kinder-upgrade-control-plane-1 did not change after 5m0s: timed out waiting for the condition\ncouldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.rollbackOldManifests\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:525\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.upgradeComponent\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:254\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.performEtcdStaticPodUpgrade\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:338\n...\nThe reason for this failure is that the affected versions generate an etcd manifest file with\nunwanted defaults in the PodSpec. This will result in a diff from the manifest comparison,\nand kubeadm will expect a change in the Pod hash, but the kubelet will never update the hash.\nThere are two way to workaround this issue if you see it in your cluster:\nThe etcd upgrade can be skipped between the affected versions and v1.28.3 (or later) by using:\nkubeadm upgrade\n{\napply|node\n}\n[\nversion\n]\n--etcd-upgrade\n=\nfalse\nThis is not recommended in case a new etcd version was introduced by a later v1.28 patch version.\nBefore upgrade, patch the manifest for the etcd static pod, to remove the problematic defaulted attributes:\ndiff --git a/etc/kubernetes/manifests/etcd_defaults.yaml b/etc/kubernetes/manifests/etcd_origin.yaml\nindex d807ccbe0aa..46b35f00e15 100644\n--- a/etc/kubernetes/manifests/etcd_defaults.yaml\n+++ b/etc/kubernetes/manifests/etcd_origin.yaml\n@@ -43,7 +43,6 @@ spec:\nscheme: HTTP\ninitialDelaySeconds: 10\nperiodSeconds: 10\n-      successThreshold: 1\ntimeoutSeconds: 15\nname: etcd\nresources:\n@@ -59,26 +58,18 @@ spec:\nscheme: HTTP\ninitialDelaySeconds: 10\nperiodSeconds: 10\n-      successThreshold: 1\ntimeoutSeconds: 15\n-    terminationMessagePath: /dev/termination-log\n-    terminationMessagePolicy: File\nvolumeMounts:\n- mountPath: /var/lib/etcd\nname: etcd-data\n- mountPath: /etc/kubernetes/pki/etcd\nname: etcd-certs\n-  dnsPolicy: ClusterFirst\n-  enableServiceLinks: true\nhostNetwork: true\npriority: 2000001000\npriorityClassName: system-node-critical\n-  restartPolicy: Always\n-  schedulerName: default-scheduler\nsecurityContext:\nseccompProfile:\ntype: RuntimeDefault\n-  terminationGracePeriodSeconds: 30\nvolumes:\n- hostPath:\npath: /etc/kubernetes/pki/etcd\nMore information can be found in the\ntracking issue\nfor this bug.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified July 05, 2024 at 4:06 PM PST:\nkubeadm: use v1beta4 in all docs examples (efc1133fa4)",
      "code_examples": [
        {
          "language": "",
          "code": "#kubernetes",
          "element": "<code>#kubernetes</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "kubeadm init phase bootstrap-token",
          "element": "<code>kubeadm init phase bootstrap-token</code>"
        },
        {
          "language": "",
          "code": "kubectl apply -f ...",
          "element": "<code>kubectl apply -f ...</code>"
        },
        {
          "language": "",
          "code": "apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:kubeadm:get-nodesrules:-apiGroups:-\"\"resources:- nodesverbs:- get---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:kubeadm:get-nodesroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:kubeadm:get-nodessubjects:-apiGroup:rbac.authorization.k8s.iokind:Groupname:system:bootstrappers:kubeadm:default-node-token",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:kubeadm:get-nodesrules:-apiGroups:-\"\"resources:- nodesverbs:- get---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:kubeadm:get-nodesroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:kubeadm:get-nodessubjects:-apiGroup:rbac.authorization.k8s.iokind:Groupname:system:bootstrappers:kubeadm:default-node-token",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>rbac.authorization.k8s.io/v1<..."
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "[preflight] WARNING: ebtables not found in system path[preflight] WARNING: ethtool not found in system path",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex\"><span><span style=\"color:#888\"..."
        },
        {
          "language": "language-console",
          "code": "[preflight] WARNING: ebtables not found in system path[preflight] WARNING: ethtool not found in system path",
          "element": "<code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex\"><span><span style=\"color:#888\">[preflight] WARNING: ebtables not found in system path\n</span></span></span><span style=\"di..."
        },
        {
          "language": "",
          "code": "apt install ebtables ethtool",
          "element": "<code>apt install ebtables ethtool</code>"
        },
        {
          "language": "",
          "code": "yum install ebtables ethtool",
          "element": "<code>yum install ebtables ethtool</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "[apiclient] Created API client, waiting for the control plane to become ready",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex\"><span><span style=\"color:#888\"..."
        },
        {
          "language": "language-console",
          "code": "[apiclient] Created API client, waiting for the control plane to become ready",
          "element": "<code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex\"><span><span style=\"color:#888\">[apiclient] Created API client, waiting for the control plane to become ready\n</span></span..."
        },
        {
          "language": "",
          "code": "docker logs",
          "element": "<code>docker logs</code>"
        },
        {
          "language": "",
          "code": "sudo kubeadm reset",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo kubeadm reset\n</span></..."
        },
        {
          "language": "language-shell",
          "code": "sudo kubeadm reset",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo kubeadm reset\n</span></span></code>"
        },
        {
          "language": "",
          "code": "[preflight] Running pre-flight checks[reset] Stopping the kubelet service[reset] Unmounting mounted directories in \"/var/lib/kubelet\"[reset] Removing kubernetes-managed containers(block)",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex\"><span><span style=\"color:#888\"..."
        },
        {
          "language": "language-console",
          "code": "[preflight] Running pre-flight checks[reset] Stopping the kubelet service[reset] Unmounting mounted directories in \"/var/lib/kubelet\"[reset] Removing kubernetes-managed containers(block)",
          "element": "<code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex\"><span><span style=\"color:#888\">[preflight] Running pre-flight checks\n</span></span></span><span style=\"display:flex\"><span..."
        },
        {
          "language": "",
          "code": "kubeadm reset",
          "element": "<code>kubeadm reset</code>"
        },
        {
          "language": "",
          "code": "RunContainerError",
          "element": "<code>RunContainerError</code>"
        },
        {
          "language": "",
          "code": "CrashLoopBackOff",
          "element": "<code>CrashLoopBackOff</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "RunContainerError",
          "element": "<code>RunContainerError</code>"
        },
        {
          "language": "",
          "code": "CrashLoopBackOff",
          "element": "<code>CrashLoopBackOff</code>"
        },
        {
          "language": "",
          "code": "HostNetwork=true",
          "element": "<code>HostNetwork=true</code>"
        },
        {
          "language": "",
          "code": "hostname -i",
          "element": "<code>hostname -i</code>"
        },
        {
          "language": "",
          "code": "# kubectl get pods\nUnable to connect to the server: x509: certificate signed by unknown authority (possibly because of \"crypto/rsa: verification error\" while trying to verify candidate authority certificate \"kubernetes\")",
          "element": "<pre tabindex=\"0\"><code class=\"language-none\" data-lang=\"none\"># kubectl get pods\nUnable to connect to the server: x509: certificate signed by unknown authority (possibly because of \"crypto/rsa: verif..."
        },
        {
          "language": "language-none",
          "code": "# kubectl get pods\nUnable to connect to the server: x509: certificate signed by unknown authority (possibly because of \"crypto/rsa: verification error\" while trying to verify candidate authority certificate \"kubernetes\")",
          "element": "<code class=\"language-none\" data-lang=\"none\"># kubectl get pods\nUnable to connect to the server: x509: certificate signed by unknown authority (possibly because of \"crypto/rsa: verification error\" whi..."
        },
        {
          "language": "",
          "code": "$HOME/.kube/config",
          "element": "<code>$HOME/.kube/config</code>"
        },
        {
          "language": "",
          "code": "base64 --decode",
          "element": "<code>base64 --decode</code>"
        },
        {
          "language": "",
          "code": "openssl x509 -text -noout",
          "element": "<code>openssl x509 -text -noout</code>"
        },
        {
          "language": "",
          "code": "unsetKUBECONFIG",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#a2f\">unset</sp..."
        },
        {
          "language": "language-sh",
          "code": "unsetKUBECONFIG",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#a2f\">unset</span> KUBECONFIG\n</span></span></code>"
        },
        {
          "language": "",
          "code": "exportKUBECONFIG=/etc/kubernetes/admin.conf",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#a2f\">export</s..."
        },
        {
          "language": "language-sh",
          "code": "exportKUBECONFIG=/etc/kubernetes/admin.conf",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#a2f\">export</span> <span style=\"color:#b8860b\">KUBECONFIG</span><span style=\"color:#666\">=</span>/etc/kube..."
        },
        {
          "language": "",
          "code": "mv$HOME/.kube$HOME/.kube.bakmkdir$HOME/.kubesudo cp -i /etc/kubernetes/admin.conf$HOME/.kube/configsudo chown$(id -u):$(id -g)$HOME/.kube/config",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>mv <span style=\"color:#b8860b\">$HO..."
        },
        {
          "language": "language-sh",
          "code": "mv$HOME/.kube$HOME/.kube.bakmkdir$HOME/.kubesudo cp -i /etc/kubernetes/admin.conf$HOME/.kube/configsudo chown$(id -u):$(id -g)$HOME/.kube/config",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>mv <span style=\"color:#b8860b\">$HOME</span>/.kube <span style=\"color:#b8860b\">$HOME</span>/.kube.bak\n</span></span><span style..."
        },
        {
          "language": "",
          "code": "/var/lib/kubelet/pki/kubelet-client-current.pem",
          "element": "<code>/var/lib/kubelet/pki/kubelet-client-current.pem</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/kubelet.conf",
          "element": "<code>/etc/kubernetes/kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "x509: certificate has expired or is not yet valid",
          "element": "<code>x509: certificate has expired or is not yet valid</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/kubelet.conf",
          "element": "<code>/etc/kubernetes/kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet/pki/kubelet-client*",
          "element": "<code>/var/lib/kubelet/pki/kubelet-client*</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki/ca.key",
          "element": "<code>/etc/kubernetes/pki/ca.key</code>"
        },
        {
          "language": "",
          "code": "kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE > kubelet.conf",
          "element": "<code>kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE &gt; kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "kubelet.conf",
          "element": "<code>kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "kubeconfig user --config",
          "element": "<code>kubeconfig user --config</code>"
        },
        {
          "language": "",
          "code": "kubelet.conf",
          "element": "<code>kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "kubelet.conf",
          "element": "<code>kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/kubelet.conf",
          "element": "<code>/etc/kubernetes/kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "systemctl restart kubelet",
          "element": "<code>systemctl restart kubelet</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet/pki/kubelet-client-current.pem",
          "element": "<code>/var/lib/kubelet/pki/kubelet-client-current.pem</code>"
        },
        {
          "language": "",
          "code": "kubelet.conf",
          "element": "<code>kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "client-certificate-data",
          "element": "<code>client-certificate-data</code>"
        },
        {
          "language": "",
          "code": "client-key-data",
          "element": "<code>client-key-data</code>"
        },
        {
          "language": "",
          "code": "client-certificate:/var/lib/kubelet/pki/kubelet-client-current.pemclient-key:/var/lib/kubelet/pki/kubelet-client-current.pem",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "client-certificate:/var/lib/kubelet/pki/kubelet-client-current.pemclient-key:/var/lib/kubelet/pki/kubelet-client-current.pem",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">client-certificate</span>:<span style=\"color:#bbb\"> </span>/var/lib/kubelet/pki/..."
        },
        {
          "language": "",
          "code": "Error from server(NotFound): the server could not find the requested resource",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>Error from server <span style=\"col..."
        },
        {
          "language": "language-sh",
          "code": "Error from server(NotFound): the server could not find the requested resource",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>Error from server <span style=\"color:#666\">(</span>NotFound<span style=\"color:#666\">)</span>: the server could not find the re..."
        },
        {
          "language": "",
          "code": "--iface eth1",
          "element": "<code>--iface eth1</code>"
        },
        {
          "language": "",
          "code": "kubectl logs",
          "element": "<code>kubectl logs</code>"
        },
        {
          "language": "",
          "code": "kubectl run",
          "element": "<code>kubectl run</code>"
        },
        {
          "language": "",
          "code": "Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex\"><span><span style=\"color:#888\"..."
        },
        {
          "language": "language-console",
          "code": "Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host",
          "element": "<code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex\"><span><span style=\"color:#888\">Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m..."
        },
        {
          "language": "",
          "code": "ip addr show",
          "element": "<code>ip addr show</code>"
        },
        {
          "language": "",
          "code": "curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>curl http://169.254.169.254/metada..."
        },
        {
          "language": "language-sh",
          "code": "curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubeletExtraArgs",
          "element": "<code>kubeletExtraArgs</code>"
        },
        {
          "language": "",
          "code": "NodeRegistrationOptions",
          "element": "<code>NodeRegistrationOptions</code>"
        },
        {
          "language": "",
          "code": "systemctl daemon-reloadsystemctl restart kubelet",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>systemctl daemon-reload\n</span></s..."
        },
        {
          "language": "language-sh",
          "code": "systemctl daemon-reloadsystemctl restart kubelet",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>systemctl daemon-reload\n</span></span><span style=\"display:flex\"><span>systemctl restart kubelet\n</span></span></code>"
        },
        {
          "language": "",
          "code": "CrashLoopBackOff",
          "element": "<code>CrashLoopBackOff</code>"
        },
        {
          "language": "",
          "code": "allowPrivilegeEscalation",
          "element": "<code>allowPrivilegeEscalation</code>"
        },
        {
          "language": "",
          "code": "kubectl -n kube-system get deployment coredns -o yaml |\\sed's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g'|\\kubectl apply -f -",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl -n kube-system get dep..."
        },
        {
          "language": "language-bash",
          "code": "kubectl -n kube-system get deployment coredns -o yaml |\\sed's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g'|\\kubectl apply -f -",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl -n kube-system get deployment coredns -o yaml | <span style=\"color:#b62;font-weight:700\">\\\n</span></span></span><s..."
        },
        {
          "language": "",
          "code": "CrashLoopBackOff",
          "element": "<code>CrashLoopBackOff</code>"
        },
        {
          "language": "",
          "code": "allowPrivilegeEscalation",
          "element": "<code>allowPrivilegeEscalation</code>"
        },
        {
          "language": "",
          "code": "rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused \"process_linux.go:110: decoding init error from pipe caused \\\"read parent: connection reset by peer\\\"\"",
          "element": "<pre tabindex=\"0\"><code>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused \"process_linux.go:110: decoding init error from pipe caused..."
        },
        {
          "language": "",
          "code": "rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused \"process_linux.go:110: decoding init error from pipe caused \\\"read parent: connection reset by peer\\\"\"",
          "element": "<code>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused \"process_linux.go:110: decoding init error from pipe caused \\\"read parent: co..."
        },
        {
          "language": "",
          "code": "yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64",
          "element": "<pre tabindex=\"0\"><code>yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64\n</code></p..."
        },
        {
          "language": "",
          "code": "yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64",
          "element": "<code>yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64\n</code>"
        },
        {
          "language": "",
          "code": "sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoyum install docker-ce-18.06.1.ce-3.el7.x86_64",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>sudo yum-config-manager --add-..."
        },
        {
          "language": "language-bash",
          "code": "sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoyum install docker-ce-18.06.1.ce-3.el7.x86_64",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n</span></span><span style=\"disp..."
        },
        {
          "language": "",
          "code": "--component-extra-args",
          "element": "<code>--component-extra-args</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "--component-extra-args",
          "element": "<code>--component-extra-args</code>"
        },
        {
          "language": "",
          "code": "mapStringString",
          "element": "<code>mapStringString</code>"
        },
        {
          "language": "",
          "code": "--apiserver-extra-args \"enable-admission-plugins=LimitRanger,NamespaceExists\"",
          "element": "<code>--apiserver-extra-args \"enable-admission-plugins=LimitRanger,NamespaceExists\"</code>"
        },
        {
          "language": "",
          "code": "flag: malformed pair, expect string=string",
          "element": "<code>flag: malformed pair, expect string=string</code>"
        },
        {
          "language": "",
          "code": "--apiserver-extra-args",
          "element": "<code>--apiserver-extra-args</code>"
        },
        {
          "language": "",
          "code": "NamespacesExists",
          "element": "<code>NamespacesExists</code>"
        },
        {
          "language": "",
          "code": "--apiserver-extra-args \"enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists\"",
          "element": "<code>--apiserver-extra-args \"enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists\"</code>"
        },
        {
          "language": "",
          "code": "enable-admission-plugins",
          "element": "<code>enable-admission-plugins</code>"
        },
        {
          "language": "",
          "code": "NamespaceExists",
          "element": "<code>NamespaceExists</code>"
        },
        {
          "language": "",
          "code": "server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []\nproxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP",
          "element": "<pre tabindex=\"0\"><code>server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []\nproxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP\n</code></pre>"
        },
        {
          "language": "",
          "code": "server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []\nproxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP",
          "element": "<code>server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []\nproxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP\n</code>"
        },
        {
          "language": "",
          "code": "kubectl -n kube-system patch ds kube-proxy -p='{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"tolerations\": [\n          {\n            \"key\": \"CriticalAddonsOnly\",\n            \"operator\": \"Exists\"\n          },\n          {\n            \"effect\": \"NoSchedule\",\n            \"key\": \"node-role.kubernetes.io/control-plane\"\n          }\n        ]\n      }\n    }\n  }\n}'",
          "element": "<pre tabindex=\"0\"><code>kubectl -n kube-system patch ds kube-proxy -p='{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"tolerations\": [\n          {\n            \"key\": \"CriticalAddonsOnly\",\n   ..."
        },
        {
          "language": "",
          "code": "kubectl -n kube-system patch ds kube-proxy -p='{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"tolerations\": [\n          {\n            \"key\": \"CriticalAddonsOnly\",\n            \"operator\": \"Exists\"\n          },\n          {\n            \"effect\": \"NoSchedule\",\n            \"key\": \"node-role.kubernetes.io/control-plane\"\n          }\n        ]\n      }\n    }\n  }\n}'",
          "element": "<code>kubectl -n kube-system patch ds kube-proxy -p='{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"tolerations\": [\n          {\n            \"key\": \"CriticalAddonsOnly\",\n            \"operator..."
        },
        {
          "language": "",
          "code": "/usr/libexec/kubernetes/kubelet-plugins/volume/exec/",
          "element": "<code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:InitConfigurationnodeRegistration:kubeletExtraArgs:-name:\"volume-plugin-dir\"value:\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"---apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationcontrollerManager:extraArgs:-name:\"flex-volume-plugin-dir\"value:\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:InitConfigurationnodeRegistration:kubeletExtraArgs:-name:\"volume-plugin-dir\"value:\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"---apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationcontrollerManager:extraArgs:-name:\"flex-volume-plugin-dir\"value:\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:JoinConfigurationnodeRegistration:kubeletExtraArgs:-name:\"volume-plugin-dir\"value:\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:JoinConfigurationnodeRegistration:kubeletExtraArgs:-name:\"volume-plugin-dir\"value:\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        },
        {
          "language": "",
          "code": "kubeadm upgrade plan",
          "element": "<code>kubeadm upgrade plan</code>"
        },
        {
          "language": "",
          "code": "context deadline exceeded",
          "element": "<code>context deadline exceeded</code>"
        },
        {
          "language": "",
          "code": "kubeadm upgrade apply ...",
          "element": "<code>kubeadm upgrade apply ...</code>"
        },
        {
          "language": "",
          "code": "kubeadm reset",
          "element": "<code>kubeadm reset</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet",
          "element": "<code>/var/lib/kubelet</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet",
          "element": "<code>/var/lib/kubelet</code>"
        },
        {
          "language": "",
          "code": "kubeadm reset",
          "element": "<code>kubeadm reset</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet",
          "element": "<code>/var/lib/kubelet</code>"
        },
        {
          "language": "",
          "code": "kubeadm reset",
          "element": "<code>kubeadm reset</code>"
        },
        {
          "language": "",
          "code": "--kubelet-insecure-tls",
          "element": "<code>--kubelet-insecure-tls</code>"
        },
        {
          "language": "",
          "code": "x509: certificate signed by unknown authority\nx509: certificate is valid for IP-foo not IP-bar",
          "element": "<pre tabindex=\"0\"><code>x509: certificate signed by unknown authority\nx509: certificate is valid for IP-foo not IP-bar\n</code></pre>"
        },
        {
          "language": "",
          "code": "x509: certificate signed by unknown authority\nx509: certificate is valid for IP-foo not IP-bar",
          "element": "<code>x509: certificate signed by unknown authority\nx509: certificate is valid for IP-foo not IP-bar\n</code>"
        },
        {
          "language": "",
          "code": "[upgrade/etcd] Failed to upgrade etcd: couldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced: static Pod hash for component etcd on Node kinder-upgrade-control-plane-1 did not change after 5m0s: timed out waiting for the condition\n[upgrade/etcd] Waiting for previous etcd to become available\nI0907 10:10:09.109104    3704 etcd.go:588] [etcd] attempting to see if all cluster endpoints ([https://172.17.0.6:2379/ https://172.17.0.4:2379/ https://172.17.0.3:2379/]) are available 1/10\n[upgrade/etcd] Etcd was rolled back and is now available\nstatic Pod hash for component etcd on Node kinder-upgrade-control-plane-1 did not change after 5m0s: timed out waiting for the condition\ncouldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.rollbackOldManifests\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:525\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.upgradeComponent\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:254\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.performEtcdStaticPodUpgrade\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:338\n...",
          "element": "<pre tabindex=\"0\"><code>[upgrade/etcd] Failed to upgrade etcd: couldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced: static Pod hash for compone..."
        },
        {
          "language": "",
          "code": "[upgrade/etcd] Failed to upgrade etcd: couldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced: static Pod hash for component etcd on Node kinder-upgrade-control-plane-1 did not change after 5m0s: timed out waiting for the condition\n[upgrade/etcd] Waiting for previous etcd to become available\nI0907 10:10:09.109104    3704 etcd.go:588] [etcd] attempting to see if all cluster endpoints ([https://172.17.0.6:2379/ https://172.17.0.4:2379/ https://172.17.0.3:2379/]) are available 1/10\n[upgrade/etcd] Etcd was rolled back and is now available\nstatic Pod hash for component etcd on Node kinder-upgrade-control-plane-1 did not change after 5m0s: timed out waiting for the condition\ncouldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.rollbackOldManifests\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:525\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.upgradeComponent\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:254\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.performEtcdStaticPodUpgrade\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:338\n...",
          "element": "<code>[upgrade/etcd] Failed to upgrade etcd: couldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced: static Pod hash for component etcd on Node ki..."
        },
        {
          "language": "",
          "code": "kubeadm upgrade{apply|node}[version]--etcd-upgrade=false",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>kubeadm upgrade <span style=..."
        },
        {
          "language": "language-shell",
          "code": "kubeadm upgrade{apply|node}[version]--etcd-upgrade=false",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>kubeadm upgrade <span style=\"color:#666\">{</span>apply|node<span style=\"color:#666\">}</span> <span style=\"color:#666\">[<..."
        },
        {
          "language": "",
          "code": "diff --git a/etc/kubernetes/manifests/etcd_defaults.yaml b/etc/kubernetes/manifests/etcd_origin.yamlindex d807ccbe0aa..46b35f00e15 100644--- a/etc/kubernetes/manifests/etcd_defaults.yaml+++ b/etc/kubernetes/manifests/etcd_origin.yaml@@ -43,7 +43,6 @@ spec:scheme: HTTPinitialDelaySeconds: 10periodSeconds: 10-      successThreshold: 1timeoutSeconds: 15name: etcdresources:@@ -59,26 +58,18 @@ spec:scheme: HTTPinitialDelaySeconds: 10periodSeconds: 10-      successThreshold: 1timeoutSeconds: 15-    terminationMessagePath: /dev/termination-log-    terminationMessagePolicy: FilevolumeMounts:- mountPath: /var/lib/etcdname: etcd-data- mountPath: /etc/kubernetes/pki/etcdname: etcd-certs-  dnsPolicy: ClusterFirst-  enableServiceLinks: truehostNetwork: truepriority: 2000001000priorityClassName: system-node-critical-  restartPolicy: Always-  schedulerName: default-schedulersecurityContext:seccompProfile:type: RuntimeDefault-  terminationGracePeriodSeconds: 30volumes:- hostPath:path: /etc/kubernetes/pki/etcd",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-patch\" data-lang=\"patch\"><span style=\"display:flex\"><span><span style=\"color:navy;font..."
        },
        {
          "language": "language-patch",
          "code": "diff --git a/etc/kubernetes/manifests/etcd_defaults.yaml b/etc/kubernetes/manifests/etcd_origin.yamlindex d807ccbe0aa..46b35f00e15 100644--- a/etc/kubernetes/manifests/etcd_defaults.yaml+++ b/etc/kubernetes/manifests/etcd_origin.yaml@@ -43,7 +43,6 @@ spec:scheme: HTTPinitialDelaySeconds: 10periodSeconds: 10-      successThreshold: 1timeoutSeconds: 15name: etcdresources:@@ -59,26 +58,18 @@ spec:scheme: HTTPinitialDelaySeconds: 10periodSeconds: 10-      successThreshold: 1timeoutSeconds: 15-    terminationMessagePath: /dev/termination-log-    terminationMessagePolicy: FilevolumeMounts:- mountPath: /var/lib/etcdname: etcd-data- mountPath: /etc/kubernetes/pki/etcdname: etcd-certs-  dnsPolicy: ClusterFirst-  enableServiceLinks: truehostNetwork: truepriority: 2000001000priorityClassName: system-node-critical-  restartPolicy: Always-  schedulerName: default-schedulersecurityContext:seccompProfile:type: RuntimeDefault-  terminationGracePeriodSeconds: 30volumes:- hostPath:path: /etc/kubernetes/pki/etcd",
          "element": "<code class=\"language-patch\" data-lang=\"patch\"><span style=\"display:flex\"><span><span style=\"color:navy;font-weight:700\">diff --git a/etc/kubernetes/manifests/etcd_defaults.yaml b/etc/kubernetes/manif..."
        },
        {
          "language": "",
          "code": "RunContainerError",
          "element": "<code>RunContainerError</code>"
        },
        {
          "language": "",
          "code": "CrashLoopBackOff",
          "element": "<code>CrashLoopBackOff</code>"
        },
        {
          "language": "",
          "code": "CrashLoopBackOff",
          "element": "<code>CrashLoopBackOff</code>"
        },
        {
          "language": "",
          "code": "--component-extra-args",
          "element": "<code>--component-extra-args</code>"
        },
        {
          "language": "",
          "code": "kubeadm upgrade plan",
          "element": "<code>kubeadm upgrade plan</code>"
        },
        {
          "language": "",
          "code": "context deadline exceeded",
          "element": "<code>context deadline exceeded</code>"
        },
        {
          "language": "",
          "code": "kubeadm reset",
          "element": "<code>kubeadm reset</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet",
          "element": "<code>/var/lib/kubelet</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Troubleshooting kubeadm",
          "id": ""
        },
        {
          "level": 2,
          "text": "Not possible to join a v1.18 Node to a v1.17 cluster due to missing RBAC",
          "id": "not-possible-to-join-a-v1-18-node-to-a-v1-17-cluster-due-to-missing-rbac"
        },
        {
          "level": 2,
          "text": "ebtablesor some similar executable not found during installation",
          "id": "ebtables-or-some-similar-executable-not-found-during-installation"
        },
        {
          "level": 2,
          "text": "kubeadm blocks waiting for control plane during installation",
          "id": "kubeadm-blocks-waiting-for-control-plane-during-installation"
        },
        {
          "level": 2,
          "text": "kubeadm blocks when removing managed containers",
          "id": "kubeadm-blocks-when-removing-managed-containers"
        },
        {
          "level": 2,
          "text": "Pods inRunContainerError,CrashLoopBackOfforErrorstate",
          "id": "pods-in-runcontainererror-crashloopbackoff-or-error-state"
        },
        {
          "level": 2,
          "text": "corednsis stuck in thePendingstate",
          "id": "coredns-is-stuck-in-the-pending-state"
        },
        {
          "level": 2,
          "text": "HostPortservices do not work",
          "id": "hostport-services-do-not-work"
        },
        {
          "level": 2,
          "text": "Pods are not accessible via their Service IP",
          "id": "pods-are-not-accessible-via-their-service-ip"
        },
        {
          "level": 2,
          "text": "TLS certificate errors",
          "id": "tls-certificate-errors"
        },
        {
          "level": 2,
          "text": "Kubelet client certificate rotation fails",
          "id": "kubelet-client-cert"
        },
        {
          "level": 2,
          "text": "Default NIC When using flannel as the pod network in Vagrant",
          "id": "default-nic-when-using-flannel-as-the-pod-network-in-vagrant"
        },
        {
          "level": 2,
          "text": "Non-public IP used for containers",
          "id": "non-public-ip-used-for-containers"
        },
        {
          "level": 2,
          "text": "corednspods haveCrashLoopBackOfforErrorstate",
          "id": "coredns-pods-have-crashloopbackoff-or-error-state"
        },
        {
          "level": 2,
          "text": "etcd pods restart continually",
          "id": "etcd-pods-restart-continually"
        },
        {
          "level": 2,
          "text": "Not possible to pass a comma separated list of values to arguments inside a--component-extra-argsflag",
          "id": "not-possible-to-pass-a-comma-separated-list-of-values-to-arguments-inside-a-component-extra-args-flag"
        },
        {
          "level": 2,
          "text": "kube-proxy scheduled before node is initialized by cloud-controller-manager",
          "id": "kube-proxy-scheduled-before-node-is-initialized-by-cloud-controller-manager"
        },
        {
          "level": 2,
          "text": "/usris mounted read-only on nodes",
          "id": "usr-mounted-read-only"
        },
        {
          "level": 2,
          "text": "kubeadm upgrade planprints outcontext deadline exceedederror message",
          "id": "kubeadm-upgrade-plan-prints-out-context-deadline-exceeded-error-message"
        },
        {
          "level": 2,
          "text": "kubeadm resetunmounts/var/lib/kubelet",
          "id": "kubeadm-reset-unmounts-var-lib-kubelet"
        },
        {
          "level": 2,
          "text": "Cannot use the metrics-server securely in a kubeadm cluster",
          "id": "cannot-use-the-metrics-server-securely-in-a-kubeadm-cluster"
        },
        {
          "level": 2,
          "text": "Upgrade fails due to etcd hash not changing",
          "id": "upgrade-fails-due-to-etcd-hash-not-changing"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 4,
          "text": "Warning:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731666.29985
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/",
      "title": "Creating a cluster with kubeadm | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nCreating a cluster with kubeadm\nCreating a cluster with kubeadm\nUsing\nkubeadm\n, you can create a minimum viable Kubernetes cluster that conforms to best practices.\nIn fact, you can use\nkubeadm\nto set up a cluster that will pass the\nKubernetes Conformance tests\n.\nkubeadm\nalso supports other cluster lifecycle functions, such as\nbootstrap tokens\nand cluster upgrades.\nThe\nkubeadm\ntool is good if you need:\nA simple way for you to try out Kubernetes, possibly for the first time.\nA way for existing users to automate setting up a cluster and test their application.\nA building block in other ecosystem and/or installer tools with a larger\nscope.\nYou can install and use\nkubeadm\non various machines: your laptop, a set\nof cloud servers, a Raspberry Pi, and more. Whether you're deploying into the\ncloud or on-premises, you can integrate\nkubeadm\ninto provisioning systems such\nas Ansible or Terraform.\nBefore you begin\nTo follow this guide, you need:\nOne or more machines running a deb/rpm-compatible Linux OS; for example: Ubuntu or CentOS.\n2 GiB or more of RAM per machine--any less leaves little room for your apps.\nAt least 2 CPUs on the machine that you use as a control-plane node.\nFull network connectivity among all machines in the cluster. You can use either a\npublic or a private network.\nYou also need to use a version of\nkubeadm\nthat can deploy the version\nof Kubernetes that you want to use in your new cluster.\nKubernetes' version and version skew support policy\napplies to\nkubeadm\nas well as to Kubernetes overall.\nCheck that policy to learn about what versions of Kubernetes and\nkubeadm\nare supported. This page is written for Kubernetes v1.33.\nThe\nkubeadm\ntool's overall feature state is General Availability (GA). Some sub-features are\nstill under active development. The implementation of creating the cluster may change\nslightly as the tool evolves, but the overall implementation should be pretty stable.\nNote:\nAny commands under\nkubeadm alpha\nare, by definition, supported on an alpha level.\nObjectives\nInstall a single control-plane Kubernetes cluster\nInstall a Pod network on the cluster so that your Pods can\ntalk to each other\nInstructions\nPreparing the hosts\nComponent installation\nInstall a\ncontainer runtime\nand kubeadm on all the hosts. For detailed instructions and other prerequisites, see\nInstalling kubeadm\n.\nNote:\nIf you have already installed kubeadm, see the first two steps of the\nUpgrading Linux nodes\ndocument for instructions on how to upgrade kubeadm.\nWhen you upgrade, the kubelet restarts every few seconds as it waits in a crashloop for\nkubeadm to tell it what to do. This crashloop is expected and normal.\nAfter you initialize your control-plane, the kubelet runs normally.\nNetwork setup\nkubeadm similarly to other Kubernetes components tries to find a usable IP on\nthe network interfaces associated with a default gateway on a host. Such\nan IP is then used for the advertising and/or listening performed by a component.\nTo find out what this IP is on a Linux host you can use:\nip route show\n# Look for a line starting with \"default via\"\nNote:\nIf two or more default gateways are present on the host, a Kubernetes component will\ntry to use the first one it encounters that has a suitable global unicast IP address.\nWhile making this choice, the exact ordering of gateways might vary between different\noperating systems and kernel versions.\nKubernetes components do not accept custom network interface as an option,\ntherefore a custom IP address must be passed as a flag to all components instances\nthat need such a custom configuration.\nNote:\nIf the host does not have a default gateway and if a custom IP address is not passed\nto a Kubernetes component, the component may exit with an error.\nTo configure the API server advertise address for control plane nodes created with both\ninit\nand\njoin\n, the flag\n--apiserver-advertise-address\ncan be used.\nPreferably, this option can be set in the\nkubeadm API\nas\nInitConfiguration.localAPIEndpoint\nand\nJoinConfiguration.controlPlane.localAPIEndpoint\n.\nFor kubelets on all nodes, the\n--node-ip\noption can be passed in\n.nodeRegistration.kubeletExtraArgs\ninside a kubeadm configuration file\n(\nInitConfiguration\nor\nJoinConfiguration\n).\nFor dual-stack see\nDual-stack support with kubeadm\n.\nThe IP addresses that you assign to control plane components become part of their X.509 certificates'\nsubject alternative name fields. Changing these IP addresses would require\nsigning new certificates and restarting the affected components, so that the change in\ncertificate files is reflected. See\nManual certificate renewal\nfor more details on this topic.\nWarning:\nThe Kubernetes project recommends against this approach (configuring all component instances\nwith custom IP addresses). Instead, the Kubernetes maintainers recommend to setup the host network,\nso that the default gateway IP is the one that Kubernetes components auto-detect and use.\nOn Linux nodes, you can use commands such as\nip route\nto configure networking; your operating\nsystem might also provide higher level network management tools. If your node's default gateway\nis a public IP address, you should configure packet filtering or other security measures that\nprotect the nodes and your cluster.\nPreparing the required container images\nThis step is optional and only applies in case you wish\nkubeadm init\nand\nkubeadm join\nto not download the default container images which are hosted at\nregistry.k8s.io\n.\nKubeadm has commands that can help you pre-pull the required images\nwhen creating a cluster without an internet connection on its nodes.\nSee\nRunning kubeadm without an internet connection\nfor more details.\nKubeadm allows you to use a custom image repository for the required images.\nSee\nUsing custom images\nfor more details.\nInitializing your control-plane node\nThe control-plane node is the machine where the control plane components run, including\netcd\n(the cluster database) and the\nAPI Server\n(which the\nkubectl\ncommand line tool\ncommunicates with).\n(Recommended) If you have plans to upgrade this single control-plane\nkubeadm\ncluster\nto\nhigh availability\nyou should specify the\n--control-plane-endpoint\nto set the shared endpoint for all control-plane nodes.\nSuch an endpoint can be either a DNS name or an IP address of a load-balancer.\nChoose a Pod network add-on, and verify whether it requires any arguments to\nbe passed to\nkubeadm init\n. Depending on which\nthird-party provider you choose, you might need to set the\n--pod-network-cidr\nto\na provider-specific value. See\nInstalling a Pod network add-on\n.\n(Optional)\nkubeadm\ntries to detect the container runtime by using a list of well\nknown endpoints. To use different container runtime or if there are more than one installed\non the provisioned node, specify the\n--cri-socket\nargument to\nkubeadm\n. See\nInstalling a runtime\n.\nTo initialize the control-plane node run:\nkubeadm init <args>\nConsiderations about apiserver-advertise-address and ControlPlaneEndpoint\nWhile\n--apiserver-advertise-address\ncan be used to set the advertised address for this particular\ncontrol-plane node's API server,\n--control-plane-endpoint\ncan be used to set the shared endpoint\nfor all control-plane nodes.\n--control-plane-endpoint\nallows both IP addresses and DNS names that can map to IP addresses.\nPlease contact your network administrator to evaluate possible solutions with respect to such mapping.\nHere is an example mapping:\n192.168.0.102 cluster-endpoint\nWhere\n192.168.0.102\nis the IP address of this node and\ncluster-endpoint\nis a custom DNS name that maps to this IP.\nThis will allow you to pass\n--control-plane-endpoint=cluster-endpoint\nto\nkubeadm init\nand pass the same DNS name to\nkubeadm join\n. Later you can modify\ncluster-endpoint\nto point to the address of your load-balancer in a\nhigh availability scenario.\nTurning a single control plane cluster created without\n--control-plane-endpoint\ninto a highly available cluster\nis not supported by kubeadm.\nMore information\nFor more information about\nkubeadm init\narguments, see the\nkubeadm reference guide\n.\nTo configure\nkubeadm init\nwith a configuration file see\nUsing kubeadm init with a configuration file\n.\nTo customize control plane components, including optional IPv6 assignment to liveness probe\nfor control plane components and etcd server, provide extra arguments to each component as documented in\ncustom arguments\n.\nTo reconfigure a cluster that has already been created see\nReconfiguring a kubeadm cluster\n.\nTo run\nkubeadm init\nagain, you must first\ntear down the cluster\n.\nIf you join a node with a different architecture to your cluster, make sure that your deployed DaemonSets\nhave container image support for this architecture.\nkubeadm init\nfirst runs a series of prechecks to ensure that the machine\nis ready to run Kubernetes. These prechecks expose warnings and exit on errors.\nkubeadm init\nthen downloads and installs the cluster control plane components. This may take several minutes.\nAfter it finishes you should see:\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a Pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  /docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>\nTo make kubectl work for your non-root user, run these commands, which are\nalso part of the\nkubeadm init\noutput:\nmkdir -p\n$HOME\n/.kube\nsudo cp -i /etc/kubernetes/admin.conf\n$HOME\n/.kube/config\nsudo chown\n$(\nid -u\n)\n:\n$(\nid -g\n)\n$HOME\n/.kube/config\nAlternatively, if you are the\nroot\nuser, you can run:\nexport\nKUBECONFIG\n=\n/etc/kubernetes/admin.conf\nWarning:\nThe kubeconfig file\nadmin.conf\nthat\nkubeadm init\ngenerates contains a certificate with\nSubject: O = kubeadm:cluster-admins, CN = kubernetes-admin\n. The group\nkubeadm:cluster-admins\nis bound to the built-in\ncluster-admin\nClusterRole.\nDo not share the\nadmin.conf\nfile with anyone.\nkubeadm init\ngenerates another kubeconfig file\nsuper-admin.conf\nthat contains a certificate with\nSubject: O = system:masters, CN = kubernetes-super-admin\n.\nsystem:masters\nis a break-glass, super user group that bypasses the authorization layer (for example RBAC).\nDo not share the\nsuper-admin.conf\nfile with anyone. It is recommended to move the file to a safe location.\nSee\nGenerating kubeconfig files for additional users\non how to use\nkubeadm kubeconfig user\nto generate kubeconfig files for additional users.\nMake a record of the\nkubeadm join\ncommand that\nkubeadm init\noutputs. You\nneed this command to\njoin nodes to your cluster\n.\nThe token is used for mutual authentication between the control-plane node and the joining\nnodes. The token included here is secret. Keep it safe, because anyone with this\ntoken can add authenticated nodes to your cluster. These tokens can be listed,\ncreated, and deleted with the\nkubeadm token\ncommand. See the\nkubeadm reference guide\n.\nInstalling a Pod network add-on\nCaution:\nThis section contains important information about networking setup and\ndeployment order.\nRead all of this advice carefully before proceeding.\nYou must deploy a\nContainer Network Interface\n(CNI) based Pod network add-on so that your Pods can communicate with each other.\nCluster DNS (CoreDNS) will not start up before a network is installed.\nTake care that your Pod network must not overlap with any of the host\nnetworks: you are likely to see problems if there is any overlap.\n(If you find a collision between your network plugin's preferred Pod\nnetwork and some of your host networks, you should think of a suitable\nCIDR block to use instead, then use that during\nkubeadm init\nwith\n--pod-network-cidr\nand as a replacement in your network plugin's YAML).\nBy default,\nkubeadm\nsets up your cluster to use and enforce use of\nRBAC\n(role based access\ncontrol).\nMake sure that your Pod network plugin supports RBAC, and so do any manifests\nthat you use to deploy it.\nIf you want to use IPv6--either dual-stack, or single-stack IPv6 only\nnetworking--for your cluster, make sure that your Pod network plugin\nsupports IPv6.\nIPv6 support was added to CNI in\nv0.6.0\n.\nNote:\nKubeadm should be CNI agnostic and the validation of CNI providers is out of the scope of our current e2e testing.\nIf you find an issue related to a CNI plugin you should log a ticket in its respective issue\ntracker instead of the kubeadm or kubernetes issue trackers.\nSeveral external projects provide Kubernetes Pod networks using CNI, some of which also\nsupport\nNetwork Policy\n.\nSee a list of add-ons that implement the\nKubernetes networking model\n.\nPlease refer to the\nInstalling Addons\npage for a non-exhaustive list of networking addons supported by Kubernetes.\nYou can install a Pod network add-on with the following command on the\ncontrol-plane node or a node that has the kubeconfig credentials:\nkubectl apply -f <add-on.yaml>\nNote:\nOnly a few CNI plugins support Windows. More details and setup instructions can be found\nin\nAdding Windows worker nodes\n.\nYou can install only one Pod network per cluster.\nOnce a Pod network has been installed, you can confirm that it is working by\nchecking that the CoreDNS Pod is\nRunning\nin the output of\nkubectl get pods --all-namespaces\n.\nAnd once the CoreDNS Pod is up and running, you can continue by joining your nodes.\nIf your network is not working or CoreDNS is not in the\nRunning\nstate, check out the\ntroubleshooting guide\nfor\nkubeadm\n.\nManaged node labels\nBy default, kubeadm enables the\nNodeRestriction\nadmission controller that restricts what labels can be self-applied by kubelets on node registration.\nThe admission controller documentation covers what labels are permitted to be used with the kubelet\n--node-labels\noption.\nThe\nnode-role.kubernetes.io/control-plane\nlabel is such a restricted label and kubeadm manually applies it using\na privileged client after a node has been created. To do that manually you can do the same by using\nkubectl label\nand ensure it is using a privileged kubeconfig such as the kubeadm managed\n/etc/kubernetes/admin.conf\n.\nControl plane node isolation\nBy default, your cluster will not schedule Pods on the control plane nodes for security\nreasons. If you want to be able to schedule Pods on the control plane nodes,\nfor example for a single machine Kubernetes cluster, run:\nkubectl taint nodes --all node-role.kubernetes.io/control-plane-\nThe output will look something like:\nnode \"test-01\" untainted\n...\nThis will remove the\nnode-role.kubernetes.io/control-plane:NoSchedule\ntaint\nfrom any nodes that have it, including the control plane nodes, meaning that the\nscheduler will then be able to schedule Pods everywhere.\nAdditionally, you can execute the following command to remove the\nnode.kubernetes.io/exclude-from-external-load-balancers\nlabel\nfrom the control plane node, which excludes it from the list of backend servers:\nkubectl label nodes --all node.kubernetes.io/exclude-from-external-load-balancers-\nAdding more control plane nodes\nSee\nCreating Highly Available Clusters with kubeadm\nfor steps on creating a high availability kubeadm cluster by adding more control plane nodes.\nAdding worker nodes\nThe worker nodes are where your workloads run.\nThe following pages show how to add Linux and Windows worker nodes to the cluster by using\nthe\nkubeadm join\ncommand:\nAdding Linux worker nodes\nAdding Windows worker nodes\n(Optional) Controlling your cluster from machines other than the control-plane node\nIn order to get a kubectl on some other computer (e.g. laptop) to talk to your\ncluster, you need to copy the administrator kubeconfig file from your control-plane node\nto your workstation like this:\nscp root@<control-plane-host>:/etc/kubernetes/admin.conf .\nkubectl --kubeconfig ./admin.conf get nodes\nNote:\nThe example above assumes SSH access is enabled for root. If that is not the\ncase, you can copy the\nadmin.conf\nfile to be accessible by some other user\nand\nscp\nusing that other user instead.\nThe\nadmin.conf\nfile gives the user\nsuperuser\nprivileges over the cluster.\nThis file should be used sparingly. For normal users, it's recommended to\ngenerate an unique credential to which you grant privileges. You can do\nthis with the\nkubeadm kubeconfig user --client-name <CN>\ncommand. That command will print out a KubeConfig file to STDOUT which you\nshould save to a file and distribute to your user. After that, grant\nprivileges by using\nkubectl create (cluster)rolebinding\n.\n(Optional) Proxying API Server to localhost\nIf you want to connect to the API Server from outside the cluster, you can use\nkubectl proxy\n:\nscp root@<control-plane-host>:/etc/kubernetes/admin.conf .\nkubectl --kubeconfig ./admin.conf proxy\nYou can now access the API Server locally at\nhttp://localhost:8001/api/v1\nClean up\nIf you used disposable servers for your cluster, for testing, you can\nswitch those off and do no further clean up. You can use\nkubectl config delete-cluster\nto delete your local references to the\ncluster.\nHowever, if you want to deprovision your cluster more cleanly, you should\nfirst\ndrain the node\nand make sure that the node is empty, then deconfigure the node.\nRemove the node\nTalking to the control-plane node with the appropriate credentials, run:\nkubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets\nBefore removing the node, reset the state installed by\nkubeadm\n:\nkubeadm reset\nThe reset process does not reset or clean up iptables rules or IPVS tables.\nIf you wish to reset iptables, you must do so manually:\niptables -F\n&&\niptables -t nat -F\n&&\niptables -t mangle -F\n&&\niptables -X\nIf you want to reset the IPVS tables, you must run the following command:\nipvsadm -C\nNow remove the node:\nkubectl delete node <node name>\nIf you wish to start over, run\nkubeadm init\nor\nkubeadm join\nwith the\nappropriate arguments.\nClean up the control plane\nYou can use\nkubeadm reset\non the control plane host to trigger a best-effort\nclean up.\nSee the\nkubeadm reset\nreference documentation for more information about this subcommand and its\noptions.\nVersion skew policy\nWhile kubeadm allows version skew against some components that it manages, it is recommended that you\nmatch the kubeadm version with the versions of the control plane components, kube-proxy and kubelet.\nkubeadm's skew against the Kubernetes version\nkubeadm can be used with Kubernetes components that are the same version as kubeadm\nor one version older. The Kubernetes version can be specified to kubeadm by using the\n--kubernetes-version\nflag of\nkubeadm init\nor the\nClusterConfiguration.kubernetesVersion\nfield when using\n--config\n. This option will control the versions\nof kube-apiserver, kube-controller-manager, kube-scheduler and kube-proxy.\nExample:\nkubeadm is at 1.33\nkubernetesVersion\nmust be at 1.33 or 1.32\nkubeadm's skew against the kubelet\nSimilarly to the Kubernetes version, kubeadm can be used with a kubelet version that is\nthe same version as kubeadm or three versions older.\nExample:\nkubeadm is at 1.33\nkubelet on the host must be at 1.33, 1.32,\n1.31 or 1.30\nkubeadm's skew against kubeadm\nThere are certain limitations on how kubeadm commands can operate on existing nodes or whole clusters\nmanaged by kubeadm.\nIf new nodes are joined to the cluster, the kubeadm binary used for\nkubeadm join\nmust match\nthe last version of kubeadm used to either create the cluster with\nkubeadm init\nor to upgrade\nthe same node with\nkubeadm upgrade\n. Similar rules apply to the rest of the kubeadm commands\nwith the exception of\nkubeadm upgrade\n.\nExample for\nkubeadm join\n:\nkubeadm version 1.33 was used to create a cluster with\nkubeadm init\nJoining nodes must use a kubeadm binary that is at version 1.33\nNodes that are being upgraded must use a version of kubeadm that is the same MINOR\nversion or one MINOR version newer than the version of kubeadm used for managing the\nnode.\nExample for\nkubeadm upgrade\n:\nkubeadm version 1.32 was used to create or upgrade the node\nThe version of kubeadm used for upgrading the node must be at 1.32\nor 1.33\nTo learn more about the version skew between the different Kubernetes component see\nthe\nVersion Skew Policy\n.\nLimitations\nCluster resilience\nThe cluster created here has a single control-plane node, with a single etcd database\nrunning on it. This means that if the control-plane node fails, your cluster may lose\ndata and may need to be recreated from scratch.\nWorkarounds:\nRegularly\nback up etcd\n. The\netcd data directory configured by kubeadm is at\n/var/lib/etcd\non the control-plane node.\nUse multiple control-plane nodes. You can read\nOptions for Highly Available topology\nto pick a cluster\ntopology that provides\nhigh-availability\n.\nPlatform compatibility\nkubeadm deb/rpm packages and binaries are built for amd64, arm (32-bit), arm64, ppc64le, and s390x\nfollowing the\nmulti-platform proposal\n.\nMultiplatform container images for the control plane and addons are also supported since v1.12.\nOnly some of the network providers offer solutions for all platforms. Please consult the list of\nnetwork providers above or the documentation from each provider to figure out whether the provider\nsupports your chosen platform.\nTroubleshooting\nIf you are running into difficulties with kubeadm, please consult our\ntroubleshooting docs\n.\nWhat's next\nVerify that your cluster is running properly with\nSonobuoy\nSee\nUpgrading kubeadm clusters\nfor details about upgrading your cluster using\nkubeadm\n.\nLearn about advanced\nkubeadm\nusage in the\nkubeadm reference documentation\nLearn more about Kubernetes\nconcepts\nand\nkubectl\n.\nSee the\nCluster Networking\npage for a bigger list\nof Pod network add-ons.\nSee the\nlist of add-ons\nto\nexplore other add-ons, including tools for logging, monitoring, network policy, visualization &\ncontrol of your Kubernetes cluster.\nConfigure how your cluster handles logs for cluster events and from\napplications running in Pods.\nSee\nLogging Architecture\nfor\nan overview of what is involved.\nFeedback\nFor bugs, visit the\nkubeadm GitHub issue tracker\nFor support, visit the\n#kubeadm\nSlack channel\nGeneral SIG Cluster Lifecycle development Slack channel:\n#sig-cluster-lifecycle\nSIG Cluster Lifecycle\nSIG information\nSIG Cluster Lifecycle mailing list:\nkubernetes-sig-cluster-lifecycle\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 16, 2024 at 9:28 AM PST:\nTweak and clean up four kubeadm files (67c5917e32)",
      "code_examples": [
        {
          "language": "",
          "code": "kubeadm alpha",
          "element": "<code>kubeadm alpha</code>"
        },
        {
          "language": "",
          "code": "ip route show# Look for a line starting with \"default via\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>ip route show <span style=\"c..."
        },
        {
          "language": "language-shell",
          "code": "ip route show# Look for a line starting with \"default via\"",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>ip route show <span style=\"color:#080;font-style:italic\"># Look for a line starting with \"default via\"</span>\n</span></s..."
        },
        {
          "language": "",
          "code": "--apiserver-advertise-address",
          "element": "<code>--apiserver-advertise-address</code>"
        },
        {
          "language": "",
          "code": "InitConfiguration.localAPIEndpoint",
          "element": "<code>InitConfiguration.localAPIEndpoint</code>"
        },
        {
          "language": "",
          "code": "JoinConfiguration.controlPlane.localAPIEndpoint",
          "element": "<code>JoinConfiguration.controlPlane.localAPIEndpoint</code>"
        },
        {
          "language": "",
          "code": ".nodeRegistration.kubeletExtraArgs",
          "element": "<code>.nodeRegistration.kubeletExtraArgs</code>"
        },
        {
          "language": "",
          "code": "InitConfiguration",
          "element": "<code>InitConfiguration</code>"
        },
        {
          "language": "",
          "code": "JoinConfiguration",
          "element": "<code>JoinConfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "registry.k8s.io",
          "element": "<code>registry.k8s.io</code>"
        },
        {
          "language": "",
          "code": "--control-plane-endpoint",
          "element": "<code>--control-plane-endpoint</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "--pod-network-cidr",
          "element": "<code>--pod-network-cidr</code>"
        },
        {
          "language": "",
          "code": "--cri-socket",
          "element": "<code>--cri-socket</code>"
        },
        {
          "language": "",
          "code": "kubeadm init <args>",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubeadm init &lt;args&gt;\n</sp..."
        },
        {
          "language": "language-bash",
          "code": "kubeadm init <args>",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubeadm init &lt;args&gt;\n</span></span></code>"
        },
        {
          "language": "",
          "code": "--apiserver-advertise-address",
          "element": "<code>--apiserver-advertise-address</code>"
        },
        {
          "language": "",
          "code": "--control-plane-endpoint",
          "element": "<code>--control-plane-endpoint</code>"
        },
        {
          "language": "",
          "code": "--control-plane-endpoint",
          "element": "<code>--control-plane-endpoint</code>"
        },
        {
          "language": "",
          "code": "192.168.0.102 cluster-endpoint",
          "element": "<pre tabindex=\"0\"><code>192.168.0.102 cluster-endpoint\n</code></pre>"
        },
        {
          "language": "",
          "code": "192.168.0.102 cluster-endpoint",
          "element": "<code>192.168.0.102 cluster-endpoint\n</code>"
        },
        {
          "language": "",
          "code": "192.168.0.102",
          "element": "<code>192.168.0.102</code>"
        },
        {
          "language": "",
          "code": "cluster-endpoint",
          "element": "<code>cluster-endpoint</code>"
        },
        {
          "language": "",
          "code": "--control-plane-endpoint=cluster-endpoint",
          "element": "<code>--control-plane-endpoint=cluster-endpoint</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "cluster-endpoint",
          "element": "<code>cluster-endpoint</code>"
        },
        {
          "language": "",
          "code": "--control-plane-endpoint",
          "element": "<code>--control-plane-endpoint</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a Pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  /docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>",
          "element": "<pre tabindex=\"0\"><code class=\"language-none\" data-lang=\"none\">Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user..."
        },
        {
          "language": "language-none",
          "code": "Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a Pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  /docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>",
          "element": "<code class=\"language-none\" data-lang=\"none\">Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOM..."
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "mkdir -p$HOME/.kubesudo cp -i /etc/kubernetes/admin.conf$HOME/.kube/configsudo chown$(id -u):$(id -g)$HOME/.kube/config",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>mkdir -p <span style=\"color:#b..."
        },
        {
          "language": "language-bash",
          "code": "mkdir -p$HOME/.kubesudo cp -i /etc/kubernetes/admin.conf$HOME/.kube/configsudo chown$(id -u):$(id -g)$HOME/.kube/config",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>mkdir -p <span style=\"color:#b8860b\">$HOME</span>/.kube\n</span></span><span style=\"display:flex\"><span>sudo cp -i /etc/kub..."
        },
        {
          "language": "",
          "code": "exportKUBECONFIG=/etc/kubernetes/admin.conf",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#a2f\">expor..."
        },
        {
          "language": "language-bash",
          "code": "exportKUBECONFIG=/etc/kubernetes/admin.conf",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#a2f\">export</span> <span style=\"color:#b8860b\">KUBECONFIG</span><span style=\"color:#666\">=</span>/etc/..."
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin",
          "element": "<code>Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin</code>"
        },
        {
          "language": "",
          "code": "kubeadm:cluster-admins",
          "element": "<code>kubeadm:cluster-admins</code>"
        },
        {
          "language": "",
          "code": "cluster-admin",
          "element": "<code>cluster-admin</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "super-admin.conf",
          "element": "<code>super-admin.conf</code>"
        },
        {
          "language": "",
          "code": "Subject: O = system:masters, CN = kubernetes-super-admin",
          "element": "<code>Subject: O = system:masters, CN = kubernetes-super-admin</code>"
        },
        {
          "language": "",
          "code": "system:masters",
          "element": "<code>system:masters</code>"
        },
        {
          "language": "",
          "code": "super-admin.conf",
          "element": "<code>super-admin.conf</code>"
        },
        {
          "language": "",
          "code": "kubeadm kubeconfig user",
          "element": "<code>kubeadm kubeconfig user</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm token",
          "element": "<code>kubeadm token</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "--pod-network-cidr",
          "element": "<code>--pod-network-cidr</code>"
        },
        {
          "language": "",
          "code": "kubectl apply -f <add-on.yaml>",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl apply -f &lt;add-on.ya..."
        },
        {
          "language": "language-bash",
          "code": "kubectl apply -f <add-on.yaml>",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl apply -f &lt;add-on.yaml&gt;\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubectl get pods --all-namespaces",
          "element": "<code>kubectl get pods --all-namespaces</code>"
        },
        {
          "language": "",
          "code": "--node-labels",
          "element": "<code>--node-labels</code>"
        },
        {
          "language": "",
          "code": "node-role.kubernetes.io/control-plane",
          "element": "<code>node-role.kubernetes.io/control-plane</code>"
        },
        {
          "language": "",
          "code": "kubectl label",
          "element": "<code>kubectl label</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/admin.conf",
          "element": "<code>/etc/kubernetes/admin.conf</code>"
        },
        {
          "language": "",
          "code": "kubectl taint nodes --all node-role.kubernetes.io/control-plane-",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl taint nodes --all node..."
        },
        {
          "language": "language-bash",
          "code": "kubectl taint nodes --all node-role.kubernetes.io/control-plane-",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl taint nodes --all node-role.kubernetes.io/control-plane-\n</span></span></code>"
        },
        {
          "language": "",
          "code": "node \"test-01\" untainted\n...",
          "element": "<pre tabindex=\"0\"><code>node \"test-01\" untainted\n...\n</code></pre>"
        },
        {
          "language": "",
          "code": "node \"test-01\" untainted\n...",
          "element": "<code>node \"test-01\" untainted\n...\n</code>"
        },
        {
          "language": "",
          "code": "node-role.kubernetes.io/control-plane:NoSchedule",
          "element": "<code>node-role.kubernetes.io/control-plane:NoSchedule</code>"
        },
        {
          "language": "",
          "code": "node.kubernetes.io/exclude-from-external-load-balancers",
          "element": "<code>node.kubernetes.io/exclude-from-external-load-balancers</code>"
        },
        {
          "language": "",
          "code": "kubectl label nodes --all node.kubernetes.io/exclude-from-external-load-balancers-",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl label nodes --all node..."
        },
        {
          "language": "language-bash",
          "code": "kubectl label nodes --all node.kubernetes.io/exclude-from-external-load-balancers-",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl label nodes --all node.kubernetes.io/exclude-from-external-load-balancers-\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "scp root@<control-plane-host>:/etc/kubernetes/admin.conf .kubectl --kubeconfig ./admin.conf get nodes",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>scp root@&lt;control-plane-hos..."
        },
        {
          "language": "language-bash",
          "code": "scp root@<control-plane-host>:/etc/kubernetes/admin.conf .kubectl --kubeconfig ./admin.conf get nodes",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .\n</span></span><span style=\"display:flex\"><span>kubectl --..."
        },
        {
          "language": "",
          "code": "kubeadm kubeconfig user --client-name <CN>",
          "element": "<code>kubeadm kubeconfig user --client-name &lt;CN&gt;</code>"
        },
        {
          "language": "",
          "code": "kubectl create (cluster)rolebinding",
          "element": "<code>kubectl create (cluster)rolebinding</code>"
        },
        {
          "language": "",
          "code": "kubectl proxy",
          "element": "<code>kubectl proxy</code>"
        },
        {
          "language": "",
          "code": "scp root@<control-plane-host>:/etc/kubernetes/admin.conf .kubectl --kubeconfig ./admin.conf proxy",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>scp root@&lt;control-plane-hos..."
        },
        {
          "language": "language-bash",
          "code": "scp root@<control-plane-host>:/etc/kubernetes/admin.conf .kubectl --kubeconfig ./admin.conf proxy",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .\n</span></span><span style=\"display:flex\"><span>kubectl --..."
        },
        {
          "language": "",
          "code": "http://localhost:8001/api/v1",
          "element": "<code>http://localhost:8001/api/v1</code>"
        },
        {
          "language": "",
          "code": "kubectl config delete-cluster",
          "element": "<code>kubectl config delete-cluster</code>"
        },
        {
          "language": "",
          "code": "kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl drain &lt;node name&gt..."
        },
        {
          "language": "language-bash",
          "code": "kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl drain &lt;node name&gt; --delete-emptydir-data --force --ignore-daemonsets\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubeadm reset",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubeadm reset\n</span></span></..."
        },
        {
          "language": "language-bash",
          "code": "kubeadm reset",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubeadm reset\n</span></span></code>"
        },
        {
          "language": "",
          "code": "iptables -F&&iptables -t nat -F&&iptables -t mangle -F&&iptables -X",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>iptables -F <span style=\"color..."
        },
        {
          "language": "language-bash",
          "code": "iptables -F&&iptables -t nat -F&&iptables -t mangle -F&&iptables -X",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>iptables -F <span style=\"color:#666\">&amp;&amp;</span> iptables -t nat -F <span style=\"color:#666\">&amp;&amp;</span> iptab..."
        },
        {
          "language": "",
          "code": "kubectl delete node <node name>",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl delete node &lt;node n..."
        },
        {
          "language": "language-bash",
          "code": "kubectl delete node <node name>",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubectl delete node &lt;node name&gt;\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "kubeadm reset",
          "element": "<code>kubeadm reset</code>"
        },
        {
          "language": "",
          "code": "kubeadm reset",
          "element": "<code>kubeadm reset</code>"
        },
        {
          "language": "",
          "code": "--kubernetes-version",
          "element": "<code>--kubernetes-version</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "ClusterConfiguration.kubernetesVersion",
          "element": "<code>ClusterConfiguration.kubernetesVersion</code>"
        },
        {
          "language": "",
          "code": "kubernetesVersion",
          "element": "<code>kubernetesVersion</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm upgrade",
          "element": "<code>kubeadm upgrade</code>"
        },
        {
          "language": "",
          "code": "kubeadm upgrade",
          "element": "<code>kubeadm upgrade</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm upgrade",
          "element": "<code>kubeadm upgrade</code>"
        },
        {
          "language": "",
          "code": "/var/lib/etcd",
          "element": "<code>/var/lib/etcd</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Creating a cluster with kubeadm",
          "id": ""
        },
        {
          "level": 2,
          "text": "Before you begin",
          "id": "before-you-begin"
        },
        {
          "level": 2,
          "text": "Objectives",
          "id": "objectives"
        },
        {
          "level": 2,
          "text": "Instructions",
          "id": "instructions"
        },
        {
          "level": 2,
          "text": "Clean up",
          "id": "tear-down"
        },
        {
          "level": 2,
          "text": "Version skew policy",
          "id": "version-skew-policy"
        },
        {
          "level": 2,
          "text": "Limitations",
          "id": "limitations"
        },
        {
          "level": 2,
          "text": "Troubleshooting",
          "id": "troubleshooting"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Preparing the hosts",
          "id": "preparing-the-hosts"
        },
        {
          "level": 3,
          "text": "Preparing the required container images",
          "id": "preparing-the-required-container-images"
        },
        {
          "level": 3,
          "text": "Initializing your control-plane node",
          "id": "initializing-your-control-plane-node"
        },
        {
          "level": 3,
          "text": "Considerations about apiserver-advertise-address and ControlPlaneEndpoint",
          "id": "considerations-about-apiserver-advertise-address-and-controlplaneendpoint"
        },
        {
          "level": 3,
          "text": "More information",
          "id": "more-information"
        },
        {
          "level": 3,
          "text": "Installing a Pod network add-on",
          "id": "pod-network"
        },
        {
          "level": 3,
          "text": "Managed node labels",
          "id": "managed-node-labels"
        },
        {
          "level": 3,
          "text": "Control plane node isolation",
          "id": "control-plane-node-isolation"
        },
        {
          "level": 3,
          "text": "Adding more control plane nodes",
          "id": "adding-more-control-plane-nodes"
        },
        {
          "level": 3,
          "text": "Adding worker nodes",
          "id": "join-nodes"
        },
        {
          "level": 3,
          "text": "(Optional) Controlling your cluster from machines other than the control-plane node",
          "id": "optional-controlling-your-cluster-from-machines-other-than-the-control-plane-node"
        },
        {
          "level": 3,
          "text": "(Optional) Proxying API Server to localhost",
          "id": "optional-proxying-api-server-to-localhost"
        },
        {
          "level": 3,
          "text": "Remove the node",
          "id": "remove-the-node"
        },
        {
          "level": 3,
          "text": "Clean up the control plane",
          "id": "clean-up-the-control-plane"
        },
        {
          "level": 3,
          "text": "kubeadm's skew against the Kubernetes version",
          "id": "kubeadm-s-skew-against-the-kubernetes-version"
        },
        {
          "level": 3,
          "text": "kubeadm's skew against the kubelet",
          "id": "kubeadm-s-skew-against-the-kubelet"
        },
        {
          "level": 3,
          "text": "kubeadm's skew against kubeadm",
          "id": "kubeadm-s-skew-against-kubeadm"
        },
        {
          "level": 3,
          "text": "Cluster resilience",
          "id": "resilience"
        },
        {
          "level": 3,
          "text": "Platform compatibility",
          "id": "multi-platform"
        },
        {
          "level": 3,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Component installation",
          "id": "component-installation"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Network setup",
          "id": "network-setup"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Warning:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Warning:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Caution:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731669.758683
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/",
      "title": "Customizing components with the kubeadm API | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nCustomizing components with the kubeadm API\nCustomizing components with the kubeadm API\nThis page covers how to customize the components that kubeadm deploys. For control plane components\nyou can use flags in the\nClusterConfiguration\nstructure or patches per-node. For the kubelet\nand kube-proxy you can use\nKubeletConfiguration\nand\nKubeProxyConfiguration\n, accordingly.\nAll of these options are possible via the kubeadm configuration API.\nFor more details on each field in the configuration you can navigate to our\nAPI reference pages\n.\nNote:\nCustomizing the CoreDNS deployment of kubeadm is currently not supported. You must manually\npatch the\nkube-system/coredns\nConfigMap\nand recreate the CoreDNS\nPods\nafter that. Alternatively,\nyou can skip the default CoreDNS deployment and deploy your own variant.\nFor more details on that see\nUsing init phases with kubeadm\n.\nNote:\nTo reconfigure a cluster that has already been created see\nReconfiguring a kubeadm cluster\n.\nCustomizing the control plane with flags in\nClusterConfiguration\nThe kubeadm\nClusterConfiguration\nobject exposes a way for users to override the default\nflags passed to control plane components such as the APIServer, ControllerManager, Scheduler and Etcd.\nThe components are defined using the following structures:\napiServer\ncontrollerManager\nscheduler\netcd\nThese structures contain a common\nextraArgs\nfield, that consists of\nname\n/\nvalue\npairs.\nTo override a flag for a control plane component:\nAdd the appropriate\nextraArgs\nto your configuration.\nAdd flags to the\nextraArgs\nfield.\nRun\nkubeadm init\nwith\n--config <YOUR CONFIG YAML>\n.\nNote:\nYou can generate a\nClusterConfiguration\nobject with default values by running\nkubeadm config print init-defaults\nand saving the output to a file of your choice.\nNote:\nThe\nClusterConfiguration\nobject is currently global in kubeadm clusters. This means that any flags that you add,\nwill apply to all instances of the same component on different nodes. To apply individual configuration per component\non different nodes you can use\npatches\n.\nNote:\nDuplicate flags (keys), or passing the same flag\n--foo\nmultiple times, is currently not supported.\nTo workaround that you must use\npatches\n.\nAPIServer flags\nFor details, see the\nreference documentation for kube-apiserver\n.\nExample usage:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nkubernetesVersion\n:\nv1.16.0\napiServer\n:\nextraArgs\n:\n-\nname\n:\n\"enable-admission-plugins\"\nvalue\n:\n\"AlwaysPullImages,DefaultStorageClass\"\n-\nname\n:\n\"audit-log-path\"\nvalue\n:\n\"/home/johndoe/audit.log\"\nControllerManager flags\nFor details, see the\nreference documentation for kube-controller-manager\n.\nExample usage:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nkubernetesVersion\n:\nv1.16.0\ncontrollerManager\n:\nextraArgs\n:\n-\nname\n:\n\"cluster-signing-key-file\"\nvalue\n:\n\"/home/johndoe/keys/ca.key\"\n-\nname\n:\n\"deployment-controller-sync-period\"\nvalue\n:\n\"50\"\nScheduler flags\nFor details, see the\nreference documentation for kube-scheduler\n.\nExample usage:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nkubernetesVersion\n:\nv1.16.0\nscheduler\n:\nextraArgs\n:\n-\nname\n:\n\"config\"\nvalue\n:\n\"/etc/kubernetes/scheduler-config.yaml\"\nextraVolumes\n:\n-\nname\n:\nschedulerconfig\nhostPath\n:\n/home/johndoe/schedconfig.yaml\nmountPath\n:\n/etc/kubernetes/scheduler-config.yaml\nreadOnly\n:\ntrue\npathType\n:\n\"File\"\nEtcd flags\nFor details, see the\netcd server documentation\n.\nExample usage:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\netcd\n:\nlocal\n:\nextraArgs\n:\n-\nname\n:\n\"election-timeout\"\nvalue\n:\n1000\nCustomizing with patches\nFEATURE STATE:\nKubernetes v1.22 [beta]\nKubeadm allows you to pass a directory with patch files to\nInitConfiguration\nand\nJoinConfiguration\non individual nodes. These patches can be used as the last customization step before component configuration\nis written to disk.\nYou can pass this file to\nkubeadm init\nwith\n--config <YOUR CONFIG YAML>\n:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nInitConfiguration\npatches\n:\ndirectory\n:\n/home/user/somedir\nNote:\nFor\nkubeadm init\nyou can pass a file containing both a\nClusterConfiguration\nand\nInitConfiguration\nseparated by\n---\n.\nYou can pass this file to\nkubeadm join\nwith\n--config <YOUR CONFIG YAML>\n:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nJoinConfiguration\npatches\n:\ndirectory\n:\n/home/user/somedir\nThe directory must contain files named\ntarget[suffix][+patchtype].extension\n.\nFor example,\nkube-apiserver0+merge.yaml\nor just\netcd.json\n.\ntarget\ncan be one of\nkube-apiserver\n,\nkube-controller-manager\n,\nkube-scheduler\n,\netcd\nand\nkubeletconfiguration\n.\nsuffix\nis an optional string that can be used to determine which patches are applied first\nalpha-numerically.\npatchtype\ncan be one of\nstrategic\n,\nmerge\nor\njson\nand these must match the patching formats\nsupported by kubectl\n.\nThe default\npatchtype\nis\nstrategic\n.\nextension\nmust be either\njson\nor\nyaml\n.\nNote:\nIf you are using\nkubeadm upgrade\nto upgrade your kubeadm nodes you must again provide the same\npatches, so that the customization is preserved after upgrade. To do that you can use the\n--patches\nflag, which must point to the same directory.\nkubeadm upgrade\ncurrently does not support a configuration\nAPI structure that can be used for the same purpose.\nCustomizing the kubelet\nTo customize the kubelet you can add a\nKubeletConfiguration\nnext to the\nClusterConfiguration\nor\nInitConfiguration\nseparated by\n---\nwithin the same configuration file.\nThis file can then be passed to\nkubeadm init\nand kubeadm will apply the same base\nKubeletConfiguration\nto all nodes in the cluster.\nFor applying instance-specific configuration over the base\nKubeletConfiguration\nyou can use the\nkubeletconfiguration\npatch target\n.\nAlternatively, you can use kubelet flags as overrides by passing them in the\nnodeRegistration.kubeletExtraArgs\nfield supported by both\nInitConfiguration\nand\nJoinConfiguration\n.\nSome kubelet flags are deprecated, so check their status in the\nkubelet reference documentation\nbefore using them.\nFor additional details see\nConfiguring each kubelet in your cluster using kubeadm\nCustomizing kube-proxy\nTo customize kube-proxy you can pass a\nKubeProxyConfiguration\nnext your\nClusterConfiguration\nor\nInitConfiguration\nto\nkubeadm init\nseparated by\n---\n.\nFor more details you can navigate to our\nAPI reference pages\n.\nNote:\nkubeadm deploys kube-proxy as a\nDaemonSet\n, which means\nthat the\nKubeProxyConfiguration\nwould apply to all instances of kube-proxy in the cluster.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified July 05, 2024 at 4:06 PM PST:\nkubeadm: use v1beta4 in all docs examples (efc1133fa4)",
      "code_examples": [
        {
          "language": "",
          "code": "ClusterConfiguration",
          "element": "<code>ClusterConfiguration</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "KubeProxyConfiguration",
          "element": "<code>KubeProxyConfiguration</code>"
        },
        {
          "language": "",
          "code": "kube-system/coredns",
          "element": "<code>kube-system/coredns</code>"
        },
        {
          "language": "",
          "code": "ClusterConfiguration",
          "element": "<code>ClusterConfiguration</code>"
        },
        {
          "language": "",
          "code": "ClusterConfiguration",
          "element": "<code>ClusterConfiguration</code>"
        },
        {
          "language": "",
          "code": "controllerManager",
          "element": "<code>controllerManager</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "--config <YOUR CONFIG YAML>",
          "element": "<code>--config &lt;YOUR CONFIG YAML&gt;</code>"
        },
        {
          "language": "",
          "code": "ClusterConfiguration",
          "element": "<code>ClusterConfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeadm config print init-defaults",
          "element": "<code>kubeadm config print init-defaults</code>"
        },
        {
          "language": "",
          "code": "ClusterConfiguration",
          "element": "<code>ClusterConfiguration</code>"
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationkubernetesVersion:v1.16.0apiServer:extraArgs:-name:\"enable-admission-plugins\"value:\"AlwaysPullImages,DefaultStorageClass\"-name:\"audit-log-path\"value:\"/home/johndoe/audit.log\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationkubernetesVersion:v1.16.0apiServer:extraArgs:-name:\"enable-admission-plugins\"value:\"AlwaysPullImages,DefaultStorageClass\"-name:\"audit-log-path\"value:\"/home/johndoe/audit.log\"",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationkubernetesVersion:v1.16.0controllerManager:extraArgs:-name:\"cluster-signing-key-file\"value:\"/home/johndoe/keys/ca.key\"-name:\"deployment-controller-sync-period\"value:\"50\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationkubernetesVersion:v1.16.0controllerManager:extraArgs:-name:\"cluster-signing-key-file\"value:\"/home/johndoe/keys/ca.key\"-name:\"deployment-controller-sync-period\"value:\"50\"",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationkubernetesVersion:v1.16.0scheduler:extraArgs:-name:\"config\"value:\"/etc/kubernetes/scheduler-config.yaml\"extraVolumes:-name:schedulerconfighostPath:/home/johndoe/schedconfig.yamlmountPath:/etc/kubernetes/scheduler-config.yamlreadOnly:truepathType:\"File\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationkubernetesVersion:v1.16.0scheduler:extraArgs:-name:\"config\"value:\"/etc/kubernetes/scheduler-config.yaml\"extraVolumes:-name:schedulerconfighostPath:/home/johndoe/schedconfig.yamlmountPath:/etc/kubernetes/scheduler-config.yamlreadOnly:truepathType:\"File\"",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationetcd:local:extraArgs:-name:\"election-timeout\"value:1000",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationetcd:local:extraArgs:-name:\"election-timeout\"value:1000",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        },
        {
          "language": "",
          "code": "Kubernetes v1.22 [beta]",
          "element": "<code>Kubernetes v1.22 [beta]</code>"
        },
        {
          "language": "",
          "code": "InitConfiguration",
          "element": "<code>InitConfiguration</code>"
        },
        {
          "language": "",
          "code": "JoinConfiguration",
          "element": "<code>JoinConfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "--config <YOUR CONFIG YAML>",
          "element": "<code>--config &lt;YOUR CONFIG YAML&gt;</code>"
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:InitConfigurationpatches:directory:/home/user/somedir",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:InitConfigurationpatches:directory:/home/user/somedir",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "ClusterConfiguration",
          "element": "<code>ClusterConfiguration</code>"
        },
        {
          "language": "",
          "code": "InitConfiguration",
          "element": "<code>InitConfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "--config <YOUR CONFIG YAML>",
          "element": "<code>--config &lt;YOUR CONFIG YAML&gt;</code>"
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:JoinConfigurationpatches:directory:/home/user/somedir",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:JoinConfigurationpatches:directory:/home/user/somedir",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        },
        {
          "language": "",
          "code": "target[suffix][+patchtype].extension",
          "element": "<code>target[suffix][+patchtype].extension</code>"
        },
        {
          "language": "",
          "code": "kube-apiserver0+merge.yaml",
          "element": "<code>kube-apiserver0+merge.yaml</code>"
        },
        {
          "language": "",
          "code": "kube-apiserver",
          "element": "<code>kube-apiserver</code>"
        },
        {
          "language": "",
          "code": "kube-controller-manager",
          "element": "<code>kube-controller-manager</code>"
        },
        {
          "language": "",
          "code": "kube-scheduler",
          "element": "<code>kube-scheduler</code>"
        },
        {
          "language": "",
          "code": "kubeletconfiguration",
          "element": "<code>kubeletconfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeadm upgrade",
          "element": "<code>kubeadm upgrade</code>"
        },
        {
          "language": "",
          "code": "kubeadm upgrade",
          "element": "<code>kubeadm upgrade</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "ClusterConfiguration",
          "element": "<code>ClusterConfiguration</code>"
        },
        {
          "language": "",
          "code": "InitConfiguration",
          "element": "<code>InitConfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeletconfiguration",
          "element": "<code>kubeletconfiguration</code>"
        },
        {
          "language": "",
          "code": "nodeRegistration.kubeletExtraArgs",
          "element": "<code>nodeRegistration.kubeletExtraArgs</code>"
        },
        {
          "language": "",
          "code": "InitConfiguration",
          "element": "<code>InitConfiguration</code>"
        },
        {
          "language": "",
          "code": "JoinConfiguration",
          "element": "<code>JoinConfiguration</code>"
        },
        {
          "language": "",
          "code": "KubeProxyConfiguration",
          "element": "<code>KubeProxyConfiguration</code>"
        },
        {
          "language": "",
          "code": "ClusterConfiguration",
          "element": "<code>ClusterConfiguration</code>"
        },
        {
          "language": "",
          "code": "InitConfiguration",
          "element": "<code>InitConfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "KubeProxyConfiguration",
          "element": "<code>KubeProxyConfiguration</code>"
        },
        {
          "language": "",
          "code": "ClusterConfiguration",
          "element": "<code>ClusterConfiguration</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Customizing components with the kubeadm API",
          "id": ""
        },
        {
          "level": 2,
          "text": "Customizing the control plane with flags inClusterConfiguration",
          "id": "customizing-the-control-plane-with-flags-in-clusterconfiguration"
        },
        {
          "level": 2,
          "text": "Customizing with patches",
          "id": "patches"
        },
        {
          "level": 2,
          "text": "Customizing the kubelet",
          "id": "kubelet"
        },
        {
          "level": 2,
          "text": "Customizing kube-proxy",
          "id": "customizing-kube-proxy"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "APIServer flags",
          "id": "apiserver-flags"
        },
        {
          "level": 3,
          "text": "ControllerManager flags",
          "id": "controllermanager-flags"
        },
        {
          "level": 3,
          "text": "Scheduler flags",
          "id": "scheduler-flags"
        },
        {
          "level": 3,
          "text": "Etcd flags",
          "id": "etcd-flags"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731673.055495
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/",
      "title": "Options for Highly Available Topology | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nOptions for Highly Available Topology\nOptions for Highly Available Topology\nThis page explains the two options for configuring the topology of your highly available (HA) Kubernetes clusters.\nYou can set up an HA cluster:\nWith stacked control plane nodes, where etcd nodes are colocated with control plane nodes\nWith external etcd nodes, where etcd runs on separate nodes from the control plane\nYou should carefully consider the advantages and disadvantages of each topology before setting up an HA cluster.\nNote:\nkubeadm bootstraps the etcd cluster statically. Read the etcd\nClustering Guide\nfor more details.\nStacked etcd topology\nA stacked HA cluster is a\ntopology\nwhere the distributed\ndata storage cluster provided by etcd is stacked on top of the cluster formed by the nodes managed by\nkubeadm that run control plane components.\nEach control plane node runs an instance of the\nkube-apiserver\n,\nkube-scheduler\n, and\nkube-controller-manager\n.\nThe\nkube-apiserver\nis exposed to worker nodes using a load balancer.\nEach control plane node creates a local etcd member and this etcd member communicates only with\nthe\nkube-apiserver\nof this node. The same applies to the local\nkube-controller-manager\nand\nkube-scheduler\ninstances.\nThis topology couples the control planes and etcd members on the same nodes. It is simpler to set up than a cluster\nwith external etcd nodes, and simpler to manage for replication.\nHowever, a stacked cluster runs the risk of failed coupling. If one node goes down, both an etcd member and a control\nplane instance are lost, and redundancy is compromised. You can mitigate this risk by adding more control plane nodes.\nYou should therefore run a minimum of three stacked control plane nodes for an HA cluster.\nThis is the default topology in kubeadm. A local etcd member is created automatically\non control plane nodes when using\nkubeadm init\nand\nkubeadm join --control-plane\n.\nExternal etcd topology\nAn HA cluster with external etcd is a\ntopology\nwhere the distributed data storage cluster provided by etcd is external to the cluster formed by\nthe nodes that run control plane components.\nLike the stacked etcd topology, each control plane node in an external etcd topology runs\nan instance of the\nkube-apiserver\n,\nkube-scheduler\n, and\nkube-controller-manager\n.\nAnd the\nkube-apiserver\nis exposed to worker nodes using a load balancer. However,\netcd members run on separate hosts, and each etcd host communicates with the\nkube-apiserver\nof each control plane node.\nThis topology decouples the control plane and etcd member. It therefore provides an HA setup where\nlosing a control plane instance or an etcd member has less impact and does not affect\nthe cluster redundancy as much as the stacked HA topology.\nHowever, this topology requires twice the number of hosts as the stacked HA topology.\nA minimum of three hosts for control plane nodes and three hosts for etcd nodes are\nrequired for an HA cluster with this topology.\nWhat's next\nSet up a highly available cluster with kubeadm\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 16, 2024 at 9:28 AM PST:\nTweak and clean up four kubeadm files (67c5917e32)",
      "code_examples": [
        {
          "language": "",
          "code": "kube-apiserver",
          "element": "<code>kube-apiserver</code>"
        },
        {
          "language": "",
          "code": "kube-scheduler",
          "element": "<code>kube-scheduler</code>"
        },
        {
          "language": "",
          "code": "kube-controller-manager",
          "element": "<code>kube-controller-manager</code>"
        },
        {
          "language": "",
          "code": "kube-apiserver",
          "element": "<code>kube-apiserver</code>"
        },
        {
          "language": "",
          "code": "kube-apiserver",
          "element": "<code>kube-apiserver</code>"
        },
        {
          "language": "",
          "code": "kube-controller-manager",
          "element": "<code>kube-controller-manager</code>"
        },
        {
          "language": "",
          "code": "kube-scheduler",
          "element": "<code>kube-scheduler</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm join --control-plane",
          "element": "<code>kubeadm join --control-plane</code>"
        },
        {
          "language": "",
          "code": "kube-apiserver",
          "element": "<code>kube-apiserver</code>"
        },
        {
          "language": "",
          "code": "kube-scheduler",
          "element": "<code>kube-scheduler</code>"
        },
        {
          "language": "",
          "code": "kube-controller-manager",
          "element": "<code>kube-controller-manager</code>"
        },
        {
          "language": "",
          "code": "kube-apiserver",
          "element": "<code>kube-apiserver</code>"
        },
        {
          "language": "",
          "code": "kube-apiserver",
          "element": "<code>kube-apiserver</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Options for Highly Available Topology",
          "id": ""
        },
        {
          "level": 2,
          "text": "Stacked etcd topology",
          "id": "stacked-etcd-topology"
        },
        {
          "level": 2,
          "text": "External etcd topology",
          "id": "external-etcd-topology"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731676.229604
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/",
      "title": "Creating Highly Available Clusters with kubeadm | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nCreating Highly Available Clusters with kubeadm\nCreating Highly Available Clusters with kubeadm\nThis page explains two different approaches to setting up a highly available Kubernetes\ncluster using kubeadm:\nWith stacked control plane nodes. This approach requires less infrastructure. The etcd members\nand control plane nodes are co-located.\nWith an external etcd cluster. This approach requires more infrastructure. The\ncontrol plane nodes and etcd members are separated.\nBefore proceeding, you should carefully consider which approach best meets the needs of your applications\nand environment.\nOptions for Highly Available topology\noutlines the advantages and disadvantages of each.\nIf you encounter issues with setting up the HA cluster, please report these\nin the kubeadm\nissue tracker\n.\nSee also the\nupgrade documentation\n.\nCaution:\nThis page does not address running your cluster on a cloud provider. In a cloud\nenvironment, neither approach documented here works with Service objects of type\nLoadBalancer, or with dynamic PersistentVolumes.\nBefore you begin\nThe prerequisites depend on which topology you have selected for your cluster's\ncontrol plane:\nStacked etcd\nExternal etcd\nYou need:\nThree or more machines that meet\nkubeadm's minimum requirements\nfor\nthe control-plane nodes. Having an odd number of control plane nodes can help\nwith leader selection in the case of machine or zone failure.\nincluding a\ncontainer runtime\n, already set up and working\nThree or more machines that meet\nkubeadm's minimum\nrequirements\nfor the workers\nincluding a container runtime, already set up and working\nFull network connectivity between all machines in the cluster (public or\nprivate network)\nSuperuser privileges on all machines using\nsudo\nYou can use a different tool; this guide uses\nsudo\nin the examples.\nSSH access from one device to all nodes in the system\nkubeadm\nand\nkubelet\nalready installed on all machines.\nSee\nStacked etcd topology\nfor context.\nYou need:\nThree or more machines that meet\nkubeadm's minimum requirements\nfor\nthe control-plane nodes. Having an odd number of control plane nodes can help\nwith leader selection in the case of machine or zone failure.\nincluding a\ncontainer runtime\n, already set up and working\nThree or more machines that meet\nkubeadm's minimum\nrequirements\nfor the workers\nincluding a container runtime, already set up and working\nFull network connectivity between all machines in the cluster (public or\nprivate network)\nSuperuser privileges on all machines using\nsudo\nYou can use a different tool; this guide uses\nsudo\nin the examples.\nSSH access from one device to all nodes in the system\nkubeadm\nand\nkubelet\nalready installed on all machines.\nAnd you also need:\nThree or more additional machines, that will become etcd cluster members.\nHaving an odd number of members in the etcd cluster is a requirement for achieving\noptimal voting quorum.\nThese machines again need to have\nkubeadm\nand\nkubelet\ninstalled.\nThese machines also require a container runtime, that is already set up and working.\nSee\nExternal etcd topology\nfor context.\nContainer images\nEach host should have access read and fetch images from the Kubernetes container image registry,\nregistry.k8s.io\n. If you want to deploy a highly-available cluster where the hosts do not have\naccess to pull images, this is possible. You must ensure by some other means that the correct\ncontainer images are already available on the relevant hosts.\nCommand line interface\nTo manage Kubernetes once your cluster is set up, you should\ninstall kubectl\non your PC. It is also useful\nto install the\nkubectl\ntool on each control plane node, as this can be\nhelpful for troubleshooting.\nFirst steps for both methods\nCreate load balancer for kube-apiserver\nNote:\nThere are many configurations for load balancers. The following example is only one\noption. Your cluster requirements may need a different configuration.\nCreate a kube-apiserver load balancer with a name that resolves to DNS.\nIn a cloud environment you should place your control plane nodes behind a TCP\nforwarding load balancer. This load balancer distributes traffic to all\nhealthy control plane nodes in its target list. The health check for\nan apiserver is a TCP check on the port the kube-apiserver listens on\n(default value\n:6443\n).\nIt is not recommended to use an IP address directly in a cloud environment.\nThe load balancer must be able to communicate with all control plane nodes\non the apiserver port. It must also allow incoming traffic on its\nlistening port.\nMake sure the address of the load balancer always matches\nthe address of kubeadm's\nControlPlaneEndpoint\n.\nRead the\nOptions for Software Load Balancing\nguide for more details.\nAdd the first control plane node to the load balancer, and test the\nconnection:\nnc -zv -w\n2\n<LOAD_BALANCER_IP> <PORT>\nA connection refused error is expected because the API server is not yet\nrunning. A timeout, however, means the load balancer cannot communicate\nwith the control plane node. If a timeout occurs, reconfigure the load\nbalancer to communicate with the control plane node.\nAdd the remaining control plane nodes to the load balancer target group.\nStacked control plane and etcd nodes\nSteps for the first control plane node\nInitialize the control plane:\nsudo kubeadm init --control-plane-endpoint\n\"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\"\n--upload-certs\nYou can use the\n--kubernetes-version\nflag to set the Kubernetes version to use.\nIt is recommended that the versions of kubeadm, kubelet, kubectl and Kubernetes match.\nThe\n--control-plane-endpoint\nflag should be set to the address or DNS and port of the load balancer.\nThe\n--upload-certs\nflag is used to upload the certificates that should be shared\nacross all the control-plane instances to the cluster. If instead, you prefer to copy certs across\ncontrol-plane nodes manually or using automation tools, please remove this flag and refer to\nManual\ncertificate distribution\nsection below.\nNote:\nThe\nkubeadm init\nflags\n--config\nand\n--certificate-key\ncannot be mixed, therefore if you want\nto use the\nkubeadm configuration\nyou must add the\ncertificateKey\nfield in the appropriate config locations\n(under\nInitConfiguration\nand\nJoinConfiguration: controlPlane\n).\nNote:\nSome CNI network plugins require additional configuration, for example specifying the pod IP CIDR, while others do not.\nSee the\nCNI network documentation\n.\nTo add a pod CIDR pass the flag\n--pod-network-cidr\n, or if you are using a kubeadm configuration file\nset the\npodSubnet\nfield under the\nnetworking\nobject of\nClusterConfiguration\n.\nThe output looks similar to:\n...\nYou can now join any number of control-plane node by running the following\ncommand\non each as a root:\nkubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.\nThen you can join any number of worker nodes by running the following on each as root:\nkubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866\nCopy this output to a text file. You will need it later to join control plane and worker nodes to\nthe cluster.\nWhen\n--upload-certs\nis used with\nkubeadm init\n, the certificates of the primary control plane\nare encrypted and uploaded in the\nkubeadm-certs\nSecret.\nTo re-upload the certificates and generate a new decryption key, use the following command on a\ncontrol plane\nnode that is already joined to the cluster:\nsudo kubeadm init phase upload-certs --upload-certs\nYou can also specify a custom\n--certificate-key\nduring\ninit\nthat can later be used by\njoin\n.\nTo generate such a key you can use the following command:\nkubeadm certs certificate-key\nThe certificate key is a hex encoded string that is an AES key of size 32 bytes.\nNote:\nThe\nkubeadm-certs\nSecret and the decryption key expire after two hours.\nCaution:\nAs stated in the command output, the certificate key gives access to cluster sensitive data, keep it secret!\nApply the CNI plugin of your choice:\nFollow these instructions\nto install the CNI provider. Make sure the configuration corresponds to the Pod CIDR specified in the\nkubeadm configuration file (if applicable).\nNote:\nYou must pick a network plugin that suits your use case and deploy it before you move on to next step.\nIf you don't do this, you will not be able to launch your cluster properly.\nType the following and watch the pods of the control plane components get started:\nkubectl get pod -n kube-system -w\nSteps for the rest of the control plane nodes\nFor each additional control plane node you should:\nExecute the join command that was previously given to you by the\nkubeadm init\noutput on the first node.\nIt should look something like this:\nsudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07\nThe\n--control-plane\nflag tells\nkubeadm join\nto create a new control plane.\nThe\n--certificate-key ...\nwill cause the control plane certificates to be downloaded\nfrom the\nkubeadm-certs\nSecret in the cluster and be decrypted using the given key.\nYou can join multiple control-plane nodes in parallel.\nExternal etcd nodes\nSetting up a cluster with external etcd nodes is similar to the procedure used for stacked etcd\nwith the exception that you should setup etcd first, and you should pass the etcd information\nin the kubeadm config file.\nSet up the etcd cluster\nFollow these\ninstructions\nto set up the etcd cluster.\nSet up SSH as described\nhere\n.\nCopy the following files from any etcd node in the cluster to the first control plane node:\nexport\nCONTROL_PLANE\n=\n\"ubuntu@10.0.0.7\"\nscp /etc/kubernetes/pki/etcd/ca.crt\n\"\n${\nCONTROL_PLANE\n}\n\"\n:\nscp /etc/kubernetes/pki/apiserver-etcd-client.crt\n\"\n${\nCONTROL_PLANE\n}\n\"\n:\nscp /etc/kubernetes/pki/apiserver-etcd-client.key\n\"\n${\nCONTROL_PLANE\n}\n\"\n:\nReplace the value of\nCONTROL_PLANE\nwith the\nuser@host\nof the first control-plane node.\nSet up the first control plane node\nCreate a file called\nkubeadm-config.yaml\nwith the following contents:\n---\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nkubernetesVersion\n:\nstable\ncontrolPlaneEndpoint\n:\n\"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\"\n# change this (see below)\netcd\n:\nexternal\n:\nendpoints\n:\n- https://ETCD_0_IP:2379\n# change ETCD_0_IP appropriately\n- https://ETCD_1_IP:2379\n# change ETCD_1_IP appropriately\n- https://ETCD_2_IP:2379\n# change ETCD_2_IP appropriately\ncaFile\n:\n/etc/kubernetes/pki/etcd/ca.crt\ncertFile\n:\n/etc/kubernetes/pki/apiserver-etcd-client.crt\nkeyFile\n:\n/etc/kubernetes/pki/apiserver-etcd-client.key\nNote:\nThe difference between stacked etcd and external etcd here is that the external etcd setup requires\na configuration file with the etcd endpoints under the\nexternal\nobject for\netcd\n.\nIn the case of the stacked etcd topology, this is managed automatically.\nReplace the following variables in the config template with the appropriate values for your cluster:\nLOAD_BALANCER_DNS\nLOAD_BALANCER_PORT\nETCD_0_IP\nETCD_1_IP\nETCD_2_IP\nThe following steps are similar to the stacked etcd setup:\nRun\nsudo kubeadm init --config kubeadm-config.yaml --upload-certs\non this node.\nWrite the output join commands that are returned to a text file for later use.\nApply the CNI plugin of your choice.\nNote:\nYou must pick a network plugin that suits your use case and deploy it before you move on to next step.\nIf you don't do this, you will not be able to launch your cluster properly.\nSteps for the rest of the control plane nodes\nThe steps are the same as for the stacked etcd setup:\nMake sure the first control plane node is fully initialized.\nJoin each control plane node with the join command you saved to a text file. It's recommended\nto join the control plane nodes one at a time.\nDon't forget that the decryption key from\n--certificate-key\nexpires after two hours, by default.\nCommon tasks after bootstrapping control plane\nInstall workers\nWorker nodes can be joined to the cluster with the command you stored previously\nas the output from the\nkubeadm init\ncommand:\nsudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866\nManual certificate distribution\nIf you choose to not use\nkubeadm init\nwith the\n--upload-certs\nflag this means that\nyou are going to have to manually copy the certificates from the primary control plane node to the\njoining control plane nodes.\nThere are many ways to do this. The following example uses\nssh\nand\nscp\n:\nSSH is required if you want to control all nodes from a single machine.\nEnable ssh-agent on your main device that has access to all other nodes in\nthe system:\neval\n$(\nssh-agent\n)\nAdd your SSH identity to the session:\nssh-add ~/.ssh/path_to_private_key\nSSH between nodes to check that the connection is working correctly.\nWhen you SSH to any node, add the\n-A\nflag. This flag allows the node that you\nhave logged into via SSH to access the SSH agent on your PC. Consider alternative\nmethods if you do not fully trust the security of your user session on the node.\nssh -A 10.0.0.7\nWhen using sudo on any node, make sure to preserve the environment so SSH\nforwarding works:\nsudo -E -s\nAfter configuring SSH on all the nodes you should run the following script on the first\ncontrol plane node after running\nkubeadm init\n. This script will copy the certificates from\nthe first control plane node to the other control plane nodes:\nIn the following example, replace\nCONTROL_PLANE_IPS\nwith the IP addresses of the\nother control plane nodes.\nUSER\n=\nubuntu\n# customizable\nCONTROL_PLANE_IPS\n=\n\"10.0.0.7 10.0.0.8\"\nfor\nhost in\n${\nCONTROL_PLANE_IPS\n}\n;\ndo\nscp /etc/kubernetes/pki/ca.crt\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/ca.key\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/sa.key\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/sa.pub\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/front-proxy-ca.crt\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/front-proxy-ca.key\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/etcd/ca.crt\n\"\n${\nUSER\n}\n\"\n@\n$host\n:etcd-ca.crt\n# Skip the next line if you are using external etcd\nscp /etc/kubernetes/pki/etcd/ca.key\n\"\n${\nUSER\n}\n\"\n@\n$host\n:etcd-ca.key\ndone\nCaution:\nCopy only the certificates in the above list. kubeadm will take care of generating the rest of the certificates\nwith the required SANs for the joining control-plane instances. If you copy all the certificates by mistake,\nthe creation of additional nodes could fail due to a lack of required SANs.\nThen on each joining control plane node you have to run the following script before running\nkubeadm join\n.\nThis script will move the previously copied certificates from the home directory to\n/etc/kubernetes/pki\n:\nUSER\n=\nubuntu\n# customizable\nmkdir -p /etc/kubernetes/pki/etcd\nmv /home/\n${\nUSER\n}\n/ca.crt /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/ca.key /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/sa.pub /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/sa.key /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/front-proxy-ca.crt /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/front-proxy-ca.key /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt\n# Skip the next line if you are using external etcd\nmv /home/\n${\nUSER\n}\n/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified March 08, 2025 at 3:13 PM PST:\nUpdate high-availability.md (358fef3e0a)",
      "code_examples": [
        {
          "language": "",
          "code": "registry.k8s.io",
          "element": "<code>registry.k8s.io</code>"
        },
        {
          "language": "",
          "code": "ControlPlaneEndpoint",
          "element": "<code>ControlPlaneEndpoint</code>"
        },
        {
          "language": "",
          "code": "nc -zv -w2<LOAD_BALANCER_IP> <PORT>",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>nc -zv -w <span style=\"color..."
        },
        {
          "language": "language-shell",
          "code": "nc -zv -w2<LOAD_BALANCER_IP> <PORT>",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>nc -zv -w <span style=\"color:#666\">2</span> &lt;LOAD_BALANCER_IP&gt; &lt;PORT&gt;\n</span></span></code>"
        },
        {
          "language": "",
          "code": "sudo kubeadm init --control-plane-endpoint\"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\"--upload-certs",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>sudo kubeadm init --control-plane-..."
        },
        {
          "language": "language-sh",
          "code": "sudo kubeadm init --control-plane-endpoint\"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\"--upload-certs",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>sudo kubeadm init --control-plane-endpoint <span style=\"color:#b44\">\"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\"</span> --upload-cer..."
        },
        {
          "language": "",
          "code": "--kubernetes-version",
          "element": "<code>--kubernetes-version</code>"
        },
        {
          "language": "",
          "code": "--control-plane-endpoint",
          "element": "<code>--control-plane-endpoint</code>"
        },
        {
          "language": "",
          "code": "--upload-certs",
          "element": "<code>--upload-certs</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "--certificate-key",
          "element": "<code>--certificate-key</code>"
        },
        {
          "language": "",
          "code": "certificateKey",
          "element": "<code>certificateKey</code>"
        },
        {
          "language": "",
          "code": "InitConfiguration",
          "element": "<code>InitConfiguration</code>"
        },
        {
          "language": "",
          "code": "JoinConfiguration: controlPlane",
          "element": "<code>JoinConfiguration: controlPlane</code>"
        },
        {
          "language": "",
          "code": "--pod-network-cidr",
          "element": "<code>--pod-network-cidr</code>"
        },
        {
          "language": "",
          "code": "ClusterConfiguration",
          "element": "<code>ClusterConfiguration</code>"
        },
        {
          "language": "",
          "code": "...You can now join any number of control-plane node by running the followingcommandon each as a root:kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07Please note that the certificate-key gives access to cluster sensitive data, keep it secret!As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>...\n</span></span><span style=\"dis..."
        },
        {
          "language": "language-sh",
          "code": "...You can now join any number of control-plane node by running the followingcommandon each as a root:kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07Please note that the certificate-key gives access to cluster sensitive data, keep it secret!As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>...\n</span></span><span style=\"display:flex\"><span>You can now join any number of control-plane node by running the following ..."
        },
        {
          "language": "",
          "code": "--upload-certs",
          "element": "<code>--upload-certs</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm-certs",
          "element": "<code>kubeadm-certs</code>"
        },
        {
          "language": "",
          "code": "sudo kubeadm init phase upload-certs --upload-certs",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>sudo kubeadm init phase upload-cer..."
        },
        {
          "language": "language-sh",
          "code": "sudo kubeadm init phase upload-certs --upload-certs",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>sudo kubeadm init phase upload-certs --upload-certs\n</span></span></code>"
        },
        {
          "language": "",
          "code": "--certificate-key",
          "element": "<code>--certificate-key</code>"
        },
        {
          "language": "",
          "code": "kubeadm certs certificate-key",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubeadm certs certificate-key\n</sp..."
        },
        {
          "language": "language-sh",
          "code": "kubeadm certs certificate-key",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubeadm certs certificate-key\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubeadm-certs",
          "element": "<code>kubeadm-certs</code>"
        },
        {
          "language": "",
          "code": "kubectl get pod -n kube-system -w",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl get pod -n kube-system -w\n..."
        },
        {
          "language": "language-sh",
          "code": "kubectl get pod -n kube-system -w",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl get pod -n kube-system -w\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>sudo kubeadm join 192.168.0.200:64..."
        },
        {
          "language": "language-sh",
          "code": "sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042..."
        },
        {
          "language": "",
          "code": "--control-plane",
          "element": "<code>--control-plane</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "--certificate-key ...",
          "element": "<code>--certificate-key ...</code>"
        },
        {
          "language": "",
          "code": "kubeadm-certs",
          "element": "<code>kubeadm-certs</code>"
        },
        {
          "language": "",
          "code": "exportCONTROL_PLANE=\"ubuntu@10.0.0.7\"scp /etc/kubernetes/pki/etcd/ca.crt\"${CONTROL_PLANE}\":scp /etc/kubernetes/pki/apiserver-etcd-client.crt\"${CONTROL_PLANE}\":scp /etc/kubernetes/pki/apiserver-etcd-client.key\"${CONTROL_PLANE}\":",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#a2f\">export</s..."
        },
        {
          "language": "language-sh",
          "code": "exportCONTROL_PLANE=\"ubuntu@10.0.0.7\"scp /etc/kubernetes/pki/etcd/ca.crt\"${CONTROL_PLANE}\":scp /etc/kubernetes/pki/apiserver-etcd-client.crt\"${CONTROL_PLANE}\":scp /etc/kubernetes/pki/apiserver-etcd-client.key\"${CONTROL_PLANE}\":",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#a2f\">export</span> <span style=\"color:#b8860b\">CONTROL_PLANE</span><span style=\"color:#666\">=</span><span ..."
        },
        {
          "language": "",
          "code": "CONTROL_PLANE",
          "element": "<code>CONTROL_PLANE</code>"
        },
        {
          "language": "",
          "code": "kubeadm-config.yaml",
          "element": "<code>kubeadm-config.yaml</code>"
        },
        {
          "language": "",
          "code": "---apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationkubernetesVersion:stablecontrolPlaneEndpoint:\"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\"# change this (see below)etcd:external:endpoints:- https://ETCD_0_IP:2379# change ETCD_0_IP appropriately- https://ETCD_1_IP:2379# change ETCD_1_IP appropriately- https://ETCD_2_IP:2379# change ETCD_2_IP appropriatelycaFile:/etc/kubernetes/pki/etcd/ca.crtcertFile:/etc/kubernetes/pki/apiserver-etcd-client.crtkeyFile:/etc/kubernetes/pki/apiserver-etcd-client.key",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:#00f;font-w..."
        },
        {
          "language": "language-yaml",
          "code": "---apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationkubernetesVersion:stablecontrolPlaneEndpoint:\"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\"# change this (see below)etcd:external:endpoints:- https://ETCD_0_IP:2379# change ETCD_0_IP appropriately- https://ETCD_1_IP:2379# change ETCD_1_IP appropriately- https://ETCD_2_IP:2379# change ETCD_2_IP appropriatelycaFile:/etc/kubernetes/pki/etcd/ca.crtcertFile:/etc/kubernetes/pki/apiserver-etcd-client.crtkeyFile:/etc/kubernetes/pki/apiserver-etcd-client.key",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:#00f;font-weight:700\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:fle..."
        },
        {
          "language": "",
          "code": "LOAD_BALANCER_DNS",
          "element": "<code>LOAD_BALANCER_DNS</code>"
        },
        {
          "language": "",
          "code": "LOAD_BALANCER_PORT",
          "element": "<code>LOAD_BALANCER_PORT</code>"
        },
        {
          "language": "",
          "code": "sudo kubeadm init --config kubeadm-config.yaml --upload-certs",
          "element": "<code>sudo kubeadm init --config kubeadm-config.yaml --upload-certs</code>"
        },
        {
          "language": "",
          "code": "--certificate-key",
          "element": "<code>--certificate-key</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>sudo kubeadm join 192.168.0.200:64..."
        },
        {
          "language": "language-sh",
          "code": "sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042..."
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "--upload-certs",
          "element": "<code>--upload-certs</code>"
        },
        {
          "language": "",
          "code": "eval$(ssh-agent)",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#a2f\">eva..."
        },
        {
          "language": "language-shell",
          "code": "eval$(ssh-agent)",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#a2f\">eval</span> <span style=\"color:#a2f;font-weight:700\">$(</span>ssh-agent<span style=\"color:#a2f;..."
        },
        {
          "language": "",
          "code": "ssh-add ~/.ssh/path_to_private_key",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>ssh-add ~/.ssh/path_to_priva..."
        },
        {
          "language": "language-shell",
          "code": "ssh-add ~/.ssh/path_to_private_key",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>ssh-add ~/.ssh/path_to_private_key\n</span></span></code>"
        },
        {
          "language": "",
          "code": "ssh -A 10.0.0.7",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>ssh -A 10.0.0.7\n</span></spa..."
        },
        {
          "language": "language-shell",
          "code": "ssh -A 10.0.0.7",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>ssh -A 10.0.0.7\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "CONTROL_PLANE_IPS",
          "element": "<code>CONTROL_PLANE_IPS</code>"
        },
        {
          "language": "",
          "code": "USER=ubuntu# customizableCONTROL_PLANE_IPS=\"10.0.0.7 10.0.0.8\"forhost in${CONTROL_PLANE_IPS};doscp /etc/kubernetes/pki/ca.crt\"${USER}\"@$host:scp /etc/kubernetes/pki/ca.key\"${USER}\"@$host:scp /etc/kubernetes/pki/sa.key\"${USER}\"@$host:scp /etc/kubernetes/pki/sa.pub\"${USER}\"@$host:scp /etc/kubernetes/pki/front-proxy-ca.crt\"${USER}\"@$host:scp /etc/kubernetes/pki/front-proxy-ca.key\"${USER}\"@$host:scp /etc/kubernetes/pki/etcd/ca.crt\"${USER}\"@$host:etcd-ca.crt# Skip the next line if you are using external etcdscp /etc/kubernetes/pki/etcd/ca.key\"${USER}\"@$host:etcd-ca.keydone",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">USER</..."
        },
        {
          "language": "language-sh",
          "code": "USER=ubuntu# customizableCONTROL_PLANE_IPS=\"10.0.0.7 10.0.0.8\"forhost in${CONTROL_PLANE_IPS};doscp /etc/kubernetes/pki/ca.crt\"${USER}\"@$host:scp /etc/kubernetes/pki/ca.key\"${USER}\"@$host:scp /etc/kubernetes/pki/sa.key\"${USER}\"@$host:scp /etc/kubernetes/pki/sa.pub\"${USER}\"@$host:scp /etc/kubernetes/pki/front-proxy-ca.crt\"${USER}\"@$host:scp /etc/kubernetes/pki/front-proxy-ca.key\"${USER}\"@$host:scp /etc/kubernetes/pki/etcd/ca.crt\"${USER}\"@$host:etcd-ca.crt# Skip the next line if you are using external etcdscp /etc/kubernetes/pki/etcd/ca.key\"${USER}\"@$host:etcd-ca.keydone",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">USER</span><span style=\"color:#666\">=</span>ubuntu <span style=\"color:#080;font-style:italic\"># cu..."
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki",
          "element": "<code>/etc/kubernetes/pki</code>"
        },
        {
          "language": "",
          "code": "USER=ubuntu# customizablemkdir -p /etc/kubernetes/pki/etcdmv /home/${USER}/ca.crt /etc/kubernetes/pki/mv /home/${USER}/ca.key /etc/kubernetes/pki/mv /home/${USER}/sa.pub /etc/kubernetes/pki/mv /home/${USER}/sa.key /etc/kubernetes/pki/mv /home/${USER}/front-proxy-ca.crt /etc/kubernetes/pki/mv /home/${USER}/front-proxy-ca.key /etc/kubernetes/pki/mv /home/${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt# Skip the next line if you are using external etcdmv /home/${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">USER</..."
        },
        {
          "language": "language-sh",
          "code": "USER=ubuntu# customizablemkdir -p /etc/kubernetes/pki/etcdmv /home/${USER}/ca.crt /etc/kubernetes/pki/mv /home/${USER}/ca.key /etc/kubernetes/pki/mv /home/${USER}/sa.pub /etc/kubernetes/pki/mv /home/${USER}/sa.key /etc/kubernetes/pki/mv /home/${USER}/front-proxy-ca.crt /etc/kubernetes/pki/mv /home/${USER}/front-proxy-ca.key /etc/kubernetes/pki/mv /home/${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt# Skip the next line if you are using external etcdmv /home/${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">USER</span><span style=\"color:#666\">=</span>ubuntu <span style=\"color:#080;font-style:italic\"># cu..."
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Creating Highly Available Clusters with kubeadm",
          "id": ""
        },
        {
          "level": 2,
          "text": "Before you begin",
          "id": "before-you-begin"
        },
        {
          "level": 2,
          "text": "First steps for both methods",
          "id": "first-steps-for-both-methods"
        },
        {
          "level": 2,
          "text": "Stacked control plane and etcd nodes",
          "id": "stacked-control-plane-and-etcd-nodes"
        },
        {
          "level": 2,
          "text": "External etcd nodes",
          "id": "external-etcd-nodes"
        },
        {
          "level": 2,
          "text": "Common tasks after bootstrapping control plane",
          "id": "common-tasks-after-bootstrapping-control-plane"
        },
        {
          "level": 2,
          "text": "Manual certificate distribution",
          "id": "manual-certs"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Container images",
          "id": "container-images"
        },
        {
          "level": 3,
          "text": "Command line interface",
          "id": "kubectl"
        },
        {
          "level": 3,
          "text": "Create load balancer for kube-apiserver",
          "id": "create-load-balancer-for-kube-apiserver"
        },
        {
          "level": 3,
          "text": "Steps for the first control plane node",
          "id": "steps-for-the-first-control-plane-node"
        },
        {
          "level": 3,
          "text": "Steps for the rest of the control plane nodes",
          "id": "steps-for-the-rest-of-the-control-plane-nodes"
        },
        {
          "level": 3,
          "text": "Set up the etcd cluster",
          "id": "set-up-the-etcd-cluster"
        },
        {
          "level": 3,
          "text": "Set up the first control plane node",
          "id": "set-up-the-first-control-plane-node"
        },
        {
          "level": 3,
          "text": "Steps for the rest of the control plane nodes",
          "id": "steps-for-the-rest-of-the-control-plane-nodes-1"
        },
        {
          "level": 3,
          "text": "Install workers",
          "id": "install-workers"
        },
        {
          "level": 4,
          "text": "Caution:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Caution:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Caution:",
          "id": ""
        }
      ],
      "timestamp": 1750731679.4652731
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/",
      "title": "Set up a High Availability etcd Cluster with kubeadm | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nSet up a High Availability etcd Cluster with kubeadm\nSet up a High Availability etcd Cluster with kubeadm\nBy default, kubeadm runs a local etcd instance on each control plane node.\nIt is also possible to treat the etcd cluster as external and provision\netcd instances on separate hosts. The differences between the two approaches are covered in the\nOptions for Highly Available topology\npage.\nThis task walks through the process of creating a high availability external\netcd cluster of three members that can be used by kubeadm during cluster creation.\nBefore you begin\nThree hosts that can talk to each other over TCP ports 2379 and 2380. This\ndocument assumes these default ports. However, they are configurable through\nthe kubeadm config file.\nEach host must have systemd and a bash compatible shell installed.\nEach host must\nhave a container runtime, kubelet, and kubeadm installed\n.\nEach host should have access to the Kubernetes container image registry (\nregistry.k8s.io\n) or list/pull the required etcd image using\nkubeadm config images list/pull\n. This guide will set up etcd instances as\nstatic pods\nmanaged by a kubelet.\nSome infrastructure to copy files between hosts. For example\nssh\nand\nscp\ncan satisfy this requirement.\nSetting up the cluster\nThe general approach is to generate all certs on one node and only distribute\nthe\nnecessary\nfiles to the other nodes.\nNote:\nkubeadm contains all the necessary cryptographic machinery to generate\nthe certificates described below; no other cryptographic tooling is required for\nthis example.\nNote:\nThe examples below use IPv4 addresses but you can also configure kubeadm, the kubelet and etcd\nto use IPv6 addresses. Dual-stack is supported by some Kubernetes options, but not by etcd. For more details\non Kubernetes dual-stack support see\nDual-stack support with kubeadm\n.\nConfigure the kubelet to be a service manager for etcd.\nNote:\nYou must do this on every host where etcd should be running.\nSince etcd was created first, you must override the service priority by creating a new unit file\nthat has higher precedence than the kubeadm-provided kubelet unit file.\ncat\n<< EOF > /etc/systemd/system/kubelet.service.d/kubelet.conf\n# Replace \"systemd\" with the cgroup driver of your container runtime. The default value in the kubelet is \"cgroupfs\".\n# Replace the value of \"containerRuntimeEndpoint\" for a different container runtime if needed.\n#\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nauthentication:\nanonymous:\nenabled: false\nwebhook:\nenabled: false\nauthorization:\nmode: AlwaysAllow\ncgroupDriver: systemd\naddress: 127.0.0.1\ncontainerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock\nstaticPodPath: /etc/kubernetes/manifests\nEOF\ncat\n<< EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf\n[Service]\nExecStart=\nExecStart=/usr/bin/kubelet --config=/etc/systemd/system/kubelet.service.d/kubelet.conf\nRestart=always\nEOF\nsystemctl daemon-reload\nsystemctl restart kubelet\nCheck the kubelet status to ensure it is running.\nsystemctl status kubelet\nCreate configuration files for kubeadm.\nGenerate one kubeadm configuration file for each host that will have an etcd\nmember running on it using the following script.\n# Update HOST0, HOST1 and HOST2 with the IPs of your hosts\nexport\nHOST0\n=\n10.0.0.6\nexport\nHOST1\n=\n10.0.0.7\nexport\nHOST2\n=\n10.0.0.8\n# Update NAME0, NAME1 and NAME2 with the hostnames of your hosts\nexport\nNAME0\n=\n\"infra0\"\nexport\nNAME1\n=\n\"infra1\"\nexport\nNAME2\n=\n\"infra2\"\n# Create temp directories to store files that will end up on other hosts\nmkdir -p /tmp/\n${\nHOST0\n}\n/ /tmp/\n${\nHOST1\n}\n/ /tmp/\n${\nHOST2\n}\n/\nHOSTS\n=(\n${\nHOST0\n}\n${\nHOST1\n}\n${\nHOST2\n}\n)\nNAMES\n=(\n${\nNAME0\n}\n${\nNAME1\n}\n${\nNAME2\n}\n)\nfor\ni in\n\"\n${\n!HOSTS[@]\n}\n\"\n;\ndo\nHOST\n=\n${\nHOSTS\n[\n$i\n]\n}\nNAME\n=\n${\nNAMES\n[\n$i\n]\n}\ncat\n<< EOF > /tmp/${HOST}/kubeadmcfg.yaml\n---\napiVersion: \"kubeadm.k8s.io/v1beta4\"\nkind: InitConfiguration\nnodeRegistration:\nname: ${NAME}\nlocalAPIEndpoint:\nadvertiseAddress: ${HOST}\n---\napiVersion: \"kubeadm.k8s.io/v1beta4\"\nkind: ClusterConfiguration\netcd:\nlocal:\nserverCertSANs:\n- \"${HOST}\"\npeerCertSANs:\n- \"${HOST}\"\nextraArgs:\n- name: initial-cluster\nvalue: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380\n- name: initial-cluster-state\nvalue: new\n- name: name\nvalue: ${NAME}\n- name: listen-peer-urls\nvalue: https://${HOST}:2380\n- name: listen-client-urls\nvalue: https://${HOST}:2379\n- name: advertise-client-urls\nvalue: https://${HOST}:2379\n- name: initial-advertise-peer-urls\nvalue: https://${HOST}:2380\nEOF\ndone\nGenerate the certificate authority.\nIf you already have a CA then the only action that is copying the CA's\ncrt\nand\nkey\nfile to\n/etc/kubernetes/pki/etcd/ca.crt\nand\n/etc/kubernetes/pki/etcd/ca.key\n. After those files have been copied,\nproceed to the next step, \"Create certificates for each member\".\nIf you do not already have a CA then run this command on\n$HOST0\n(where you\ngenerated the configuration files for kubeadm).\nkubeadm init phase certs etcd-ca\nThis creates two files:\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/etcd/ca.key\nCreate certificates for each member.\nkubeadm init phase certs etcd-server --config\n=\n/tmp/\n${\nHOST2\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-peer --config\n=\n/tmp/\n${\nHOST2\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-healthcheck-client --config\n=\n/tmp/\n${\nHOST2\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs apiserver-etcd-client --config\n=\n/tmp/\n${\nHOST2\n}\n/kubeadmcfg.yaml\ncp -R /etc/kubernetes/pki /tmp/\n${\nHOST2\n}\n/\n# cleanup non-reusable certificates\nfind /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete\nkubeadm init phase certs etcd-server --config\n=\n/tmp/\n${\nHOST1\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-peer --config\n=\n/tmp/\n${\nHOST1\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-healthcheck-client --config\n=\n/tmp/\n${\nHOST1\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs apiserver-etcd-client --config\n=\n/tmp/\n${\nHOST1\n}\n/kubeadmcfg.yaml\ncp -R /etc/kubernetes/pki /tmp/\n${\nHOST1\n}\n/\nfind /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete\nkubeadm init phase certs etcd-server --config\n=\n/tmp/\n${\nHOST0\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-peer --config\n=\n/tmp/\n${\nHOST0\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-healthcheck-client --config\n=\n/tmp/\n${\nHOST0\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs apiserver-etcd-client --config\n=\n/tmp/\n${\nHOST0\n}\n/kubeadmcfg.yaml\n# No need to move the certs because they are for HOST0\n# clean up certs that should not be copied off this host\nfind /tmp/\n${\nHOST2\n}\n-name ca.key -type f -delete\nfind /tmp/\n${\nHOST1\n}\n-name ca.key -type f -delete\nCopy certificates and kubeadm configs.\nThe certificates have been generated and now they must be moved to their\nrespective hosts.\nUSER\n=\nubuntu\nHOST\n=\n${\nHOST1\n}\nscp -r /tmp/\n${\nHOST\n}\n/*\n${\nUSER\n}\n@\n${\nHOST\n}\n:\nssh\n${\nUSER\n}\n@\n${\nHOST\n}\nUSER@HOST $ sudo -Es\nroot@HOST $ chown -R root:root pki\nroot@HOST $ mv pki /etc/kubernetes/\nEnsure all expected files exist.\nThe complete list of required files on\n$HOST0\nis:\n/tmp/${HOST0}\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── ca.key\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key\nOn\n$HOST1\n:\n$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key\nOn\n$HOST2\n:\n$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key\nCreate the static pod manifests.\nNow that the certificates and configs are in place it's time to create the\nmanifests. On each host run the\nkubeadm\ncommand to generate a static manifest\nfor etcd.\nroot@HOST0 $ kubeadm init phase etcd\nlocal\n--config\n=\n/tmp/\n${\nHOST0\n}\n/kubeadmcfg.yaml\nroot@HOST1 $ kubeadm init phase etcd\nlocal\n--config\n=\n$HOME\n/kubeadmcfg.yaml\nroot@HOST2 $ kubeadm init phase etcd\nlocal\n--config\n=\n$HOME\n/kubeadmcfg.yaml\nOptional: Check the cluster health.\nIf\netcdctl\nisn't available, you can run this tool inside a container image.\nYou would do that directly with your container runtime using a tool such as\ncrictl run\nand not through Kubernetes\nETCDCTL_API\n=\n3\netcdctl\n\\\n--cert /etc/kubernetes/pki/etcd/peer.crt\n\\\n--key /etc/kubernetes/pki/etcd/peer.key\n\\\n--cacert /etc/kubernetes/pki/etcd/ca.crt\n\\\n--endpoints https://\n${\nHOST0\n}\n:2379 endpoint health\n...\nhttps://\n[\nHOST0 IP\n]\n:2379 is healthy: successfully committed proposal:\ntook\n=\n16.283339ms\nhttps://\n[\nHOST1 IP\n]\n:2379 is healthy: successfully committed proposal:\ntook\n=\n19.44402ms\nhttps://\n[\nHOST2 IP\n]\n:2379 is healthy: successfully committed proposal:\ntook\n=\n35.926451ms\nSet\n${HOST0}\nto the IP address of the host you are testing.\nWhat's next\nOnce you have an etcd cluster with 3 working members, you can continue setting up a\nhighly available control plane using the\nexternal etcd method with kubeadm\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 02, 2024 at 12:45 AM PST:\nRemove deprecated note on \"Set up a High Availability etcd Cluster with kubeadm\" page (#48120) (40419a5ef5)",
      "code_examples": [
        {
          "language": "",
          "code": "registry.k8s.io",
          "element": "<code>registry.k8s.io</code>"
        },
        {
          "language": "",
          "code": "kubeadm config images list/pull",
          "element": "<code>kubeadm config images list/pull</code>"
        },
        {
          "language": "",
          "code": "cat<< EOF > /etc/systemd/system/kubelet.service.d/kubelet.conf# Replace \"systemd\" with the cgroup driver of your container runtime. The default value in the kubelet is \"cgroupfs\".# Replace the value of \"containerRuntimeEndpoint\" for a different container runtime if needed.#apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationauthentication:anonymous:enabled: falsewebhook:enabled: falseauthorization:mode: AlwaysAllowcgroupDriver: systemdaddress: 127.0.0.1containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sockstaticPodPath: /etc/kubernetes/manifestsEOFcat<< EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf[Service]ExecStart=ExecStart=/usr/bin/kubelet --config=/etc/systemd/system/kubelet.service.d/kubelet.confRestart=alwaysEOFsystemctl daemon-reloadsystemctl restart kubelet",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>cat <span style=\"color:#b44\">&lt;&..."
        },
        {
          "language": "language-sh",
          "code": "cat<< EOF > /etc/systemd/system/kubelet.service.d/kubelet.conf# Replace \"systemd\" with the cgroup driver of your container runtime. The default value in the kubelet is \"cgroupfs\".# Replace the value of \"containerRuntimeEndpoint\" for a different container runtime if needed.#apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationauthentication:anonymous:enabled: falsewebhook:enabled: falseauthorization:mode: AlwaysAllowcgroupDriver: systemdaddress: 127.0.0.1containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sockstaticPodPath: /etc/kubernetes/manifestsEOFcat<< EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf[Service]ExecStart=ExecStart=/usr/bin/kubelet --config=/etc/systemd/system/kubelet.service.d/kubelet.confRestart=alwaysEOFsystemctl daemon-reloadsystemctl restart kubelet",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>cat <span style=\"color:#b44\">&lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/kubelet.conf\n</span></span></span><span s..."
        },
        {
          "language": "",
          "code": "systemctl status kubelet",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>systemctl status kubelet\n</span></..."
        },
        {
          "language": "language-sh",
          "code": "systemctl status kubelet",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>systemctl status kubelet\n</span></span></code>"
        },
        {
          "language": "",
          "code": "# Update HOST0, HOST1 and HOST2 with the IPs of your hostsexportHOST0=10.0.0.6exportHOST1=10.0.0.7exportHOST2=10.0.0.8# Update NAME0, NAME1 and NAME2 with the hostnames of your hostsexportNAME0=\"infra0\"exportNAME1=\"infra1\"exportNAME2=\"infra2\"# Create temp directories to store files that will end up on other hostsmkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/HOSTS=(${HOST0}${HOST1}${HOST2})NAMES=(${NAME0}${NAME1}${NAME2})fori in\"${!HOSTS[@]}\";doHOST=${HOSTS[$i]}NAME=${NAMES[$i]}cat<< EOF > /tmp/${HOST}/kubeadmcfg.yaml---apiVersion: \"kubeadm.k8s.io/v1beta4\"kind: InitConfigurationnodeRegistration:name: ${NAME}localAPIEndpoint:advertiseAddress: ${HOST}---apiVersion: \"kubeadm.k8s.io/v1beta4\"kind: ClusterConfigurationetcd:local:serverCertSANs:- \"${HOST}\"peerCertSANs:- \"${HOST}\"extraArgs:- name: initial-clustervalue: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380- name: initial-cluster-statevalue: new- name: namevalue: ${NAME}- name: listen-peer-urlsvalue: https://${HOST}:2380- name: listen-client-urlsvalue: https://${HOST}:2379- name: advertise-client-urlsvalue: https://${HOST}:2379- name: initial-advertise-peer-urlsvalue: https://${HOST}:2380EOFdone",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#080;font-style..."
        },
        {
          "language": "language-sh",
          "code": "# Update HOST0, HOST1 and HOST2 with the IPs of your hostsexportHOST0=10.0.0.6exportHOST1=10.0.0.7exportHOST2=10.0.0.8# Update NAME0, NAME1 and NAME2 with the hostnames of your hostsexportNAME0=\"infra0\"exportNAME1=\"infra1\"exportNAME2=\"infra2\"# Create temp directories to store files that will end up on other hostsmkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/HOSTS=(${HOST0}${HOST1}${HOST2})NAMES=(${NAME0}${NAME1}${NAME2})fori in\"${!HOSTS[@]}\";doHOST=${HOSTS[$i]}NAME=${NAMES[$i]}cat<< EOF > /tmp/${HOST}/kubeadmcfg.yaml---apiVersion: \"kubeadm.k8s.io/v1beta4\"kind: InitConfigurationnodeRegistration:name: ${NAME}localAPIEndpoint:advertiseAddress: ${HOST}---apiVersion: \"kubeadm.k8s.io/v1beta4\"kind: ClusterConfigurationetcd:local:serverCertSANs:- \"${HOST}\"peerCertSANs:- \"${HOST}\"extraArgs:- name: initial-clustervalue: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380- name: initial-cluster-statevalue: new- name: namevalue: ${NAME}- name: listen-peer-urlsvalue: https://${HOST}:2380- name: listen-client-urlsvalue: https://${HOST}:2379- name: advertise-client-urlsvalue: https://${HOST}:2379- name: initial-advertise-peer-urlsvalue: https://${HOST}:2380EOFdone",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#080;font-style:italic\"># Update HOST0, HOST1 and HOST2 with the IPs of your hosts</span>\n</span></span><sp..."
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki/etcd/ca.crt",
          "element": "<code>/etc/kubernetes/pki/etcd/ca.crt</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki/etcd/ca.key",
          "element": "<code>/etc/kubernetes/pki/etcd/ca.key</code>"
        },
        {
          "language": "",
          "code": "kubeadm init phase certs etcd-ca",
          "element": "<pre tabindex=\"0\"><code>kubeadm init phase certs etcd-ca\n</code></pre>"
        },
        {
          "language": "",
          "code": "kubeadm init phase certs etcd-ca",
          "element": "<code>kubeadm init phase certs etcd-ca\n</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki/etcd/ca.crt",
          "element": "<code>/etc/kubernetes/pki/etcd/ca.crt</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki/etcd/ca.key",
          "element": "<code>/etc/kubernetes/pki/etcd/ca.key</code>"
        },
        {
          "language": "",
          "code": "kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yamlkubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yamlkubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yamlkubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yamlcp -R /etc/kubernetes/pki /tmp/${HOST2}/# cleanup non-reusable certificatesfind /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -deletekubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yamlkubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yamlkubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yamlkubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yamlcp -R /etc/kubernetes/pki /tmp/${HOST1}/find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -deletekubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yamlkubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yamlkubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yamlkubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml# No need to move the certs because they are for HOST0# clean up certs that should not be copied off this hostfind /tmp/${HOST2}-name ca.key -type f -deletefind /tmp/${HOST1}-name ca.key -type f -delete",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubeadm init phase certs etcd-serv..."
        },
        {
          "language": "language-sh",
          "code": "kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yamlkubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yamlkubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yamlkubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yamlcp -R /etc/kubernetes/pki /tmp/${HOST2}/# cleanup non-reusable certificatesfind /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -deletekubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yamlkubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yamlkubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yamlkubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yamlcp -R /etc/kubernetes/pki /tmp/${HOST1}/find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -deletekubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yamlkubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yamlkubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yamlkubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml# No need to move the certs because they are for HOST0# clean up certs that should not be copied off this hostfind /tmp/${HOST2}-name ca.key -type f -deletefind /tmp/${HOST1}-name ca.key -type f -delete",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubeadm init phase certs etcd-server --config<span style=\"color:#666\">=</span>/tmp/<span style=\"color:#b68;font-weight:700\">${..."
        },
        {
          "language": "",
          "code": "USER=ubuntuHOST=${HOST1}scp -r /tmp/${HOST}/*${USER}@${HOST}:ssh${USER}@${HOST}USER@HOST $ sudo -Esroot@HOST $ chown -R root:root pkiroot@HOST $ mv pki /etc/kubernetes/",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">USER</..."
        },
        {
          "language": "language-sh",
          "code": "USER=ubuntuHOST=${HOST1}scp -r /tmp/${HOST}/*${USER}@${HOST}:ssh${USER}@${HOST}USER@HOST $ sudo -Esroot@HOST $ chown -R root:root pkiroot@HOST $ mv pki /etc/kubernetes/",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">USER</span><span style=\"color:#666\">=</span>ubuntu\n</span></span><span style=\"display:flex\"><span>..."
        },
        {
          "language": "",
          "code": "/tmp/${HOST0}\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── ca.key\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key",
          "element": "<pre tabindex=\"0\"><code>/tmp/${HOST0}\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── ca.key\n    ├── healthcheck..."
        },
        {
          "language": "",
          "code": "/tmp/${HOST0}\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── ca.key\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key",
          "element": "<code>/tmp/${HOST0}\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── ca.key\n    ├── healthcheck-client.crt\n    ├─..."
        },
        {
          "language": "",
          "code": "$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key",
          "element": "<pre tabindex=\"0\"><code>$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── hea..."
        },
        {
          "language": "",
          "code": "$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key",
          "element": "<code>$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.ke..."
        },
        {
          "language": "",
          "code": "$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key",
          "element": "<pre tabindex=\"0\"><code>$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── hea..."
        },
        {
          "language": "",
          "code": "$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key",
          "element": "<code>$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.ke..."
        },
        {
          "language": "",
          "code": "root@HOST0 $ kubeadm init phase etcdlocal--config=/tmp/${HOST0}/kubeadmcfg.yamlroot@HOST1 $ kubeadm init phase etcdlocal--config=$HOME/kubeadmcfg.yamlroot@HOST2 $ kubeadm init phase etcdlocal--config=$HOME/kubeadmcfg.yaml",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>root@HOST0 $ kubeadm init phase et..."
        },
        {
          "language": "language-sh",
          "code": "root@HOST0 $ kubeadm init phase etcdlocal--config=/tmp/${HOST0}/kubeadmcfg.yamlroot@HOST1 $ kubeadm init phase etcdlocal--config=$HOME/kubeadmcfg.yamlroot@HOST2 $ kubeadm init phase etcdlocal--config=$HOME/kubeadmcfg.yaml",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>root@HOST0 $ kubeadm init phase etcd <span style=\"color:#a2f\">local</span> --config<span style=\"color:#666\">=</span>/tmp/<span..."
        },
        {
          "language": "",
          "code": "ETCDCTL_API=3etcdctl\\--cert /etc/kubernetes/pki/etcd/peer.crt\\--key /etc/kubernetes/pki/etcd/peer.key\\--cacert /etc/kubernetes/pki/etcd/ca.crt\\--endpoints https://${HOST0}:2379 endpoint health...https://[HOST0 IP]:2379 is healthy: successfully committed proposal:took=16.283339mshttps://[HOST1 IP]:2379 is healthy: successfully committed proposal:took=19.44402mshttps://[HOST2 IP]:2379 is healthy: successfully committed proposal:took=35.926451ms",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">ETCDCT..."
        },
        {
          "language": "language-sh",
          "code": "ETCDCTL_API=3etcdctl\\--cert /etc/kubernetes/pki/etcd/peer.crt\\--key /etc/kubernetes/pki/etcd/peer.key\\--cacert /etc/kubernetes/pki/etcd/ca.crt\\--endpoints https://${HOST0}:2379 endpoint health...https://[HOST0 IP]:2379 is healthy: successfully committed proposal:took=16.283339mshttps://[HOST1 IP]:2379 is healthy: successfully committed proposal:took=19.44402mshttps://[HOST2 IP]:2379 is healthy: successfully committed proposal:took=35.926451ms",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">ETCDCTL_API</span><span style=\"color:#666\">=</span><span style=\"color:#666\">3</span> etcdctl <span..."
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Set up a High Availability etcd Cluster with kubeadm",
          "id": ""
        },
        {
          "level": 2,
          "text": "Before you begin",
          "id": "before-you-begin"
        },
        {
          "level": 2,
          "text": "Setting up the cluster",
          "id": "setting-up-the-cluster"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731682.855318
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/",
      "title": "Configuring each kubelet in your cluster using kubeadm | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nConfiguring each kubelet in your cluster using kubeadm\nConfiguring each kubelet in your cluster using kubeadm\nNote:\nDockershim has been removed from the Kubernetes project as of release 1.24. Read the\nDockershim Removal FAQ\nfor further details.\nFEATURE STATE:\nKubernetes v1.11 [stable]\nThe lifecycle of the kubeadm CLI tool is decoupled from the\nkubelet\n, which is a daemon that runs\non each node within the Kubernetes cluster. The kubeadm CLI tool is executed by the user when Kubernetes is\ninitialized or upgraded, whereas the kubelet is always running in the background.\nSince the kubelet is a daemon, it needs to be maintained by some kind of an init\nsystem or service manager. When the kubelet is installed using DEBs or RPMs,\nsystemd is configured to manage the kubelet. You can use a different service\nmanager instead, but you need to configure it manually.\nSome kubelet configuration details need to be the same across all kubelets involved in the cluster, while\nother configuration aspects need to be set on a per-kubelet basis to accommodate the different\ncharacteristics of a given machine (such as OS, storage, and networking). You can manage the configuration\nof your kubelets manually, but kubeadm now provides a\nKubeletConfiguration\nAPI type for\nmanaging your kubelet configurations centrally\n.\nKubelet configuration patterns\nThe following sections describe patterns to kubelet configuration that are simplified by\nusing kubeadm, rather than managing the kubelet configuration for each Node manually.\nPropagating cluster-level configuration to each kubelet\nYou can provide the kubelet with default values to be used by\nkubeadm init\nand\nkubeadm join\ncommands. Interesting examples include using a different container runtime or setting the default subnet\nused by services.\nIf you want your services to use the subnet\n10.96.0.0/12\nas the default for services, you can pass\nthe\n--service-cidr\nparameter to kubeadm:\nkubeadm init --service-cidr 10.96.0.0/12\nVirtual IPs for services are now allocated from this subnet. You also need to set the DNS address used\nby the kubelet, using the\n--cluster-dns\nflag. This setting needs to be the same for every kubelet\non every manager and Node in the cluster. The kubelet provides a versioned, structured API object\nthat can configure most parameters in the kubelet and push out this configuration to each running\nkubelet in the cluster. This object is called\nKubeletConfiguration\n.\nThe\nKubeletConfiguration\nallows the user to specify flags such as the cluster DNS IP addresses expressed as\na list of values to a camelCased key, illustrated by the following example:\napiVersion\n:\nkubelet.config.k8s.io/v1beta1\nkind\n:\nKubeletConfiguration\nclusterDNS\n:\n-\n10.96.0.10\nFor more details on the\nKubeletConfiguration\nhave a look at\nthis section\n.\nProviding instance-specific configuration details\nSome hosts require specific kubelet configurations due to differences in hardware, operating system,\nnetworking, or other host-specific parameters. The following list provides a few examples.\nThe path to the DNS resolution file, as specified by the\n--resolv-conf\nkubelet\nconfiguration flag, may differ among operating systems, or depending on whether you are using\nsystemd-resolved\n. If this path is wrong, DNS resolution will fail on the Node whose kubelet\nis configured incorrectly.\nThe Node API object\n.metadata.name\nis set to the machine's hostname by default,\nunless you are using a cloud provider. You can use the\n--hostname-override\nflag to override the\ndefault behavior if you need to specify a Node name different from the machine's hostname.\nCurrently, the kubelet cannot automatically detect the cgroup driver used by the container runtime,\nbut the value of\n--cgroup-driver\nmust match the cgroup driver used by the container runtime to ensure\nthe health of the kubelet.\nTo specify the container runtime you must set its endpoint with the\n--container-runtime-endpoint=<path>\nflag.\nThe recommended way of applying such instance-specific configuration is by using\nKubeletConfiguration\npatches\n.\nConfigure kubelets using kubeadm\nIt is possible to configure the kubelet that kubeadm will start if a custom\nKubeletConfiguration\nAPI object is passed with a configuration file like so\nkubeadm ... --config some-config-file.yaml\n.\nBy calling\nkubeadm config print init-defaults --component-configs KubeletConfiguration\nyou can\nsee all the default values for this structure.\nIt is also possible to apply instance-specific patches over the base\nKubeletConfiguration\n.\nHave a look at\nCustomizing the kubelet\nfor more details.\nWorkflow when using\nkubeadm init\nWhen you call\nkubeadm init\n, the kubelet configuration is marshalled to disk\nat\n/var/lib/kubelet/config.yaml\n, and also uploaded to a\nkubelet-config\nConfigMap in the\nkube-system\nnamespace of the cluster. A kubelet configuration file is also written to\n/etc/kubernetes/kubelet.conf\nwith the baseline cluster-wide configuration for all kubelets in the cluster. This configuration file\npoints to the client certificates that allow the kubelet to communicate with the API server. This\naddresses the need to\npropagate cluster-level configuration to each kubelet\n.\nTo address the second pattern of\nproviding instance-specific configuration details\n,\nkubeadm writes an environment file to\n/var/lib/kubelet/kubeadm-flags.env\n, which contains a list of\nflags to pass to the kubelet when it starts. The flags are presented in the file like this:\nKUBELET_KUBEADM_ARGS\n=\n\"--flag1=value1 --flag2=value2 ...\"\nIn addition to the flags used when starting the kubelet, the file also contains dynamic\nparameters such as the cgroup driver and whether to use a different container runtime socket\n(\n--cri-socket\n).\nAfter marshalling these two files to disk, kubeadm attempts to run the following two\ncommands, if you are using systemd:\nsystemctl daemon-reload\n&&\nsystemctl restart kubelet\nIf the reload and restart are successful, the normal\nkubeadm init\nworkflow continues.\nWorkflow when using\nkubeadm join\nWhen you run\nkubeadm join\n, kubeadm uses the Bootstrap Token credential to perform\na TLS bootstrap, which fetches the credential needed to download the\nkubelet-config\nConfigMap and writes it to\n/var/lib/kubelet/config.yaml\n. The dynamic\nenvironment file is generated in exactly the same way as\nkubeadm init\n.\nNext,\nkubeadm\nruns the following two commands to load the new configuration into the kubelet:\nsystemctl daemon-reload\n&&\nsystemctl restart kubelet\nAfter the kubelet loads the new configuration, kubeadm writes the\n/etc/kubernetes/bootstrap-kubelet.conf\nKubeConfig file, which contains a CA certificate and Bootstrap\nToken. These are used by the kubelet to perform the TLS Bootstrap and obtain a unique\ncredential, which is stored in\n/etc/kubernetes/kubelet.conf\n.\nWhen the\n/etc/kubernetes/kubelet.conf\nfile is written, the kubelet has finished performing the TLS Bootstrap.\nKubeadm deletes the\n/etc/kubernetes/bootstrap-kubelet.conf\nfile after completing the TLS Bootstrap.\nThe kubelet drop-in file for systemd\nkubeadm\nships with configuration for how systemd should run the kubelet.\nNote that the kubeadm CLI command never touches this drop-in file.\nThis configuration file installed by the\nkubeadm\npackage\nis written to\n/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf\nand is used by systemd.\nIt augments the basic\nkubelet.service\n.\nIf you want to override that further, you can make a directory\n/etc/systemd/system/kubelet.service.d/\n(not\n/usr/lib/systemd/system/kubelet.service.d/\n) and put your own customizations into a file there.\nFor example, you might add a new local file\n/etc/systemd/system/kubelet.service.d/local-overrides.conf\nto override the unit settings configured by\nkubeadm\n.\nHere is what you are likely to find in\n/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf\n:\nNote:\nThe contents below are just an example. If you don't want to use a package manager\nfollow the guide outlined in the (\nWithout a package manager\n)\nsection.\n[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\"\nEnvironment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\"\n# This is a file that \"kubeadm init\" and \"kubeadm join\" generate at runtime, populating\n# the KUBELET_KUBEADM_ARGS variable dynamically\nEnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env\n# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably,\n# the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead.\n# KUBELET_EXTRA_ARGS should be sourced from this file.\nEnvironmentFile=-/etc/default/kubelet\nExecStart=\nExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS\nThis file specifies the default locations for all of the files managed by kubeadm for the kubelet.\nThe KubeConfig file to use for the TLS Bootstrap is\n/etc/kubernetes/bootstrap-kubelet.conf\n,\nbut it is only used if\n/etc/kubernetes/kubelet.conf\ndoes not exist.\nThe KubeConfig file with the unique kubelet identity is\n/etc/kubernetes/kubelet.conf\n.\nThe file containing the kubelet's ComponentConfig is\n/var/lib/kubelet/config.yaml\n.\nThe dynamic environment file that contains\nKUBELET_KUBEADM_ARGS\nis sourced from\n/var/lib/kubelet/kubeadm-flags.env\n.\nThe file that can contain user-specified flag overrides with\nKUBELET_EXTRA_ARGS\nis sourced from\n/etc/default/kubelet\n(for DEBs), or\n/etc/sysconfig/kubelet\n(for RPMs).\nKUBELET_EXTRA_ARGS\nis last in the flag chain and has the highest priority in the event of conflicting settings.\nKubernetes binaries and package contents\nThe DEB and RPM packages shipped with the Kubernetes releases are:\nPackage name\nDescription\nkubeadm\nInstalls the\n/usr/bin/kubeadm\nCLI tool and the\nkubelet drop-in file\nfor the kubelet.\nkubelet\nInstalls the\n/usr/bin/kubelet\nbinary.\nkubectl\nInstalls the\n/usr/bin/kubectl\nbinary.\ncri-tools\nInstalls the\n/usr/bin/crictl\nbinary from the\ncri-tools git repository\n.\nkubernetes-cni\nInstalls the\n/opt/cni/bin\nbinaries from the\nplugins git repository\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified March 12, 2024 at 8:18 PM PST:\nFix path of kubelet systemd config file (6b2e5dfac6)",
      "code_examples": [
        {
          "language": "",
          "code": "Kubernetes v1.11 [stable]",
          "element": "<code>Kubernetes v1.11 [stable]</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "10.96.0.0/12",
          "element": "<code>10.96.0.0/12</code>"
        },
        {
          "language": "",
          "code": "--service-cidr",
          "element": "<code>--service-cidr</code>"
        },
        {
          "language": "",
          "code": "kubeadm init --service-cidr 10.96.0.0/12",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubeadm init --service-cidr 10..."
        },
        {
          "language": "language-bash",
          "code": "kubeadm init --service-cidr 10.96.0.0/12",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>kubeadm init --service-cidr 10.96.0.0/12\n</span></span></code>"
        },
        {
          "language": "",
          "code": "--cluster-dns",
          "element": "<code>--cluster-dns</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "apiVersion:kubelet.config.k8s.io/v1beta1kind:KubeletConfigurationclusterDNS:-10.96.0.10",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubelet.config.k8s.io/v1beta1kind:KubeletConfigurationclusterDNS:-10.96.0.10",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubelet.config.k8s.io/v1beta1..."
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "--resolv-conf",
          "element": "<code>--resolv-conf</code>"
        },
        {
          "language": "",
          "code": "systemd-resolved",
          "element": "<code>systemd-resolved</code>"
        },
        {
          "language": "",
          "code": ".metadata.name",
          "element": "<code>.metadata.name</code>"
        },
        {
          "language": "",
          "code": "--hostname-override",
          "element": "<code>--hostname-override</code>"
        },
        {
          "language": "",
          "code": "--cgroup-driver",
          "element": "<code>--cgroup-driver</code>"
        },
        {
          "language": "",
          "code": "--container-runtime-endpoint=<path>",
          "element": "<code>--container-runtime-endpoint=&lt;path&gt;</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeadm ... --config some-config-file.yaml",
          "element": "<code>kubeadm ... --config some-config-file.yaml</code>"
        },
        {
          "language": "",
          "code": "kubeadm config print init-defaults --component-configs KubeletConfiguration",
          "element": "<code>kubeadm config print init-defaults --component-configs KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "KubeletConfiguration",
          "element": "<code>KubeletConfiguration</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet/config.yaml",
          "element": "<code>/var/lib/kubelet/config.yaml</code>"
        },
        {
          "language": "",
          "code": "kubelet-config",
          "element": "<code>kubelet-config</code>"
        },
        {
          "language": "",
          "code": "kube-system",
          "element": "<code>kube-system</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/kubelet.conf",
          "element": "<code>/etc/kubernetes/kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet/kubeadm-flags.env",
          "element": "<code>/var/lib/kubelet/kubeadm-flags.env</code>"
        },
        {
          "language": "",
          "code": "KUBELET_KUBEADM_ARGS=\"--flag1=value1 --flag2=value2 ...\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">KU..."
        },
        {
          "language": "language-bash",
          "code": "KUBELET_KUBEADM_ARGS=\"--flag1=value1 --flag2=value2 ...\"",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#b8860b\">KUBELET_KUBEADM_ARGS</span><span style=\"color:#666\">=</span><span style=\"color:#b44\">\"--flag1=..."
        },
        {
          "language": "",
          "code": "--cri-socket",
          "element": "<code>--cri-socket</code>"
        },
        {
          "language": "",
          "code": "systemctl daemon-reload&&systemctl restart kubelet",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>systemctl daemon-reload <span ..."
        },
        {
          "language": "language-bash",
          "code": "systemctl daemon-reload&&systemctl restart kubelet",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>systemctl daemon-reload <span style=\"color:#666\">&amp;&amp;</span> systemctl restart kubelet\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "kubelet-config",
          "element": "<code>kubelet-config</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet/config.yaml",
          "element": "<code>/var/lib/kubelet/config.yaml</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "systemctl daemon-reload&&systemctl restart kubelet",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>systemctl daemon-reload <span ..."
        },
        {
          "language": "language-bash",
          "code": "systemctl daemon-reload&&systemctl restart kubelet",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>systemctl daemon-reload <span style=\"color:#666\">&amp;&amp;</span> systemctl restart kubelet\n</span></span></code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/bootstrap-kubelet.conf",
          "element": "<code>/etc/kubernetes/bootstrap-kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/kubelet.conf",
          "element": "<code>/etc/kubernetes/kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/kubelet.conf",
          "element": "<code>/etc/kubernetes/kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/bootstrap-kubelet.conf",
          "element": "<code>/etc/kubernetes/bootstrap-kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf",
          "element": "<code>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</code>"
        },
        {
          "language": "",
          "code": "kubelet.service",
          "element": "<code>kubelet.service</code>"
        },
        {
          "language": "",
          "code": "/etc/systemd/system/kubelet.service.d/",
          "element": "<code>/etc/systemd/system/kubelet.service.d/</code>"
        },
        {
          "language": "",
          "code": "/usr/lib/systemd/system/kubelet.service.d/",
          "element": "<code>/usr/lib/systemd/system/kubelet.service.d/</code>"
        },
        {
          "language": "",
          "code": "/etc/systemd/system/kubelet.service.d/local-overrides.conf",
          "element": "<code>/etc/systemd/system/kubelet.service.d/local-overrides.conf</code>"
        },
        {
          "language": "",
          "code": "/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf",
          "element": "<code>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</code>"
        },
        {
          "language": "",
          "code": "[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\"\nEnvironment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\"\n# This is a file that \"kubeadm init\" and \"kubeadm join\" generate at runtime, populating\n# the KUBELET_KUBEADM_ARGS variable dynamically\nEnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env\n# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably,\n# the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead.\n# KUBELET_EXTRA_ARGS should be sourced from this file.\nEnvironmentFile=-/etc/default/kubelet\nExecStart=\nExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS",
          "element": "<pre tabindex=\"0\"><code class=\"language-none\" data-lang=\"none\">[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes..."
        },
        {
          "language": "language-none",
          "code": "[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\"\nEnvironment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\"\n# This is a file that \"kubeadm init\" and \"kubeadm join\" generate at runtime, populating\n# the KUBELET_KUBEADM_ARGS variable dynamically\nEnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env\n# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably,\n# the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead.\n# KUBELET_EXTRA_ARGS should be sourced from this file.\nEnvironmentFile=-/etc/default/kubelet\nExecStart=\nExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS",
          "element": "<code class=\"language-none\" data-lang=\"none\">[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\"\nEnv..."
        },
        {
          "language": "",
          "code": "/etc/kubernetes/bootstrap-kubelet.conf",
          "element": "<code>/etc/kubernetes/bootstrap-kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/kubelet.conf",
          "element": "<code>/etc/kubernetes/kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/kubelet.conf",
          "element": "<code>/etc/kubernetes/kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet/config.yaml",
          "element": "<code>/var/lib/kubelet/config.yaml</code>"
        },
        {
          "language": "",
          "code": "KUBELET_KUBEADM_ARGS",
          "element": "<code>KUBELET_KUBEADM_ARGS</code>"
        },
        {
          "language": "",
          "code": "/var/lib/kubelet/kubeadm-flags.env",
          "element": "<code>/var/lib/kubelet/kubeadm-flags.env</code>"
        },
        {
          "language": "",
          "code": "KUBELET_EXTRA_ARGS",
          "element": "<code>KUBELET_EXTRA_ARGS</code>"
        },
        {
          "language": "",
          "code": "/etc/default/kubelet",
          "element": "<code>/etc/default/kubelet</code>"
        },
        {
          "language": "",
          "code": "/etc/sysconfig/kubelet",
          "element": "<code>/etc/sysconfig/kubelet</code>"
        },
        {
          "language": "",
          "code": "KUBELET_EXTRA_ARGS",
          "element": "<code>KUBELET_EXTRA_ARGS</code>"
        },
        {
          "language": "",
          "code": "/usr/bin/kubeadm",
          "element": "<code>/usr/bin/kubeadm</code>"
        },
        {
          "language": "",
          "code": "/usr/bin/kubelet",
          "element": "<code>/usr/bin/kubelet</code>"
        },
        {
          "language": "",
          "code": "/usr/bin/kubectl",
          "element": "<code>/usr/bin/kubectl</code>"
        },
        {
          "language": "",
          "code": "/usr/bin/crictl",
          "element": "<code>/usr/bin/crictl</code>"
        },
        {
          "language": "",
          "code": "kubernetes-cni",
          "element": "<code>kubernetes-cni</code>"
        },
        {
          "language": "",
          "code": "/opt/cni/bin",
          "element": "<code>/opt/cni/bin</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Configuring each kubelet in your cluster using kubeadm",
          "id": ""
        },
        {
          "level": 2,
          "text": "Kubelet configuration patterns",
          "id": "kubelet-configuration-patterns"
        },
        {
          "level": 2,
          "text": "Configure kubelets using kubeadm",
          "id": "configure-kubelets-using-kubeadm"
        },
        {
          "level": 2,
          "text": "The kubelet drop-in file for systemd",
          "id": "the-kubelet-drop-in-file-for-systemd"
        },
        {
          "level": 2,
          "text": "Kubernetes binaries and package contents",
          "id": "kubernetes-binaries-and-package-contents"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Propagating cluster-level configuration to each kubelet",
          "id": "propagating-cluster-level-configuration-to-each-kubelet"
        },
        {
          "level": 3,
          "text": "Providing instance-specific configuration details",
          "id": "providing-instance-specific-configuration-details"
        },
        {
          "level": 3,
          "text": "Workflow when usingkubeadm init",
          "id": "workflow-when-using-kubeadm-init"
        },
        {
          "level": 3,
          "text": "Workflow when usingkubeadm join",
          "id": "workflow-when-using-kubeadm-join"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731686.245135
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/dual-stack-support/",
      "title": "Dual-stack support with kubeadm | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nDual-stack support with kubeadm\nDual-stack support with kubeadm\nFEATURE STATE:\nKubernetes v1.23 [stable]\nYour Kubernetes cluster includes\ndual-stack\nnetworking, which means that cluster networking lets you use either address family.\nIn a cluster, the control plane can assign both an IPv4 address and an IPv6 address to a single\nPod\nor a\nService\n.\nBefore you begin\nYou need to have installed the\nkubeadm\ntool,\nfollowing the steps from\nInstalling kubeadm\n.\nFor each server that you want to use as a\nnode\n,\nmake sure it allows IPv6 forwarding.\nEnable IPv6 packet forwarding\nTo check if IPv6 packet forwarding is enabled:\nsysctl net.ipv6.conf.all.forwarding\nIf the output is\nnet.ipv6.conf.all.forwarding = 1\nit is already enabled.\nOtherwise it is not enabled yet.\nTo manually enable IPv6 packet forwarding:\n# sysctl params required by setup, params persist across reboots\ncat\n<<EOF | sudo tee -a /etc/sysctl.d/k8s.conf\nnet.ipv6.conf.all.forwarding = 1\nEOF\n# Apply sysctl params without reboot\nsudo sysctl --system\nYou need to have an IPv4 and and IPv6 address range to use. Cluster operators typically\nuse private address ranges for IPv4. For IPv6, a cluster operator typically chooses a global\nunicast address block from within\n2000::/3\n, using a range that is assigned to the operator.\nYou don't have to route the cluster's IP address ranges to the public internet.\nThe size of the IP address allocations should be suitable for the number of Pods and\nServices that you are planning to run.\nNote:\nIf you are upgrading an existing cluster with the\nkubeadm upgrade\ncommand,\nkubeadm\ndoes not support making modifications to the pod IP address range\n(“cluster CIDR”) nor to the cluster's Service address range (“Service CIDR”).\nCreate a dual-stack cluster\nTo create a dual-stack cluster with\nkubeadm init\nyou can pass command line arguments\nsimilar to the following example:\n# These address ranges are examples\nkubeadm init --pod-network-cidr\n=\n10.244.0.0/16,2001:db8:42:0::/56 --service-cidr\n=\n10.96.0.0/16,2001:db8:42:1::/112\nTo make things clearer, here is an example kubeadm\nconfiguration file\nkubeadm-config.yaml\nfor the primary dual-stack control plane node.\n---\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nnetworking\n:\npodSubnet\n:\n10.244.0.0\n/16,2001:db8:42:0::/56\nserviceSubnet\n:\n10.96.0.0\n/16,2001:db8:42:1::/112\n---\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nInitConfiguration\nlocalAPIEndpoint\n:\nadvertiseAddress\n:\n\"10.100.0.1\"\nbindPort\n:\n6443\nnodeRegistration\n:\nkubeletExtraArgs\n:\n-\nname\n:\n\"node-ip\"\nvalue\n:\n\"10.100.0.2,fd00:1:2:3::2\"\nadvertiseAddress\nin InitConfiguration specifies the IP address that the API Server\nwill advertise it is listening on. The value of\nadvertiseAddress\nequals the\n--apiserver-advertise-address\nflag of\nkubeadm init\n.\nRun kubeadm to initiate the dual-stack control plane node:\nkubeadm init --config\n=\nkubeadm-config.yaml\nThe kube-controller-manager flags\n--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6\nare set with default values. See\nconfigure IPv4/IPv6 dual stack\n.\nNote:\nThe\n--apiserver-advertise-address\nflag does not support dual-stack.\nJoin a node to dual-stack cluster\nBefore joining a node, make sure that the node has IPv6 routable network interface and allows IPv6 forwarding.\nHere is an example kubeadm\nconfiguration file\nkubeadm-config.yaml\nfor joining a worker node to the cluster.\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nJoinConfiguration\ndiscovery\n:\nbootstrapToken\n:\napiServerEndpoint\n:\n10.100.0.1\n:\n6443\ntoken\n:\n\"clvldh.vjjwg16ucnhp94qr\"\ncaCertHashes\n:\n-\n\"sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e\"\n# change auth info above to match the actual token and CA certificate hash for your cluster\nnodeRegistration\n:\nkubeletExtraArgs\n:\n-\nname\n:\n\"node-ip\"\nvalue\n:\n\"10.100.0.2,fd00:1:2:3::3\"\nAlso, here is an example kubeadm\nconfiguration file\nkubeadm-config.yaml\nfor joining another control plane node to the cluster.\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nJoinConfiguration\ncontrolPlane\n:\nlocalAPIEndpoint\n:\nadvertiseAddress\n:\n\"10.100.0.2\"\nbindPort\n:\n6443\ndiscovery\n:\nbootstrapToken\n:\napiServerEndpoint\n:\n10.100.0.1\n:\n6443\ntoken\n:\n\"clvldh.vjjwg16ucnhp94qr\"\ncaCertHashes\n:\n-\n\"sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e\"\n# change auth info above to match the actual token and CA certificate hash for your cluster\nnodeRegistration\n:\nkubeletExtraArgs\n:\n-\nname\n:\n\"node-ip\"\nvalue\n:\n\"10.100.0.2,fd00:1:2:3::4\"\nadvertiseAddress\nin JoinConfiguration.controlPlane specifies the IP address that the\nAPI Server will advertise it is listening on. The value of\nadvertiseAddress\nequals\nthe\n--apiserver-advertise-address\nflag of\nkubeadm join\n.\nkubeadm join --config\n=\nkubeadm-config.yaml\nCreate a single-stack cluster\nNote:\nDual-stack support doesn't mean that you need to use dual-stack addressing.\nYou can deploy a single-stack cluster that has the dual-stack networking feature enabled.\nTo make things more clear, here is an example kubeadm\nconfiguration file\nkubeadm-config.yaml\nfor the single-stack control plane node.\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nnetworking\n:\npodSubnet\n:\n10.244.0.0\n/16\nserviceSubnet\n:\n10.96.0.0\n/16\nWhat's next\nValidate IPv4/IPv6 dual-stack\nnetworking\nRead about\nDual-stack\ncluster networking\nLearn more about the kubeadm\nconfiguration format\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 28, 2024 at 1:01 PM PST:\nupdate how to persist enabling ipv6 forwarding across reboot (85e4a6c251)",
      "code_examples": [
        {
          "language": "",
          "code": "Kubernetes v1.23 [stable]",
          "element": "<code>Kubernetes v1.23 [stable]</code>"
        },
        {
          "language": "",
          "code": "sysctl net.ipv6.conf.all.forwarding",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>sysctl net.ipv6.conf.all.forwa..."
        },
        {
          "language": "language-bash",
          "code": "sysctl net.ipv6.conf.all.forwarding",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span>sysctl net.ipv6.conf.all.forwarding\n</span></span></code>"
        },
        {
          "language": "",
          "code": "net.ipv6.conf.all.forwarding = 1",
          "element": "<code>net.ipv6.conf.all.forwarding = 1</code>"
        },
        {
          "language": "",
          "code": "# sysctl params required by setup, params persist across rebootscat<<EOF | sudo tee -a /etc/sysctl.d/k8s.confnet.ipv6.conf.all.forwarding = 1EOF# Apply sysctl params without rebootsudo sysctl --system",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#080;font-s..."
        },
        {
          "language": "language-bash",
          "code": "# sysctl params required by setup, params persist across rebootscat<<EOF | sudo tee -a /etc/sysctl.d/k8s.confnet.ipv6.conf.all.forwarding = 1EOF# Apply sysctl params without rebootsudo sysctl --system",
          "element": "<code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex\"><span><span style=\"color:#080;font-style:italic\"># sysctl params required by setup, params persist across reboots</span>\n</span>..."
        },
        {
          "language": "",
          "code": "kubeadm upgrade",
          "element": "<code>kubeadm upgrade</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "# These address ranges are exampleskubeadm init --pod-network-cidr=10.244.0.0/16,2001:db8:42:0::/56 --service-cidr=10.96.0.0/16,2001:db8:42:1::/112",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font..."
        },
        {
          "language": "language-shell",
          "code": "# These address ranges are exampleskubeadm init --pod-network-cidr=10.244.0.0/16,2001:db8:42:0::/56 --service-cidr=10.96.0.0/16,2001:db8:42:1::/112",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font-style:italic\"># These address ranges are examples</span>\n</span></span><span style=\"display..."
        },
        {
          "language": "",
          "code": "kubeadm-config.yaml",
          "element": "<code>kubeadm-config.yaml</code>"
        },
        {
          "language": "",
          "code": "---apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationnetworking:podSubnet:10.244.0.0/16,2001:db8:42:0::/56serviceSubnet:10.96.0.0/16,2001:db8:42:1::/112---apiVersion:kubeadm.k8s.io/v1beta4kind:InitConfigurationlocalAPIEndpoint:advertiseAddress:\"10.100.0.1\"bindPort:6443nodeRegistration:kubeletExtraArgs:-name:\"node-ip\"value:\"10.100.0.2,fd00:1:2:3::2\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:#00f;font-w..."
        },
        {
          "language": "language-yaml",
          "code": "---apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationnetworking:podSubnet:10.244.0.0/16,2001:db8:42:0::/56serviceSubnet:10.96.0.0/16,2001:db8:42:1::/112---apiVersion:kubeadm.k8s.io/v1beta4kind:InitConfigurationlocalAPIEndpoint:advertiseAddress:\"10.100.0.1\"bindPort:6443nodeRegistration:kubeletExtraArgs:-name:\"node-ip\"value:\"10.100.0.2,fd00:1:2:3::2\"",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:#00f;font-weight:700\">---</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:fle..."
        },
        {
          "language": "",
          "code": "advertiseAddress",
          "element": "<code>advertiseAddress</code>"
        },
        {
          "language": "",
          "code": "advertiseAddress",
          "element": "<code>advertiseAddress</code>"
        },
        {
          "language": "",
          "code": "--apiserver-advertise-address",
          "element": "<code>--apiserver-advertise-address</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "kubeadm init --config=kubeadm-config.yaml",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>kubeadm init --config<span s..."
        },
        {
          "language": "language-shell",
          "code": "kubeadm init --config=kubeadm-config.yaml",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>kubeadm init --config<span style=\"color:#666\">=</span>kubeadm-config.yaml\n</span></span></code>"
        },
        {
          "language": "",
          "code": "--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6",
          "element": "<code>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code>"
        },
        {
          "language": "",
          "code": "--apiserver-advertise-address",
          "element": "<code>--apiserver-advertise-address</code>"
        },
        {
          "language": "",
          "code": "kubeadm-config.yaml",
          "element": "<code>kubeadm-config.yaml</code>"
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:JoinConfigurationdiscovery:bootstrapToken:apiServerEndpoint:10.100.0.1:6443token:\"clvldh.vjjwg16ucnhp94qr\"caCertHashes:-\"sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e\"# change auth info above to match the actual token and CA certificate hash for your clusternodeRegistration:kubeletExtraArgs:-name:\"node-ip\"value:\"10.100.0.2,fd00:1:2:3::3\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:JoinConfigurationdiscovery:bootstrapToken:apiServerEndpoint:10.100.0.1:6443token:\"clvldh.vjjwg16ucnhp94qr\"caCertHashes:-\"sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e\"# change auth info above to match the actual token and CA certificate hash for your clusternodeRegistration:kubeletExtraArgs:-name:\"node-ip\"value:\"10.100.0.2,fd00:1:2:3::3\"",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        },
        {
          "language": "",
          "code": "kubeadm-config.yaml",
          "element": "<code>kubeadm-config.yaml</code>"
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:JoinConfigurationcontrolPlane:localAPIEndpoint:advertiseAddress:\"10.100.0.2\"bindPort:6443discovery:bootstrapToken:apiServerEndpoint:10.100.0.1:6443token:\"clvldh.vjjwg16ucnhp94qr\"caCertHashes:-\"sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e\"# change auth info above to match the actual token and CA certificate hash for your clusternodeRegistration:kubeletExtraArgs:-name:\"node-ip\"value:\"10.100.0.2,fd00:1:2:3::4\"",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:JoinConfigurationcontrolPlane:localAPIEndpoint:advertiseAddress:\"10.100.0.2\"bindPort:6443discovery:bootstrapToken:apiServerEndpoint:10.100.0.1:6443token:\"clvldh.vjjwg16ucnhp94qr\"caCertHashes:-\"sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e\"# change auth info above to match the actual token and CA certificate hash for your clusternodeRegistration:kubeletExtraArgs:-name:\"node-ip\"value:\"10.100.0.2,fd00:1:2:3::4\"",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        },
        {
          "language": "",
          "code": "advertiseAddress",
          "element": "<code>advertiseAddress</code>"
        },
        {
          "language": "",
          "code": "advertiseAddress",
          "element": "<code>advertiseAddress</code>"
        },
        {
          "language": "",
          "code": "--apiserver-advertise-address",
          "element": "<code>--apiserver-advertise-address</code>"
        },
        {
          "language": "",
          "code": "kubeadm join",
          "element": "<code>kubeadm join</code>"
        },
        {
          "language": "",
          "code": "kubeadm join --config=kubeadm-config.yaml",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>kubeadm join --config<span s..."
        },
        {
          "language": "language-shell",
          "code": "kubeadm join --config=kubeadm-config.yaml",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>kubeadm join --config<span style=\"color:#666\">=</span>kubeadm-config.yaml\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubeadm-config.yaml",
          "element": "<code>kubeadm-config.yaml</code>"
        },
        {
          "language": "",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationnetworking:podSubnet:10.244.0.0/16serviceSubnet:10.96.0.0/16",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:kubeadm.k8s.io/v1beta4kind:ClusterConfigurationnetworking:podSubnet:10.244.0.0/16serviceSubnet:10.96.0.0/16",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubeadm.k8s.io/v1beta4<span s..."
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Dual-stack support with kubeadm",
          "id": ""
        },
        {
          "level": 2,
          "text": "Before you begin",
          "id": "before-you-begin"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Enable IPv6 packet forwarding",
          "id": "prerequisite-ipv6-forwarding"
        },
        {
          "level": 3,
          "text": "Create a dual-stack cluster",
          "id": "create-a-dual-stack-cluster"
        },
        {
          "level": 3,
          "text": "Join a node to dual-stack cluster",
          "id": "join-a-node-to-dual-stack-cluster"
        },
        {
          "level": 3,
          "text": "Create a single-stack cluster",
          "id": "create-a-single-stack-cluster"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731689.5309298
    },
    {
      "url": "https://kubernetes.io/docs/setup/production-environment/turnkey-solutions/",
      "title": "Turnkey Cloud Solutions | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nProduction environment\nTurnkey Cloud Solutions\nTurnkey Cloud Solutions\nThis page provides a list of Kubernetes certified solution providers. From each\nprovider page, you can learn how to install and setup production\nready clusters.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified January 18, 2024 at 10:32 AM PST:\nUpdate references to CNCF landscape (v2) (26e760da6e)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Turnkey Cloud Solutions",
          "id": ""
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        }
      ],
      "timestamp": 1750731692.716602
    },
    {
      "url": "https://kubernetes.io/docs/setup/best-practices/",
      "title": "Best practices | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nBest practices\nBest practices\nConsiderations for large clusters\nRunning in multiple zones\nValidate node setup\nEnforcing Pod Security Standards\nPKI certificates and requirements\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified June 12, 2019 at 5:27 PM PST:\nRestructure the left navigation pane of setup (#14826) (55ac801bc4)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Best practices",
          "id": ""
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 5,
          "text": "Considerations for large clusters",
          "id": ""
        },
        {
          "level": 5,
          "text": "Running in multiple zones",
          "id": ""
        },
        {
          "level": 5,
          "text": "Validate node setup",
          "id": ""
        },
        {
          "level": 5,
          "text": "Enforcing Pod Security Standards",
          "id": ""
        },
        {
          "level": 5,
          "text": "PKI certificates and requirements",
          "id": ""
        }
      ],
      "timestamp": 1750731695.919928
    },
    {
      "url": "https://kubernetes.io/docs/setup/best-practices/cluster-large/",
      "title": "Considerations for large clusters | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nBest practices\nConsiderations for large clusters\nConsiderations for large clusters\nA cluster is a set of\nnodes\n(physical\nor virtual machines) running Kubernetes agents, managed by the\ncontrol plane\n.\nKubernetes v1.33 supports clusters with up to 5,000 nodes. More specifically,\nKubernetes is designed to accommodate configurations that meet\nall\nof the following criteria:\nNo more than 110 pods per node\nNo more than 5,000 nodes\nNo more than 150,000 total pods\nNo more than 300,000 total containers\nYou can scale your cluster by adding or removing nodes. The way you do this depends\non how your cluster is deployed.\nCloud provider resource quotas\nTo avoid running into cloud provider quota issues, when creating a cluster with many nodes,\nconsider:\nRequesting a quota increase for cloud resources such as:\nComputer instances\nCPUs\nStorage volumes\nIn-use IP addresses\nPacket filtering rule sets\nNumber of load balancers\nNetwork subnets\nLog streams\nGating the cluster scaling actions to bring up new nodes in batches, with a pause\nbetween batches, because some cloud providers rate limit the creation of new instances.\nControl plane components\nFor a large cluster, you need a control plane with sufficient compute and other\nresources.\nTypically you would run one or two control plane instances per failure zone,\nscaling those instances vertically first and then scaling horizontally after reaching\nthe point of falling returns to (vertical) scale.\nYou should run at least one instance per failure zone to provide fault-tolerance. Kubernetes\nnodes do not automatically steer traffic towards control-plane endpoints that are in the\nsame failure zone; however, your cloud provider might have its own mechanisms to do this.\nFor example, using a managed load balancer, you configure the load balancer to send traffic\nthat originates from the kubelet and Pods in failure zone\nA\n, and direct that traffic only\nto the control plane hosts that are also in zone\nA\n. If a single control-plane host or\nendpoint failure zone\nA\ngoes offline, that means that all the control-plane traffic for\nnodes in zone\nA\nis now being sent between zones. Running multiple control plane hosts in\neach zone makes that outcome less likely.\netcd storage\nTo improve performance of large clusters, you can store Event objects in a separate\ndedicated etcd instance.\nWhen creating a cluster, you can (using custom tooling):\nstart and configure additional etcd instance\nconfigure the\nAPI server\nto use it for storing events\nSee\nOperating etcd clusters for Kubernetes\nand\nSet up a High Availability etcd cluster with kubeadm\nfor details on configuring and managing etcd for a large cluster.\nAddon resources\nKubernetes\nresource limits\nhelp to minimize the impact of memory leaks and other ways that pods and containers can\nimpact on other components. These resource limits apply to\naddon\nresources just as they apply to application workloads.\nFor example, you can set CPU and memory limits for a logging component:\n...\ncontainers\n:\n-\nname\n:\nfluentd-cloud-logging\nimage\n:\nfluent/fluentd-kubernetes-daemonset:v1\nresources\n:\nlimits\n:\ncpu\n:\n100m\nmemory\n:\n200Mi\nAddons' default limits are typically based on data collected from experience running\neach addon on small or medium Kubernetes clusters. When running on large\nclusters, addons often consume more of some resources than their default limits.\nIf a large cluster is deployed without adjusting these values, the addon(s)\nmay continuously get killed because they keep hitting the memory limit.\nAlternatively, the addon may run but with poor performance due to CPU time\nslice restrictions.\nTo avoid running into cluster addon resource issues, when creating a cluster with\nmany nodes, consider the following:\nSome addons scale vertically - there is one replica of the addon for the cluster\nor serving a whole failure zone. For these addons, increase requests and limits\nas you scale out your cluster.\nMany addons scale horizontally - you add capacity by running more pods - but with\na very large cluster you may also need to raise CPU or memory limits slightly.\nThe\nVertical Pod Autoscaler\ncan run in\nrecommender\nmode to provide suggested\nfigures for requests and limits.\nSome addons run as one copy per node, controlled by a\nDaemonSet\n: for example, a node-level log aggregator. Similar to\nthe case with horizontally-scaled addons, you may also need to raise CPU or memory\nlimits slightly.\nWhat's next\nVerticalPodAutoscaler\nis a custom resource that you can deploy into your cluster\nto help you manage resource requests and limits for pods.\nLearn more about\nVertical Pod Autoscaler\nand how you can use it to scale cluster\ncomponents, including cluster-critical addons.\nRead about\nNode autoscaling\nThe\naddon resizer\nhelps you in resizing the addons automatically as your cluster's scale changes.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified April 26, 2024 at 3:39 PM PST:\ncluster-autoscaling -> node-autoscaling clean-up (dc530ffd6a)",
      "code_examples": [
        {
          "language": "",
          "code": "...containers:-name:fluentd-cloud-loggingimage:fluent/fluentd-kubernetes-daemonset:v1resources:limits:cpu:100mmemory:200Mi",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:#bbb\">  </s..."
        },
        {
          "language": "language-yaml",
          "code": "...containers:-name:fluentd-cloud-loggingimage:fluent/fluentd-kubernetes-daemonset:v1resources:limits:cpu:100mmemory:200Mi",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:#bbb\">  </span>...<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex\"><span><span..."
        },
        {
          "language": "",
          "code": "VerticalPodAutoscaler",
          "element": "<code>VerticalPodAutoscaler</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Considerations for large clusters",
          "id": ""
        },
        {
          "level": 2,
          "text": "Cloud provider resource quotas",
          "id": "quota-issues"
        },
        {
          "level": 2,
          "text": "Control plane components",
          "id": "control-plane-components"
        },
        {
          "level": 2,
          "text": "Addon resources",
          "id": "addon-resources"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "etcd storage",
          "id": "etcd-storage"
        }
      ],
      "timestamp": 1750731699.445847
    },
    {
      "url": "https://kubernetes.io/docs/setup/best-practices/multiple-zones/",
      "title": "Running in multiple zones | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nBest practices\nRunning in multiple zones\nRunning in multiple zones\nThis page describes running Kubernetes across multiple zones.\nBackground\nKubernetes is designed so that a single Kubernetes cluster can run\nacross multiple failure zones, typically where these zones fit within\na logical grouping called a\nregion\n. Major cloud providers define a region\nas a set of failure zones (also called\navailability zones\n) that provide\na consistent set of features: within a region, each zone offers the same\nAPIs and services.\nTypical cloud architectures aim to minimize the chance that a failure in\none zone also impairs services in another zone.\nControl plane behavior\nAll\ncontrol plane components\nsupport running as a pool of interchangeable resources, replicated per\ncomponent.\nWhen you deploy a cluster control plane, place replicas of\ncontrol plane components across multiple failure zones. If availability is\nan important concern, select at least three failure zones and replicate\neach individual control plane component (API server, scheduler, etcd,\ncluster controller manager) across at least three failure zones.\nIf you are running a cloud controller manager then you should\nalso replicate this across all the failure zones you selected.\nNote:\nKubernetes does not provide cross-zone resilience for the API server\nendpoints. You can use various techniques to improve availability for\nthe cluster API server, including DNS round-robin, SRV records, or\na third-party load balancing solution with health checking.\nNode behavior\nKubernetes automatically spreads the Pods for\nworkload resources (such as\nDeployment\nor\nStatefulSet\n)\nacross different nodes in a cluster. This spreading helps\nreduce the impact of failures.\nWhen nodes start up, the kubelet on each node automatically adds\nlabels\nto the Node object\nthat represents that specific kubelet in the Kubernetes API.\nThese labels can include\nzone information\n.\nIf your cluster spans multiple zones or regions, you can use node labels\nin conjunction with\nPod topology spread constraints\nto control how Pods are spread across your cluster among fault domains:\nregions, zones, and even specific nodes.\nThese hints enable the\nscheduler\nto place\nPods for better expected availability, reducing the risk that a correlated\nfailure affects your whole workload.\nFor example, you can set a constraint to make sure that the\n3 replicas of a StatefulSet are all running in different zones to each\nother, whenever that is feasible. You can define this declaratively\nwithout explicitly defining which availability zones are in use for\neach workload.\nDistributing nodes across zones\nKubernetes' core does not create nodes for you; you need to do that yourself,\nor use a tool such as the\nCluster API\nto\nmanage nodes on your behalf.\nUsing tools such as the Cluster API you can define sets of machines to run as\nworker nodes for your cluster across multiple failure domains, and rules to\nautomatically heal the cluster in case of whole-zone service disruption.\nManual zone assignment for Pods\nYou can apply\nnode selector constraints\nto Pods that you create, as well as to Pod templates in workload resources\nsuch as Deployment, StatefulSet, or Job.\nStorage access for zones\nWhen persistent volumes are created, Kubernetes automatically adds zone labels\nto any PersistentVolumes that are linked to a specific zone.\nThe\nscheduler\nthen ensures,\nthrough its\nNoVolumeZoneConflict\npredicate, that pods which claim a given PersistentVolume\nare only placed into the same zone as that volume.\nPlease note that the method of adding zone labels can depend on your\ncloud provider and the storage provisioner you’re using. Always refer to the specific\ndocumentation for your environment to ensure correct configuration.\nYou can specify a\nStorageClass\nfor PersistentVolumeClaims that specifies the failure domains (zones) that the\nstorage in that class may use.\nTo learn about configuring a StorageClass that is aware of failure domains or zones,\nsee\nAllowed topologies\n.\nNetworking\nBy itself, Kubernetes does not include zone-aware networking. You can use a\nnetwork plugin\nto configure cluster networking, and that network solution might have zone-specific\nelements. For example, if your cloud provider supports Services with\ntype=LoadBalancer\n, the load balancer might only send traffic to Pods running in the\nsame zone as the load balancer element processing a given connection.\nCheck your cloud provider's documentation for details.\nFor custom or on-premises deployments, similar considerations apply.\nService\nand\nIngress\nbehavior, including handling\nof different failure zones, does vary depending on exactly how your cluster is set up.\nFault recovery\nWhen you set up your cluster, you might also need to consider whether and how\nyour setup can restore service if all the failure zones in a region go\noff-line at the same time. For example, do you rely on there being at least\none node able to run Pods in a zone?\nMake sure that any cluster-critical repair work does not rely\non there being at least one healthy node in your cluster. For example: if all nodes\nare unhealthy, you might need to run a repair Job with a special\ntoleration\nso that the repair\ncan complete enough to bring at least one node into service.\nKubernetes doesn't come with an answer for this challenge; however, it's\nsomething to consider.\nWhat's next\nTo learn how the scheduler places Pods in a cluster, honoring the configured constraints,\nvisit\nScheduling and Eviction\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified September 01, 2024 at 1:54 AM PST:\nFix broken links from \"overview/components/#...\" to \"architecture/#...\" (#47724) (7e64c2db82)",
      "code_examples": [
        {
          "language": "",
          "code": "NoVolumeZoneConflict",
          "element": "<code>NoVolumeZoneConflict</code>"
        },
        {
          "language": "",
          "code": "type=LoadBalancer",
          "element": "<code>type=LoadBalancer</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Running in multiple zones",
          "id": ""
        },
        {
          "level": 2,
          "text": "Background",
          "id": "background"
        },
        {
          "level": 2,
          "text": "Control plane behavior",
          "id": "control-plane-behavior"
        },
        {
          "level": 2,
          "text": "Node behavior",
          "id": "node-behavior"
        },
        {
          "level": 2,
          "text": "Manual zone assignment for Pods",
          "id": "manual-zone-assignment-for-pods"
        },
        {
          "level": 2,
          "text": "Storage access for zones",
          "id": "storage-access-for-zones"
        },
        {
          "level": 2,
          "text": "Networking",
          "id": "networking"
        },
        {
          "level": 2,
          "text": "Fault recovery",
          "id": "fault-recovery"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Distributing nodes across zones",
          "id": "distributing-nodes-across-zones"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731702.7292721
    },
    {
      "url": "https://kubernetes.io/docs/setup/best-practices/node-conformance/",
      "title": "Validate node setup | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nBest practices\nValidate node setup\nValidate node setup\nNode Conformance Test\nNode conformance test\nis a containerized test framework that provides a system\nverification and functionality test for a node. The test validates whether the\nnode meets the minimum requirements for Kubernetes; a node that passes the test\nis qualified to join a Kubernetes cluster.\nNode Prerequisite\nTo run node conformance test, a node must satisfy the same prerequisites as a\nstandard Kubernetes node. At a minimum, the node should have the following\ndaemons installed:\nCRI-compatible container runtimes such as Docker, containerd and CRI-O\nkubelet\nRunning Node Conformance Test\nTo run the node conformance test, perform the following steps:\nWork out the value of the\n--kubeconfig\noption for the kubelet; for example:\n--kubeconfig=/var/lib/kubelet/config.yaml\n.\nBecause the test framework starts a local control plane to test the kubelet,\nuse\nhttp://localhost:8080\nas the URL of the API server.\nThere are some other kubelet command line parameters you may want to use:\n--cloud-provider\n: If you are using\n--cloud-provider=gce\n, you should\nremove the flag to run the test.\nRun the node conformance test with command:\n# $CONFIG_DIR is the pod manifest path of your kubelet.\n# $LOG_DIR is the test output path.\nsudo docker run -it --rm --privileged --net\n=\nhost\n\\\n-v /:/rootfs -v\n$CONFIG_DIR\n:\n$CONFIG_DIR\n-v\n$LOG_DIR\n:/var/result\n\\\nregistry.k8s.io/node-test:0.2\nRunning Node Conformance Test for Other Architectures\nKubernetes also provides node conformance test docker images for other\narchitectures:\nArch\nImage\namd64\nnode-test-amd64\narm\nnode-test-arm\narm64\nnode-test-arm64\nRunning Selected Test\nTo run specific tests, overwrite the environment variable\nFOCUS\nwith the\nregular expression of tests you want to run.\nsudo docker run -it --rm --privileged --net\n=\nhost\n\\\n-v /:/rootfs:ro -v\n$CONFIG_DIR\n:\n$CONFIG_DIR\n-v\n$LOG_DIR\n:/var/result\n\\\n-e\nFOCUS\n=\nMirrorPod\n\\\n# Only run MirrorPod test\nregistry.k8s.io/node-test:0.2\nTo skip specific tests, overwrite the environment variable\nSKIP\nwith the\nregular expression of tests you want to skip.\nsudo docker run -it --rm --privileged --net\n=\nhost\n\\\n-v /:/rootfs:ro -v\n$CONFIG_DIR\n:\n$CONFIG_DIR\n-v\n$LOG_DIR\n:/var/result\n\\\n-e\nSKIP\n=\nMirrorPod\n\\\n# Run all conformance tests but skip MirrorPod test\nregistry.k8s.io/node-test:0.2\nNode conformance test is a containerized version of\nnode e2e test\n.\nBy default, it runs all conformance tests.\nTheoretically, you can run any node e2e test if you configure the container and\nmount required volumes properly. But\nit is strongly recommended to only run conformance\ntest\n, because it requires much more complex configuration to run non-conformance test.\nCaveats\nThe test leaves some docker images on the node, including the node conformance\ntest image and images of containers used in the functionality\ntest.\nThe test leaves dead containers on the node. These containers are created\nduring the functionality test.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 17, 2024 at 6:09 PM PST:\nClean up best-practices: certificates and node-conformance (1d24167b04)",
      "code_examples": [
        {
          "language": "",
          "code": "--kubeconfig",
          "element": "<code>--kubeconfig</code>"
        },
        {
          "language": "",
          "code": "--kubeconfig=/var/lib/kubelet/config.yaml",
          "element": "<code>--kubeconfig=/var/lib/kubelet/config.yaml</code>"
        },
        {
          "language": "",
          "code": "http://localhost:8080",
          "element": "<code>http://localhost:8080</code>"
        },
        {
          "language": "",
          "code": "--cloud-provider",
          "element": "<code>--cloud-provider</code>"
        },
        {
          "language": "",
          "code": "--cloud-provider=gce",
          "element": "<code>--cloud-provider=gce</code>"
        },
        {
          "language": "",
          "code": "# $CONFIG_DIR is the pod manifest path of your kubelet.# $LOG_DIR is the test output path.sudo docker run -it --rm --privileged --net=host\\-v /:/rootfs -v$CONFIG_DIR:$CONFIG_DIR-v$LOG_DIR:/var/result\\registry.k8s.io/node-test:0.2",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font..."
        },
        {
          "language": "language-shell",
          "code": "# $CONFIG_DIR is the pod manifest path of your kubelet.# $LOG_DIR is the test output path.sudo docker run -it --rm --privileged --net=host\\-v /:/rootfs -v$CONFIG_DIR:$CONFIG_DIR-v$LOG_DIR:/var/result\\registry.k8s.io/node-test:0.2",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span><span style=\"color:#080;font-style:italic\"># $CONFIG_DIR is the pod manifest path of your kubelet.</span>\n</span></span>..."
        },
        {
          "language": "",
          "code": "sudo docker run -it --rm --privileged --net=host\\-v /:/rootfs:ro -v$CONFIG_DIR:$CONFIG_DIR-v$LOG_DIR:/var/result\\-eFOCUS=MirrorPod\\# Only run MirrorPod testregistry.k8s.io/node-test:0.2",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo docker run -it --rm --p..."
        },
        {
          "language": "language-shell",
          "code": "sudo docker run -it --rm --privileged --net=host\\-v /:/rootfs:ro -v$CONFIG_DIR:$CONFIG_DIR-v$LOG_DIR:/var/result\\-eFOCUS=MirrorPod\\# Only run MirrorPod testregistry.k8s.io/node-test:0.2",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo docker run -it --rm --privileged --net<span style=\"color:#666\">=</span>host <span style=\"color:#b62;font-weight:700..."
        },
        {
          "language": "",
          "code": "sudo docker run -it --rm --privileged --net=host\\-v /:/rootfs:ro -v$CONFIG_DIR:$CONFIG_DIR-v$LOG_DIR:/var/result\\-eSKIP=MirrorPod\\# Run all conformance tests but skip MirrorPod testregistry.k8s.io/node-test:0.2",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo docker run -it --rm --p..."
        },
        {
          "language": "language-shell",
          "code": "sudo docker run -it --rm --privileged --net=host\\-v /:/rootfs:ro -v$CONFIG_DIR:$CONFIG_DIR-v$LOG_DIR:/var/result\\-eSKIP=MirrorPod\\# Run all conformance tests but skip MirrorPod testregistry.k8s.io/node-test:0.2",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>sudo docker run -it --rm --privileged --net<span style=\"color:#666\">=</span>host <span style=\"color:#b62;font-weight:700..."
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Validate node setup",
          "id": ""
        },
        {
          "level": 2,
          "text": "Node Conformance Test",
          "id": "node-conformance-test"
        },
        {
          "level": 2,
          "text": "Node Prerequisite",
          "id": "node-prerequisite"
        },
        {
          "level": 2,
          "text": "Running Node Conformance Test",
          "id": "running-node-conformance-test"
        },
        {
          "level": 2,
          "text": "Running Node Conformance Test for Other Architectures",
          "id": "running-node-conformance-test-for-other-architectures"
        },
        {
          "level": 2,
          "text": "Running Selected Test",
          "id": "running-selected-test"
        },
        {
          "level": 2,
          "text": "Caveats",
          "id": "caveats"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        }
      ],
      "timestamp": 1750731706.6141288
    },
    {
      "url": "https://kubernetes.io/docs/setup/best-practices/enforcing-pod-security-standards/",
      "title": "Enforcing Pod Security Standards | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nBest practices\nEnforcing Pod Security Standards\nEnforcing Pod Security Standards\nThis page provides an overview of best practices when it comes to enforcing\nPod Security Standards\n.\nUsing the built-in Pod Security Admission Controller\nFEATURE STATE:\nKubernetes v1.25 [stable]\nThe\nPod Security Admission Controller\nintends to replace the deprecated PodSecurityPolicies.\nConfigure all cluster namespaces\nNamespaces that lack any configuration at all should be considered significant gaps in your cluster\nsecurity model. We recommend taking the time to analyze the types of workloads occurring in each\nnamespace, and by referencing the Pod Security Standards, decide on an appropriate level for\neach of them. Unlabeled namespaces should only indicate that they've yet to be evaluated.\nIn the scenario that all workloads in all namespaces have the same security requirements,\nwe provide an\nexample\nthat illustrates how the PodSecurity labels can be applied in bulk.\nEmbrace the principle of least privilege\nIn an ideal world, every pod in every namespace would meet the requirements of the\nrestricted\npolicy. However, this is not possible nor practical, as some workloads will require elevated\nprivileges for legitimate reasons.\nNamespaces allowing\nprivileged\nworkloads should establish and enforce appropriate access controls.\nFor workloads running in those permissive namespaces, maintain documentation about their unique\nsecurity requirements. If at all possible, consider how those requirements could be further\nconstrained.\nAdopt a multi-mode strategy\nThe\naudit\nand\nwarn\nmodes of the Pod Security Standards admission controller make it easy to\ncollect important security insights about your pods without breaking existing workloads.\nIt is good practice to enable these modes for all namespaces, setting them to the\ndesired\nlevel\nand version you would eventually like to\nenforce\n. The warnings and audit annotations generated in\nthis phase can guide you toward that state. If you expect workload authors to make changes to fit\nwithin the desired level, enable the\nwarn\nmode. If you expect to use audit logs to monitor/drive\nchanges to fit within the desired level, enable the\naudit\nmode.\nWhen you have the\nenforce\nmode set to your desired value, these modes can still be useful in a\nfew different ways:\nBy setting\nwarn\nto the same level as\nenforce\n, clients will receive warnings when attempting\nto create Pods (or resources that have Pod templates) that do not pass validation. This will help\nthem update those resources to become compliant.\nIn Namespaces that pin\nenforce\nto a specific non-latest version, setting the\naudit\nand\nwarn\nmodes to the same level as\nenforce\n, but to the\nlatest\nversion, gives visibility into settings\nthat were allowed by previous versions but are not allowed per current best practices.\nThird-party alternatives\nNote:\nThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the\ncontent guide\nbefore submitting a change.\nMore information.\nOther alternatives for enforcing security profiles are being developed in the Kubernetes\necosystem:\nKubewarden\n.\nKyverno\n.\nOPA Gatekeeper\n.\nThe decision to go with a\nbuilt-in\nsolution (e.g. PodSecurity admission controller) versus a\nthird-party tool is entirely dependent on your own situation. When evaluating any solution,\ntrust of your supply chain is crucial. Ultimately, using\nany\nof the aforementioned approaches\nwill be better than doing nothing.\nItems on this page refer to third party products or projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. See the\nCNCF website guidelines\nfor more details.\nYou should read the\ncontent guide\nbefore proposing a change that adds an extra third-party link.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified September 22, 2022 at 3:12 PM PST:\nBump the feature state of the Pod Security plugin to stable (1ab76fba82)",
      "code_examples": [
        {
          "language": "",
          "code": "Kubernetes v1.25 [stable]",
          "element": "<code>Kubernetes v1.25 [stable]</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Enforcing Pod Security Standards",
          "id": ""
        },
        {
          "level": 2,
          "text": "Using the built-in Pod Security Admission Controller",
          "id": "using-the-built-in-pod-security-admission-controller"
        },
        {
          "level": 2,
          "text": "Third-party alternatives",
          "id": "third-party-alternatives"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Configure all cluster namespaces",
          "id": "configure-all-cluster-namespaces"
        },
        {
          "level": 3,
          "text": "Embrace the principle of least privilege",
          "id": "embrace-the-principle-of-least-privilege"
        },
        {
          "level": 3,
          "text": "Adopt a multi-mode strategy",
          "id": "adopt-a-multi-mode-strategy"
        }
      ],
      "timestamp": 1750731709.764377
    },
    {
      "url": "https://kubernetes.io/docs/setup/best-practices/certificates/",
      "title": "PKI certificates and requirements | Kubernetes",
      "content": "Kubernetes Documentation\nGetting started\nBest practices\nPKI certificates and requirements\nPKI certificates and requirements\nKubernetes requires PKI certificates for authentication over TLS.\nIf you install Kubernetes with\nkubeadm\n, the certificates\nthat your cluster requires are automatically generated.\nYou can also generate your own certificates -- for example, to keep your private keys more secure\nby not storing them on the API server.\nThis page explains the certificates that your cluster requires.\nHow certificates are used by your cluster\nKubernetes requires PKI for the following operations:\nServer certificates\nServer certificate for the API server endpoint\nServer certificate for the etcd server\nServer certificates\nfor each kubelet (every\nnode\nruns a kubelet)\nOptional server certificate for the\nfront-proxy\nClient certificates\nClient certificates for each kubelet, used to authenticate to the API server as a client of\nthe Kubernetes API\nClient certificate for each API server, used to authenticate to etcd\nClient certificate for the controller manager to securely communicate with the API server\nClient certificate for the scheduler to securely communicate with the API server\nClient certificates, one for each node, for kube-proxy to authenticate to the API server\nOptional client certificates for administrators of the cluster to authenticate to the API server\nOptional client certificate for the\nfront-proxy\nKubelet's server and client certificates\nTo establish a secure connection and authenticate itself to the kubelet, the API Server\nrequires a client certificate and key pair.\nIn this scenario, there are two approaches for certificate usage:\nShared Certificates: The kube-apiserver can utilize the same certificate and key pair it uses\nto authenticate its clients. This means that the existing certificates, such as\napiserver.crt\nand\napiserver.key\n, can be used for communicating with the kubelet servers.\nSeparate Certificates: Alternatively, the kube-apiserver can generate a new client certificate\nand key pair to authenticate its communication with the kubelet servers. In this case,\na distinct certificate named\nkubelet-client.crt\nand its corresponding private key,\nkubelet-client.key\nare created.\nNote:\nfront-proxy\ncertificates are required only if you run kube-proxy to support\nan extension API server\n.\netcd also implements mutual TLS to authenticate clients and peers.\nWhere certificates are stored\nIf you install Kubernetes with kubeadm, most certificates are stored in\n/etc/kubernetes/pki\n.\nAll paths in this documentation are relative to that directory, with the exception of user account\ncertificates which kubeadm places in\n/etc/kubernetes\n.\nConfigure certificates manually\nIf you don't want kubeadm to generate the required certificates, you can create them using a\nsingle root CA or by providing all certificates. See\nCertificates\nfor details on creating your own certificate authority. See\nCertificate Management with kubeadm\nfor more on managing certificates.\nSingle root CA\nYou can create a single root CA, controlled by an administrator. This root CA can then create\nmultiple intermediate CAs, and delegate all further creation to Kubernetes itself.\nRequired CAs:\nPath\nDefault CN\nDescription\nca.crt,key\nkubernetes-ca\nKubernetes general CA\netcd/ca.crt,key\netcd-ca\nFor all etcd-related functions\nfront-proxy-ca.crt,key\nkubernetes-front-proxy-ca\nFor the\nfront-end proxy\nOn top of the above CAs, it is also necessary to get a public/private key pair for service account\nmanagement,\nsa.key\nand\nsa.pub\n.\nThe following example illustrates the CA key and certificate files shown in the previous table:\n/etc/kubernetes/pki/ca.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/front-proxy-ca.crt\n/etc/kubernetes/pki/front-proxy-ca.key\nAll certificates\nIf you don't wish to copy the CA private keys to your cluster, you can generate all certificates yourself.\nRequired certificates:\nDefault CN\nParent CA\nO (in Subject)\nkind\nhosts (SAN)\nkube-etcd\netcd-ca\nserver, client\n<hostname>\n,\n<Host_IP>\n,\nlocalhost\n,\n127.0.0.1\nkube-etcd-peer\netcd-ca\nserver, client\n<hostname>\n,\n<Host_IP>\n,\nlocalhost\n,\n127.0.0.1\nkube-etcd-healthcheck-client\netcd-ca\nclient\nkube-apiserver-etcd-client\netcd-ca\nclient\nkube-apiserver\nkubernetes-ca\nserver\n<hostname>\n,\n<Host_IP>\n,\n<advertise_IP>\n1\nkube-apiserver-kubelet-client\nkubernetes-ca\nsystem:masters\nclient\nfront-proxy-client\nkubernetes-front-proxy-ca\nclient\nNote:\nInstead of using the super-user group\nsystem:masters\nfor\nkube-apiserver-kubelet-client\na less privileged group can be used. kubeadm uses the\nkubeadm:cluster-admins\ngroup for\nthat purpose.\nwhere\nkind\nmaps to one or more of the x509 key usage, which is also documented in the\n.spec.usages\nof a\nCertificateSigningRequest\ntype:\nkind\nKey usage\nserver\ndigital signature, key encipherment, server auth\nclient\ndigital signature, key encipherment, client auth\nNote:\nHosts/SAN listed above are the recommended ones for getting a working cluster; if required by a\nspecific setup, it is possible to add additional SANs on all the server certificates.\nNote:\nFor kubeadm users only:\nThe scenario where you are copying to your cluster CA certificates without private keys is\nreferred as external CA in the kubeadm documentation.\nIf you are comparing the above list with a kubeadm generated PKI, please be aware that\nkube-etcd\n,\nkube-etcd-peer\nand\nkube-etcd-healthcheck-client\ncertificates are not generated\nin case of external etcd.\nCertificate paths\nCertificates should be placed in a recommended path (as used by\nkubeadm\n).\nPaths should be specified using the given argument regardless of location.\nDefaultCN\nrecommendedkeypath\nrecommendedcertpath\ncommand\nkeyargument\ncertargument\netcd-ca\netcd/ca.key\netcd/ca.crt\nkube-apiserver\n--etcd-cafile\nkube-apiserver-etcd-client\napiserver-etcd-client.key\napiserver-etcd-client.crt\nkube-apiserver\n--etcd-keyfile\n--etcd-certfile\nkubernetes-ca\nca.key\nca.crt\nkube-apiserver\n--client-ca-file\nkubernetes-ca\nca.key\nca.crt\nkube-controller-manager\n--cluster-signing-key-file\n--client-ca-file,--root-ca-file,--cluster-signing-cert-file\nkube-apiserver\napiserver.key\napiserver.crt\nkube-apiserver\n--tls-private-key-file\n--tls-cert-file\nkube-apiserver-kubelet-client\napiserver-kubelet-client.key\napiserver-kubelet-client.crt\nkube-apiserver\n--kubelet-client-key\n--kubelet-client-certificate\nfront-proxy-ca\nfront-proxy-ca.key\nfront-proxy-ca.crt\nkube-apiserver\n--requestheader-client-ca-file\nfront-proxy-ca\nfront-proxy-ca.key\nfront-proxy-ca.crt\nkube-controller-manager\n--requestheader-client-ca-file\nfront-proxy-client\nfront-proxy-client.key\nfront-proxy-client.crt\nkube-apiserver\n--proxy-client-key-file\n--proxy-client-cert-file\netcd-ca\netcd/ca.key\netcd/ca.crt\netcd\n--trusted-ca-file,--peer-trusted-ca-file\nkube-etcd\netcd/server.key\netcd/server.crt\netcd\n--key-file\n--cert-file\nkube-etcd-peer\netcd/peer.key\netcd/peer.crt\netcd\n--peer-key-file\n--peer-cert-file\netcd-ca\netcd/ca.crt\netcdctl\n--cacert\nkube-etcd-healthcheck-client\netcd/healthcheck-client.key\netcd/healthcheck-client.crt\netcdctl\n--key\n--cert\nSame considerations apply for the service account key pair:\nprivate key path\npublic key path\ncommand\nargument\nsa.key\nkube-controller-manager\n--service-account-private-key-file\nsa.pub\nkube-apiserver\n--service-account-key-file\nThe following example illustrates the file paths\nfrom the previous tables\nyou need to provide if you are generating all of your own keys and certificates:\n/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/apiserver-etcd-client.key\n/etc/kubernetes/pki/apiserver-etcd-client.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubernetes/pki/ca.crt\n/etc/kubernetes/pki/apiserver.key\n/etc/kubernetes/pki/apiserver.crt\n/etc/kubernetes/pki/apiserver-kubelet-client.key\n/etc/kubernetes/pki/apiserver-kubelet-client.crt\n/etc/kubernetes/pki/front-proxy-ca.key\n/etc/kubernetes/pki/front-proxy-ca.crt\n/etc/kubernetes/pki/front-proxy-client.key\n/etc/kubernetes/pki/front-proxy-client.crt\n/etc/kubernetes/pki/etcd/server.key\n/etc/kubernetes/pki/etcd/server.crt\n/etc/kubernetes/pki/etcd/peer.key\n/etc/kubernetes/pki/etcd/peer.crt\n/etc/kubernetes/pki/etcd/healthcheck-client.key\n/etc/kubernetes/pki/etcd/healthcheck-client.crt\n/etc/kubernetes/pki/sa.key\n/etc/kubernetes/pki/sa.pub\nConfigure certificates for user accounts\nYou must manually configure these administrator accounts and service accounts:\nFilename\nCredential name\nDefault CN\nO (in Subject)\nadmin.conf\ndefault-admin\nkubernetes-admin\n<admin-group>\nsuper-admin.conf\ndefault-super-admin\nkubernetes-super-admin\nsystem:masters\nkubelet.conf\ndefault-auth\nsystem:node:\n<nodeName>\n(see note)\nsystem:nodes\ncontroller-manager.conf\ndefault-controller-manager\nsystem:kube-controller-manager\nscheduler.conf\ndefault-scheduler\nsystem:kube-scheduler\nNote:\nThe value of\n<nodeName>\nfor\nkubelet.conf\nmust\nmatch precisely the value of the node name\nprovided by the kubelet as it registers with the apiserver. For further details, read the\nNode Authorization\n.\nNote:\nIn the above example\n<admin-group>\nis implementation specific. Some tools sign the\ncertificate in the default\nadmin.conf\nto be part of the\nsystem:masters\ngroup.\nsystem:masters\nis a break-glass, super user group can bypass the authorization\nlayer of Kubernetes, such as RBAC. Also some tools do not generate a separate\nsuper-admin.conf\nwith a certificate bound to this super user group.\nkubeadm generates two separate administrator certificates in kubeconfig files.\nOne is in\nadmin.conf\nand has\nSubject: O = kubeadm:cluster-admins, CN = kubernetes-admin\n.\nkubeadm:cluster-admins\nis a custom group bound to the\ncluster-admin\nClusterRole.\nThis file is generated on all kubeadm managed control plane machines.\nAnother is in\nsuper-admin.conf\nthat has\nSubject: O = system:masters, CN = kubernetes-super-admin\n.\nThis file is generated only on the node where\nkubeadm init\nwas called.\nFor each configuration, generate an x509 certificate/key pair with the\ngiven Common Name (CN) and Organization (O).\nRun\nkubectl\nas follows for each configuration:\nKUBECONFIG=<filename> kubectl config set-cluster default-cluster --server=https://<host ip>:6443 --certificate-authority <path-to-kubernetes-ca> --embed-certs\nKUBECONFIG=<filename> kubectl config set-credentials <credential-name> --client-key <path-to-key>.pem --client-certificate <path-to-cert>.pem --embed-certs\nKUBECONFIG=<filename> kubectl config set-context default-system --cluster default-cluster --user <credential-name>\nKUBECONFIG=<filename> kubectl config use-context default-system\nThese files are used as follows:\nFilename\nCommand\nComment\nadmin.conf\nkubectl\nConfigures administrator user for the cluster\nsuper-admin.conf\nkubectl\nConfigures super administrator user for the cluster\nkubelet.conf\nkubelet\nOne required for each node in the cluster.\ncontroller-manager.conf\nkube-controller-manager\nMust be added to manifest in\nmanifests/kube-controller-manager.yaml\nscheduler.conf\nkube-scheduler\nMust be added to manifest in\nmanifests/kube-scheduler.yaml\nThe following files illustrate full paths to the files listed in the previous table:\n/etc/kubernetes/admin.conf\n/etc/kubernetes/super-admin.conf\n/etc/kubernetes/kubelet.conf\n/etc/kubernetes/controller-manager.conf\n/etc/kubernetes/scheduler.conf\nany other IP or DNS name you contact your cluster on (as used by\nkubeadm\nthe load balancer stable IP and/or DNS name,\nkubernetes\n,\nkubernetes.default\n,\nkubernetes.default.svc\n,\nkubernetes.default.svc.cluster\n,\nkubernetes.default.svc.cluster.local\n)\n↩︎\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 17, 2024 at 6:09 PM PST:\nClean up best-practices: certificates and node-conformance (1d24167b04)",
      "code_examples": [
        {
          "language": "",
          "code": "apiserver.crt",
          "element": "<code>apiserver.crt</code>"
        },
        {
          "language": "",
          "code": "apiserver.key",
          "element": "<code>apiserver.key</code>"
        },
        {
          "language": "",
          "code": "kubelet-client.crt",
          "element": "<code>kubelet-client.crt</code>"
        },
        {
          "language": "",
          "code": "kubelet-client.key",
          "element": "<code>kubelet-client.key</code>"
        },
        {
          "language": "",
          "code": "front-proxy",
          "element": "<code>front-proxy</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki",
          "element": "<code>/etc/kubernetes/pki</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes",
          "element": "<code>/etc/kubernetes</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki/ca.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/front-proxy-ca.crt\n/etc/kubernetes/pki/front-proxy-ca.key",
          "element": "<pre tabindex=\"0\"><code>/etc/kubernetes/pki/ca.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/front-proxy-ca.crt\n/etc/kubernetes/pki..."
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki/ca.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/front-proxy-ca.crt\n/etc/kubernetes/pki/front-proxy-ca.key",
          "element": "<code>/etc/kubernetes/pki/ca.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/front-proxy-ca.crt\n/etc/kubernetes/pki/front-proxy-ca.ke..."
        },
        {
          "language": "",
          "code": "<advertise_IP>",
          "element": "<code>&lt;advertise_IP&gt;</code>"
        },
        {
          "language": "",
          "code": "system:masters",
          "element": "<code>system:masters</code>"
        },
        {
          "language": "",
          "code": "kube-apiserver-kubelet-client",
          "element": "<code>kube-apiserver-kubelet-client</code>"
        },
        {
          "language": "",
          "code": "kubeadm:cluster-admins",
          "element": "<code>kubeadm:cluster-admins</code>"
        },
        {
          "language": "",
          "code": ".spec.usages",
          "element": "<code>.spec.usages</code>"
        },
        {
          "language": "",
          "code": "kube-etcd-peer",
          "element": "<code>kube-etcd-peer</code>"
        },
        {
          "language": "",
          "code": "kube-etcd-healthcheck-client",
          "element": "<code>kube-etcd-healthcheck-client</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/apiserver-etcd-client.key\n/etc/kubernetes/pki/apiserver-etcd-client.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubernetes/pki/ca.crt\n/etc/kubernetes/pki/apiserver.key\n/etc/kubernetes/pki/apiserver.crt\n/etc/kubernetes/pki/apiserver-kubelet-client.key\n/etc/kubernetes/pki/apiserver-kubelet-client.crt\n/etc/kubernetes/pki/front-proxy-ca.key\n/etc/kubernetes/pki/front-proxy-ca.crt\n/etc/kubernetes/pki/front-proxy-client.key\n/etc/kubernetes/pki/front-proxy-client.crt\n/etc/kubernetes/pki/etcd/server.key\n/etc/kubernetes/pki/etcd/server.crt\n/etc/kubernetes/pki/etcd/peer.key\n/etc/kubernetes/pki/etcd/peer.crt\n/etc/kubernetes/pki/etcd/healthcheck-client.key\n/etc/kubernetes/pki/etcd/healthcheck-client.crt\n/etc/kubernetes/pki/sa.key\n/etc/kubernetes/pki/sa.pub",
          "element": "<pre tabindex=\"0\"><code>/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/apiserver-etcd-client.key\n/etc/kubernetes/pki/apiserver-etcd-client.crt\n/etc/kubernetes/pki/..."
        },
        {
          "language": "",
          "code": "/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/apiserver-etcd-client.key\n/etc/kubernetes/pki/apiserver-etcd-client.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubernetes/pki/ca.crt\n/etc/kubernetes/pki/apiserver.key\n/etc/kubernetes/pki/apiserver.crt\n/etc/kubernetes/pki/apiserver-kubelet-client.key\n/etc/kubernetes/pki/apiserver-kubelet-client.crt\n/etc/kubernetes/pki/front-proxy-ca.key\n/etc/kubernetes/pki/front-proxy-ca.crt\n/etc/kubernetes/pki/front-proxy-client.key\n/etc/kubernetes/pki/front-proxy-client.crt\n/etc/kubernetes/pki/etcd/server.key\n/etc/kubernetes/pki/etcd/server.crt\n/etc/kubernetes/pki/etcd/peer.key\n/etc/kubernetes/pki/etcd/peer.crt\n/etc/kubernetes/pki/etcd/healthcheck-client.key\n/etc/kubernetes/pki/etcd/healthcheck-client.crt\n/etc/kubernetes/pki/sa.key\n/etc/kubernetes/pki/sa.pub",
          "element": "<code>/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/apiserver-etcd-client.key\n/etc/kubernetes/pki/apiserver-etcd-client.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubern..."
        },
        {
          "language": "",
          "code": "<admin-group>",
          "element": "<code>&lt;admin-group&gt;</code>"
        },
        {
          "language": "",
          "code": "kubelet.conf",
          "element": "<code>kubelet.conf</code>"
        },
        {
          "language": "",
          "code": "<admin-group>",
          "element": "<code>&lt;admin-group&gt;</code>"
        },
        {
          "language": "",
          "code": "system:masters",
          "element": "<code>system:masters</code>"
        },
        {
          "language": "",
          "code": "system:masters",
          "element": "<code>system:masters</code>"
        },
        {
          "language": "",
          "code": "super-admin.conf",
          "element": "<code>super-admin.conf</code>"
        },
        {
          "language": "",
          "code": "Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin",
          "element": "<code>Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin</code>"
        },
        {
          "language": "",
          "code": "kubeadm:cluster-admins",
          "element": "<code>kubeadm:cluster-admins</code>"
        },
        {
          "language": "",
          "code": "cluster-admin",
          "element": "<code>cluster-admin</code>"
        },
        {
          "language": "",
          "code": "super-admin.conf",
          "element": "<code>super-admin.conf</code>"
        },
        {
          "language": "",
          "code": "Subject: O = system:masters, CN = kubernetes-super-admin",
          "element": "<code>Subject: O = system:masters, CN = kubernetes-super-admin</code>"
        },
        {
          "language": "",
          "code": "kubeadm init",
          "element": "<code>kubeadm init</code>"
        },
        {
          "language": "",
          "code": "KUBECONFIG=<filename> kubectl config set-cluster default-cluster --server=https://<host ip>:6443 --certificate-authority <path-to-kubernetes-ca> --embed-certs\nKUBECONFIG=<filename> kubectl config set-credentials <credential-name> --client-key <path-to-key>.pem --client-certificate <path-to-cert>.pem --embed-certs\nKUBECONFIG=<filename> kubectl config set-context default-system --cluster default-cluster --user <credential-name>\nKUBECONFIG=<filename> kubectl config use-context default-system",
          "element": "<pre tabindex=\"0\"><code>KUBECONFIG=&lt;filename&gt; kubectl config set-cluster default-cluster --server=https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs..."
        },
        {
          "language": "",
          "code": "KUBECONFIG=<filename> kubectl config set-cluster default-cluster --server=https://<host ip>:6443 --certificate-authority <path-to-kubernetes-ca> --embed-certs\nKUBECONFIG=<filename> kubectl config set-credentials <credential-name> --client-key <path-to-key>.pem --client-certificate <path-to-cert>.pem --embed-certs\nKUBECONFIG=<filename> kubectl config set-context default-system --cluster default-cluster --user <credential-name>\nKUBECONFIG=<filename> kubectl config use-context default-system",
          "element": "<code>KUBECONFIG=&lt;filename&gt; kubectl config set-cluster default-cluster --server=https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs\nKUBECONFIG=&lt;fi..."
        },
        {
          "language": "",
          "code": "manifests/kube-controller-manager.yaml",
          "element": "<code>manifests/kube-controller-manager.yaml</code>"
        },
        {
          "language": "",
          "code": "manifests/kube-scheduler.yaml",
          "element": "<code>manifests/kube-scheduler.yaml</code>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/admin.conf\n/etc/kubernetes/super-admin.conf\n/etc/kubernetes/kubelet.conf\n/etc/kubernetes/controller-manager.conf\n/etc/kubernetes/scheduler.conf",
          "element": "<pre tabindex=\"0\"><code>/etc/kubernetes/admin.conf\n/etc/kubernetes/super-admin.conf\n/etc/kubernetes/kubelet.conf\n/etc/kubernetes/controller-manager.conf\n/etc/kubernetes/scheduler.conf\n</code></pre>"
        },
        {
          "language": "",
          "code": "/etc/kubernetes/admin.conf\n/etc/kubernetes/super-admin.conf\n/etc/kubernetes/kubelet.conf\n/etc/kubernetes/controller-manager.conf\n/etc/kubernetes/scheduler.conf",
          "element": "<code>/etc/kubernetes/admin.conf\n/etc/kubernetes/super-admin.conf\n/etc/kubernetes/kubelet.conf\n/etc/kubernetes/controller-manager.conf\n/etc/kubernetes/scheduler.conf\n</code>"
        },
        {
          "language": "",
          "code": "kubernetes.default",
          "element": "<code>kubernetes.default</code>"
        },
        {
          "language": "",
          "code": "kubernetes.default.svc",
          "element": "<code>kubernetes.default.svc</code>"
        },
        {
          "language": "",
          "code": "kubernetes.default.svc.cluster",
          "element": "<code>kubernetes.default.svc.cluster</code>"
        },
        {
          "language": "",
          "code": "kubernetes.default.svc.cluster.local",
          "element": "<code>kubernetes.default.svc.cluster.local</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "PKI certificates and requirements",
          "id": ""
        },
        {
          "level": 2,
          "text": "How certificates are used by your cluster",
          "id": "how-certificates-are-used-by-your-cluster"
        },
        {
          "level": 2,
          "text": "Where certificates are stored",
          "id": "where-certificates-are-stored"
        },
        {
          "level": 2,
          "text": "Configure certificates manually",
          "id": "configure-certificates-manually"
        },
        {
          "level": 2,
          "text": "Configure certificates for user accounts",
          "id": "configure-certificates-for-user-accounts"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Server certificates",
          "id": "server-certificates"
        },
        {
          "level": 3,
          "text": "Client certificates",
          "id": "client-certificates"
        },
        {
          "level": 3,
          "text": "Kubelet's server and client certificates",
          "id": "kubelet-s-server-and-client-certificates"
        },
        {
          "level": 3,
          "text": "Single root CA",
          "id": "single-root-ca"
        },
        {
          "level": 3,
          "text": "All certificates",
          "id": "all-certificates"
        },
        {
          "level": 3,
          "text": "Certificate paths",
          "id": "certificate-paths"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731712.920392
    },
    {
      "url": "https://kubernetes.io/docs/concepts/",
      "title": "Concepts | Kubernetes",
      "content": "Kubernetes Documentation\nConcepts\nThe Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your\ncluster\n, and helps you obtain a deeper understanding of how Kubernetes works.\nOverview\nKubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\nCluster Architecture\nThe architectural concepts behind Kubernetes.\nContainers\nTechnology for packaging an application along with its runtime dependencies.\nWorkloads\nUnderstand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.\nServices, Load Balancing, and Networking\nConcepts and resources behind networking in Kubernetes.\nStorage\nWays to provide both long-term and temporary storage to Pods in your cluster.\nConfiguration\nResources that Kubernetes provides for configuring Pods.\nSecurity\nConcepts for keeping your cloud-native workload secure.\nPolicies\nManage security and best-practices with policies.\nScheduling, Preemption and Eviction\nCluster Administration\nLower-level detail relevant to creating or administering a Kubernetes cluster.\nWindows in Kubernetes\nKubernetes supports nodes that run Microsoft Windows.\nExtending Kubernetes\nDifferent ways to change the behavior of your Kubernetes cluster.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified June 22, 2020 at 11:01 PM PST:\nAdd descriptions to Concept sections (3ff7312cff)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Concepts",
          "id": ""
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 5,
          "text": "Overview",
          "id": ""
        },
        {
          "level": 5,
          "text": "Cluster Architecture",
          "id": ""
        },
        {
          "level": 5,
          "text": "Containers",
          "id": ""
        },
        {
          "level": 5,
          "text": "Workloads",
          "id": ""
        },
        {
          "level": 5,
          "text": "Services, Load Balancing, and Networking",
          "id": ""
        },
        {
          "level": 5,
          "text": "Storage",
          "id": ""
        },
        {
          "level": 5,
          "text": "Configuration",
          "id": ""
        },
        {
          "level": 5,
          "text": "Security",
          "id": ""
        },
        {
          "level": 5,
          "text": "Policies",
          "id": ""
        },
        {
          "level": 5,
          "text": "Scheduling, Preemption and Eviction",
          "id": ""
        },
        {
          "level": 5,
          "text": "Cluster Administration",
          "id": ""
        },
        {
          "level": 5,
          "text": "Windows in Kubernetes",
          "id": ""
        },
        {
          "level": 5,
          "text": "Extending Kubernetes",
          "id": ""
        }
      ],
      "timestamp": 1750731716.039349
    },
    {
      "url": "https://kubernetes.io/docs/concepts/overview/",
      "title": "Overview | Kubernetes",
      "content": "Kubernetes Documentation\nConcepts\nOverview\nOverview\nKubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\nThis page is an overview of Kubernetes.\nThe name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation\nresults from counting the eight letters between the \"K\" and the \"s\". Google open-sourced the\nKubernetes project in 2014. Kubernetes combines\nover 15 years of Google's experience\nrunning\nproduction workloads at scale with best-of-breed ideas and practices from the community.\nWhy you need Kubernetes and what it can do\nContainers are a good way to bundle and run your applications. In a production\nenvironment, you need to manage the containers that run the applications and\nensure that there is no downtime. For example, if a container goes down, another\ncontainer needs to start. Wouldn't it be easier if this behavior was handled by a system?\nThat's how Kubernetes comes to the rescue! Kubernetes provides you with a framework\nto run distributed systems resiliently. It takes care of scaling and failover for\nyour application, provides deployment patterns, and more. For example: Kubernetes\ncan easily manage a canary deployment for your system.\nKubernetes provides you with:\nService discovery and load balancing\nKubernetes can expose a container using the DNS name or using their own IP address.\nIf traffic to a container is high, Kubernetes is able to load balance and distribute\nthe network traffic so that the deployment is stable.\nStorage orchestration\nKubernetes allows you to automatically mount a storage system of your choice, such as\nlocal storages, public cloud providers, and more.\nAutomated rollouts and rollbacks\nYou can describe the desired state for your deployed containers using Kubernetes,\nand it can change the actual state to the desired state at a controlled rate.\nFor example, you can automate Kubernetes to create new containers for your\ndeployment, remove existing containers and adopt all their resources to the new container.\nAutomatic bin packing\nYou provide Kubernetes with a cluster of nodes that it can use to run containerized tasks.\nYou tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit\ncontainers onto your nodes to make the best use of your resources.\nSelf-healing\nKubernetes restarts containers that fail, replaces containers, kills containers that don't\nrespond to your user-defined health check, and doesn't advertise them to clients until they\nare ready to serve.\nSecret and configuration management\nKubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens,\nand SSH keys. You can deploy and update secrets and application configuration without\nrebuilding your container images, and without exposing secrets in your stack configuration.\nBatch execution\nIn addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.\nHorizontal scaling\nScale your application up and down with a simple command, with a UI, or automatically based on CPU usage.\nIPv4/IPv6 dual-stack\nAllocation of IPv4 and IPv6 addresses to Pods and Services\nDesigned for extensibility\nAdd features to your Kubernetes cluster without changing upstream source code.\nWhat Kubernetes is not\nKubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system.\nSince Kubernetes operates at the container level rather than at the hardware level,\nit provides some generally applicable features common to PaaS offerings, such as\ndeployment, scaling, load balancing, and lets users integrate their logging, monitoring,\nand alerting solutions. However, Kubernetes is not monolithic, and these default solutions\nare optional and pluggable. Kubernetes provides the building blocks for building developer\nplatforms, but preserves user choice and flexibility where it is important.\nKubernetes:\nDoes not limit the types of applications supported. Kubernetes aims to support an\nextremely diverse variety of workloads, including stateless, stateful, and data-processing\nworkloads. If an application can run in a container, it should run great on Kubernetes.\nDoes not deploy source code and does not build your application. Continuous Integration,\nDelivery, and Deployment (CI/CD) workflows are determined by organization cultures and\npreferences as well as technical requirements.\nDoes not provide application-level services, such as middleware (for example, message buses),\ndata-processing frameworks (for example, Spark), databases (for example, MySQL), caches, nor\ncluster storage systems (for example, Ceph) as built-in services. Such components can run on\nKubernetes, and/or can be accessed by applications running on Kubernetes through portable\nmechanisms, such as the\nOpen Service Broker\n.\nDoes not dictate logging, monitoring, or alerting solutions. It provides some integrations\nas proof of concept, and mechanisms to collect and export metrics.\nDoes not provide nor mandate a configuration language/system (for example, Jsonnet). It provides\na declarative API that may be targeted by arbitrary forms of declarative specifications.\nDoes not provide nor adopt any comprehensive machine configuration, maintenance, management,\nor self-healing systems.\nAdditionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need\nfor orchestration. The technical definition of orchestration is execution of a defined workflow:\nfirst do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable\ncontrol processes that continuously drive the current state towards the provided desired state.\nIt shouldn't matter how you get from A to C. Centralized control is also not required. This\nresults in a system that is easier to use and more powerful, robust, resilient, and extensible.\nHistorical context for Kubernetes\nLet's take a look at why Kubernetes is so useful by going back in time.\nTraditional deployment era:\nEarly on, organizations ran applications on physical servers. There was no way to define\nresource boundaries for applications in a physical server, and this caused resource\nallocation issues. For example, if multiple applications run on a physical server, there\ncan be instances where one application would take up most of the resources, and as a result,\nthe other applications would underperform. A solution for this would be to run each application\non a different physical server. But this did not scale as resources were underutilized, and it\nwas expensive for organizations to maintain many physical servers.\nVirtualized deployment era:\nAs a solution, virtualization was introduced. It allows you\nto run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization\nallows applications to be isolated between VMs and provides a level of security as the\ninformation of one application cannot be freely accessed by another application.\nVirtualization allows better utilization of resources in a physical server and allows\nbetter scalability because an application can be added or updated easily, reduces\nhardware costs, and much more. With virtualization you can present a set of physical\nresources as a cluster of disposable virtual machines.\nEach VM is a full machine running all the components, including its own operating\nsystem, on top of the virtualized hardware.\nContainer deployment era:\nContainers are similar to VMs, but they have relaxed\nisolation properties to share the Operating System (OS) among the applications.\nTherefore, containers are considered lightweight. Similar to a VM, a container\nhas its own filesystem, share of CPU, memory, process space, and more. As they\nare decoupled from the underlying infrastructure, they are portable across clouds\nand OS distributions.\nContainers have become popular because they provide extra benefits, such as:\nAgile application creation and deployment: increased ease and efficiency of\ncontainer image creation compared to VM image use.\nContinuous development, integration, and deployment: provides for reliable\nand frequent container image build and deployment with quick and efficient\nrollbacks (due to image immutability).\nDev and Ops separation of concerns: create application container images at\nbuild/release time rather than deployment time, thereby decoupling\napplications from infrastructure.\nObservability: not only surfaces OS-level information and metrics, but also\napplication health and other signals.\nEnvironmental consistency across development, testing, and production: runs\nthe same on a laptop as it does in the cloud.\nCloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises,\non major public clouds, and anywhere else.\nApplication-centric management: raises the level of abstraction from running an\nOS on virtual hardware to running an application on an OS using logical resources.\nLoosely coupled, distributed, elastic, liberated micro-services: applications are\nbroken into smaller, independent pieces and can be deployed and managed dynamically –\nnot a monolithic stack running on one big single-purpose machine.\nResource isolation: predictable application performance.\nResource utilization: high efficiency and density.\nWhat's next\nTake a look at the\nKubernetes Components\nTake a look at the\nThe Kubernetes API\nTake a look at the\nCluster Architecture\nReady to\nGet Started\n?\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified September 11, 2024 at 3:17 PM PST:\nRemoved duplicated paragraph (7e71096044)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Overview",
          "id": ""
        },
        {
          "level": 2,
          "text": "Why you need Kubernetes and what it can do",
          "id": "why-you-need-kubernetes-and-what-can-it-do"
        },
        {
          "level": 2,
          "text": "What Kubernetes is not",
          "id": "what-kubernetes-is-not"
        },
        {
          "level": 2,
          "text": "Historical context for Kubernetes",
          "id": "going-back-in-time"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        }
      ],
      "timestamp": 1750731719.127939
    },
    {
      "url": "https://kubernetes.io/docs/concepts/overview/components/",
      "title": "Kubernetes Components | Kubernetes",
      "content": "Kubernetes Documentation\nConcepts\nOverview\nKubernetes Components\nKubernetes Components\nAn overview of the key components that make up a Kubernetes cluster.\nThis page provides a high-level overview of the essential components that make up a Kubernetes cluster.\nThe components of a Kubernetes cluster\nCore Components\nA Kubernetes cluster consists of a control plane and one or more worker nodes.\nHere's a brief overview of the main components:\nControl Plane Components\nManage the overall state of the cluster:\nkube-apiserver\nThe core component server that exposes the Kubernetes HTTP API.\netcd\nConsistent and highly-available key value store for all API server data.\nkube-scheduler\nLooks for Pods not yet bound to a node, and assigns each Pod to a suitable node.\nkube-controller-manager\nRuns\ncontrollers\nto implement Kubernetes API behavior.\ncloud-controller-manager\n(optional)\nIntegrates with underlying cloud provider(s).\nNode Components\nRun on every node, maintaining running pods and providing the Kubernetes runtime environment:\nkubelet\nEnsures that Pods are running, including their containers.\nkube-proxy\n(optional)\nMaintains network rules on nodes to implement\nServices\n.\nContainer runtime\nSoftware responsible for running containers. Read\nContainer Runtimes\nto learn more.\n🛇 This item links to a third party project or product that is not part of Kubernetes itself.\nMore information\nYour cluster may require additional software on each node; for example, you might also\nrun\nsystemd\non a Linux node to supervise local components.\nAddons\nAddons extend the functionality of Kubernetes. A few important examples include:\nDNS\nFor cluster-wide DNS resolution.\nWeb UI\n(Dashboard)\nFor cluster management via a web interface.\nContainer Resource Monitoring\nFor collecting and storing container metrics.\nCluster-level Logging\nFor saving container logs to a central log store.\nFlexibility in Architecture\nKubernetes allows for flexibility in how these components are deployed and managed.\nThe architecture can be adapted to various needs, from small development environments\nto large-scale production deployments.\nFor more detailed information about each component and various ways to configure your\ncluster architecture, see the\nCluster Architecture\npage.\nItems on this page refer to third party products or projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. See the\nCNCF website guidelines\nfor more details.\nYou should read the\ncontent guide\nbefore proposing a change that adds an extra third-party link.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified May 31, 2025 at 8:36 AM PST:\nFix missing periods at the end of sentences (f0b3dc1f07)",
      "code_examples": [],
      "headings": [
        {
          "level": 1,
          "text": "Kubernetes Components",
          "id": ""
        },
        {
          "level": 2,
          "text": "Core Components",
          "id": "core-components"
        },
        {
          "level": 2,
          "text": "Addons",
          "id": "addons"
        },
        {
          "level": 2,
          "text": "Flexibility in Architecture",
          "id": "flexibility-in-architecture"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Control Plane Components",
          "id": "control-plane-components"
        },
        {
          "level": 3,
          "text": "Node Components",
          "id": "node-components"
        }
      ],
      "timestamp": 1750731722.261329
    },
    {
      "url": "https://kubernetes.io/docs/concepts/overview/working-with-objects/",
      "title": "Objects In Kubernetes | Kubernetes",
      "content": "Kubernetes Documentation\nConcepts\nOverview\nObjects In Kubernetes\nObjects In Kubernetes\nKubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Learn about the Kubernetes object model and how to work with these objects.\nThis page explains how Kubernetes objects are represented in the Kubernetes API, and how you can\nexpress them in\n.yaml\nformat.\nUnderstanding Kubernetes objects\nKubernetes objects\nare persistent entities in the Kubernetes system. Kubernetes uses these\nentities to represent the state of your cluster. Specifically, they can describe:\nWhat containerized applications are running (and on which nodes)\nThe resources available to those applications\nThe policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance\nA Kubernetes object is a \"record of intent\"--once you create the object, the Kubernetes system\nwill constantly work to ensure that the object exists. By creating an object, you're effectively\ntelling the Kubernetes system what you want your cluster's workload to look like; this is your\ncluster's\ndesired state\n.\nTo work with Kubernetes objects—whether to create, modify, or delete them—you'll need to use the\nKubernetes API\n. When you use the\nkubectl\ncommand-line\ninterface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use\nthe Kubernetes API directly in your own programs using one of the\nClient Libraries\n.\nObject spec and status\nAlmost every Kubernetes object includes two nested object fields that govern\nthe object's configuration: the object\nspec\nand the object\nstatus\n.\nFor objects that have a\nspec\n, you have to set this when you create the object,\nproviding a description of the characteristics you want the resource to have:\nits\ndesired state\n.\nThe\nstatus\ndescribes the\ncurrent state\nof the object, supplied and updated\nby the Kubernetes system and its components. The Kubernetes\ncontrol plane\ncontinually\nand actively manages every object's actual state to match the desired state you\nsupplied.\nFor example: in Kubernetes, a Deployment is an object that can represent an\napplication running on your cluster. When you create the Deployment, you\nmight set the Deployment\nspec\nto specify that you want three replicas of\nthe application to be running. The Kubernetes system reads the Deployment\nspec and starts three instances of your desired application--updating\nthe status to match your spec. If any of those instances should fail\n(a status change), the Kubernetes system responds to the difference\nbetween spec and status by making a correction--in this case, starting\na replacement instance.\nFor more information on the object spec, status, and metadata, see the\nKubernetes API Conventions\n.\nDescribing a Kubernetes object\nWhen you create an object in Kubernetes, you must provide the object spec that describes its\ndesired state, as well as some basic information about the object (such as a name). When you use\nthe Kubernetes API to create the object (either directly or via\nkubectl\n), that API request must\ninclude that information as JSON in the request body.\nMost often, you provide the information to\nkubectl\nin a file known as a\nmanifest\n.\nBy convention, manifests are YAML (you could also use JSON format).\nTools such as\nkubectl\nconvert the information from a manifest into JSON or another supported\nserialization format when making the API request over HTTP.\nHere's an example manifest that shows the required fields and object spec for a Kubernetes\nDeployment:\napplication/deployment.yaml\napiVersion\n:\napps/v1\nkind\n:\nDeployment\nmetadata\n:\nname\n:\nnginx-deployment\nspec\n:\nselector\n:\nmatchLabels\n:\napp\n:\nnginx\nreplicas\n:\n2\n# tells deployment to run 2 pods matching the template\ntemplate\n:\nmetadata\n:\nlabels\n:\napp\n:\nnginx\nspec\n:\ncontainers\n:\n-\nname\n:\nnginx\nimage\n:\nnginx:1.14.2\nports\n:\n-\ncontainerPort\n:\n80\nOne way to create a Deployment using a manifest file like the one above is to use the\nkubectl apply\ncommand\nin the\nkubectl\ncommand-line interface, passing the\n.yaml\nfile as an argument. Here's an example:\nkubectl apply -f https://k8s.io/examples/application/deployment.yaml\nThe output is similar to this:\ndeployment.apps/nginx-deployment created\nRequired fields\nIn the manifest (YAML or JSON file) for the Kubernetes object you want to create, you'll need to set values for\nthe following fields:\napiVersion\n- Which version of the Kubernetes API you're using to create this object\nkind\n- What kind of object you want to create\nmetadata\n- Data that helps uniquely identify the object, including a\nname\nstring,\nUID\n, and optional\nnamespace\nspec\n- What state you desire for the object\nThe precise format of the object\nspec\nis different for every Kubernetes object, and contains\nnested fields specific to that object. The\nKubernetes API Reference\ncan help you find the spec format for all of the objects you can create using Kubernetes.\nFor example, see the\nspec\nfield\nfor the Pod API reference.\nFor each Pod, the\n.spec\nfield specifies the pod and its desired state (such as the container image name for\neach container within that pod).\nAnother example of an object specification is the\nspec\nfield\nfor the StatefulSet API. For StatefulSet, the\n.spec\nfield specifies the StatefulSet and\nits desired state.\nWithin the\n.spec\nof a StatefulSet is a\ntemplate\nfor Pod objects. That template describes Pods that the StatefulSet controller will create in order to\nsatisfy the StatefulSet specification.\nDifferent kinds of objects can also have different\n.status\n; again, the API reference pages\ndetail the structure of that\n.status\nfield, and its content for each different type of object.\nNote:\nSee\nConfiguration Best Practices\nfor additional\ninformation on writing YAML configuration files.\nServer side field validation\nStarting with Kubernetes v1.25, the API server offers server side\nfield validation\nthat detects unrecognized or duplicate fields in an object. It provides all the functionality\nof\nkubectl --validate\non the server side.\nThe\nkubectl\ntool uses the\n--validate\nflag to set the level of field validation. It accepts the\nvalues\nignore\n,\nwarn\n, and\nstrict\nwhile also accepting the values\ntrue\n(equivalent to\nstrict\n)\nand\nfalse\n(equivalent to\nignore\n). The default validation setting for\nkubectl\nis\n--validate=true\n.\nStrict\nStrict field validation, errors on validation failure\nWarn\nField validation is performed, but errors are exposed as warnings rather than failing the request\nIgnore\nNo server side field validation is performed\nWhen\nkubectl\ncannot connect to an API server that supports field validation it will fall back\nto using client-side validation. Kubernetes 1.27 and later versions always offer field validation;\nolder Kubernetes releases might not. If your cluster is older than v1.27, check the documentation\nfor your version of Kubernetes.\nWhat's next\nIf you're new to Kubernetes, read more about the following:\nPods\nwhich are the most important basic Kubernetes objects.\nDeployment\nobjects.\nControllers\nin Kubernetes.\nkubectl\nand\nkubectl commands\n.\nKubernetes Object Management\nexplains how to use\nkubectl\nto manage objects.\nYou might need to\ninstall kubectl\nif you don't already have it available.\nTo learn about the Kubernetes API in general, visit:\nKubernetes API overview\nTo learn about objects in Kubernetes in more depth, read other pages in this section:\nKubernetes Object Management\nObject Names and IDs\nLabels and Selectors\nNamespaces\nAnnotations\nField Selectors\nFinalizers\nOwners and Dependents\nRecommended Labels\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified August 25, 2024 at 8:24 PM PST:\nReorder overview pages (42da717f16)",
      "code_examples": [
        {
          "language": "",
          "code": "application/deployment.yaml",
          "element": "<code>application/deployment.yaml</code>"
        },
        {
          "language": "",
          "code": "apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginxreplicas:2# tells deployment to run 2 pods matching the templatetemplate:metadata:labels:app:nginxspec:containers:-name:nginximage:nginx:1.14.2ports:-containerPort:80",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-..."
        },
        {
          "language": "language-yaml",
          "code": "apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginxreplicas:2# tells deployment to run 2 pods matching the templatetemplate:metadata:labels:app:nginxspec:containers:-name:nginximage:nginx:1.14.2ports:-containerPort:80",
          "element": "<code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex\"><span><span style=\"color:green;font-weight:700\">apiVersion</span>:<span style=\"color:#bbb\"> </span>apps/v1<span style=\"color:#bb..."
        },
        {
          "language": "",
          "code": "kubectl apply",
          "element": "<code>kubectl apply</code>"
        },
        {
          "language": "",
          "code": "kubectl apply -f https://k8s.io/examples/application/deployment.yaml",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>kubectl apply -f https://k8s..."
        },
        {
          "language": "language-shell",
          "code": "kubectl apply -f https://k8s.io/examples/application/deployment.yaml",
          "element": "<code class=\"language-shell\" data-lang=\"shell\"><span style=\"display:flex\"><span>kubectl apply -f https://k8s.io/examples/application/deployment.yaml\n</span></span></code>"
        },
        {
          "language": "",
          "code": "deployment.apps/nginx-deployment created",
          "element": "<pre tabindex=\"0\"><code>deployment.apps/nginx-deployment created\n</code></pre>"
        },
        {
          "language": "",
          "code": "deployment.apps/nginx-deployment created",
          "element": "<code>deployment.apps/nginx-deployment created\n</code>"
        },
        {
          "language": "",
          "code": "kubectl --validate",
          "element": "<code>kubectl --validate</code>"
        },
        {
          "language": "",
          "code": "--validate=true",
          "element": "<code>--validate=true</code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Objects In Kubernetes",
          "id": ""
        },
        {
          "level": 2,
          "text": "Understanding Kubernetes objects",
          "id": "kubernetes-objects"
        },
        {
          "level": 2,
          "text": "Server side field validation",
          "id": "server-side-field-validation"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Object spec and status",
          "id": "object-spec-and-status"
        },
        {
          "level": 3,
          "text": "Describing a Kubernetes object",
          "id": "describing-a-kubernetes-object"
        },
        {
          "level": 3,
          "text": "Required fields",
          "id": "required-fields"
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731725.385215
    },
    {
      "url": "https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/",
      "title": "Kubernetes Object Management | Kubernetes",
      "content": "Kubernetes Documentation\nConcepts\nOverview\nObjects In Kubernetes\nKubernetes Object Management\nKubernetes Object Management\nThe\nkubectl\ncommand-line tool supports several different ways to create and manage\nKubernetes\nobjects\n. This document provides an overview of the different\napproaches. Read the\nKubectl book\nfor\ndetails of managing objects by Kubectl.\nManagement techniques\nWarning:\nA Kubernetes object should be managed using only one technique. Mixing\nand matching techniques for the same object results in undefined behavior.\nManagement technique\nOperates on\nRecommended environment\nSupported writers\nLearning curve\nImperative commands\nLive objects\nDevelopment projects\n1+\nLowest\nImperative object configuration\nIndividual files\nProduction projects\n1\nModerate\nDeclarative object configuration\nDirectories of files\nProduction projects\n1+\nHighest\nImperative commands\nWhen using imperative commands, a user operates directly on live objects\nin a cluster. The user provides operations to\nthe\nkubectl\ncommand as arguments or flags.\nThis is the recommended way to get started or to run a one-off task in\na cluster. Because this technique operates directly on live\nobjects, it provides no history of previous configurations.\nExamples\nRun an instance of the nginx container by creating a Deployment object:\nkubectl create deployment nginx --image nginx\nTrade-offs\nAdvantages compared to object configuration:\nCommands are expressed as a single action word.\nCommands require only a single step to make changes to the cluster.\nDisadvantages compared to object configuration:\nCommands do not integrate with change review processes.\nCommands do not provide an audit trail associated with changes.\nCommands do not provide a source of records except for what is live.\nCommands do not provide a template for creating new objects.\nImperative object configuration\nIn imperative object configuration, the kubectl command specifies the\noperation (create, replace, etc.), optional flags and at least one file\nname. The file specified must contain a full definition of the object\nin YAML or JSON format.\nSee the\nAPI reference\nfor more details on object definitions.\nWarning:\nThe imperative\nreplace\ncommand replaces the existing\nspec with the newly provided one, dropping all changes to the object missing from\nthe configuration file. This approach should not be used with resource\ntypes whose specs are updated independently of the configuration file.\nServices of type\nLoadBalancer\n, for example, have their\nexternalIPs\nfield updated\nindependently from the configuration by the cluster.\nExamples\nCreate the objects defined in a configuration file:\nkubectl create -f nginx.yaml\nDelete the objects defined in two configuration files:\nkubectl delete -f nginx.yaml -f redis.yaml\nUpdate the objects defined in a configuration file by overwriting\nthe live configuration:\nkubectl replace -f nginx.yaml\nTrade-offs\nAdvantages compared to imperative commands:\nObject configuration can be stored in a source control system such as Git.\nObject configuration can integrate with processes such as reviewing changes before push and audit trails.\nObject configuration provides a template for creating new objects.\nDisadvantages compared to imperative commands:\nObject configuration requires basic understanding of the object schema.\nObject configuration requires the additional step of writing a YAML file.\nAdvantages compared to declarative object configuration:\nImperative object configuration behavior is simpler and easier to understand.\nAs of Kubernetes version 1.5, imperative object configuration is more mature.\nDisadvantages compared to declarative object configuration:\nImperative object configuration works best on files, not directories.\nUpdates to live objects must be reflected in configuration files, or they will be lost during the next replacement.\nDeclarative object configuration\nWhen using declarative object configuration, a user operates on object\nconfiguration files stored locally, however the user does not define the\noperations to be taken on the files. Create, update, and delete operations\nare automatically detected per-object by\nkubectl\n. This enables working on\ndirectories, where different operations might be needed for different objects.\nNote:\nDeclarative object configuration retains changes made by other\nwriters, even if the changes are not merged back to the object configuration file.\nThis is possible by using the\npatch\nAPI operation to write only\nobserved differences, instead of using the\nreplace\nAPI operation to replace the entire object configuration.\nExamples\nProcess all object configuration files in the\nconfigs\ndirectory, and create or\npatch the live objects. You can first\ndiff\nto see what changes are going to be\nmade, and then apply:\nkubectl diff -f configs/\nkubectl apply -f configs/\nRecursively process directories:\nkubectl diff -R -f configs/\nkubectl apply -R -f configs/\nTrade-offs\nAdvantages compared to imperative object configuration:\nChanges made directly to live objects are retained, even if they are not merged back into the configuration files.\nDeclarative object configuration has better support for operating on directories and automatically detecting operation types (create, patch, delete) per-object.\nDisadvantages compared to imperative object configuration:\nDeclarative object configuration is harder to debug and understand results when they are unexpected.\nPartial updates using diffs create complex merge and patch operations.\nWhat's next\nManaging Kubernetes Objects Using Imperative Commands\nImperative Management of Kubernetes Objects Using Configuration Files\nDeclarative Management of Kubernetes Objects Using Configuration Files\nDeclarative Management of Kubernetes Objects Using Kustomize\nKubectl Command Reference\nKubectl Book\nKubernetes API Reference\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified January 08, 2022 at 6:09 PM PST:\nReorganize Working with Kubernetes Objects section (634c17f61c)",
      "code_examples": [
        {
          "language": "",
          "code": "kubectl create deployment nginx --image nginx",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl create deployment nginx --..."
        },
        {
          "language": "language-sh",
          "code": "kubectl create deployment nginx --image nginx",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl create deployment nginx --image nginx\n</span></span></code>"
        },
        {
          "language": "",
          "code": "LoadBalancer",
          "element": "<code>LoadBalancer</code>"
        },
        {
          "language": "",
          "code": "externalIPs",
          "element": "<code>externalIPs</code>"
        },
        {
          "language": "",
          "code": "kubectl create -f nginx.yaml",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl create -f nginx.yaml\n</spa..."
        },
        {
          "language": "language-sh",
          "code": "kubectl create -f nginx.yaml",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl create -f nginx.yaml\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubectl delete -f nginx.yaml -f redis.yaml",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl delete -f nginx.yaml -f re..."
        },
        {
          "language": "language-sh",
          "code": "kubectl delete -f nginx.yaml -f redis.yaml",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl delete -f nginx.yaml -f redis.yaml\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubectl replace -f nginx.yaml",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl replace -f nginx.yaml\n</sp..."
        },
        {
          "language": "language-sh",
          "code": "kubectl replace -f nginx.yaml",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl replace -f nginx.yaml\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubectl diff -f configs/kubectl apply -f configs/",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl diff -f configs/\n</span></..."
        },
        {
          "language": "language-sh",
          "code": "kubectl diff -f configs/kubectl apply -f configs/",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl diff -f configs/\n</span></span><span style=\"display:flex\"><span>kubectl apply -f configs/\n</span></span></code>"
        },
        {
          "language": "",
          "code": "kubectl diff -R -f configs/kubectl apply -R -f configs/",
          "element": "<pre style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4\" tabindex=\"0\"><code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl diff -R -f configs/\n</span..."
        },
        {
          "language": "language-sh",
          "code": "kubectl diff -R -f configs/kubectl apply -R -f configs/",
          "element": "<code class=\"language-sh\" data-lang=\"sh\"><span style=\"display:flex\"><span>kubectl diff -R -f configs/\n</span></span><span style=\"display:flex\"><span>kubectl apply -R -f configs/\n</span></span></code>"
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Kubernetes Object Management",
          "id": ""
        },
        {
          "level": 2,
          "text": "Management techniques",
          "id": "management-techniques"
        },
        {
          "level": 2,
          "text": "Imperative commands",
          "id": "imperative-commands"
        },
        {
          "level": 2,
          "text": "Imperative object configuration",
          "id": "imperative-object-configuration"
        },
        {
          "level": 2,
          "text": "Declarative object configuration",
          "id": "declarative-object-configuration"
        },
        {
          "level": 2,
          "text": "What's next",
          "id": "what-s-next"
        },
        {
          "level": 2,
          "text": "Feedback",
          "id": "feedback"
        },
        {
          "level": 3,
          "text": "Examples",
          "id": "examples"
        },
        {
          "level": 3,
          "text": "Trade-offs",
          "id": "trade-offs"
        },
        {
          "level": 3,
          "text": "Examples",
          "id": "examples-1"
        },
        {
          "level": 3,
          "text": "Trade-offs",
          "id": "trade-offs-1"
        },
        {
          "level": 3,
          "text": "Examples",
          "id": "examples-2"
        },
        {
          "level": 3,
          "text": "Trade-offs",
          "id": "trade-offs-2"
        },
        {
          "level": 4,
          "text": "Warning:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Warning:",
          "id": ""
        },
        {
          "level": 4,
          "text": "Note:",
          "id": ""
        }
      ],
      "timestamp": 1750731728.48241
    }
  ],
  "total_pages": 30,
  "errors": []
}