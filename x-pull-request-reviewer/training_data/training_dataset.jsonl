{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nToolkit Components\nSource Controller\nAPI Reference\nAPI Reference\nSource API reference v1\nThe GitOps Toolkit Custom Resource Definitions documentation.\nLast modified 2023-03-23:\nGA: adapt to SC, NC and KC APIs graduating to v1 (3d32a85)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nToolkit Components\nSource Controller\nAPI Reference\nSource API reference v1\nSource API reference v1\nThe GitOps Toolkit Custom Resource Definitions documentation.\nPackages:\nsource.toolkit.fluxcd.io/v1\nsource.toolkit.fluxcd.io/v1\nPackage v1 contains API Schema definitions for the source v1 API group\nResource Types:\nBucket\nGitRepository\nHelmChart\nHelmRepository\nOCIRepository\nBucket\nBucket is the Schema for the buckets API.\nField\nDescription\napiVersion\nstring\nsource.toolkit.fluxcd.io/v1\nkind\nstring\nBucket\nmetadata\nKubernetes meta/v1.ObjectMeta\nRefer to the Kubernetes API documentation for the fields of the\nmetadata\nfield.\nspec\nBucketSpec\nprovider\nstring\n(Optional)\nProvider of the object storage bucket.\nDefaults to ‘generic’, which expects an S3 (API) compatible object\nstorage.\nbucketName\nstring\nBucketName is the name of the object storage bucket.\nendpoint\nstring\nEndpoint is the object storage address the BucketName is located at.\nsts\nBucketSTSSpec\n(Optional)\nSTS specifies the required configuration to use a Security Token\nService for fetching temporary credentials to authenticate in a\nBucket provider.\nThis field is only supported for the\naws\nand\ngeneric\nproviders.\ninsecure\nbool\n(Optional)\nInsecure allows connecting to a non-TLS HTTP Endpoint.\nregion\nstring\n(Optional)\nRegion of the Endpoint where the BucketName is located in.\nprefix\nstring\n(Optional)\nPrefix to use for server-side filtering of files in the Bucket.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Secret containing authentication credentials\nfor the Bucket.\ncertSecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nCertSecretRef can be given the name of a Secret containing\neither or both of\na PEM-encoded client certificate (\ntls.crt\n) and private\nkey (\ntls.key\n);\na PEM-encoded CA certificate (\nca.crt\n)\nand whichever are supplied, will be used for connecting to the\nbucket. The client cert and key are useful if you are\nauthenticating with a certificate; the CA cert is useful if\nyou are using a self-signed server certificate. The Secret must\nbe of type\nOpaque\nor\nkubernetes.io/tls\n.\nThis field is only supported for the\ngeneric\nprovider.\nproxySecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nProxySecretRef specifies the Secret containing the proxy configuration\nto use while communicating with the Bucket server.\ninterval\nKubernetes meta/v1.Duration\nInterval at which the Bucket Endpoint is checked for updates.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout for fetch operations, defaults to 60s.\nignore\nstring\n(Optional)\nIgnore overrides the set of excluded patterns in the .sourceignore format\n(which is the same as .gitignore). If not provided, a default will be used,\nconsult the documentation for your version to find out what those are.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend the reconciliation of this\nBucket.\nstatus\nBucketStatus\nGitRepository\nGitRepository is the Schema for the gitrepositories API.\nField\nDescription\napiVersion\nstring\nsource.toolkit.fluxcd.io/v1\nkind\nstring\nGitRepository\nmetadata\nKubernetes meta/v1.ObjectMeta\nRefer to the Kubernetes API documentation for the fields of the\nmetadata\nfield.\nspec\nGitRepositorySpec\nurl\nstring\nURL specifies the Git repository URL, it can be an HTTP/S or SSH address.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Secret containing authentication credentials for\nthe GitRepository.\nFor HTTPS repositories the Secret must contain ‘username’ and ‘password’\nfields for basic auth or ‘bearerToken’ field for token auth.\nFor SSH repositories the Secret must contain ‘identity’\nand ‘known_hosts’ fields.\nprovider\nstring\n(Optional)\nProvider used for authentication, can be ‘azure’, ‘github’, ‘generic’.\nWhen not specified, defaults to ‘generic’.\ninterval\nKubernetes meta/v1.Duration\nInterval at which the GitRepository URL is checked for updates.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout for Git operations like cloning, defaults to 60s.\nref\nGitRepositoryRef\n(Optional)\nReference specifies the Git reference to resolve and monitor for\nchanges, defaults to the ‘master’ branch.\nverify\nGitRepositoryVerification\n(Optional)\nVerification specifies the configuration to verify the Git commit\nsignature(s).\nproxySecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nProxySecretRef specifies the Secret containing the proxy configuration\nto use while communicating with the Git server.\nignore\nstring\n(Optional)\nIgnore overrides the set of excluded patterns in the .sourceignore format\n(which is the same as .gitignore). If not provided, a default will be used,\nconsult the documentation for your version to find out what those are.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend the reconciliation of this\nGitRepository.\nrecurseSubmodules\nbool\n(Optional)\nRecurseSubmodules enables the initialization of all submodules within\nthe GitRepository as cloned from the URL, using their default settings.\ninclude\n[]GitRepositoryInclude\n(Optional)\nInclude specifies a list of GitRepository resources which Artifacts\nshould be included in the Artifact produced for this GitRepository.\nsparseCheckout\n[]string\n(Optional)\nSparseCheckout specifies a list of directories to checkout when cloning\nthe repository. If specified, only these directories are included in the\nArtifact produced for this GitRepository.\nstatus\nGitRepositoryStatus\nHelmChart\nHelmChart is the Schema for the helmcharts API.\nField\nDescription\napiVersion\nstring\nsource.toolkit.fluxcd.io/v1\nkind\nstring\nHelmChart\nmetadata\nKubernetes meta/v1.ObjectMeta\nRefer to the Kubernetes API documentation for the fields of the\nmetadata\nfield.\nspec\nHelmChartSpec\nchart\nstring\nChart is the name or path the Helm chart is available at in the\nSourceRef.\nversion\nstring\n(Optional)\nVersion is the chart version semver expression, ignored for charts from\nGitRepository and Bucket sources. Defaults to latest when omitted.\nsourceRef\nLocalHelmChartSourceReference\nSourceRef is the reference to the Source the chart is available at.\ninterval\nKubernetes meta/v1.Duration\nInterval at which the HelmChart SourceRef is checked for updates.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\nreconcileStrategy\nstring\n(Optional)\nReconcileStrategy determines what enables the creation of a new artifact.\nValid values are (‘ChartVersion’, ‘Revision’).\nSee the documentation of the values for an explanation on their behavior.\nDefaults to ChartVersion when omitted.\nvaluesFiles\n[]string\n(Optional)\nValuesFiles is an alternative list of values files to use as the chart\nvalues (values.yaml is not included by default), expected to be a\nrelative path in the SourceRef.\nValues files are merged in the order of this list with the last file\noverriding the first. Ignored when omitted.\nignoreMissingValuesFiles\nbool\n(Optional)\nIgnoreMissingValuesFiles controls whether to silently ignore missing values\nfiles rather than failing.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend the reconciliation of this\nsource.\nverify\nOCIRepositoryVerification\n(Optional)\nVerify contains the secret name containing the trusted public keys\nused to verify the signature and specifies which provider to use to check\nwhether OCI image is authentic.\nThis field is only supported when using HelmRepository source with spec.type ‘oci’.\nChart dependencies, which are not bundled in the umbrella chart artifact, are not verified.\nstatus\nHelmChartStatus\nHelmRepository\nHelmRepository is the Schema for the helmrepositories API.\nField\nDescription\napiVersion\nstring\nsource.toolkit.fluxcd.io/v1\nkind\nstring\nHelmRepository\nmetadata\nKubernetes meta/v1.ObjectMeta\nRefer to the Kubernetes API documentation for the fields of the\nmetadata\nfield.\nspec\nHelmRepositorySpec\nurl\nstring\nURL of the Helm repository, a valid URL contains at least a protocol and\nhost.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Secret containing authentication credentials\nfor the HelmRepository.\nFor HTTP/S basic auth the secret must contain ‘username’ and ‘password’\nfields.\nSupport for TLS auth using the ‘certFile’ and ‘keyFile’, and/or ‘caFile’\nkeys is deprecated. Please use\n.spec.certSecretRef\ninstead.\ncertSecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nCertSecretRef can be given the name of a Secret containing\neither or both of\na PEM-encoded client certificate (\ntls.crt\n) and private\nkey (\ntls.key\n);\na PEM-encoded CA certificate (\nca.crt\n)\nand whichever are supplied, will be used for connecting to the\nregistry. The client cert and key are useful if you are\nauthenticating with a certificate; the CA cert is useful if\nyou are using a self-signed server certificate. The Secret must\nbe of type\nOpaque\nor\nkubernetes.io/tls\n.\nIt takes precedence over the values specified in the Secret referred\nto by\n.spec.secretRef\n.\npassCredentials\nbool\n(Optional)\nPassCredentials allows the credentials from the SecretRef to be passed\non to a host that does not match the host as defined in URL.\nThis may be required if the host of the advertised chart URLs in the\nindex differ from the defined URL.\nEnabling this should be done with caution, as it can potentially result\nin credentials getting stolen in a MITM-attack.\ninterval\nKubernetes meta/v1.Duration\n(Optional)\nInterval at which the HelmRepository URL is checked for updates.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\ninsecure\nbool\n(Optional)\nInsecure allows connecting to a non-TLS HTTP container registry.\nThis field is only taken into account if the .spec.type field is set to ‘oci’.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout is used for the index fetch operation for an HTTPS helm repository,\nand for remote OCI Repository operations like pulling for an OCI helm\nchart by the associated HelmChart.\nIts default value is 60s.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend the reconciliation of this\nHelmRepository.\naccessFrom\ngithub.com/fluxcd/pkg/apis/acl.AccessFrom\n(Optional)\nAccessFrom specifies an Access Control List for allowing cross-namespace\nreferences to this object.\nNOTE: Not implemented, provisional as of\nhttps://github.com/fluxcd/flux2/pull/2092\ntype\nstring\n(Optional)\nType of the HelmRepository.\nWhen this field is set to “oci”, the URL field value must be prefixed with “oci://”.\nprovider\nstring\n(Optional)\nProvider used for authentication, can be ‘aws’, ‘azure’, ‘gcp’ or ‘generic’.\nThis field is optional, and only taken into account if the .spec.type field is set to ‘oci’.\nWhen not specified, defaults to ‘generic’.\nstatus\nHelmRepositoryStatus\nOCIRepository\nOCIRepository is the Schema for the ocirepositories API\nField\nDescription\napiVersion\nstring\nsource.toolkit.fluxcd.io/v1\nkind\nstring\nOCIRepository\nmetadata\nKubernetes meta/v1.ObjectMeta\nRefer to the Kubernetes API documentation for the fields of the\nmetadata\nfield.\nspec\nOCIRepositorySpec\nurl\nstring\nURL is a reference to an OCI artifact repository hosted\non a remote container registry.\nref\nOCIRepositoryRef\n(Optional)\nThe OCI reference to pull and monitor for changes,\ndefaults to the latest tag.\nlayerSelector\nOCILayerSelector\n(Optional)\nLayerSelector specifies which layer should be extracted from the OCI artifact.\nWhen not specified, the first layer found in the artifact is selected.\nprovider\nstring\n(Optional)\nThe provider used for authentication, can be ‘aws’, ‘azure’, ‘gcp’ or ‘generic’.\nWhen not specified, defaults to ‘generic’.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef contains the secret name containing the registry login\ncredentials to resolve image metadata.\nThe secret must be of type kubernetes.io/dockerconfigjson.\nverify\nOCIRepositoryVerification\n(Optional)\nVerify contains the secret name containing the trusted public keys\nused to verify the signature and specifies which provider to use to check\nwhether OCI image is authentic.\nserviceAccountName\nstring\n(Optional)\nServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate\nthe image pull if the service account has attached pull secrets. For more information:\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account\ncertSecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nCertSecretRef can be given the name of a Secret containing\neither or both of\na PEM-encoded client certificate (\ntls.crt\n) and private\nkey (\ntls.key\n);\na PEM-encoded CA certificate (\nca.crt\n)\nand whichever are supplied, will be used for connecting to the\nregistry. The client cert and key are useful if you are\nauthenticating with a certificate; the CA cert is useful if\nyou are using a self-signed server certificate. The Secret must\nbe of type\nOpaque\nor\nkubernetes.io/tls\n.\nproxySecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nProxySecretRef specifies the Secret containing the proxy configuration\nto use while communicating with the container registry.\ninterval\nKubernetes meta/v1.Duration\nInterval at which the OCIRepository URL is checked for updates.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nThe timeout for remote OCI Repository operations like pulling, defaults to 60s.\nignore\nstring\n(Optional)\nIgnore overrides the set of excluded patterns in the .sourceignore format\n(which is the same as .gitignore). If not provided, a default will be used,\nconsult the documentation for your version to find out what those are.\ninsecure\nbool\n(Optional)\nInsecure allows connecting to a non-TLS HTTP container registry.\nsuspend\nbool\n(Optional)\nThis flag tells the controller to suspend the reconciliation of this source.\nstatus\nOCIRepositoryStatus\nArtifact\n(\nAppears on:\nBucketStatus\n,\nGitRepositoryStatus\n,\nHelmChartStatus\n,\nHelmRepositoryStatus\n,\nOCIRepositoryStatus\n)\nArtifact represents the output of a Source reconciliation.\nField\nDescription\npath\nstring\nPath is the relative file path of the Artifact. It can be used to locate\nthe file in the root of the Artifact storage on the local file system of\nthe controller managing the Source.\nurl\nstring\nURL is the HTTP address of the Artifact as exposed by the controller\nmanaging the Source. It can be used to retrieve the Artifact for\nconsumption, e.g. by another controller applying the Artifact contents.\nrevision\nstring\nRevision is a human-readable identifier traceable in the origin source\nsystem. It can be a Git commit SHA, Git tag, a Helm chart version, etc.\ndigest\nstring\n(Optional)\nDigest is the digest of the file in the form of ‘\n:\n’.\nlastUpdateTime\nKubernetes meta/v1.Time\nLastUpdateTime is the timestamp corresponding to the last update of the\nArtifact.\nsize\nint64\n(Optional)\nSize is the number of bytes in the file.\nmetadata\nmap[string]string\n(Optional)\nMetadata holds upstream information such as OCI annotations.\nBucketSTSSpec\n(\nAppears on:\nBucketSpec\n)\nBucketSTSSpec specifies the required configuration to use a Security Token\nService for fetching temporary credentials to authenticate in a Bucket\nprovider.\nField\nDescription\nprovider\nstring\nProvider of the Security Token Service.\nendpoint\nstring\nEndpoint is the HTTP/S endpoint of the Security Token Service from\nwhere temporary credentials will be fetched.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Secret containing authentication credentials\nfor the STS endpoint. This Secret must contain the fields\nusername\nand\npassword\nand is supported only for the\nldap\nprovider.\ncertSecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nCertSecretRef can be given the name of a Secret containing\neither or both of\na PEM-encoded client certificate (\ntls.crt\n) and private\nkey (\ntls.key\n);\na PEM-encoded CA certificate (\nca.crt\n)\nand whichever are supplied, will be used for connecting to the\nSTS endpoint. The client cert and key are useful if you are\nauthenticating with a certificate; the CA cert is useful if\nyou are using a self-signed server certificate. The Secret must\nbe of type\nOpaque\nor\nkubernetes.io/tls\n.\nThis field is only supported for the\nldap\nprovider.\nBucketSpec\n(\nAppears on:\nBucket\n)\nBucketSpec specifies the required configuration to produce an Artifact for\nan object storage bucket.\nField\nDescription\nprovider\nstring\n(Optional)\nProvider of the object storage bucket.\nDefaults to ‘generic’, which expects an S3 (API) compatible object\nstorage.\nbucketName\nstring\nBucketName is the name of the object storage bucket.\nendpoint\nstring\nEndpoint is the object storage address the BucketName is located at.\nsts\nBucketSTSSpec\n(Optional)\nSTS specifies the required configuration to use a Security Token\nService for fetching temporary credentials to authenticate in a\nBucket provider.\nThis field is only supported for the\naws\nand\ngeneric\nproviders.\ninsecure\nbool\n(Optional)\nInsecure allows connecting to a non-TLS HTTP Endpoint.\nregion\nstring\n(Optional)\nRegion of the Endpoint where the BucketName is located in.\nprefix\nstring\n(Optional)\nPrefix to use for server-side filtering of files in the Bucket.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Secret containing authentication credentials\nfor the Bucket.\ncertSecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nCertSecretRef can be given the name of a Secret containing\neither or both of\na PEM-encoded client certificate (\ntls.crt\n) and private\nkey (\ntls.key\n);\na PEM-encoded CA certificate (\nca.crt\n)\nand whichever are supplied, will be used for connecting to the\nbucket. The client cert and key are useful if you are\nauthenticating with a certificate; the CA cert is useful if\nyou are using a self-signed server certificate. The Secret must\nbe of type\nOpaque\nor\nkubernetes.io/tls\n.\nThis field is only supported for the\ngeneric\nprovider.\nproxySecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nProxySecretRef specifies the Secret containing the proxy configuration\nto use while communicating with the Bucket server.\ninterval\nKubernetes meta/v1.Duration\nInterval at which the Bucket Endpoint is checked for updates.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout for fetch operations, defaults to 60s.\nignore\nstring\n(Optional)\nIgnore overrides the set of excluded patterns in the .sourceignore format\n(which is the same as .gitignore). If not provided, a default will be used,\nconsult the documentation for your version to find out what those are.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend the reconciliation of this\nBucket.\nBucketStatus\n(\nAppears on:\nBucket\n)\nBucketStatus records the observed state of a Bucket.\nField\nDescription\nobservedGeneration\nint64\n(Optional)\nObservedGeneration is the last observed generation of the Bucket object.\nconditions\n[]Kubernetes meta/v1.Condition\n(Optional)\nConditions holds the conditions for the Bucket.\nurl\nstring\n(Optional)\nURL is the dynamic fetch link for the latest Artifact.\nIt is provided on a “best effort” basis, and using the precise\nBucketStatus.Artifact data is recommended.\nartifact\nArtifact\n(Optional)\nArtifact represents the last successful Bucket reconciliation.\nobservedIgnore\nstring\n(Optional)\nObservedIgnore is the observed exclusion patterns used for constructing\nthe source artifact.\nReconcileRequestStatus\ngithub.com/fluxcd/pkg/apis/meta.ReconcileRequestStatus\n(Members of\nReconcileRequestStatus\nare embedded into this type.)\nGitRepositoryInclude\n(\nAppears on:\nGitRepositorySpec\n,\nGitRepositoryStatus\n)\nGitRepositoryInclude specifies a local reference to a GitRepository which\nArtifact (sub-)contents must be included, and where they should be placed.\nField\nDescription\nrepository\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\nGitRepositoryRef specifies the GitRepository which Artifact contents\nmust be included.\nfromPath\nstring\n(Optional)\nFromPath specifies the path to copy contents from, defaults to the root\nof the Artifact.\ntoPath\nstring\n(Optional)\nToPath specifies the path to copy contents to, defaults to the name of\nthe GitRepositoryRef.\nGitRepositoryRef\n(\nAppears on:\nGitRepositorySpec\n)\nGitRepositoryRef specifies the Git reference to resolve and checkout.\nField\nDescription\nbranch\nstring\n(Optional)\nBranch to check out, defaults to ‘master’ if no other field is defined.\ntag\nstring\n(Optional)\nTag to check out, takes precedence over Branch.\nsemver\nstring\n(Optional)\nSemVer tag expression to check out, takes precedence over Tag.\nname\nstring\n(Optional)\nName of the reference to check out; takes precedence over Branch, Tag and SemVer.\nIt must be a valid Git reference:\nhttps://git-scm.com/docs/git-check-ref-format#_description\nExamples: “refs/heads/main”, “refs/tags/v0.1.0”, “refs/pull/420/head”, “refs/merge-requests/1/head”\ncommit\nstring\n(Optional)\nCommit SHA to check out, takes precedence over all reference fields.\nThis can be combined with Branch to shallow clone the branch, in which\nthe commit is expected to exist.\nGitRepositorySpec\n(\nAppears on:\nGitRepository\n)\nGitRepositorySpec specifies the required configuration to produce an\nArtifact for a Git repository.\nField\nDescription\nurl\nstring\nURL specifies the Git repository URL, it can be an HTTP/S or SSH address.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Secret containing authentication credentials for\nthe GitRepository.\nFor HTTPS repositories the Secret must contain ‘username’ and ‘password’\nfields for basic auth or ‘bearerToken’ field for token auth.\nFor SSH repositories the Secret must contain ‘identity’\nand ‘known_hosts’ fields.\nprovider\nstring\n(Optional)\nProvider used for authentication, can be ‘azure’, ‘github’, ‘generic’.\nWhen not specified, defaults to ‘generic’.\ninterval\nKubernetes meta/v1.Duration\nInterval at which the GitRepository URL is checked for updates.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout for Git operations like cloning, defaults to 60s.\nref\nGitRepositoryRef\n(Optional)\nReference specifies the Git reference to resolve and monitor for\nchanges, defaults to the ‘master’ branch.\nverify\nGitRepositoryVerification\n(Optional)\nVerification specifies the configuration to verify the Git commit\nsignature(s).\nproxySecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nProxySecretRef specifies the Secret containing the proxy configuration\nto use while communicating with the Git server.\nignore\nstring\n(Optional)\nIgnore overrides the set of excluded patterns in the .sourceignore format\n(which is the same as .gitignore). If not provided, a default will be used,\nconsult the documentation for your version to find out what those are.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend the reconciliation of this\nGitRepository.\nrecurseSubmodules\nbool\n(Optional)\nRecurseSubmodules enables the initialization of all submodules within\nthe GitRepository as cloned from the URL, using their default settings.\ninclude\n[]GitRepositoryInclude\n(Optional)\nInclude specifies a list of GitRepository resources which Artifacts\nshould be included in the Artifact produced for this GitRepository.\nsparseCheckout\n[]string\n(Optional)\nSparseCheckout specifies a list of directories to checkout when cloning\nthe repository. If specified, only these directories are included in the\nArtifact produced for this GitRepository.\nGitRepositoryStatus\n(\nAppears on:\nGitRepository\n)\nGitRepositoryStatus records the observed state of a Git repository.\nField\nDescription\nobservedGeneration\nint64\n(Optional)\nObservedGeneration is the last observed generation of the GitRepository\nobject.\nconditions\n[]Kubernetes meta/v1.Condition\n(Optional)\nConditions holds the conditions for the GitRepository.\nartifact\nArtifact\n(Optional)\nArtifact represents the last successful GitRepository reconciliation.\nincludedArtifacts\n[]Artifact\n(Optional)\nIncludedArtifacts contains a list of the last successfully included\nArtifacts as instructed by GitRepositorySpec.Include.\nobservedIgnore\nstring\n(Optional)\nObservedIgnore is the observed exclusion patterns used for constructing\nthe source artifact.\nobservedRecurseSubmodules\nbool\n(Optional)\nObservedRecurseSubmodules is the observed resource submodules\nconfiguration used to produce the current Artifact.\nobservedInclude\n[]GitRepositoryInclude\n(Optional)\nObservedInclude is the observed list of GitRepository resources used to\nproduce the current Artifact.\nobservedSparseCheckout\n[]string\n(Optional)\nObservedSparseCheckout is the observed list of directories used to\nproduce the current Artifact.\nsourceVerificationMode\nGitVerificationMode\n(Optional)\nSourceVerificationMode is the last used verification mode indicating\nwhich Git object(s) have been verified.\nReconcileRequestStatus\ngithub.com/fluxcd/pkg/apis/meta.ReconcileRequestStatus\n(Members of\nReconcileRequestStatus\nare embedded into this type.)\nGitRepositoryVerification\n(\nAppears on:\nGitRepositorySpec\n)\nGitRepositoryVerification specifies the Git commit signature verification\nstrategy.\nField\nDescription\nmode\nGitVerificationMode\n(Optional)\nMode specifies which Git object(s) should be verified.\nThe variants “head” and “HEAD” both imply the same thing, i.e. verify\nthe commit that the HEAD of the Git repository points to. The variant\n“head” solely exists to ensure backwards compatibility.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\nSecretRef specifies the Secret containing the public keys of trusted Git\nauthors.\nGitVerificationMode\n(\nstring\nalias)\n(\nAppears on:\nGitRepositoryStatus\n,\nGitRepositoryVerification\n)\nGitVerificationMode specifies the verification mode for a Git repository.\nHelmChartSpec\n(\nAppears on:\nHelmChart\n)\nHelmChartSpec specifies the desired state of a Helm chart.\nField\nDescription\nchart\nstring\nChart is the name or path the Helm chart is available at in the\nSourceRef.\nversion\nstring\n(Optional)\nVersion is the chart version semver expression, ignored for charts from\nGitRepository and Bucket sources. Defaults to latest when omitted.\nsourceRef\nLocalHelmChartSourceReference\nSourceRef is the reference to the Source the chart is available at.\ninterval\nKubernetes meta/v1.Duration\nInterval at which the HelmChart SourceRef is checked for updates.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\nreconcileStrategy\nstring\n(Optional)\nReconcileStrategy determines what enables the creation of a new artifact.\nValid values are (‘ChartVersion’, ‘Revision’).\nSee the documentation of the values for an explanation on their behavior.\nDefaults to ChartVersion when omitted.\nvaluesFiles\n[]string\n(Optional)\nValuesFiles is an alternative list of values files to use as the chart\nvalues (values.yaml is not included by default), expected to be a\nrelative path in the SourceRef.\nValues files are merged in the order of this list with the last file\noverriding the first. Ignored when omitted.\nignoreMissingValuesFiles\nbool\n(Optional)\nIgnoreMissingValuesFiles controls whether to silently ignore missing values\nfiles rather than failing.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend the reconciliation of this\nsource.\nverify\nOCIRepositoryVerification\n(Optional)\nVerify contains the secret name containing the trusted public keys\nused to verify the signature and specifies which provider to use to check\nwhether OCI image is authentic.\nThis field is only supported when using HelmRepository source with spec.type ‘oci’.\nChart dependencies, which are not bundled in the umbrella chart artifact, are not verified.\nHelmChartStatus\n(\nAppears on:\nHelmChart\n)\nHelmChartStatus records the observed state of the HelmChart.\nField\nDescription\nobservedGeneration\nint64\n(Optional)\nObservedGeneration is the last observed generation of the HelmChart\nobject.\nobservedSourceArtifactRevision\nstring\n(Optional)\nObservedSourceArtifactRevision is the last observed Artifact.Revision\nof the HelmChartSpec.SourceRef.\nobservedChartName\nstring\n(Optional)\nObservedChartName is the last observed chart name as specified by the\nresolved chart reference.\nobservedValuesFiles\n[]string\n(Optional)\nObservedValuesFiles are the observed value files of the last successful\nreconciliation.\nIt matches the chart in the last successfully reconciled artifact.\nconditions\n[]Kubernetes meta/v1.Condition\n(Optional)\nConditions holds the conditions for the HelmChart.\nurl\nstring\n(Optional)\nURL is the dynamic fetch link for the latest Artifact.\nIt is provided on a “best effort” basis, and using the precise\nBucketStatus.Artifact data is recommended.\nartifact\nArtifact\n(Optional)\nArtifact represents the output of the last successful reconciliation.\nReconcileRequestStatus\ngithub.com/fluxcd/pkg/apis/meta.ReconcileRequestStatus\n(Members of\nReconcileRequestStatus\nare embedded into this type.)\nHelmRepositorySpec\n(\nAppears on:\nHelmRepository\n)\nHelmRepositorySpec specifies the required configuration to produce an\nArtifact for a Helm repository index YAML.\nField\nDescription\nurl\nstring\nURL of the Helm repository, a valid URL contains at least a protocol and\nhost.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Secret containing authentication credentials\nfor the HelmRepository.\nFor HTTP/S basic auth the secret must contain ‘username’ and ‘password’\nfields.\nSupport for TLS auth using the ‘certFile’ and ‘keyFile’, and/or ‘caFile’\nkeys is deprecated. Please use\n.spec.certSecretRef\ninstead.\ncertSecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nCertSecretRef can be given the name of a Secret containing\neither or both of\na PEM-encoded client certificate (\ntls.crt\n) and private\nkey (\ntls.key\n);\na PEM-encoded CA certificate (\nca.crt\n)\nand whichever are supplied, will be used for connecting to the\nregistry. The client cert and key are useful if you are\nauthenticating with a certificate; the CA cert is useful if\nyou are using a self-signed server certificate. The Secret must\nbe of type\nOpaque\nor\nkubernetes.io/tls\n.\nIt takes precedence over the values specified in the Secret referred\nto by\n.spec.secretRef\n.\npassCredentials\nbool\n(Optional)\nPassCredentials allows the credentials from the SecretRef to be passed\non to a host that does not match the host as defined in URL.\nThis may be required if the host of the advertised chart URLs in the\nindex differ from the defined URL.\nEnabling this should be done with caution, as it can potentially result\nin credentials getting stolen in a MITM-attack.\ninterval\nKubernetes meta/v1.Duration\n(Optional)\nInterval at which the HelmRepository URL is checked for updates.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\ninsecure\nbool\n(Optional)\nInsecure allows connecting to a non-TLS HTTP container registry.\nThis field is only taken into account if the .spec.type field is set to ‘oci’.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout is used for the index fetch operation for an HTTPS helm repository,\nand for remote OCI Repository operations like pulling for an OCI helm\nchart by the associated HelmChart.\nIts default value is 60s.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend the reconciliation of this\nHelmRepository.\naccessFrom\ngithub.com/fluxcd/pkg/apis/acl.AccessFrom\n(Optional)\nAccessFrom specifies an Access Control List for allowing cross-namespace\nreferences to this object.\nNOTE: Not implemented, provisional as of\nhttps://github.com/fluxcd/flux2/pull/2092\ntype\nstring\n(Optional)\nType of the HelmRepository.\nWhen this field is set to “oci”, the URL field value must be prefixed with “oci://”.\nprovider\nstring\n(Optional)\nProvider used for authentication, can be ‘aws’, ‘azure’, ‘gcp’ or ‘generic’.\nThis field is optional, and only taken into account if the .spec.type field is set to ‘oci’.\nWhen not specified, defaults to ‘generic’.\nHelmRepositoryStatus\n(\nAppears on:\nHelmRepository\n)\nHelmRepositoryStatus records the observed state of the HelmRepository.\nField\nDescription\nobservedGeneration\nint64\n(Optional)\nObservedGeneration is the last observed generation of the HelmRepository\nobject.\nconditions\n[]Kubernetes meta/v1.Condition\n(Optional)\nConditions holds the conditions for the HelmRepository.\nurl\nstring\n(Optional)\nURL is the dynamic fetch link for the latest Artifact.\nIt is provided on a “best effort” basis, and using the precise\nHelmRepositoryStatus.Artifact data is recommended.\nartifact\nArtifact\n(Optional)\nArtifact represents the last successful HelmRepository reconciliation.\nReconcileRequestStatus\ngithub.com/fluxcd/pkg/apis/meta.ReconcileRequestStatus\n(Members of\nReconcileRequestStatus\nare embedded into this type.)\nLocalHelmChartSourceReference\n(\nAppears on:\nHelmChartSpec\n)\nLocalHelmChartSourceReference contains enough information to let you locate\nthe typed referenced object at namespace level.\nField\nDescription\napiVersion\nstring\n(Optional)\nAPIVersion of the referent.\nkind\nstring\nKind of the referent, valid values are (‘HelmRepository’, ‘GitRepository’,\n‘Bucket’).\nname\nstring\nName of the referent.\nOCILayerSelector\n(\nAppears on:\nOCIRepositorySpec\n,\nOCIRepositoryStatus\n)\nOCILayerSelector specifies which layer should be extracted from an OCI Artifact\nField\nDescription\nmediaType\nstring\n(Optional)\nMediaType specifies the OCI media type of the layer\nwhich should be extracted from the OCI Artifact. The\nfirst layer matching this type is selected.\noperation\nstring\n(Optional)\nOperation specifies how the selected layer should be processed.\nBy default, the layer compressed content is extracted to storage.\nWhen the operation is set to ‘copy’, the layer compressed content\nis persisted to storage as it is.\nOCIRepositoryRef\n(\nAppears on:\nOCIRepositorySpec\n)\nOCIRepositoryRef defines the image reference for the OCIRepository’s URL\nField\nDescription\ndigest\nstring\n(Optional)\nDigest is the image digest to pull, takes precedence over SemVer.\nThe value should be in the format ‘sha256:\n’.\nsemver\nstring\n(Optional)\nSemVer is the range of tags to pull selecting the latest within\nthe range, takes precedence over Tag.\nsemverFilter\nstring\n(Optional)\nSemverFilter is a regex pattern to filter the tags within the SemVer range.\ntag\nstring\n(Optional)\nTag is the image tag to pull, defaults to latest.\nOCIRepositorySpec\n(\nAppears on:\nOCIRepository\n)\nOCIRepositorySpec defines the desired state of OCIRepository\nField\nDescription\nurl\nstring\nURL is a reference to an OCI artifact repository hosted\non a remote container registry.\nref\nOCIRepositoryRef\n(Optional)\nThe OCI reference to pull and monitor for changes,\ndefaults to the latest tag.\nlayerSelector\nOCILayerSelector\n(Optional)\nLayerSelector specifies which layer should be extracted from the OCI artifact.\nWhen not specified, the first layer found in the artifact is selected.\nprovider\nstring\n(Optional)\nThe provider used for authentication, can be ‘aws’, ‘azure’, ‘gcp’ or ‘generic’.\nWhen not specified, defaults to ‘generic’.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef contains the secret name containing the registry login\ncredentials to resolve image metadata.\nThe secret must be of type kubernetes.io/dockerconfigjson.\nverify\nOCIRepositoryVerification\n(Optional)\nVerify contains the secret name containing the trusted public keys\nused to verify the signature and specifies which provider to use to check\nwhether OCI image is authentic.\nserviceAccountName\nstring\n(Optional)\nServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate\nthe image pull if the service account has attached pull secrets. For more information:\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account\ncertSecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nCertSecretRef can be given the name of a Secret containing\neither or both of\na PEM-encoded client certificate (\ntls.crt\n) and private\nkey (\ntls.key\n);\na PEM-encoded CA certificate (\nca.crt\n)\nand whichever are supplied, will be used for connecting to the\nregistry. The client cert and key are useful if you are\nauthenticating with a certificate; the CA cert is useful if\nyou are using a self-signed server certificate. The Secret must\nbe of type\nOpaque\nor\nkubernetes.io/tls\n.\nproxySecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nProxySecretRef specifies the Secret containing the proxy configuration\nto use while communicating with the container registry.\ninterval\nKubernetes meta/v1.Duration\nInterval at which the OCIRepository URL is checked for updates.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nThe timeout for remote OCI Repository operations like pulling, defaults to 60s.\nignore\nstring\n(Optional)\nIgnore overrides the set of excluded patterns in the .sourceignore format\n(which is the same as .gitignore). If not provided, a default will be used,\nconsult the documentation for your version to find out what those are.\ninsecure\nbool\n(Optional)\nInsecure allows connecting to a non-TLS HTTP container registry.\nsuspend\nbool\n(Optional)\nThis flag tells the controller to suspend the reconciliation of this source.\nOCIRepositoryStatus\n(\nAppears on:\nOCIRepository\n)\nOCIRepositoryStatus defines the observed state of OCIRepository\nField\nDescription\nobservedGeneration\nint64\n(Optional)\nObservedGeneration is the last observed generation.\nconditions\n[]Kubernetes meta/v1.Condition\n(Optional)\nConditions holds the conditions for the OCIRepository.\nurl\nstring\n(Optional)\nURL is the download link for the artifact output of the last OCI Repository sync.\nartifact\nArtifact\n(Optional)\nArtifact represents the output of the last successful OCI Repository sync.\nobservedIgnore\nstring\n(Optional)\nObservedIgnore is the observed exclusion patterns used for constructing\nthe source artifact.\nobservedLayerSelector\nOCILayerSelector\n(Optional)\nObservedLayerSelector is the observed layer selector used for constructing\nthe source artifact.\nReconcileRequestStatus\ngithub.com/fluxcd/pkg/apis/meta.ReconcileRequestStatus\n(Members of\nReconcileRequestStatus\nare embedded into this type.)\nOCIRepositoryVerification\n(\nAppears on:\nHelmChartSpec\n,\nOCIRepositorySpec\n)\nOCIRepositoryVerification verifies the authenticity of an OCI Artifact\nField\nDescription\nprovider\nstring\nProvider specifies the technology used to sign the OCI Artifact.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Kubernetes Secret containing the\ntrusted public keys.\nmatchOIDCIdentity\n[]OIDCIdentityMatch\n(Optional)\nMatchOIDCIdentity specifies the identity matching criteria to use\nwhile verifying an OCI artifact which was signed using Cosign keyless\nsigning. The artifact’s identity is deemed to be verified if any of the\nspecified matchers match against the identity.\nOIDCIdentityMatch\n(\nAppears on:\nOCIRepositoryVerification\n)\nOIDCIdentityMatch specifies options for verifying the certificate identity,\ni.e. the issuer and the subject of the certificate.\nField\nDescription\nissuer\nstring\nIssuer specifies the regex pattern to match against to verify\nthe OIDC issuer in the Fulcio certificate. The pattern must be a\nvalid Go regular expression.\nsubject\nstring\nSubject specifies the regex pattern to match against to verify\nthe identity subject in the Fulcio certificate. The pattern must\nbe a valid Go regular expression.\nSource\nSource interface must be supported by all API types.\nSource is the interface that provides generic access to the Artifact and\ninterval. It must be supported by all kinds of the source.toolkit.fluxcd.io\nAPI group.\nThis page was automatically generated with\ngen-crd-api-reference-docs\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nToolkit Components\nKustomize Controller\nAPI Reference\nAPI Reference\nKustomize API reference v1\nThe GitOps Toolkit Custom Resource Definitions documentation.\nLast modified 2023-03-23:\nGA: adapt to SC, NC and KC APIs graduating to v1 (3d32a85)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nToolkit Components\nKustomize Controller\nAPI Reference\nKustomize API reference v1\nKustomize API reference v1\nThe GitOps Toolkit Custom Resource Definitions documentation.\nPackages:\nkustomize.toolkit.fluxcd.io/v1\nkustomize.toolkit.fluxcd.io/v1\nPackage v1 contains API Schema definitions for the kustomize.toolkit.fluxcd.io\nv1 API group.\nResource Types:\nKustomization\nKustomization\nKustomization is the Schema for the kustomizations API.\nField\nDescription\napiVersion\nstring\nkustomize.toolkit.fluxcd.io/v1\nkind\nstring\nKustomization\nmetadata\nKubernetes meta/v1.ObjectMeta\nRefer to the Kubernetes API documentation for the fields of the\nmetadata\nfield.\nspec\nKustomizationSpec\ncommonMetadata\nCommonMetadata\n(Optional)\nCommonMetadata specifies the common labels and annotations that are\napplied to all resources. Any existing label or annotation will be\noverridden if its key matches a common one.\ndependsOn\n[]github.com/fluxcd/pkg/apis/meta.NamespacedObjectReference\n(Optional)\nDependsOn may contain a meta.NamespacedObjectReference slice\nwith references to Kustomization resources that must be ready before this\nKustomization can be reconciled.\ndecryption\nDecryption\n(Optional)\nDecrypt Kubernetes secrets before applying them on the cluster.\ninterval\nKubernetes meta/v1.Duration\nThe interval at which to reconcile the Kustomization.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\nretryInterval\nKubernetes meta/v1.Duration\n(Optional)\nThe interval at which to retry a previously failed reconciliation.\nWhen not specified, the controller uses the KustomizationSpec.Interval\nvalue to retry failures.\nkubeConfig\ngithub.com/fluxcd/pkg/apis/meta.KubeConfigReference\n(Optional)\nThe KubeConfig for reconciling the Kustomization on a remote cluster.\nWhen used in combination with KustomizationSpec.ServiceAccountName,\nforces the controller to act on behalf of that Service Account at the\ntarget cluster.\nIf the –default-service-account flag is set, its value will be used as\na controller level fallback for when KustomizationSpec.ServiceAccountName\nis empty.\npath\nstring\n(Optional)\nPath to the directory containing the kustomization.yaml file, or the\nset of plain YAMLs a kustomization.yaml should be generated for.\nDefaults to ‘None’, which translates to the root path of the SourceRef.\npostBuild\nPostBuild\n(Optional)\nPostBuild describes which actions to perform on the YAML manifest\ngenerated by building the kustomize overlay.\nprune\nbool\nPrune enables garbage collection.\ndeletionPolicy\nstring\n(Optional)\nDeletionPolicy can be used to control garbage collection when this\nKustomization is deleted. Valid values are (‘MirrorPrune’, ‘Delete’,\n‘WaitForTermination’, ‘Orphan’). ‘MirrorPrune’ mirrors the Prune field\n(orphan if false, delete if true). Defaults to ‘MirrorPrune’.\nhealthChecks\n[]github.com/fluxcd/pkg/apis/meta.NamespacedObjectKindReference\n(Optional)\nA list of resources to be included in the health assessment.\nnamePrefix\nstring\n(Optional)\nNamePrefix will prefix the names of all managed resources.\nnameSuffix\nstring\n(Optional)\nNameSuffix will suffix the names of all managed resources.\npatches\n[]github.com/fluxcd/pkg/apis/kustomize.Patch\n(Optional)\nStrategic merge and JSON patches, defined as inline YAML objects,\ncapable of targeting objects based on kind, label and annotation selectors.\nimages\n[]github.com/fluxcd/pkg/apis/kustomize.Image\n(Optional)\nImages is a list of (image name, new name, new tag or digest)\nfor changing image names, tags or digests. This can also be achieved with a\npatch, but this operator is simpler to specify.\nserviceAccountName\nstring\n(Optional)\nThe name of the Kubernetes service account to impersonate\nwhen reconciling this Kustomization.\nsourceRef\nCrossNamespaceSourceReference\nReference of the source where the kustomization file is.\nsuspend\nbool\n(Optional)\nThis flag tells the controller to suspend subsequent kustomize executions,\nit does not apply to already started executions. Defaults to false.\ntargetNamespace\nstring\n(Optional)\nTargetNamespace sets or overrides the namespace in the\nkustomization.yaml file.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout for validation, apply and health checking operations.\nDefaults to ‘Interval’ duration.\nforce\nbool\n(Optional)\nForce instructs the controller to recreate resources\nwhen patching fails due to an immutable field change.\nwait\nbool\n(Optional)\nWait instructs the controller to check the health of all the reconciled\nresources. When enabled, the HealthChecks are ignored. Defaults to false.\ncomponents\n[]string\n(Optional)\nComponents specifies relative paths to specifications of other Components.\nhealthCheckExprs\n[]github.com/fluxcd/pkg/apis/kustomize.CustomHealthCheck\n(Optional)\nHealthCheckExprs is a list of healthcheck expressions for evaluating the\nhealth of custom resources using Common Expression Language (CEL).\nThe expressions are evaluated only when Wait or HealthChecks are specified.\nstatus\nKustomizationStatus\nCommonMetadata\n(\nAppears on:\nKustomizationSpec\n)\nCommonMetadata defines the common labels and annotations.\nField\nDescription\nannotations\nmap[string]string\n(Optional)\nAnnotations to be added to the object’s metadata.\nlabels\nmap[string]string\n(Optional)\nLabels to be added to the object’s metadata.\nCrossNamespaceSourceReference\n(\nAppears on:\nKustomizationSpec\n)\nCrossNamespaceSourceReference contains enough information to let you locate the\ntyped Kubernetes resource object at cluster level.\nField\nDescription\napiVersion\nstring\n(Optional)\nAPI version of the referent.\nkind\nstring\nKind of the referent.\nname\nstring\nName of the referent.\nnamespace\nstring\n(Optional)\nNamespace of the referent, defaults to the namespace of the Kubernetes\nresource object that contains the reference.\nDecryption\n(\nAppears on:\nKustomizationSpec\n)\nDecryption defines how decryption is handled for Kubernetes manifests.\nField\nDescription\nprovider\nstring\nProvider is the name of the decryption engine.\nserviceAccountName\nstring\n(Optional)\nServiceAccountName is the name of the service account used to\nauthenticate with KMS services from cloud providers. If a\nstatic credential for a given cloud provider is defined\ninside the Secret referenced by SecretRef, that static\ncredential takes priority.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nThe secret name containing the private OpenPGP keys used for decryption.\nA static credential for a cloud provider defined inside the Secret\ntakes priority to secret-less authentication with the ServiceAccountName\nfield.\nKustomizationSpec\n(\nAppears on:\nKustomization\n)\nKustomizationSpec defines the configuration to calculate the desired state\nfrom a Source using Kustomize.\nField\nDescription\ncommonMetadata\nCommonMetadata\n(Optional)\nCommonMetadata specifies the common labels and annotations that are\napplied to all resources. Any existing label or annotation will be\noverridden if its key matches a common one.\ndependsOn\n[]github.com/fluxcd/pkg/apis/meta.NamespacedObjectReference\n(Optional)\nDependsOn may contain a meta.NamespacedObjectReference slice\nwith references to Kustomization resources that must be ready before this\nKustomization can be reconciled.\ndecryption\nDecryption\n(Optional)\nDecrypt Kubernetes secrets before applying them on the cluster.\ninterval\nKubernetes meta/v1.Duration\nThe interval at which to reconcile the Kustomization.\nThis interval is approximate and may be subject to jitter to ensure\nefficient use of resources.\nretryInterval\nKubernetes meta/v1.Duration\n(Optional)\nThe interval at which to retry a previously failed reconciliation.\nWhen not specified, the controller uses the KustomizationSpec.Interval\nvalue to retry failures.\nkubeConfig\ngithub.com/fluxcd/pkg/apis/meta.KubeConfigReference\n(Optional)\nThe KubeConfig for reconciling the Kustomization on a remote cluster.\nWhen used in combination with KustomizationSpec.ServiceAccountName,\nforces the controller to act on behalf of that Service Account at the\ntarget cluster.\nIf the –default-service-account flag is set, its value will be used as\na controller level fallback for when KustomizationSpec.ServiceAccountName\nis empty.\npath\nstring\n(Optional)\nPath to the directory containing the kustomization.yaml file, or the\nset of plain YAMLs a kustomization.yaml should be generated for.\nDefaults to ‘None’, which translates to the root path of the SourceRef.\npostBuild\nPostBuild\n(Optional)\nPostBuild describes which actions to perform on the YAML manifest\ngenerated by building the kustomize overlay.\nprune\nbool\nPrune enables garbage collection.\ndeletionPolicy\nstring\n(Optional)\nDeletionPolicy can be used to control garbage collection when this\nKustomization is deleted. Valid values are (‘MirrorPrune’, ‘Delete’,\n‘WaitForTermination’, ‘Orphan’). ‘MirrorPrune’ mirrors the Prune field\n(orphan if false, delete if true). Defaults to ‘MirrorPrune’.\nhealthChecks\n[]github.com/fluxcd/pkg/apis/meta.NamespacedObjectKindReference\n(Optional)\nA list of resources to be included in the health assessment.\nnamePrefix\nstring\n(Optional)\nNamePrefix will prefix the names of all managed resources.\nnameSuffix\nstring\n(Optional)\nNameSuffix will suffix the names of all managed resources.\npatches\n[]github.com/fluxcd/pkg/apis/kustomize.Patch\n(Optional)\nStrategic merge and JSON patches, defined as inline YAML objects,\ncapable of targeting objects based on kind, label and annotation selectors.\nimages\n[]github.com/fluxcd/pkg/apis/kustomize.Image\n(Optional)\nImages is a list of (image name, new name, new tag or digest)\nfor changing image names, tags or digests. This can also be achieved with a\npatch, but this operator is simpler to specify.\nserviceAccountName\nstring\n(Optional)\nThe name of the Kubernetes service account to impersonate\nwhen reconciling this Kustomization.\nsourceRef\nCrossNamespaceSourceReference\nReference of the source where the kustomization file is.\nsuspend\nbool\n(Optional)\nThis flag tells the controller to suspend subsequent kustomize executions,\nit does not apply to already started executions. Defaults to false.\ntargetNamespace\nstring\n(Optional)\nTargetNamespace sets or overrides the namespace in the\nkustomization.yaml file.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout for validation, apply and health checking operations.\nDefaults to ‘Interval’ duration.\nforce\nbool\n(Optional)\nForce instructs the controller to recreate resources\nwhen patching fails due to an immutable field change.\nwait\nbool\n(Optional)\nWait instructs the controller to check the health of all the reconciled\nresources. When enabled, the HealthChecks are ignored. Defaults to false.\ncomponents\n[]string\n(Optional)\nComponents specifies relative paths to specifications of other Components.\nhealthCheckExprs\n[]github.com/fluxcd/pkg/apis/kustomize.CustomHealthCheck\n(Optional)\nHealthCheckExprs is a list of healthcheck expressions for evaluating the\nhealth of custom resources using Common Expression Language (CEL).\nThe expressions are evaluated only when Wait or HealthChecks are specified.\nKustomizationStatus\n(\nAppears on:\nKustomization\n)\nKustomizationStatus defines the observed state of a kustomization.\nField\nDescription\nReconcileRequestStatus\ngithub.com/fluxcd/pkg/apis/meta.ReconcileRequestStatus\n(Members of\nReconcileRequestStatus\nare embedded into this type.)\nobservedGeneration\nint64\n(Optional)\nObservedGeneration is the last reconciled generation.\nconditions\n[]Kubernetes meta/v1.Condition\n(Optional)\nlastAppliedRevision\nstring\n(Optional)\nThe last successfully applied revision.\nEquals the Revision of the applied Artifact from the referenced Source.\nlastAppliedOriginRevision\nstring\n(Optional)\nThe last successfully applied origin revision.\nEquals the origin revision of the applied Artifact from the referenced Source.\nUsually present on the Metadata of the applied Artifact and depends on the\nSource type, e.g. for OCI it’s the value associated with the key\n“org.opencontainers.image.revision”.\nlastAttemptedRevision\nstring\n(Optional)\nLastAttemptedRevision is the revision of the last reconciliation attempt.\ninventory\nResourceInventory\n(Optional)\nInventory contains the list of Kubernetes resource object references that\nhave been successfully applied.\nPostBuild\n(\nAppears on:\nKustomizationSpec\n)\nPostBuild describes which actions to perform on the YAML manifest\ngenerated by building the kustomize overlay.\nField\nDescription\nsubstitute\nmap[string]string\n(Optional)\nSubstitute holds a map of key/value pairs.\nThe variables defined in your YAML manifests that match any of the keys\ndefined in the map will be substituted with the set value.\nIncludes support for bash string replacement functions\ne.g. ${var:=default}, ${var:position} and ${var/substring/replacement}.\nsubstituteFrom\n[]SubstituteReference\n(Optional)\nSubstituteFrom holds references to ConfigMaps and Secrets containing\nthe variables and their values to be substituted in the YAML manifests.\nThe ConfigMap and the Secret data keys represent the var names, and they\nmust match the vars declared in the manifests for the substitution to\nhappen.\nResourceInventory\n(\nAppears on:\nKustomizationStatus\n)\nResourceInventory contains a list of Kubernetes resource object references\nthat have been applied by a Kustomization.\nField\nDescription\nentries\n[]ResourceRef\nEntries of Kubernetes resource object references.\nResourceRef\n(\nAppears on:\nResourceInventory\n)\nResourceRef contains the information necessary to locate a resource within a cluster.\nField\nDescription\nid\nstring\nID is the string representation of the Kubernetes resource object’s metadata,\nin the format ‘\n_\n’.\nv\nstring\nVersion is the API version of the Kubernetes resource object’s kind.\nSubstituteReference\n(\nAppears on:\nPostBuild\n)\nSubstituteReference contains a reference to a resource containing\nthe variables name and value.\nField\nDescription\nkind\nstring\nKind of the values referent, valid values are (‘Secret’, ‘ConfigMap’).\nname\nstring\nName of the values referent. Should reside in the same namespace as the\nreferring resource.\noptional\nbool\n(Optional)\nOptional indicates whether the referenced resource must exist, or whether to\ntolerate its absence. If true and the referenced resource is absent, proceed\nas if the resource was present but empty, without any variables defined.\nThis page was automatically generated with\ngen-crd-api-reference-docs\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nToolkit Components\nHelm Controller\nAPI Reference\nAPI Reference\nHelm API reference v2\nThe GitOps Toolkit Custom Resource Definitions documentation.\nLast modified 2023-07-04:\nGeneralize import-flux2-assets.sh script (070bfee)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nToolkit Components\nHelm Controller\nAPI Reference\nHelm API reference v2\nHelm API reference v2\nThe GitOps Toolkit Custom Resource Definitions documentation.\nPackages:\nhelm.toolkit.fluxcd.io/v2\nhelm.toolkit.fluxcd.io/v2\nPackage v2 contains API Schema definitions for the helm v2 API group\nResource Types:\nHelmRelease\nHelmRelease\nHelmRelease is the Schema for the helmreleases API\nField\nDescription\napiVersion\nstring\nhelm.toolkit.fluxcd.io/v2\nkind\nstring\nHelmRelease\nmetadata\nKubernetes meta/v1.ObjectMeta\nRefer to the Kubernetes API documentation for the fields of the\nmetadata\nfield.\nspec\nHelmReleaseSpec\nchart\nHelmChartTemplate\n(Optional)\nChart defines the template of the v1.HelmChart that should be created\nfor this HelmRelease.\nchartRef\nCrossNamespaceSourceReference\n(Optional)\nChartRef holds a reference to a source controller resource containing the\nHelm chart artifact.\ninterval\nKubernetes meta/v1.Duration\nInterval at which to reconcile the Helm release.\nkubeConfig\ngithub.com/fluxcd/pkg/apis/meta.KubeConfigReference\n(Optional)\nKubeConfig for reconciling the HelmRelease on a remote cluster.\nWhen used in combination with HelmReleaseSpec.ServiceAccountName,\nforces the controller to act on behalf of that Service Account at the\ntarget cluster.\nIf the –default-service-account flag is set, its value will be used as\na controller level fallback for when HelmReleaseSpec.ServiceAccountName\nis empty.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend reconciliation for this HelmRelease,\nit does not apply to already started reconciliations. Defaults to false.\nreleaseName\nstring\n(Optional)\nReleaseName used for the Helm release. Defaults to a composition of\n‘[TargetNamespace-]Name’.\ntargetNamespace\nstring\n(Optional)\nTargetNamespace to target when performing operations for the HelmRelease.\nDefaults to the namespace of the HelmRelease.\nstorageNamespace\nstring\n(Optional)\nStorageNamespace used for the Helm storage.\nDefaults to the namespace of the HelmRelease.\ndependsOn\n[]github.com/fluxcd/pkg/apis/meta.NamespacedObjectReference\n(Optional)\nDependsOn may contain a meta.NamespacedObjectReference slice with\nreferences to HelmRelease resources that must be ready before this HelmRelease\ncan be reconciled.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout is the time to wait for any individual Kubernetes operation (like Jobs\nfor hooks) during the performance of a Helm action. Defaults to ‘5m0s’.\nmaxHistory\nint\n(Optional)\nMaxHistory is the number of revisions saved by Helm for this HelmRelease.\nUse ‘0’ for an unlimited number of revisions; defaults to ‘5’.\nserviceAccountName\nstring\n(Optional)\nThe name of the Kubernetes service account to impersonate\nwhen reconciling this HelmRelease.\npersistentClient\nbool\n(Optional)\nPersistentClient tells the controller to use a persistent Kubernetes\nclient for this release. When enabled, the client will be reused for the\nduration of the reconciliation, instead of being created and destroyed\nfor each (step of a) Helm action.\nThis can improve performance, but may cause issues with some Helm charts\nthat for example do create Custom Resource Definitions during installation\noutside Helm’s CRD lifecycle hooks, which are then not observed to be\navailable by e.g. post-install hooks.\nIf not set, it defaults to true.\ndriftDetection\nDriftDetection\n(Optional)\nDriftDetection holds the configuration for detecting and handling\ndifferences between the manifest in the Helm storage and the resources\ncurrently existing in the cluster.\ninstall\nInstall\n(Optional)\nInstall holds the configuration for Helm install actions for this HelmRelease.\nupgrade\nUpgrade\n(Optional)\nUpgrade holds the configuration for Helm upgrade actions for this HelmRelease.\ntest\nTest\n(Optional)\nTest holds the configuration for Helm test actions for this HelmRelease.\nrollback\nRollback\n(Optional)\nRollback holds the configuration for Helm rollback actions for this HelmRelease.\nuninstall\nUninstall\n(Optional)\nUninstall holds the configuration for Helm uninstall actions for this HelmRelease.\nvaluesFrom\n[]github.com/fluxcd/pkg/apis/meta.ValuesReference\nValuesFrom holds references to resources containing Helm values for this HelmRelease,\nand information about how they should be merged.\nvalues\nKubernetes pkg/apis/apiextensions/v1.JSON\n(Optional)\nValues holds the values for this Helm release.\npostRenderers\n[]PostRenderer\n(Optional)\nPostRenderers holds an array of Helm PostRenderers, which will be applied in order\nof their definition.\nstatus\nHelmReleaseStatus\nCRDsPolicy\n(\nstring\nalias)\n(\nAppears on:\nInstall\n,\nUpgrade\n)\nCRDsPolicy defines the install/upgrade approach to use for CRDs when\ninstalling or upgrading a HelmRelease.\nCrossNamespaceObjectReference\n(\nAppears on:\nHelmChartTemplateSpec\n)\nCrossNamespaceObjectReference contains enough information to let you locate\nthe typed referenced object at cluster level.\nField\nDescription\napiVersion\nstring\n(Optional)\nAPIVersion of the referent.\nkind\nstring\nKind of the referent.\nname\nstring\nName of the referent.\nnamespace\nstring\n(Optional)\nNamespace of the referent.\nCrossNamespaceSourceReference\n(\nAppears on:\nHelmReleaseSpec\n)\nCrossNamespaceSourceReference contains enough information to let you locate\nthe typed referenced object at cluster level.\nField\nDescription\napiVersion\nstring\n(Optional)\nAPIVersion of the referent.\nkind\nstring\nKind of the referent.\nname\nstring\nName of the referent.\nnamespace\nstring\n(Optional)\nNamespace of the referent, defaults to the namespace of the Kubernetes\nresource object that contains the reference.\nDriftDetection\n(\nAppears on:\nHelmReleaseSpec\n)\nDriftDetection defines the strategy for performing differential analysis and\nprovides a way to define rules for ignoring specific changes during this\nprocess.\nField\nDescription\nmode\nDriftDetectionMode\n(Optional)\nMode defines how differences should be handled between the Helm manifest\nand the manifest currently applied to the cluster.\nIf not explicitly set, it defaults to DiffModeDisabled.\nignore\n[]IgnoreRule\n(Optional)\nIgnore contains a list of rules for specifying which changes to ignore\nduring diffing.\nDriftDetectionMode\n(\nstring\nalias)\n(\nAppears on:\nDriftDetection\n)\nDriftDetectionMode represents the modes in which a controller can detect and\nhandle differences between the manifest in the Helm storage and the resources\ncurrently existing in the cluster.\nFilter\n(\nAppears on:\nTest\n)\nFilter holds the configuration for individual Helm test filters.\nField\nDescription\nname\nstring\nName is the name of the test.\nexclude\nbool\n(Optional)\nExclude specifies whether the named test should be excluded.\nHelmChartTemplate\n(\nAppears on:\nHelmReleaseSpec\n)\nHelmChartTemplate defines the template from which the controller will\ngenerate a v1.HelmChart object in the same namespace as the referenced\nv1.Source.\nField\nDescription\nmetadata\nHelmChartTemplateObjectMeta\n(Optional)\nObjectMeta holds the template for metadata like labels and annotations.\nspec\nHelmChartTemplateSpec\nSpec holds the template for the v1.HelmChartSpec for this HelmRelease.\nchart\nstring\nThe name or path the Helm chart is available at in the SourceRef.\nversion\nstring\n(Optional)\nVersion semver expression, ignored for charts from v1.GitRepository and\nv1beta2.Bucket sources. Defaults to latest when omitted.\nsourceRef\nCrossNamespaceObjectReference\nThe name and namespace of the v1.Source the chart is available at.\ninterval\nKubernetes meta/v1.Duration\n(Optional)\nInterval at which to check the v1.Source for updates. Defaults to\n‘HelmReleaseSpec.Interval’.\nreconcileStrategy\nstring\n(Optional)\nDetermines what enables the creation of a new artifact. Valid values are\n(‘ChartVersion’, ‘Revision’).\nSee the documentation of the values for an explanation on their behavior.\nDefaults to ChartVersion when omitted.\nvaluesFiles\n[]string\n(Optional)\nAlternative list of values files to use as the chart values (values.yaml\nis not included by default), expected to be a relative path in the SourceRef.\nValues files are merged in the order of this list with the last file overriding\nthe first. Ignored when omitted.\nignoreMissingValuesFiles\nbool\n(Optional)\nIgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing.\nverify\nHelmChartTemplateVerification\n(Optional)\nVerify contains the secret name containing the trusted public keys\nused to verify the signature and specifies which provider to use to check\nwhether OCI image is authentic.\nThis field is only supported for OCI sources.\nChart dependencies, which are not bundled in the umbrella chart artifact,\nare not verified.\nHelmChartTemplateObjectMeta\n(\nAppears on:\nHelmChartTemplate\n)\nHelmChartTemplateObjectMeta defines the template for the ObjectMeta of a\nv1.HelmChart.\nField\nDescription\nlabels\nmap[string]string\n(Optional)\nMap of string keys and values that can be used to organize and categorize\n(scope and select) objects.\nMore info:\nhttps://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\nannotations\nmap[string]string\n(Optional)\nAnnotations is an unstructured key value map stored with a resource that may be\nset by external tools to store and retrieve arbitrary metadata. They are not\nqueryable and should be preserved when modifying objects.\nMore info:\nhttps://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\nHelmChartTemplateSpec\n(\nAppears on:\nHelmChartTemplate\n)\nHelmChartTemplateSpec defines the template from which the controller will\ngenerate a v1.HelmChartSpec object.\nField\nDescription\nchart\nstring\nThe name or path the Helm chart is available at in the SourceRef.\nversion\nstring\n(Optional)\nVersion semver expression, ignored for charts from v1.GitRepository and\nv1beta2.Bucket sources. Defaults to latest when omitted.\nsourceRef\nCrossNamespaceObjectReference\nThe name and namespace of the v1.Source the chart is available at.\ninterval\nKubernetes meta/v1.Duration\n(Optional)\nInterval at which to check the v1.Source for updates. Defaults to\n‘HelmReleaseSpec.Interval’.\nreconcileStrategy\nstring\n(Optional)\nDetermines what enables the creation of a new artifact. Valid values are\n(‘ChartVersion’, ‘Revision’).\nSee the documentation of the values for an explanation on their behavior.\nDefaults to ChartVersion when omitted.\nvaluesFiles\n[]string\n(Optional)\nAlternative list of values files to use as the chart values (values.yaml\nis not included by default), expected to be a relative path in the SourceRef.\nValues files are merged in the order of this list with the last file overriding\nthe first. Ignored when omitted.\nignoreMissingValuesFiles\nbool\n(Optional)\nIgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing.\nverify\nHelmChartTemplateVerification\n(Optional)\nVerify contains the secret name containing the trusted public keys\nused to verify the signature and specifies which provider to use to check\nwhether OCI image is authentic.\nThis field is only supported for OCI sources.\nChart dependencies, which are not bundled in the umbrella chart artifact,\nare not verified.\nHelmChartTemplateVerification\n(\nAppears on:\nHelmChartTemplateSpec\n)\nHelmChartTemplateVerification verifies the authenticity of an OCI Helm chart.\nField\nDescription\nprovider\nstring\nProvider specifies the technology used to sign the OCI Helm chart.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Kubernetes Secret containing the\ntrusted public keys.\nHelmReleaseSpec\n(\nAppears on:\nHelmRelease\n)\nHelmReleaseSpec defines the desired state of a Helm release.\nField\nDescription\nchart\nHelmChartTemplate\n(Optional)\nChart defines the template of the v1.HelmChart that should be created\nfor this HelmRelease.\nchartRef\nCrossNamespaceSourceReference\n(Optional)\nChartRef holds a reference to a source controller resource containing the\nHelm chart artifact.\ninterval\nKubernetes meta/v1.Duration\nInterval at which to reconcile the Helm release.\nkubeConfig\ngithub.com/fluxcd/pkg/apis/meta.KubeConfigReference\n(Optional)\nKubeConfig for reconciling the HelmRelease on a remote cluster.\nWhen used in combination with HelmReleaseSpec.ServiceAccountName,\nforces the controller to act on behalf of that Service Account at the\ntarget cluster.\nIf the –default-service-account flag is set, its value will be used as\na controller level fallback for when HelmReleaseSpec.ServiceAccountName\nis empty.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend reconciliation for this HelmRelease,\nit does not apply to already started reconciliations. Defaults to false.\nreleaseName\nstring\n(Optional)\nReleaseName used for the Helm release. Defaults to a composition of\n‘[TargetNamespace-]Name’.\ntargetNamespace\nstring\n(Optional)\nTargetNamespace to target when performing operations for the HelmRelease.\nDefaults to the namespace of the HelmRelease.\nstorageNamespace\nstring\n(Optional)\nStorageNamespace used for the Helm storage.\nDefaults to the namespace of the HelmRelease.\ndependsOn\n[]github.com/fluxcd/pkg/apis/meta.NamespacedObjectReference\n(Optional)\nDependsOn may contain a meta.NamespacedObjectReference slice with\nreferences to HelmRelease resources that must be ready before this HelmRelease\ncan be reconciled.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout is the time to wait for any individual Kubernetes operation (like Jobs\nfor hooks) during the performance of a Helm action. Defaults to ‘5m0s’.\nmaxHistory\nint\n(Optional)\nMaxHistory is the number of revisions saved by Helm for this HelmRelease.\nUse ‘0’ for an unlimited number of revisions; defaults to ‘5’.\nserviceAccountName\nstring\n(Optional)\nThe name of the Kubernetes service account to impersonate\nwhen reconciling this HelmRelease.\npersistentClient\nbool\n(Optional)\nPersistentClient tells the controller to use a persistent Kubernetes\nclient for this release. When enabled, the client will be reused for the\nduration of the reconciliation, instead of being created and destroyed\nfor each (step of a) Helm action.\nThis can improve performance, but may cause issues with some Helm charts\nthat for example do create Custom Resource Definitions during installation\noutside Helm’s CRD lifecycle hooks, which are then not observed to be\navailable by e.g. post-install hooks.\nIf not set, it defaults to true.\ndriftDetection\nDriftDetection\n(Optional)\nDriftDetection holds the configuration for detecting and handling\ndifferences between the manifest in the Helm storage and the resources\ncurrently existing in the cluster.\ninstall\nInstall\n(Optional)\nInstall holds the configuration for Helm install actions for this HelmRelease.\nupgrade\nUpgrade\n(Optional)\nUpgrade holds the configuration for Helm upgrade actions for this HelmRelease.\ntest\nTest\n(Optional)\nTest holds the configuration for Helm test actions for this HelmRelease.\nrollback\nRollback\n(Optional)\nRollback holds the configuration for Helm rollback actions for this HelmRelease.\nuninstall\nUninstall\n(Optional)\nUninstall holds the configuration for Helm uninstall actions for this HelmRelease.\nvaluesFrom\n[]github.com/fluxcd/pkg/apis/meta.ValuesReference\nValuesFrom holds references to resources containing Helm values for this HelmRelease,\nand information about how they should be merged.\nvalues\nKubernetes pkg/apis/apiextensions/v1.JSON\n(Optional)\nValues holds the values for this Helm release.\npostRenderers\n[]PostRenderer\n(Optional)\nPostRenderers holds an array of Helm PostRenderers, which will be applied in order\nof their definition.\nHelmReleaseStatus\n(\nAppears on:\nHelmRelease\n)\nHelmReleaseStatus defines the observed state of a HelmRelease.\nField\nDescription\nobservedGeneration\nint64\n(Optional)\nObservedGeneration is the last observed generation.\nobservedPostRenderersDigest\nstring\n(Optional)\nObservedPostRenderersDigest is the digest for the post-renderers of\nthe last successful reconciliation attempt.\nlastAttemptedGeneration\nint64\n(Optional)\nLastAttemptedGeneration is the last generation the controller attempted\nto reconcile.\nconditions\n[]Kubernetes meta/v1.Condition\n(Optional)\nConditions holds the conditions for the HelmRelease.\nhelmChart\nstring\n(Optional)\nHelmChart is the namespaced name of the HelmChart resource created by\nthe controller for the HelmRelease.\nstorageNamespace\nstring\n(Optional)\nStorageNamespace is the namespace of the Helm release storage for the\ncurrent release.\nhistory\nSnapshots\n(Optional)\nHistory holds the history of Helm releases performed for this HelmRelease\nup to the last successfully completed release.\nlastAttemptedReleaseAction\nReleaseAction\n(Optional)\nLastAttemptedReleaseAction is the last release action performed for this\nHelmRelease. It is used to determine the active remediation strategy.\nfailures\nint64\n(Optional)\nFailures is the reconciliation failure count against the latest desired\nstate. It is reset after a successful reconciliation.\ninstallFailures\nint64\n(Optional)\nInstallFailures is the install failure count against the latest desired\nstate. It is reset after a successful reconciliation.\nupgradeFailures\nint64\n(Optional)\nUpgradeFailures is the upgrade failure count against the latest desired\nstate. It is reset after a successful reconciliation.\nlastAttemptedRevision\nstring\n(Optional)\nLastAttemptedRevision is the Source revision of the last reconciliation\nattempt. For OCIRepository sources, the 12 first characters of the digest are\nappended to the chart version e.g. “1.2.3+1234567890ab”.\nlastAttemptedRevisionDigest\nstring\n(Optional)\nLastAttemptedRevisionDigest is the digest of the last reconciliation attempt.\nThis is only set for OCIRepository sources.\nlastAttemptedValuesChecksum\nstring\n(Optional)\nLastAttemptedValuesChecksum is the SHA1 checksum for the values of the last\nreconciliation attempt.\nDeprecated: Use LastAttemptedConfigDigest instead.\nlastReleaseRevision\nint\n(Optional)\nLastReleaseRevision is the revision of the last successful Helm release.\nDeprecated: Use History instead.\nlastAttemptedConfigDigest\nstring\n(Optional)\nLastAttemptedConfigDigest is the digest for the config (better known as\n“values”) of the last reconciliation attempt.\nlastHandledForceAt\nstring\n(Optional)\nLastHandledForceAt holds the value of the most recent force request\nvalue, so a change of the annotation value can be detected.\nlastHandledResetAt\nstring\n(Optional)\nLastHandledResetAt holds the value of the most recent reset request\nvalue, so a change of the annotation value can be detected.\nReconcileRequestStatus\ngithub.com/fluxcd/pkg/apis/meta.ReconcileRequestStatus\n(Members of\nReconcileRequestStatus\nare embedded into this type.)\nIgnoreRule\n(\nAppears on:\nDriftDetection\n)\nIgnoreRule defines a rule to selectively disregard specific changes during\nthe drift detection process.\nField\nDescription\npaths\n[]string\nPaths is a list of JSON Pointer (RFC 6901) paths to be excluded from\nconsideration in a Kubernetes object.\ntarget\ngithub.com/fluxcd/pkg/apis/kustomize.Selector\n(Optional)\nTarget is a selector for specifying Kubernetes objects to which this\nrule applies.\nIf Target is not set, the Paths will be ignored for all Kubernetes\nobjects within the manifest of the Helm release.\nInstall\n(\nAppears on:\nHelmReleaseSpec\n)\nInstall holds the configuration for Helm install actions performed for this\nHelmRelease.\nField\nDescription\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout is the time to wait for any individual Kubernetes operation (like\nJobs for hooks) during the performance of a Helm install action. Defaults to\n‘HelmReleaseSpec.Timeout’.\nremediation\nInstallRemediation\n(Optional)\nRemediation holds the remediation configuration for when the Helm install\naction for the HelmRelease fails. The default is to not perform any action.\ndisableTakeOwnership\nbool\n(Optional)\nDisableTakeOwnership disables taking ownership of existing resources\nduring the Helm install action. Defaults to false.\ndisableWait\nbool\n(Optional)\nDisableWait disables the waiting for resources to be ready after a Helm\ninstall has been performed.\ndisableWaitForJobs\nbool\n(Optional)\nDisableWaitForJobs disables waiting for jobs to complete after a Helm\ninstall has been performed.\ndisableHooks\nbool\n(Optional)\nDisableHooks prevents hooks from running during the Helm install action.\ndisableOpenAPIValidation\nbool\n(Optional)\nDisableOpenAPIValidation prevents the Helm install action from validating\nrendered templates against the Kubernetes OpenAPI Schema.\ndisableSchemaValidation\nbool\n(Optional)\nDisableSchemaValidation prevents the Helm install action from validating\nthe values against the JSON Schema.\nreplace\nbool\n(Optional)\nReplace tells the Helm install action to re-use the ‘ReleaseName’, but only\nif that name is a deleted release which remains in the history.\nskipCRDs\nbool\n(Optional)\nSkipCRDs tells the Helm install action to not install any CRDs. By default,\nCRDs are installed if not already present.\nDeprecated use CRD policy (\ncrds\n) attribute with value\nSkip\ninstead.\ncrds\nCRDsPolicy\n(Optional)\nCRDs upgrade CRDs from the Helm Chart’s crds directory according\nto the CRD upgrade policy provided here. Valid values are\nSkip\n,\nCreate\nor\nCreateReplace\n. Default is\nCreate\nand if omitted\nCRDs are installed but not updated.\nSkip: do neither install nor replace (update) any CRDs.\nCreate: new CRDs are created, existing CRDs are neither updated nor deleted.\nCreateReplace: new CRDs are created, existing CRDs are updated (replaced)\nbut not deleted.\nBy default, CRDs are applied (installed) during Helm install action.\nWith this option users can opt in to CRD replace existing CRDs on Helm\ninstall actions, which is not (yet) natively supported by Helm.\nhttps://helm.sh/docs/chart_best_practices/custom_resource_definitions\n.\ncreateNamespace\nbool\n(Optional)\nCreateNamespace tells the Helm install action to create the\nHelmReleaseSpec.TargetNamespace if it does not exist yet.\nOn uninstall, the namespace will not be garbage collected.\nInstallRemediation\n(\nAppears on:\nInstall\n)\nInstallRemediation holds the configuration for Helm install remediation.\nField\nDescription\nretries\nint\n(Optional)\nRetries is the number of retries that should be attempted on failures before\nbailing. Remediation, using an uninstall, is performed between each attempt.\nDefaults to ‘0’, a negative integer equals to unlimited retries.\nignoreTestFailures\nbool\n(Optional)\nIgnoreTestFailures tells the controller to skip remediation when the Helm\ntests are run after an install action but fail. Defaults to\n‘Test.IgnoreFailures’.\nremediateLastFailure\nbool\n(Optional)\nRemediateLastFailure tells the controller to remediate the last failure, when\nno retries remain. Defaults to ‘false’.\nKustomize\n(\nAppears on:\nPostRenderer\n)\nKustomize Helm PostRenderer specification.\nField\nDescription\npatches\n[]github.com/fluxcd/pkg/apis/kustomize.Patch\n(Optional)\nStrategic merge and JSON patches, defined as inline YAML objects,\ncapable of targeting objects based on kind, label and annotation selectors.\nimages\n[]github.com/fluxcd/pkg/apis/kustomize.Image\n(Optional)\nImages is a list of (image name, new name, new tag or digest)\nfor changing image names, tags or digests. This can also be achieved with a\npatch, but this operator is simpler to specify.\nPostRenderer\n(\nAppears on:\nHelmReleaseSpec\n)\nPostRenderer contains a Helm PostRenderer specification.\nField\nDescription\nkustomize\nKustomize\n(Optional)\nKustomization to apply as PostRenderer.\nReleaseAction\n(\nstring\nalias)\n(\nAppears on:\nHelmReleaseStatus\n)\nReleaseAction is the action to perform a Helm release.\nRemediation\nRemediation defines a consistent interface for InstallRemediation and\nUpgradeRemediation.\nRemediationStrategy\n(\nstring\nalias)\n(\nAppears on:\nUpgradeRemediation\n)\nRemediationStrategy returns the strategy to use to remediate a failed install\nor upgrade.\nRollback\n(\nAppears on:\nHelmReleaseSpec\n)\nRollback holds the configuration for Helm rollback actions for this\nHelmRelease.\nField\nDescription\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout is the time to wait for any individual Kubernetes operation (like\nJobs for hooks) during the performance of a Helm rollback action. Defaults to\n‘HelmReleaseSpec.Timeout’.\ndisableWait\nbool\n(Optional)\nDisableWait disables the waiting for resources to be ready after a Helm\nrollback has been performed.\ndisableWaitForJobs\nbool\n(Optional)\nDisableWaitForJobs disables waiting for jobs to complete after a Helm\nrollback has been performed.\ndisableHooks\nbool\n(Optional)\nDisableHooks prevents hooks from running during the Helm rollback action.\nrecreate\nbool\n(Optional)\nRecreate performs pod restarts for the resource if applicable.\nforce\nbool\n(Optional)\nForce forces resource updates through a replacement strategy.\ncleanupOnFail\nbool\n(Optional)\nCleanupOnFail allows deletion of new resources created during the Helm\nrollback action when it fails.\nSnapshot\nSnapshot captures a point-in-time copy of the status information for a Helm release,\nas managed by the controller.\nField\nDescription\napiVersion\nstring\n(Optional)\nAPIVersion is the API version of the Snapshot.\nProvisional: when the calculation method of the Digest field is changed,\nthis field will be used to distinguish between the old and new methods.\ndigest\nstring\nDigest is the checksum of the release object in storage.\nIt has the format of\n<algo>:<checksum>\n.\nname\nstring\nName is the name of the release.\nnamespace\nstring\nNamespace is the namespace the release is deployed to.\nversion\nint\nVersion is the version of the release object in storage.\nstatus\nstring\nStatus is the current state of the release.\nchartName\nstring\nChartName is the chart name of the release object in storage.\nchartVersion\nstring\nChartVersion is the chart version of the release object in\nstorage.\nappVersion\nstring\n(Optional)\nAppVersion is the chart app version of the release object in storage.\nconfigDigest\nstring\nConfigDigest is the checksum of the config (better known as\n“values”) of the release object in storage.\nIt has the format of\n<algo>:<checksum>\n.\nfirstDeployed\nKubernetes meta/v1.Time\nFirstDeployed is when the release was first deployed.\nlastDeployed\nKubernetes meta/v1.Time\nLastDeployed is when the release was last deployed.\ndeleted\nKubernetes meta/v1.Time\n(Optional)\nDeleted is when the release was deleted.\ntestHooks\nTestHookStatus\n(Optional)\nTestHooks is the list of test hooks for the release as observed to be\nrun by the controller.\nociDigest\nstring\n(Optional)\nOCIDigest is the digest of the OCI artifact associated with the release.\nSnapshots\n(\n[]*./api/v2.Snapshot\nalias)\n(\nAppears on:\nHelmReleaseStatus\n)\nSnapshots is a list of Snapshot objects.\nTest\n(\nAppears on:\nHelmReleaseSpec\n)\nTest holds the configuration for Helm test actions for this HelmRelease.\nField\nDescription\nenable\nbool\n(Optional)\nEnable enables Helm test actions for this HelmRelease after an Helm install\nor upgrade action has been performed.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout is the time to wait for any individual Kubernetes operation during\nthe performance of a Helm test action. Defaults to ‘HelmReleaseSpec.Timeout’.\nignoreFailures\nbool\n(Optional)\nIgnoreFailures tells the controller to skip remediation when the Helm tests\nare run but fail. Can be overwritten for tests run after install or upgrade\nactions in ‘Install.IgnoreTestFailures’ and ‘Upgrade.IgnoreTestFailures’.\nfilters\nFilter\nFilters is a list of tests to run or exclude from running.\nTestHookStatus\n(\nAppears on:\nSnapshot\n)\nTestHookStatus holds the status information for a test hook as observed\nto be run by the controller.\nField\nDescription\nlastStarted\nKubernetes meta/v1.Time\n(Optional)\nLastStarted is the time the test hook was last started.\nlastCompleted\nKubernetes meta/v1.Time\n(Optional)\nLastCompleted is the time the test hook last completed.\nphase\nstring\n(Optional)\nPhase the test hook was observed to be in.\nUninstall\n(\nAppears on:\nHelmReleaseSpec\n)\nUninstall holds the configuration for Helm uninstall actions for this\nHelmRelease.\nField\nDescription\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout is the time to wait for any individual Kubernetes operation (like\nJobs for hooks) during the performance of a Helm uninstall action. Defaults\nto ‘HelmReleaseSpec.Timeout’.\ndisableHooks\nbool\n(Optional)\nDisableHooks prevents hooks from running during the Helm rollback action.\nkeepHistory\nbool\n(Optional)\nKeepHistory tells Helm to remove all associated resources and mark the\nrelease as deleted, but retain the release history.\ndisableWait\nbool\n(Optional)\nDisableWait disables waiting for all the resources to be deleted after\na Helm uninstall is performed.\ndeletionPropagation\nstring\n(Optional)\nDeletionPropagation specifies the deletion propagation policy when\na Helm uninstall is performed.\nUpgrade\n(\nAppears on:\nHelmReleaseSpec\n)\nUpgrade holds the configuration for Helm upgrade actions for this\nHelmRelease.\nField\nDescription\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout is the time to wait for any individual Kubernetes operation (like\nJobs for hooks) during the performance of a Helm upgrade action. Defaults to\n‘HelmReleaseSpec.Timeout’.\nremediation\nUpgradeRemediation\n(Optional)\nRemediation holds the remediation configuration for when the Helm upgrade\naction for the HelmRelease fails. The default is to not perform any action.\ndisableTakeOwnership\nbool\n(Optional)\nDisableTakeOwnership disables taking ownership of existing resources\nduring the Helm upgrade action. Defaults to false.\ndisableWait\nbool\n(Optional)\nDisableWait disables the waiting for resources to be ready after a Helm\nupgrade has been performed.\ndisableWaitForJobs\nbool\n(Optional)\nDisableWaitForJobs disables waiting for jobs to complete after a Helm\nupgrade has been performed.\ndisableHooks\nbool\n(Optional)\nDisableHooks prevents hooks from running during the Helm upgrade action.\ndisableOpenAPIValidation\nbool\n(Optional)\nDisableOpenAPIValidation prevents the Helm upgrade action from validating\nrendered templates against the Kubernetes OpenAPI Schema.\ndisableSchemaValidation\nbool\n(Optional)\nDisableSchemaValidation prevents the Helm upgrade action from validating\nthe values against the JSON Schema.\nforce\nbool\n(Optional)\nForce forces resource updates through a replacement strategy.\npreserveValues\nbool\n(Optional)\nPreserveValues will make Helm reuse the last release’s values and merge in\noverrides from ‘Values’. Setting this flag makes the HelmRelease\nnon-declarative.\ncleanupOnFail\nbool\n(Optional)\nCleanupOnFail allows deletion of new resources created during the Helm\nupgrade action when it fails.\ncrds\nCRDsPolicy\n(Optional)\nCRDs upgrade CRDs from the Helm Chart’s crds directory according\nto the CRD upgrade policy provided here. Valid values are\nSkip\n,\nCreate\nor\nCreateReplace\n. Default is\nSkip\nand if omitted\nCRDs are neither installed nor upgraded.\nSkip: do neither install nor replace (update) any CRDs.\nCreate: new CRDs are created, existing CRDs are neither updated nor deleted.\nCreateReplace: new CRDs are created, existing CRDs are updated (replaced)\nbut not deleted.\nBy default, CRDs are not applied during Helm upgrade action. With this\noption users can opt-in to CRD upgrade, which is not (yet) natively supported by Helm.\nhttps://helm.sh/docs/chart_best_practices/custom_resource_definitions\n.\nUpgradeRemediation\n(\nAppears on:\nUpgrade\n)\nUpgradeRemediation holds the configuration for Helm upgrade remediation.\nField\nDescription\nretries\nint\n(Optional)\nRetries is the number of retries that should be attempted on failures before\nbailing. Remediation, using ‘Strategy’, is performed between each attempt.\nDefaults to ‘0’, a negative integer equals to unlimited retries.\nignoreTestFailures\nbool\n(Optional)\nIgnoreTestFailures tells the controller to skip remediation when the Helm\ntests are run after an upgrade action but fail.\nDefaults to ‘Test.IgnoreFailures’.\nremediateLastFailure\nbool\n(Optional)\nRemediateLastFailure tells the controller to remediate the last failure, when\nno retries remain. Defaults to ‘false’ unless ‘Retries’ is greater than 0.\nstrategy\nRemediationStrategy\n(Optional)\nStrategy to use for failure remediation. Defaults to ‘rollback’.\nThis page was automatically generated with\ngen-crd-api-reference-docs\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nToolkit Components\nNotification Controller\nAPI Reference\nAPI Reference\nNotification API reference v1\nThe GitOps Toolkit Custom Resource Definitions documentation.\nNotification API reference v1beta3\nThe GitOps Toolkit Custom Resource Definitions documentation.\nLast modified 2023-03-23:\nGA: adapt to SC, NC and KC APIs graduating to v1 (3d32a85)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nToolkit Components\nNotification Controller\nAPI Reference\nNotification API reference v1\nNotification API reference v1\nThe GitOps Toolkit Custom Resource Definitions documentation.\nPackages:\nnotification.toolkit.fluxcd.io/v1\nnotification.toolkit.fluxcd.io/v1\nPackage v1 contains API Schema definitions for the notification v1 API group.\nResource Types:\nReceiver\nReceiver\nReceiver is the Schema for the receivers API.\nField\nDescription\napiVersion\nstring\nnotification.toolkit.fluxcd.io/v1\nkind\nstring\nReceiver\nmetadata\nKubernetes meta/v1.ObjectMeta\nRefer to the Kubernetes API documentation for the fields of the\nmetadata\nfield.\nspec\nReceiverSpec\ntype\nstring\nType of webhook sender, used to determine\nthe validation procedure and payload deserialization.\ninterval\nKubernetes meta/v1.Duration\n(Optional)\nInterval at which to reconcile the Receiver with its Secret references.\nevents\n[]string\n(Optional)\nEvents specifies the list of event types to handle,\ne.g. ‘push’ for GitHub or ‘Push Hook’ for GitLab.\nresources\n[]CrossNamespaceObjectReference\nA list of resources to be notified about changes.\nresourceFilter\nstring\n(Optional)\nResourceFilter is a CEL expression expected to return a boolean that is\nevaluated for each resource referenced in the Resources field when a\nwebhook is received. If the expression returns false then the controller\nwill not request a reconciliation for the resource.\nWhen the expression is specified the controller will parse it and mark\nthe object as terminally failed if the expression is invalid or does not\nreturn a boolean.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\nSecretRef specifies the Secret containing the token used\nto validate the payload authenticity.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend subsequent\nevents handling for this receiver.\nstatus\nReceiverStatus\nCrossNamespaceObjectReference\n(\nAppears on:\nReceiverSpec\n)\nCrossNamespaceObjectReference contains enough information to let you locate the\ntyped referenced object at cluster level\nField\nDescription\napiVersion\nstring\n(Optional)\nAPI version of the referent\nkind\nstring\nKind of the referent\nname\nstring\nName of the referent\nIf multiple resources are targeted\n*\nmay be set.\nnamespace\nstring\n(Optional)\nNamespace of the referent\nmatchLabels\nmap[string]string\n(Optional)\nMatchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\nmap is equivalent to an element of matchExpressions, whose key field is “key”, the\noperator is “In”, and the values array contains only “value”. The requirements are ANDed.\nMatchLabels requires the name to be set to\n*\n.\nReceiverSpec\n(\nAppears on:\nReceiver\n)\nReceiverSpec defines the desired state of the Receiver.\nField\nDescription\ntype\nstring\nType of webhook sender, used to determine\nthe validation procedure and payload deserialization.\ninterval\nKubernetes meta/v1.Duration\n(Optional)\nInterval at which to reconcile the Receiver with its Secret references.\nevents\n[]string\n(Optional)\nEvents specifies the list of event types to handle,\ne.g. ‘push’ for GitHub or ‘Push Hook’ for GitLab.\nresources\n[]CrossNamespaceObjectReference\nA list of resources to be notified about changes.\nresourceFilter\nstring\n(Optional)\nResourceFilter is a CEL expression expected to return a boolean that is\nevaluated for each resource referenced in the Resources field when a\nwebhook is received. If the expression returns false then the controller\nwill not request a reconciliation for the resource.\nWhen the expression is specified the controller will parse it and mark\nthe object as terminally failed if the expression is invalid or does not\nreturn a boolean.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\nSecretRef specifies the Secret containing the token used\nto validate the payload authenticity.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend subsequent\nevents handling for this receiver.\nReceiverStatus\n(\nAppears on:\nReceiver\n)\nReceiverStatus defines the observed state of the Receiver.\nField\nDescription\nReconcileRequestStatus\ngithub.com/fluxcd/pkg/apis/meta.ReconcileRequestStatus\n(Members of\nReconcileRequestStatus\nare embedded into this type.)\nconditions\n[]Kubernetes meta/v1.Condition\n(Optional)\nConditions holds the conditions for the Receiver.\nwebhookPath\nstring\n(Optional)\nWebhookPath is the generated incoming webhook address in the format\nof ‘/hook/sha256sum(token+name+namespace)’.\nobservedGeneration\nint64\n(Optional)\nObservedGeneration is the last observed generation of the Receiver object.\nThis page was automatically generated with\ngen-crd-api-reference-docs\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nToolkit Components\nNotification Controller\nAPI Reference\nNotification API reference v1beta3\nNotification API reference v1beta3\nThe GitOps Toolkit Custom Resource Definitions documentation.\nPackages:\nnotification.toolkit.fluxcd.io/v1beta3\nnotification.toolkit.fluxcd.io/v1beta3\nPackage v1beta3 contains API Schema definitions for the notification v1beta3 API group.\nResource Types:\nAlert\nProvider\nAlert\nAlert is the Schema for the alerts API\nField\nDescription\napiVersion\nstring\nnotification.toolkit.fluxcd.io/v1beta3\nkind\nstring\nAlert\nmetadata\nKubernetes meta/v1.ObjectMeta\nRefer to the Kubernetes API documentation for the fields of the\nmetadata\nfield.\nspec\nAlertSpec\nproviderRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\nProviderRef specifies which Provider this Alert should use.\neventSeverity\nstring\n(Optional)\nEventSeverity specifies how to filter events based on severity.\nIf set to ‘info’ no events will be filtered.\neventSources\n[]github.com/fluxcd/notification-controller/api/v1.CrossNamespaceObjectReference\nEventSources specifies how to filter events based\non the involved object kind, name and namespace.\ninclusionList\n[]string\n(Optional)\nInclusionList specifies a list of Golang regular expressions\nto be used for including messages.\neventMetadata\nmap[string]string\n(Optional)\nEventMetadata is an optional field for adding metadata to events dispatched by the\ncontroller. This can be used for enhancing the context of the event. If a field\nwould override one already present on the original event as generated by the emitter,\nthen the override doesn’t happen, i.e. the original value is preserved, and an info\nlog is printed.\nexclusionList\n[]string\n(Optional)\nExclusionList specifies a list of Golang regular expressions\nto be used for excluding messages.\nsummary\nstring\n(Optional)\nSummary holds a short description of the impact and affected cluster.\nDeprecated: Use EventMetadata instead.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend subsequent\nevents handling for this Alert.\nProvider\nProvider is the Schema for the providers API\nField\nDescription\napiVersion\nstring\nnotification.toolkit.fluxcd.io/v1beta3\nkind\nstring\nProvider\nmetadata\nKubernetes meta/v1.ObjectMeta\nRefer to the Kubernetes API documentation for the fields of the\nmetadata\nfield.\nspec\nProviderSpec\ntype\nstring\nType specifies which Provider implementation to use.\ninterval\nKubernetes meta/v1.Duration\n(Optional)\nInterval at which to reconcile the Provider with its Secret references.\nDeprecated and not used in v1beta3.\nchannel\nstring\n(Optional)\nChannel specifies the destination channel where events should be posted.\nusername\nstring\n(Optional)\nUsername specifies the name under which events are posted.\naddress\nstring\n(Optional)\nAddress specifies the endpoint, in a generic sense, to where alerts are sent.\nWhat kind of endpoint depends on the specific Provider type being used.\nFor the generic Provider, for example, this is an HTTP/S address.\nFor other Provider types this could be a project ID or a namespace.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout for sending alerts to the Provider.\nproxy\nstring\n(Optional)\nProxy the HTTP/S address of the proxy server.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Secret containing the authentication\ncredentials for this Provider.\nserviceAccountName\nstring\n(Optional)\nServiceAccountName is the name of the service account used to\nauthenticate with services from cloud providers. An error is thrown if a\nstatic credential is also defined inside the Secret referenced by the\nSecretRef.\ncertSecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nCertSecretRef specifies the Secret containing\na PEM-encoded CA certificate (in the\nca.crt\nkey).\nNote: Support for the\ncaFile\nkey has\nbeen deprecated.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend subsequent\nevents handling for this Provider.\ncommitStatusExpr\nstring\n(Optional)\nCommitStatusExpr is a CEL expression that evaluates to a string value\nthat can be used to generate a custom commit status message for use\nwith eligible Provider types (github, gitlab, gitea, bitbucketserver,\nbitbucket, azuredevops). Supported variables are: event, provider,\nand alert.\nAlertSpec\n(\nAppears on:\nAlert\n)\nAlertSpec defines an alerting rule for events involving a list of objects.\nField\nDescription\nproviderRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\nProviderRef specifies which Provider this Alert should use.\neventSeverity\nstring\n(Optional)\nEventSeverity specifies how to filter events based on severity.\nIf set to ‘info’ no events will be filtered.\neventSources\n[]github.com/fluxcd/notification-controller/api/v1.CrossNamespaceObjectReference\nEventSources specifies how to filter events based\non the involved object kind, name and namespace.\ninclusionList\n[]string\n(Optional)\nInclusionList specifies a list of Golang regular expressions\nto be used for including messages.\neventMetadata\nmap[string]string\n(Optional)\nEventMetadata is an optional field for adding metadata to events dispatched by the\ncontroller. This can be used for enhancing the context of the event. If a field\nwould override one already present on the original event as generated by the emitter,\nthen the override doesn’t happen, i.e. the original value is preserved, and an info\nlog is printed.\nexclusionList\n[]string\n(Optional)\nExclusionList specifies a list of Golang regular expressions\nto be used for excluding messages.\nsummary\nstring\n(Optional)\nSummary holds a short description of the impact and affected cluster.\nDeprecated: Use EventMetadata instead.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend subsequent\nevents handling for this Alert.\nProviderSpec\n(\nAppears on:\nProvider\n)\nProviderSpec defines the desired state of the Provider.\nField\nDescription\ntype\nstring\nType specifies which Provider implementation to use.\ninterval\nKubernetes meta/v1.Duration\n(Optional)\nInterval at which to reconcile the Provider with its Secret references.\nDeprecated and not used in v1beta3.\nchannel\nstring\n(Optional)\nChannel specifies the destination channel where events should be posted.\nusername\nstring\n(Optional)\nUsername specifies the name under which events are posted.\naddress\nstring\n(Optional)\nAddress specifies the endpoint, in a generic sense, to where alerts are sent.\nWhat kind of endpoint depends on the specific Provider type being used.\nFor the generic Provider, for example, this is an HTTP/S address.\nFor other Provider types this could be a project ID or a namespace.\ntimeout\nKubernetes meta/v1.Duration\n(Optional)\nTimeout for sending alerts to the Provider.\nproxy\nstring\n(Optional)\nProxy the HTTP/S address of the proxy server.\nsecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nSecretRef specifies the Secret containing the authentication\ncredentials for this Provider.\nserviceAccountName\nstring\n(Optional)\nServiceAccountName is the name of the service account used to\nauthenticate with services from cloud providers. An error is thrown if a\nstatic credential is also defined inside the Secret referenced by the\nSecretRef.\ncertSecretRef\ngithub.com/fluxcd/pkg/apis/meta.LocalObjectReference\n(Optional)\nCertSecretRef specifies the Secret containing\na PEM-encoded CA certificate (in the\nca.crt\nkey).\nNote: Support for the\ncaFile\nkey has\nbeen deprecated.\nsuspend\nbool\n(Optional)\nSuspend tells the controller to suspend subsequent\nevents handling for this Provider.\ncommitStatusExpr\nstring\n(Optional)\nCommitStatusExpr is a CEL expression that evaluates to a string value\nthat can be used to generate a custom commit status message for use\nwith eligible Provider types (github, gitlab, gitea, bitbucketserver,\nbitbucket, azuredevops). Supported variables are: event, provider,\nand alert.\nThis page was automatically generated with\ngen-crd-api-reference-docs\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nSecurity\nBest Practices\nSecurity Best Practices\nBest practices for securing Flux deployments.\nIntroduction\nThe Flux project strives to keep its components secure by design and by default.\nThis document aims to list all security-sensitive options or considerations that\nmust be taken into account when deploying Flux. And also serve as a guide for\nsecurity professionals auditing such deployments.\nNot all recommendations are required for a secure deployment. Some may impact the\nconvenience, performance or resources utilization of Flux. Therefore, use this in\ncombination with your own Security Posture and Risk Appetite.\nSome recommendations may overlap with Kubernetes security recommendations, to keep\nthis short and more easily maintainable, please refer to\nKubernetes CIS Benchmark\nfor non Flux-specific guidance.\nFor help implementing these recommendations, seek\nenterprise support\n.\nSecurity Best Practices\nThe recommendations below are based on Flux’s latest version.\nHelm Controller\nStart-up flags\nEnsure controller was not started with\n--insecure-kubeconfig-exec=true\n.\nRationale\nKubeConfigs support the execution of a binary command to return the token required to authenticate against a Kubernetes cluster.\nThis is very handy for acquiring contextual tokens that are time-bound (e.g. aws-iam-authenticator).\nHowever, this may be open for abuse in multi-tenancy environments and therefore is disabled by default.\nAudit Procedure\nCheck Helm Controller’s pod YAML for the arguments used at start-up:\nkubectl describe pod -n flux-system -l\napp\n=\nhelm-controller | grep -B\n5\n-A\n10\nArgs\nEnsure controller was not started with\n--insecure-kubeconfig-tls=true\n.\nRationale\nDisables the enforcement of TLS when accessing the API Server of remote clusters.\nThis flag was created to enable scenarios in which non-production clusters need to be accessed via HTTP. Do not disable TLS in production.\nAudit Procedure\nCheck Helm Controller’s pod YAML for the arguments used at start-up:\nkubectl describe pod -n flux-system -l\napp\n=\nhelm-controller | grep -B\n5\n-A\n10\nArgs\nKustomize Controller\nStart-up flags\nEnsure controller was not started with\n--insecure-kubeconfig-exec=true\n.\nRationale\nKubeConfigs support the execution of a binary command to return the token required to authenticate against a Kubernetes cluster.\nThis is very handy for acquiring contextual tokens that are time-bound (e.g. aws-iam-authenticator).\nHowever, this may be open for abuse in multi-tenancy environments and therefore is disabled by default.\nAudit Procedure\nCheck Kustomize Controller’s pod YAML for the arguments used at start-up:\nkubectl describe pod -n flux-system -l\napp\n=\nkustomize-controller | grep -B\n5\n-A\n10\nArgs\nEnsure controller was not started with\n--insecure-kubeconfig-tls=true\n.\nRationale\nDisables the enforcement of TLS when accessing the API Server of remote clusters.\nThis flag was created to enable scenarios in which non-production clusters need to be accessed via HTTP. Do not disable TLS in production.\nAudit Procedure\nCheck Kustomize Controller’s pod YAML for the arguments used at start-up:\nkubectl describe pod -n flux-system -l\napp\n=\nkustomize-controller | grep -B\n5\n-A\n10\nArgs\nEnsure controller was started with\n--no-remote-bases=true\n.\nRationale\nBy default the Kustomize controller allows for kustomize overlays to refer to external bases.\nThis has a performance penalty, as the bases will have to be downloaded on demand during each reconciliation.\nWhen using external bases, there can’t be any assurances that the externally declared state won’t change.\nIn this case, the source loses its hermetic properties. Changes in the external bases will result in changes on the cluster, regardless of whether the source has been modified since the last reconciliation.\nAudit Procedure\nCheck Kustomize Controller’s pod YAML for the arguments used at start-up:\nkubectl describe pod -n flux-system -l\napp\n=\nkustomize-controller | grep -B\n5\n-A\n10\nArgs\nSecret Decryption\nEnsure Secret Decryption is enabled and secrets are not being held in Flux Sources in plaintext.\nRationale\nThe kustomize-controller has an auto decryption mechanism that can decrypt cipher texts on-demand at reconciliation time using an embedded implementation of\nSOPS\n. This enables credentials (e.g. passwords, tokens) and sensitive information to be kept in an encrypted state in the sources.\nAudit Procedure\nCheck for plaintext credentials stored in the Git Repository at both HEAD and historical commits. Auto-detection tools can be used for this such as\nGitLeaks\n,\nTrufflehog\nand\nSquealer\n.\nCheck whether Secret Decryption is properly enabled in each\nspec.decryption\nfield of the cluster’s\nKustomization\nobjects.\nAdditional Best Practices for Shared Cluster Multi-tenancy\nMulti-tenancy Lock-down\nEnsure\nhelm-controller\n,\nkustomize-controller\n,\nnotification-controller\n,\nimage-reflector-controller\nand\nimage-automation-controller\nhave cross namespace references disabled via\n--no-cross-namespace-refs=true\n.\nRationale\nBlocks references to Flux objects across namespaces. This assumes that tenants would own one or multiple namespaces, and should not be allowed to consume other tenant’s objects, as this could enable them to gain access to sources they do not (or should not) have access to.\nAudit Procedure\nCheck the Controller’s YAML for the arguments used at start-up:\nkubectl describe pod -n flux-system -l\napp\n=\nhelm-controller | grep -B\n5\n-A\n10\nArgs\nkubectl describe pod -n flux-system -l\napp\n=\nkustomize-controller | grep -B\n5\n-A\n10\nArgs\nkubectl describe pod -n flux-system -l\napp\n=\nnotification-controller | grep -B\n5\n-A\n10\nArgs\nkubectl describe pod -n flux-system -l\napp\n=\nimage-reflector-controller | grep -B\n5\n-A\n10\nArgs\nkubectl describe pod -n flux-system -l\napp\n=\nimage-automation-controller | grep -B\n5\n-A\n10\nArgs\nEnsure\nhelm-controller\nand\nkustomize-controller\nhave a default service account set via\n--default-service-account=<service-account-name>\n.\nRationale\nEnforces all reconciliations to impersonate a given Service Account, effectively disabling the use of the privileged service account that would otherwise be used by the controller.\nTenants must set a service account for each object that is responsible for applying changes to the Cluster (i.e.\nHelmRelease\nand\nKustomization\n), otherwise Kubernetes’s API Server will not authorize the changes. NB: It is recommended that the default service account used has no permissions set to the control plane.\nAudit Procedure\nCheck the Controller’s YAML for the arguments used at start-up:\nkubectl describe pod -n flux-system -l\napp\n=\nhelm-controller | grep -B\n5\n-A\n10\nArgs\nkubectl describe pod -n flux-system -l\napp\n=\nkustomize-controller | grep -B\n5\n-A\n10\nArgs\nSecret Decryption\nEnsure Secret Decryption is configured correctly, such that each tenant have the correct level of isolation.\nRationale\nThe secret decryption configuration must be aligned with the level of isolation required across tenants.\nFor higher isolation, each tenant must have their own Key Encryption Key (KEK) configured. Note that the access controls to the aforementioned keys must also be aligned for better isolation.\nFor lower isolation requirements, or for secrets that are shared across multiple tenants, cluster-level keys could be used.\nAudit Procedure\nCheck whether the Secret Provider configuration is security hardened. Please seek\nSOPS\nand\nSealedSecrets\ndocumentation for how to best implement each solution.\nWhen SealedSecrets are employed, pay special attention to the scopes being used.\nResource Isolation\nEnsure additional Flux instances are deployed when mission-critical tenants/workloads must be assured.\nRationale\nSharing the same instances of Flux Components across all tenants including the Platform Admin, will lead to all reconciliations competing for the same resources. In addition, all Flux objects will be placed on the same queue for reconciliation which is limited by the number of workers set by each controller (i.e.\n--concurrent=20\n), which could cause reconciliation intervals not to be accurately honored.\nFor improved reliability, additional instances of Flux Components could be deployed, effectively creating separate “lanes” that are not disrupted by noisy neighbors. An example of this approach would be having additional instances of both Kustomize and Helm controllers that focuses on applying platform level changes, which do not compete with Tenants changes.\nRunning multiple Flux instances within the same cluster is supported by means of sharding, please consult the\nFlux sharding and horizontal scaling documentation\nfor more details.\nTo avoid conflicts among controllers while attempting to reconcile Custom Resources, controller types (e.g.\nsource-controller\n) must have be configured with unique label selectors in the\n--watch-label-selector\nflag.\nAudit Procedure\nCheck for the existence of additional Flux controllers instances and their respective scopes. Each controller must be started with\n--watch-label-selector\nand have the selector point to unique label values:\nkubectl describe pod -n flux-system -l\napp\n=\nkustomize-controller | grep -B\n5\n-A\n10\nArgs\nkubectl describe pod -n flux-system -l\napp\n=\nhelm-controller | grep -B\n5\n-A\n10\nArgs\nkubectl describe pod -n flux-system -l\napp\n=\nsource-controller | grep -B\n5\n-A\n10\nArgs\nNode Isolation\nEnsure worker nodes are not being shared across tenants and the Flux components.\nRationale\nPods sharing the same worker node may enable threat vectors which might enable a malicious tenant to have a negative impact on the Confidentiality, Integrity or Availability of the co-located pods.\nThe Flux components may have Control Plane privileges while some tenants may not. A co-located pod could leverage its privileges in the shared worker node to bypass its own Control Plane access limitations by compromising one of the co-located Flux components. For cases in which cross-tenant isolation requirements must be enforced, the same risks apply.\nEmploy techniques to enforce that untrusted workloads are sandboxed. And, ensure that worker nodes are only shared when within the acceptable risks by your security requirements.\nAudit Procedure\nCheck whether you adhere to\nKubernetes Node Isolation Guidelines\nCheck whether there are Admission Controllers/OPA blocking tenants from creating privileged containers.\nCheck whether\nRuntimeClass\nis being employed to sandbox workloads that may be scheduled in shared worker nodes.\nCheck whether\nTaints and Tolerations\nare being used to decrease the likelihood of sharing worker nodes across tenants, or with the Flux controllers. Some cloud providers have this encapsulated as Node Pools.\nNetwork Isolation\nEnsure the Container Network Interface (CNI) being used in the cluster supports Network Policies.\nRationale\nFlux relies on Network Policies to ensure that only Flux components have direct access to the source artifacts kept in the Source Controller.\nAudit Procedure\nCheck whether you adhere to\nKubernetes Network Isolation Guidelines\nConfirm that the\nNetwork Policy\nobjects created by Flux are being enforced by the CNI. Alternatively, run a tool such as\nCyclonus\nor\nSonobuoy\nto validate NetworkPolicy enforcement by the CNI plugin on your cluster.\nAdditional Best Practices for Tenant Dedicated Cluster Multi-tenancy\nEnsure tenants are not able to revoke Platform Admin access to their clusters.\nRationale\nIn environments in which a management cluster is used to bootstrap and manage other clusters, it is important to ensure that a tenant is not allowed to revoke access from the Platform Admin, effectively denying the Management Cluster the ability to further reconcile changes into the tenant’s Cluster.\nThe Platform Admin should make sure that at the tenant’s cluster bootstrap process, this is taken into the account and a breakglass procedure is in place to recover access without the need to rebuild the cluster.\nAudit Procedure\nCheck whether alerts are in place in case the Remote Apply operations fails.\nCheck the permission set given to the tenant’s users and applications is not overly privileged.\nCheck whether there are Admission Controllers/OPA rules blocking changes in Platform Admin’s permissions and overall resources.\nLast modified 2023-12-12:\nUpdate HelmRelease refs to match v2beta2 spec (fbfe03c)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nSecurity\nSecurity Documentation\nFlux Security documentation.\nIntroduction\nFlux has a multi-component design, and integrates with many other systems.\nThis document outlines an overview of security considerations for Flux components,\nproject processes, artifacts, as well as Flux configurable options and\nwhat they enable for both Kubernetes cluster and external system security.\nSee our\nsecurity processes document\nfor vulnerability reporting, handling,\nand disclosure of information for the Flux project and community.\nPlease also have a look at\nour security-related blog posts\n.\nWe are writing there to inform you what we are doing to keep Flux and you safe!\nSigned container images\nThe Flux CLI and the controllers’ images are signed using\nSigstore\nCosign and GitHub OIDC.\nThe container images along with their signatures are published on GitHub Container Registry and Docker Hub.\nTo verify the authenticity of Flux’s container images,\ninstall\ncosign\nv2 and run:\n$\ncosign verify ghcr.io/fluxcd/source-controller:v1.0.0\n\\\n--certificate-identity-regexp=^https://github\\\\.com/fluxcd/.*$ \\\n--certificate-oidc-issuer=https://token.actions.githubusercontent.com\nVerification for ghcr.io/fluxcd/source-controller:v1.0.0 --\nThe following checks were performed on each of these signatures:\n- The cosign claims were validated\n- Existence of the claims in the transparency log was verified offline\n- The code-signing certificate was verified using trusted certificate authority certificates\nWe also wrote\na blog post\nwhich discusses this in some more detail.\nSoftware Bill of Materials\nFor the Flux project we publish a Software Bill of Materials (SBOM) with each release.\nThe SBOM is generated with\nSyft\nin the\nSPDX\nformat.\nThe\nspdx.json\nfile is available for download on the GitHub release page e.g.:\ncurl -sL https://github.com/fluxcd/flux2/releases/download/v2.0.0/flux_2.0.0_sbom.spdx.json | jq\nThe Flux controllers’ images come with SBOMs for each CPU architecture,\nyou can extract the SPDX JSON using Docker’s inspect command:\ndocker buildx imagetools inspect ghcr.io/fluxcd/source-controller:v1.0.0\n\\\n--format\n\"{{ json (index .SBOM \\\"linux/amd64\\\").SPDX}}\"\nOr by using Docker’s\nsbom command\n:\ndocker sbom fluxcd/source-controller:v1.0.0\nPlease also refer to\nthis blog post\nwhich discusses the idea and value of SBOMs.\nSLSA Provenance\nStarting with Flux version 2.0.0, the build, release and provenance portions of the Flux\nproject supply chain provisionally meet\nSLSA Build Level 3\n.\nPlease see the\nSLSA Assessment\ndocumentation for more details on how the\nprovenance is generated and how Flux complies with the SLSA requirements.\nProvenance verification\nThe provenance of the Flux release artifacts (binaries, container images, SBOMs, deploy manifests)\ncan be verified using the official SLSA verifier tool and Sigstore Cosign.\nPlease see the\nSLSA provenance verification\ndocumentation\nfor more details on how to verify the provenance of Flux release artifacts.\nBuildkit attestations\nThe Flux controllers’ images come with provenance attestations which follow\nthe\nSLSA provenance schema version 0.2\n.\nThe provenance attestations are generated at build time with\nDocker Buildkit\nand\ninclude facts about the build process such as:\nBuild timestamps\nBuild parameters and environment\nVersion control metadata\nSource code details\nMaterials (files, scripts) consumed during the build\nTo extract the SLSA provenance JSON for a specific CPU architecture,\nyou can use Docker’s inspect command:\ndocker buildx imagetools inspect ghcr.io/fluxcd/source-controller:v1.0.0\n\\\n--format\n\"{{ json (index .Provenance \\\"linux/amd64\\\").SLSA}}\"\nNote that the\nlinux/amd64\ncan be replaced with another architecture variation of the image,\nfor example\nlinux/arm64\nor\nlinux/arm/v7\n.\nScanning for CVEs\nThe Flux controllers’ images are based on Alpine, they contain very few OS packages\nand the controller’s binary which is statically built using Go.\nTo properly scan Flux container images, the scanner must be able to detect the\nAlpine apk packages and the Go modules included in the controller’s Go binary.\nThe Go modules and apk packages are also available for inspection\nin the attached\nSBOM\n.\nThe Flux team recommends users to scan the container images for CVEs using\nTrivy\n,\nwhich is an OSS scanner made by\nAqua Security\n.\nTo scan a controller image with Trivy:\ntrivy image ghcr.io/fluxcd/source-controller:v1.0.0\nWe ask users to keep Flux up-to-date on their clusters,\nthis is the only way to ensure a Flux deployment is free of CVEs.\nNew Flux versions are\npublished periodically\n,\nand the container images are based on the latest Alpine and Go releases.\nWe offer a fully automated solution for keeping Flux up-to-date,\nplease see the Flux GitHub Actions\ndocumentation\nfor more details.\nReporting CVEs in Flux images\nThe Flux controllers are constantly being monitored for new CVEs, and the attack\nsurface for any vulnerability is assessed by maintainers. If a controller is considered\nto be vulnerable, a new patch release will be issued immediately.\nGiven this, and while we do appreciate the effort, reporting CVEs found by a security\nscanner through issues and/or the security mailing list is not necessary.\nPod security standard\nThe controller deployments are configured in conformance with the\nKubernetes\nrestricted pod security standard\n:\nall Linux capabilities are dropped\nthe root filesystem is set to read-only\nthe seccomp profile is set to the runtime default\nrun as non-root is enabled\nthe filesystem group is set to 1337\nthe user and group ID is set to 65534\nController permissions\nWhile Flux integrates with other systems it is built on Kubernetes core controller-runtime\nand properly adheres to Kubernetes security model including RBAC\n1\n.\nFlux installs a set of\nRBAC manifests\n.\nThese include:\nA\ncrd-controller\nClusterRole\n, which:\nHas full access to all the Custom Resource Definitions defined by Flux controllers\nCan get, list, and watch namespaces and secrets\nCan get, list, watch, create, patch, and delete configmaps and their status\nCan get, list, watch, create, patch, and delete coordination.k8s.io leases\nCan create ServiceAccount tokens used for object-level workload identity\nA\ncrd-controller\nClusterRoleBinding\n:\nReferences\ncrd-controller\nClusterRole\nabove\nBound to a service accounts for every Flux controller\nA\ncluster-reconciler\nClusterRoleBinding\n:\nReferences\ncluster-admin\nClusterRole\nBound to service accounts for only\nkustomize-controller\nand\nhelm-controller\nA\nflux-view\nClusterRole\n:\nGrants the Kubernetes builtin\nview\nrole read-only access to Flux Custom Resources\nA\nflux-edit\nClusterRole\n:\nGrants the Kubernetes builtin\nedit\nand\nadmin\nroles write access to Flux Custom Resources\nFlux uses these two\nClusterRoleBinding\nstrategies in order to allow for clear access separation using tools\npurpose-built for policy enforcement (OPA, Kyverno, admission controllers).\nFor example, the design allows all controllers to access Flux CRDs (binds to\ncrd-controller\nClusterRole\n),\nbut only binds the Flux reconciler controllers for Kustomize and Helm to\ncluster-admin\nClusterRole\n,\nas these are the only two controllers that manage resources in the cluster.\nHowever in a\nsoft multi-tenancy setup\n,\nFlux does not reconcile a tenant’s repo under the\ncluster-admin\nrole.\nInstead, you specify a different service account in your manifest, and the Flux controllers will use\nthe Kubernetes Impersonation API under\ncluster-admin\nto impersonate that service account\n2\n.\nIn this way, policy restrictions for this service account are applied to the manifests being reconciled.\nIf the binding is not defined for the correct service account and namespace, it will fail.\nThe roles and permissions for this multi-tenancy approach\nare described in detail here:\nhttps://github.com/fluxcd/flux2-multi-tenancy\n.\nCross-Namespace reference policy\nFlux’s general premise is to follow Kubernetes best RBAC practices which forbid cross-namespace references to potential sensitive data, i.e. Secrets and ConfigMaps. For sources and events, Flux allows referencing resources from other Namespaces. In these cases, the policy is governed by each controller’s\n--no-cross-namespace-refs\nflag. See the\nFlux multi-tenancy configuration page\nfor further information on this flag.\nFurther securing Flux Deployments\nBeyond the baked-in security features of Flux, there are further best\npractices that can be implemented to ensure your Flux deployment is as secure\nas it can be. For more information, checkout the\nFlux Security Best Practices\n.\nHowever, by design cross-namespace references are an exception to RBAC.\nPlatform admins have the option to turn off cross-namespace references as described in the\ninstallation documentation\n.\n↩︎\nPlatform admins have to option to enforce impersonation as described in the\ninstallation documentation\n.\n↩︎\nSecurity Best Practices\nBest practices for securing Flux deployments.\nContextual Authorization\nContextual Authorization for securing Flux deployments.\nSecrets Management\nManaging Secrets in a GitOps way using Flux.\nSLSA Assessment\nFlux assessment of SLSA Level 3 requirements.\nLast modified 2025-06-08:\nAdd RBAC patch to remove Object-level Workload Identity (8de296b)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nSecurity\nContextual Authorization\nContextual Authorization\nContextual Authorization for securing Flux deployments.\nIntroduction\nMost cloud providers support context-based authorization, enabling applications\nto benefit from strong access controls applied to a given context (e.g. Virtual\nMachine), without the need of managing authentication tokens and credentials.\nFor example, by granting a given Virtual Machine (or principal that such machine\noperates under) access to AWS S3, applications running inside that machine can\nrequest a token on-demand, which would grant them access to the AWS S3 buckets\nwithout having to store long lived credentials anywhere.\nBy leveraging such capability, Flux users can focus on the big picture, which is\naccess controls enforcement with a least-privileged approach, whilst not having to\ndo deal with security hygiene topics such as encrypting authentication secrets, and\nensure they are being rotated regularly.\nAll that is taken care of automatically by the cloud providers, as the tokens provided\nare context- and time-bound.\nCurrent Support\nBelow is a list of Flux features that support this functionality and their documentation:\nStatus\nComponent\nFeature\nProvider\nRef\nSupported\nSource Controller\nGit Repository Authentication\nAzure\nGuide\nSupported\nSource Controller\nBucket Repository Authentication\nAWS\nGuide\nSupported\nSource Controller\nBucket Repository Authentication\nAzure\nGuide\nSupported\nSource Controller\nBucket Repository Authentication\nGCP\nGuide\nSupported\nSource Controller\nOCI Repository Authentication\nAWS\nGuide\nSupported\nSource Controller\nOCI Repository Authentication\nAzure\nGuide\nSupported\nSource Controller\nOCI Repository Authentication\nGCP\nGuide\nSupported\nSource Controller\nHelm OCI Repository Authentication\nAWS\nGuide\nSupported\nSource Controller\nHelm OCI Repository Authentication\nAzure\nGuide\nSupported\nSource Controller\nHelm OCI Repository Authentication\nGCP\nGuide\nSupported\nImage Reflector Controller\nImage Repository Authentication\nAWS\nGuide\nSupported\nImage Reflector Controller\nImage Repository Authentication\nAzure\nGuide\nSupported\nImage Reflector Controller\nImage Repository Authentication\nGCP\nGuide\nSupported\nImage Automation Controller\nGit Repository Authentication\nAzure\nGuide\nSupported\nKustomize Controller\nSOPS Integration with KMS\nAWS\nGuide\nSupported\nKustomize Controller\nSOPS Integration with KMS\nAzure\nGuide\nSupported\nKustomize Controller\nSOPS Integration with KMS\nGCP\nGuide\nSupported\nNotification Controller\nAzure DevOps Commit Status Updates\nAzure\nGuide\nSupported\nNotification Controller\nAzure Event Hubs\nAzure\nGuide\nSupported\nNotification Controller\nGoogle Cloud Pub/Sub\nGCP\nGuide\nRoadmap\nSupport for context-based authorization should only increase over time.\nFor more information, please visit the tracking issue:\nhttps://github.com/fluxcd/flux2/issues/3003\n.\nLast modified 2025-05-19:\n[RFC-0010] Add section for integrations (daec6cb)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nSecurity\nSecrets Management\nSecrets Management\nManaging Secrets in a GitOps way using Flux.\nIntroduction\nFlux improves the application deployment process by continuously reconciling a\ndesired state, defined at a source, against a target cluster. One of the challenges\nwith this process is its dependency on secrets, that must not be stored in plain-text\nlike the rest of the desired state.\nSecrets are sensitive information that an application needs to operate such as:\ncredentials, passwords, keys, tokens and certificates. Managing secrets declaratively\nneeds to be done right because of its broad security implications.\nWe will cover the mechanisms supported by Flux, as well as the security principles,\nconcerns and techniques to consider when managing secrets with Flux.\nWhat’s inside the toolbox?\nFirst of all, let’s go through the different options supported by Flux and Kubernetes.\nNowadays there are a multitude of secret management options. Some are available in-cluster,\ndirectly in the comfort of your Kubernetes cluster, and others that are provided from\nout-of-cluster, for example a Cloud based KMS.\nKubernetes Secrets\nKubernetes has a\nbuilt-in mechanism\nto store and manage secrets. The secrets\nare stored in etcd either in plain-text or\nencrypted\n.\nThey are the vanilla offering, which is used during\nflux bootstrap\n, for example, to store your\nSSH Deploy Keys. Unless when the initial Flux source supports\ncontextual authorization\n,\nin which case no secrets are required.\nStoring plain-text secrets in your desired state is not recommended, so apart from the secret\nused to authenticate against your initial source, Flux users should not manage these. Instead,\nthey should rely mostly on other mechanisms covered below.\nSecrets Decryption Operators\nSometimes referred to as Encrypted Secrets, Secrets Decryption Operators enable secrets to be stored\nin ciphertext as Kubernetes resources within a Flux source. They are deployed into the cluster by\nFlux in their original CustomResourceDefinition (CRD) form, which is later used by its Secret\nDecryption Operator to decrypt those secrets and generate a Kubernetes Secret.\nThis is transparent to the consuming applications, making it a quite suitable approach to retrofit\ninto an existing setup. An example of a Secret Decryption Operator is\nSealed Secrets\n.\nStoring encrypted secrets in Git repositories enables configuration versioning to leverage the\nsame practices used for managing, versioning and releasing application and/or infrastructure\nwhen using declarative “everything as code”, for example pull-requests, tags and branch\nstrategies.\nNote that some sources may keep a history of the encrypted Secrets (e.g.\nGitRepository\n)\nthrough time. Increasing the impact when old encryption keys are leaked, especially when\nother security measures are not in place (e.g. secret rotation) or when long-lived secrets\nare being handled (e.g. Public TLS Certs).\nNotice that secrets can be stored in any Source type supported by Flux, such as\nBuckets\nand\nOCI repositories\n.\nFlux specific guides on using Secrets Decryption Operators:\nBitnami Sealed Secrets\nUsing Flux to decrypt Secrets on-demand\nFlux has the ability to decrypt secrets stored in Flux sources by itself, without the need of\nadditional controllers installed in the cluster. The approach relies on keeping in Flux sources\nencrypted Kubernetes Secrets, which are decrypted on-demand with\nSOPS\n, just before they are\ndeployed into the target clusters.\nThis approach is more flexible than using\nSealed Secrets\n, as\nSOPS\nsupports cloud-based Key\nManagement Services of the major cloud providers (Azure KeyVault, GCP KMS and AWS KMS), HashiCorp\nVault, as well as “off-line” decryption using Age and PGP.\nThis mechanism supports\nkustomize-secretgenerator\nwhich ensures that dependent workloads will\nreload automatically and start using the latest version of the secret. Notice that most approaches\nthat are based on Kubernetes Secrets would require something like\nstakater/Reloader\nto achieve\nthe same result. The\nKubernetes blog\nexplains quite well how this works.\nThe security concerns of this approach are similar to the Secrets Decryption Operators, but with\nthe added benefit that no additional controllers are required, therefore reducing resources consumption\nand the attack surface. When using external providers (e.g. KMS, Vault), remember that they can become\na single point of failure, if they are deleted by mistake (or unavailable by extended periods) this\ncould impact your solution.\nFlux supports the two main names in Encrypted Secrets and has specific how-to guides for them:\nMozilla SOPS Guide\nSecrets decryption\nSecrets Synchronized by Operators\nThe source of truth for your secrets can reside outside of the cluster, and then be synchronised\ninto the cluster as Kubernetes Secrets by operators. Much like encrypted secrets, this process\nis transparent to the workloads in the cluster.\nTwo examples of this type of operator are\n1Password Operator\nand\nExternal Secrets Operator\n.\nBut given their nature, Flux is able to support any operator that manages Kubernetes secrets.\nThis approach provides a level of redundancy by default, as secrets are kept at both the cluster\nand the remote source, so small failures can go undetected. It supports hybrid workloads\nquite well, when some secrets have to be shared with applications that are not Kubernetes-based.\nWhen using mutable secrets, it could be hard for Flux or the dependent applications to know\nwhether they are using the latest version of a given secret. In such cases, immutable secrets,\nwhere the name also contains the version of the secret, may help.\nTake into account the loading times when provisioning a new cluster, as that can become a\nbottleneck slowing down the provisioning time as the number of secrets increases.\nFlux supports all operators that provide this functionality.\nSecrets mounted via CSI Drivers\nAnother way to bring external secrets into Kubernetes, is the use of CSI Drivers,\nwhich mounts secrets as files directly into a Pod filesystem, instead of generating\nnative Kubernetes Secrets.\nDue to the way it works, the secrets are not accessible within the Kubernetes Control\nPlane, so although you can use it with your workloads, it won’t work when providing\nto CustomResourceTypes (CRDs) that need a reference to a secret\n(e.g.\n.spec.secretRef.name\nin\nGitRepository\n).\nWith CSI Drivers, the mounting takes place at Pod starting time, so issues accessing\nthe external source of the secrets may be more impactful.\nHere are a few CSI providers:\nHashiCorp Vault\nAzure KeyVault\nAWS Secrets Manager\nGCP CSI Driver\nDirect access to out-of-cluster Secrets\nDirect access to a secret management solution that resides outside of a Kubernetes\ncluster is also an option. Which could be a useful alternative when lifting and\nshifting legacy applications that already depend on such approach.\nHere the secret management solution will become a single point of failure,\nexpect issues when it goes temporarily unavailable and make sure to have disaster recovery plans.\nAlso observe throttling limits of cloud solutions, given that different applications\nmay be targeting the same Secret Manager, without a rate limiter across all of them,\nthis could easily lead to an outage at scale.\nFlux currently does not directly fetch secrets from out-of-cluster solutions, in the\nsame way that most Kubernetes native tools don’t, therefore this approach may need to\nbe combined with things such as Secrets Synchronized by Operators. However, this will\nnot block the ability of your applications to do so.\nBig Picture - Things to consider\nOnce you are aware of the different tools in the toolbox, it is important to align them\nwith your actual requirements, taking into account some key points:\nExpiration and Rotation\nSecrets should have an expiration time, and ideally such expiration should be enforced,\nso that the potential of leakage has a well-defined risk window.\nTo facilitate the uninterrupted use of the dependent applications, rotation should be\nautomated taking into account that at times different versions of the same secret\n(old and new) may need to be supported at the same time - e.g. whilst validating a\nnew version of the application that is being deployed.\nBoth secrets can remain active during a time window, but once the new version is\nvalidated after deployment, the previous secrets can be safely decommissioned.\nCloud KMS solutions tend to provide secret versioning built-in.\nAccess Management and Auditing\nAccess to secrets should be restricted to the servers and applications within the environment\nthey need to be accessed. The same goes for users and service accounts.\nWhen considering the different solutions, it is important to note how they hang together\nand what the gaps are. If you have a strong requirement for access and auditing controls,\nhaving a well-defined api-server auditing in place, together with tight RBAC policies in your\ncluster is only part of the problem. Also take into account how those secrets are sourced,\nstored and handled. Maybe having secrets stored (even if in encrypted form) in an easily\naccessible Flux source that has a loosely defined RBAC and no auditing in place may not meet\nsuch requirements.\nLeast Privileged and Segregation of duties\nThe scope of each secret must be carefully considered to decrease the blast radius in\ncase of breach. A trade-off must be reached to attain a balance between the two extremes:\nhaving a single secret that has all the access, versus having too many secrets that are\nalways used in combination.\nSharing the same secret across different scopes, just because they have the same permissions\nmay lead to disruption if such secret needs to be quickly rotated.\nDisaster Recovery\nThe entire provisioning of your infrastructure and application must take into account\nbreak the glass procedures that are secure, provide relevant security controls (e.g. auditing)\nand cannot be misused to bypass other processes (e.g. Access Management).\nAround disaster recovery scenarios, consider how they align with your Availability and\nConfidentiality requirements.\nDon’t co-locate ciphertext with encryption keys\nIt should go without saying, but never place secrets together with keys that can provide privilege\nescalation routes. For example, if you store the decryption key for your secrets in GitHub secrets,\nand all your encrypted secrets are stored in the same repository, a single GitHub account\n(with enough access) compromised is enough for all your secrets to be decrypted.\nInstead, segregate encryption keys from ciphertext and understand what needs to be compromised\nfor the data to be at risk.\nSingle Points of Failure\nIdentify all potential single points of failure and ensure that there is a way around them.\nIf all your secrets are encrypted using an encryption key stored in Vault, and due to a major\nfailure your Vault instance is completely lost, and no backup is to be found, the encrypted\nsecrets are now useless. Therefore, think big picture, and ensure that each step of the way\nhas a redundancy and that process is regularly exercised.\nThe same goes for temporary single points of failure. If you rely on a Key Hierarchy Architecture\nbased on a cloud KMS to provision an on-premises cluster/application, consider the impact\nthey would have in case of a failure pre, mid or post deploy (of either cluster or applications).\nEphemeral or Single-use Secrets\nThe easiest type of secrets to manage are the ones that ephemeral; context-bound and time-bound.\nHowever, they are not supported by all use-cases. Whenever they are, prioritise their use over\nstatic or long-lived secrets.\nAn example of an ephemeral secret that is time-bound, is a token provided by cloud providers to\nany application running within a given Cloud Machine. Those tokens are generated automatically,\nand have a short expiration time. In some cases you can even tie them to a network boundary,\nmeaning that even if they get breached, they won’t be able to be used outside the current\ncontext.\nFlux supports\ncontextual authorization\nfor the major Cloud Providers, be aware of the supported\nfeatures and use them whenever possible.\nDetect “chicken and egg” scenarios\nFlux won’t protect you from yourself. On a running cluster, it is quite easy to incrementally fall\ninto the trap of building a non-provisionable cluster. For example, if your first Kustomization\ndepends on a CustomResourceType (CRD) to deploy a secret, which is only deployed as part of another\nKustomization, Flux may not be able to redeploy your sources from scratch on a new cluster.\nMake sure that your pipeline identifies and tests such scenarios. Automate the provisioning of clusters\nthat can test the entire E2E of your deployment process, and ensure that it is executed regularly.\nSummary\nFlux supports a wide range of Secret Management solutions. And it is up to its users\nto define what works best for their use case. This subject isn’t easy, and due diligence\nis important to ensure the appropriate level of security controls are in place.\nOverall, none of the approaches covered above are inherently secure or insecure, but they\nare rather part of a big picture in which what matters the most is the weakest link\nand how it all hangs together. As with all things around security, a layered approach is\nrecommended.\nTake into account your threat model, availability and resilience requirements\nwhen deciding what works best for you, and rest assured that a combination of some of\nthe above will make more sense, especially when disaster recovery and break the glass\nscenarios are considered.\nLast modified 2023-08-24:\nFix broken API anchor links (fabe6e7)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of FLUXCD.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocs\nSecurity\nSLSA Assessment\nSLSA Assessment\nFlux assessment of SLSA Level 3 requirements.\nIntroduction\nSupply Chain Levels for Software Artifacts, or SLSA (pronounced “salsa”),\nis a security framework which aims to prevent tampering and secure artifacts in a project.\nSLSA is designed to support automation that tracks code handling from source to binary\nprotecting against tampering regardless of the complexity of the software supply chain.\nStarting with Flux version 2.0.0, the build, release and provenance portions of the Flux\nproject supply chain provisionally meet\nSLSA Build Level 3\n.\nSLSA Requirements and Flux Compliance State\nWhat follows is an assessment made by members of the Flux core maintainers team\non how Flux v2.0 complies with the Build Level 3 requirements as specified by\nSLSA v1.0\n.\nProducer Requirements\nRequirement\nRequired at SLSA L3\nMet by Flux\nChoose an appropriate build platform\nYes\nYes\nFollow a consistent build process\nYes\nYes\nDistribute provenance\nYes\nYes\nChoose an appropriate build platform\nThe producer MUST choose a builder capable of producing Build Level 3 provenance.\nThe Flux project uses Git for source code management and the Flux project’s repositories are hosted on GitHub under\nthe FluxCD organization.\nAll the Flux maintainers are required to have two-factor authentication enabled and to sign-off all their\ncontributions.\nThe Flux project uses GitHub Actions and GitHub Runners for building all its release artifacts.\nThe build and release process runs in isolation on an ephemeral environment provided by GitHub-hosted runners.\nFollow a consistent build process\nThe producer MUST build their artifact in a consistent manner such that verifiers can form expectations about the\nbuild process.\nThe build and release process is defined in code (GitHub Workflows and Makefiles) and is kept under version control.\nThe GitHub Workflows make use of GitHub Actions pinned to their Git commit SHA and are kept up-to-date using GitHub\nDependabot.\nAll changes to build and release process are done via Pull Requests that must be approved by at least one Flux\nmaintainer.\nThe release process can only be kicked off by a Flux maintainer by pushing a Git tag in the semver format.\nDistribute provenance\nThe producer MUST distribute provenance to artifact consumers.\nThe Flux project uses the\nofficial\nSLSA GitHub Generator project\nfor provenance\ngeneration and distribution.\nThe provenance for the release artifacts published to GitHub releases (binaries, SBOMs, deploy manifests, source code)\nis generated using the\ngenerator_generic_slsa3\nGitHub Workflow provided by\nthe\nSLSA GitHub Generator project\n.\nThe provenance for the release artifacts published to GitHub Container Registry and to DockerHub (Flux controllers\nmulti-arch container images) is generated using the\ngenerator_container_slsa3\nGitHub Workflow provided by\nthe\nSLSA GitHub Generator project\n.\nBuild Platform Requirements\nProvenance generation\nRequirement\nRequired at SLSA L3\nMet by Flux\nProvenance Exists\nYes\nYes\nProvenance is Authentic\nYes\nYes\nProvenance is Unforgeable\nYes\nYes\nThe build process MUST generate provenance that unambiguously identifies the output package by cryptographic digest\nand describes how that package was produced.\nThe Flux project release workflows make use of the\nofficial\nSLSA GitHub Generator project\nfor provenance\ngeneration.\nThe provenance file stores the SHA-256 hashes of the release artifacts (binaries, SBOMs, deploy manifests, source\ncode).\nThe provenance identifies the Flux container images using their digest in SHA-256 format.\nConsumers MUST be able to validate the authenticity of the provenance attestation in order to ensure integrity and\ndefine trust.\nThe provenance is signed by Sigstore Cosign using the GitHub OIDC identity and the public key to verify the provenance\nis stored in the public\nRekor transparency log\n.\nThe release process and the provenance generation are run in isolation on an ephemeral environment provided by\nGitHub-hosted runners.\nThe provenance of the Flux release artifacts (binaries, container images, SBOMs, deploy manifests) can be verified\nusing the official\nSLSA verifier tool\n.\nProvenance MUST be strongly resistant to forgery by tenants.\nThe provenance generation workflows run on ephemeral and isolated virtual machines which are fully managed by GitHub.\nThe provenance signing secrets are ephemeral and are generated through\nSigstore’s\nkeyless signing\nprocedure.\nThe\nSLSA GitHub generator\nruns on separate virtual machines\nthan the build and release process, so that the Flux build scripts don’t have access to the signing secrets.\nIsolation strength\nRequirement\nRequired at SLSA L3\nMet by Flux\nHosted\nYes\nYes\nIsolated\nYes\nYes\nAll build steps ran using a hosted build platform on shared or dedicated infrastructure.\nThe release process and the provenance generation are run in isolation on an ephemeral environment provided by\nGitHub-hosted runners.\nThe provenance generation is decoupled from the build process,\nthe\nSLSA GitHub generator\nruns on separate virtual machines\nfully managed by GitHub.\nThe build platform ensured that the build steps ran in an isolated environment, free of unintended external influence.\nThe release process can only be kicked off by a Flux maintainer by pushing a Git tag in the semver format.\nThe release process runs on ephemeral and isolated virtual machines which are fully managed by GitHub.\nThe release process can’t access the provenance signing key, because the provenance generator runs in isolation on\nseparate GitHub-hosted runners.\nProvenance verification\nThe provenance of the Flux release artifacts (binaries, container images, SBOMs, deploy manifests)\ncan be verified using the official\nSLSA verifier tool\n.\nContainer images\nThe provenance of the Flux multi-arch container images hosted on GitHub Container Registry\nand DockerHub can be verified using the official\nSLSA verifier tool\nand\nSigstore Cosign\n.\nWhat follows is the list of Flux components along with their minimum required version for provenance verification.\nGit Repository\nImages\nMin version\nProvenance (SLSA L3)\nflux2\ndocker.io/fluxcd/flux-cli\nghcr.io/fluxcd/flux-cli\nv2.0.1\nYes\nsource-controller\ndocker.io/fluxcd/source-contoller\nghcr.io/fluxcd/source-contoller\nv1.0.0\nYes\nkustomize-controller\ndocker.io/fluxcd/kustomize-contoller\nghcr.io/fluxcd/kustomize-contoller\nv1.0.0\nYes\nnotification-controller\ndocker.io/fluxcd/notification-contoller\nghcr.io/fluxcd/notification-contoller\nv1.0.0\nYes\nhelm-controller\ndocker.io/fluxcd/helm-contoller\nghcr.io/fluxcd/helm-contoller\nv0.35.0\nYes\nimage-reflector-controller\ndocker.io/fluxcd/image-reflector-contoller\nghcr.io/fluxcd/image-reflector-contoller\nv0.29.0\nYes\nimage-automation-controller\ndocker.io/fluxcd/image-automation-contoller\nghcr.io/fluxcd/image-automation-contoller\nv0.35.0\nYes\nExample\nWe will be using the\nsource-controller\ncontainer\nimage hosted on GHCR for this example, but these instructions can be used for all Flux container images.\nFirst, collect the digest of the image to verify:\n$\ncrane digest ghcr.io/fluxcd/source-controller:v1.0.0\nsha256:8dfd386a338eab2fde70cd7609e3b35a6e2f30283ecf2366da53013295fa65f3\nUsing the digest, verify the provenance of the Flux controller by specifying the repository and version:\n$\nslsa-verifier verify-image ghcr.io/fluxcd/source-controller:v1.0.0@sha256:8dfd386a338eab2fde70cd7609e3b35a6e2f30283ecf2366da53013295fa65f3 --source-uri github.com/fluxcd/source-controller --source-tag v1.0.0\nVerified build using builder https://github.comslsa-framework/slsa-github-generator/.github/workflows/generator_container_slsa3.yml@refs/tags/v1.7.0 at commit a40e0da705f26710077a7591f9dad05b7cd55acd\nPASSED: Verified SLSA provenance\nUsing Cosign, verify the SLSA provenance attestation by specifying the workflow and GitHub OIDC issuer:\n$\ncosign verify-attestation --type slsaprovenance --certificate-identity-regexp https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_container_slsa3.yml@refs/tags/v --certificate-oidc-issuer https://token.actions.githubusercontent.com ghcr.io/fluxcd/source-controller:v1.0.0\nVerification for ghcr.io/fluxcd/source-controller:v1.0.0 --\nThe following checks were performed on each of these signatures:\n- The cosign claims were validated\n- Existence of the claims in the transparency log was verified offline\n- The code-signing certificate was verified using trusted certificate authority certificates\nCertificate subject: https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_container_slsa3.yml@refs/tags/v1.7.0\nCertificate issuer URL: https://token.actions.githubusercontent.com\nGitHub Workflow Trigger: push\nGitHub Workflow SHA: a40e0da705f26710077a7591f9dad05b7cd55acd\nGitHub Workflow Name: release\nGitHub Workflow Repository: fluxcd/source-controller\nGitHub Workflow Ref: refs/tags/v1.0.0\nFlux artifacts\nThe provenance of the Flux release artifacts (binaries, container images, SBOMs, deploy manifests) published on\nGitHub can be verified using the official\nSLSA verifier tool\n.\nExample\nIn this example we use the Flux SBOM file,\nbut the instructions can be used for all artifacts included with the Flux release.\nFirst, download the release artifacts from GitHub:\nFLUX_VER\n=\n2.0.1\n&&\n\\\ngh release download v\n${\nFLUX_VER\n}\n-R\n=\nfluxcd/flux2 -p\n=\n\"*\"\nUsing the\nprovenance.intoto.jsonl\nfile,\nverify the provenance attestation of the Flux SBOM (\nflux_<version>_sbom.spdx.json\n):\n$\nslsa-verifier verify-artifact --provenance-path provenance.intoto.jsonl --source-uri github.com/fluxcd/flux2 --source-tag v\n${\nFLUX_VER\n}\nflux_\n${\nFLUX_VER\n}\n_sbom.spdx.json\nVerified signature against tlog entry index 27066821 at URL: https://rekor.sigstore.dev/api/v1/log/entries/24296fb24b8ad77ac2d2dc6381ec7f1f04d991344771214c5fb5861621dbd9da6f0551f806cbf609\nVerified build using builder https://github.comslsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@refs/tags/v1.7.0 at commit 9b3162495ce1b99b1fcdf137c553f543eafe3ec7\nVerifying artifact flux_2.0.1_sbom.spdx.json: PASSED\nPASSED: Verified SLSA provenance\nLast modified 2023-07-11:\nAdd SBOM provenance verification to SLSA guide (d362d2d)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the FLUXCD official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "fluxcd", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nThe Python Tutorial\n|\nTheme\nAuto\nLight\nDark\n|\nThe Python Tutorial\n¶\nTip\nThis tutorial is designed for\nprogrammers\nthat are new to the Python language,\nnot\nbeginners\nwho are new to programming.\nPython is an easy to learn, powerful programming language. It has efficient\nhigh-level data structures and a simple but effective approach to\nobject-oriented programming. Python’s elegant syntax and dynamic typing,\ntogether with its interpreted nature, make it an ideal language for scripting\nand rapid application development in many areas on most platforms.\nThe Python interpreter and the extensive standard library are freely available\nin source or binary form for all major platforms from the Python web site,\nhttps://www.python.org/\n, and may be freely distributed. The same site also\ncontains distributions of and pointers to many free third party Python modules,\nprograms and tools, and additional documentation.\nThe Python interpreter is easily extended with new functions and data types\nimplemented in C or C++ (or other languages callable from C). Python is also\nsuitable as an extension language for customizable applications.\nThis tutorial introduces the reader informally to the basic concepts and\nfeatures of the Python language and system. Be aware that it expects you to\nhave a basic understanding of programming in general. It helps to have a Python\ninterpreter handy for hands-on experience, but all examples are self-contained,\nso the tutorial can be read off-line as well.\nFor a description of standard objects and modules, see\nThe Python Standard Library\n.\nThe Python Language Reference\ngives a more formal definition of the language.  To write\nextensions in C or C++, read\nExtending and Embedding the Python Interpreter\nand\nPython/C API Reference Manual\n. There are also several books covering Python in depth.\nThis tutorial does not attempt to be comprehensive and cover every single\nfeature, or even every commonly used feature. Instead, it introduces many of\nPython’s most noteworthy features, and will give you a good idea of the\nlanguage’s flavor and style. After reading it, you will be able to read and\nwrite Python modules and programs, and you will be ready to learn more about the\nvarious Python library modules described in\nThe Python Standard Library\n.\nThe\nGlossary\nis also worth going through.\n1. Whetting Your Appetite\n2. Using the Python Interpreter\n2.1. Invoking the Interpreter\n2.1.1. Argument Passing\n2.1.2. Interactive Mode\n2.2. The Interpreter and Its Environment\n2.2.1. Source Code Encoding\n3. An Informal Introduction to Python\n3.1. Using Python as a Calculator\n3.1.1. Numbers\n3.1.2. Text\n3.1.3. Lists\n3.2. First Steps Towards Programming\n4. More Control Flow Tools\n4.1.\nif\nStatements\n4.2.\nfor\nStatements\n4.3. The\nrange()\nFunction\n4.4.\nbreak\nand\ncontinue\nStatements\n4.5.\nelse\nClauses on Loops\n4.6.\npass\nStatements\n4.7.\nmatch\nStatements\n4.8. Defining Functions\n4.9. More on Defining Functions\n4.9.1. Default Argument Values\n4.9.2. Keyword Arguments\n4.9.3. Special parameters\n4.9.3.1. Positional-or-Keyword Arguments\n4.9.3.2. Positional-Only Parameters\n4.9.3.3. Keyword-Only Arguments\n4.9.3.4. Function Examples\n4.9.3.5. Recap\n4.9.4. Arbitrary Argument Lists\n4.9.5. Unpacking Argument Lists\n4.9.6. Lambda Expressions\n4.9.7. Documentation Strings\n4.9.8. Function Annotations\n4.10. Intermezzo: Coding Style\n5. Data Structures\n5.1. More on Lists\n5.1.1. Using Lists as Stacks\n5.1.2. Using Lists as Queues\n5.1.3. List Comprehensions\n5.1.4. Nested List Comprehensions\n5.2. The\ndel\nstatement\n5.3. Tuples and Sequences\n5.4. Sets\n5.5. Dictionaries\n5.6. Looping Techniques\n5.7. More on Conditions\n5.8. Comparing Sequences and Other Types\n6. Modules\n6.1. More on Modules\n6.1.1. Executing modules as scripts\n6.1.2. The Module Search Path\n6.1.3. “Compiled” Python files\n6.2. Standard Modules\n6.3. The\ndir()\nFunction\n6.4. Packages\n6.4.1. Importing * From a Package\n6.4.2. Intra-package References\n6.4.3. Packages in Multiple Directories\n7. Input and Output\n7.1. Fancier Output Formatting\n7.1.1. Formatted String Literals\n7.1.2. The String format() Method\n7.1.3. Manual String Formatting\n7.1.4. Old string formatting\n7.2. Reading and Writing Files\n7.2.1. Methods of File Objects\n7.2.2. Saving structured data with\njson\n8. Errors and Exceptions\n8.1. Syntax Errors\n8.2. Exceptions\n8.3. Handling Exceptions\n8.4. Raising Exceptions\n8.5. Exception Chaining\n8.6. User-defined Exceptions\n8.7. Defining Clean-up Actions\n8.8. Predefined Clean-up Actions\n8.9. Raising and Handling Multiple Unrelated Exceptions\n8.10. Enriching Exceptions with Notes\n9. Classes\n9.1. A Word About Names and Objects\n9.2. Python Scopes and Namespaces\n9.2.1. Scopes and Namespaces Example\n9.3. A First Look at Classes\n9.3.1. Class Definition Syntax\n9.3.2. Class Objects\n9.3.3. Instance Objects\n9.3.4. Method Objects\n9.3.5. Class and Instance Variables\n9.4. Random Remarks\n9.5. Inheritance\n9.5.1. Multiple Inheritance\n9.6. Private Variables\n9.7. Odds and Ends\n9.8. Iterators\n9.9. Generators\n9.10. Generator Expressions\n10. Brief Tour of the Standard Library\n10.1. Operating System Interface\n10.2. File Wildcards\n10.3. Command Line Arguments\n10.4. Error Output Redirection and Program Termination\n10.5. String Pattern Matching\n10.6. Mathematics\n10.7. Internet Access\n10.8. Dates and Times\n10.9. Data Compression\n10.10. Performance Measurement\n10.11. Quality Control\n10.12. Batteries Included\n11. Brief Tour of the Standard Library — Part II\n11.1. Output Formatting\n11.2. Templating\n11.3. Working with Binary Data Record Layouts\n11.4. Multi-threading\n11.5. Logging\n11.6. Weak References\n11.7. Tools for Working with Lists\n11.8. Decimal Floating-Point Arithmetic\n12. Virtual Environments and Packages\n12.1. Introduction\n12.2. Creating Virtual Environments\n12.3. Managing Packages with pip\n13. What Now?\n14. Interactive Input Editing and History Substitution\n14.1. Tab Completion and History Editing\n14.2. Alternatives to the Interactive Interpreter\n15. Floating-Point Arithmetic:  Issues and Limitations\n15.1. Representation Error\n16. Appendix\n16.1. Interactive Mode\n16.1.1. Error Handling\n16.1.2. Executable Python Scripts\n16.1.3. The Interactive Startup File\n16.1.4. The Customization Modules\nPrevious topic\nChangelog\nNext topic\n1.\nWhetting Your Appetite\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nThe Python Tutorial\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nThe Python Standard Library\n|\nTheme\nAuto\nLight\nDark\n|\nThe Python Standard Library\n¶\nWhile\nThe Python Language Reference\ndescribes the exact syntax and\nsemantics of the Python language, this library reference manual\ndescribes the standard library that is distributed with Python. It also\ndescribes some of the optional components that are commonly included\nin Python distributions.\nPython’s standard library is very extensive, offering a wide range of\nfacilities as indicated by the long table of contents listed below. The\nlibrary contains built-in modules (written in C) that provide access to\nsystem functionality such as file I/O that would otherwise be\ninaccessible to Python programmers, as well as modules written in Python\nthat provide standardized solutions for many problems that occur in\neveryday programming. Some of these modules are explicitly designed to\nencourage and enhance the portability of Python programs by abstracting\naway platform-specifics into platform-neutral APIs.\nThe Python installers for the Windows platform usually include\nthe entire standard library and often also include many additional\ncomponents. For Unix-like operating systems Python is normally provided\nas a collection of packages, so it may be necessary to use the packaging\ntools provided with the operating system to obtain some or all of the\noptional components.\nIn addition to the standard library, there is an active collection of\nhundreds of thousands of components (from individual programs and modules to\npackages and entire application development frameworks), available from\nthe\nPython Package Index\n.\nIntroduction\nNotes on availability\nBuilt-in Functions\nBuilt-in Constants\nConstants added by the\nsite\nmodule\nBuilt-in Types\nTruth Value Testing\nBoolean Operations —\nand\n,\nor\n,\nnot\nComparisons\nNumeric Types —\nint\n,\nfloat\n,\ncomplex\nBoolean Type -\nbool\nIterator Types\nSequence Types —\nlist\n,\ntuple\n,\nrange\nText Sequence Type —\nstr\nBinary Sequence Types —\nbytes\n,\nbytearray\n,\nmemoryview\nSet Types —\nset\n,\nfrozenset\nMapping Types —\ndict\nContext Manager Types\nType Annotation Types —\nGeneric Alias\n,\nUnion\nOther Built-in Types\nSpecial Attributes\nInteger string conversion length limitation\nBuilt-in Exceptions\nException context\nInheriting from built-in exceptions\nBase classes\nConcrete exceptions\nWarnings\nException groups\nException hierarchy\nText Processing Services\nstring\n— Common string operations\nre\n— Regular expression operations\ndifflib\n— Helpers for computing deltas\ntextwrap\n— Text wrapping and filling\nunicodedata\n— Unicode Database\nstringprep\n— Internet String Preparation\nreadline\n— GNU readline interface\nrlcompleter\n— Completion function for GNU readline\nBinary Data Services\nstruct\n— Interpret bytes as packed binary data\ncodecs\n— Codec registry and base classes\nData Types\ndatetime\n— Basic date and time types\nzoneinfo\n— IANA time zone support\ncalendar\n— General calendar-related functions\ncollections\n— Container datatypes\ncollections.abc\n— Abstract Base Classes for Containers\nheapq\n— Heap queue algorithm\nbisect\n— Array bisection algorithm\narray\n— Efficient arrays of numeric values\nweakref\n— Weak references\ntypes\n— Dynamic type creation and names for built-in types\ncopy\n— Shallow and deep copy operations\npprint\n— Data pretty printer\nreprlib\n— Alternate\nrepr()\nimplementation\nenum\n— Support for enumerations\ngraphlib\n— Functionality to operate with graph-like structures\nNumeric and Mathematical Modules\nnumbers\n— Numeric abstract base classes\nmath\n— Mathematical functions\ncmath\n— Mathematical functions for complex numbers\ndecimal\n— Decimal fixed-point and floating-point arithmetic\nfractions\n— Rational numbers\nrandom\n— Generate pseudo-random numbers\nstatistics\n— Mathematical statistics functions\nFunctional Programming Modules\nitertools\n— Functions creating iterators for efficient looping\nfunctools\n— Higher-order functions and operations on callable objects\noperator\n— Standard operators as functions\nFile and Directory Access\npathlib\n— Object-oriented filesystem paths\nos.path\n— Common pathname manipulations\nstat\n— Interpreting\nstat()\nresults\nfilecmp\n— File and Directory Comparisons\ntempfile\n— Generate temporary files and directories\nglob\n— Unix style pathname pattern expansion\nfnmatch\n— Unix filename pattern matching\nlinecache\n— Random access to text lines\nshutil\n— High-level file operations\nData Persistence\npickle\n— Python object serialization\ncopyreg\n— Register\npickle\nsupport functions\nshelve\n— Python object persistence\nmarshal\n— Internal Python object serialization\ndbm\n— Interfaces to Unix “databases”\nsqlite3\n— DB-API 2.0 interface for SQLite databases\nData Compression and Archiving\nzlib\n— Compression compatible with\ngzip\ngzip\n— Support for\ngzip\nfiles\nbz2\n— Support for\nbzip2\ncompression\nlzma\n— Compression using the LZMA algorithm\nzipfile\n— Work with ZIP archives\ntarfile\n— Read and write tar archive files\nFile Formats\ncsv\n— CSV File Reading and Writing\nconfigparser\n— Configuration file parser\ntomllib\n— Parse TOML files\nnetrc\n— netrc file processing\nplistlib\n— Generate and parse Apple\n.plist\nfiles\nCryptographic Services\nhashlib\n— Secure hashes and message digests\nhmac\n— Keyed-Hashing for Message Authentication\nsecrets\n— Generate secure random numbers for managing secrets\nGeneric Operating System Services\nos\n— Miscellaneous operating system interfaces\nio\n— Core tools for working with streams\ntime\n— Time access and conversions\nlogging\n— Logging facility for Python\nlogging.config\n— Logging configuration\nlogging.handlers\n— Logging handlers\nplatform\n—  Access to underlying platform’s identifying data\nerrno\n— Standard errno system symbols\nctypes\n— A foreign function library for Python\nCommand Line Interface Libraries\nargparse\n— Parser for command-line options, arguments and subcommands\noptparse\n— Parser for command line options\ngetpass\n— Portable password input\nfileinput\n— Iterate over lines from multiple input streams\ncurses\n— Terminal handling for character-cell displays\ncurses.textpad\n— Text input widget for curses programs\ncurses.ascii\n— Utilities for ASCII characters\ncurses.panel\n— A panel stack extension for curses\nConcurrent Execution\nthreading\n— Thread-based parallelism\nmultiprocessing\n— Process-based parallelism\nmultiprocessing.shared_memory\n— Shared memory for direct access across processes\nThe\nconcurrent\npackage\nconcurrent.futures\n— Launching parallel tasks\nsubprocess\n— Subprocess management\nsched\n— Event scheduler\nqueue\n— A synchronized queue class\ncontextvars\n— Context Variables\n_thread\n— Low-level threading API\nNetworking and Interprocess Communication\nasyncio\n— Asynchronous I/O\nsocket\n— Low-level networking interface\nssl\n— TLS/SSL wrapper for socket objects\nselect\n— Waiting for I/O completion\nselectors\n— High-level I/O multiplexing\nsignal\n— Set handlers for asynchronous events\nmmap\n— Memory-mapped file support\nInternet Data Handling\nemail\n— An email and MIME handling package\njson\n— JSON encoder and decoder\nmailbox\n— Manipulate mailboxes in various formats\nmimetypes\n— Map filenames to MIME types\nbase64\n— Base16, Base32, Base64, Base85 Data Encodings\nbinascii\n— Convert between binary and ASCII\nquopri\n— Encode and decode MIME quoted-printable data\nStructured Markup Processing Tools\nhtml\n— HyperText Markup Language support\nhtml.parser\n— Simple HTML and XHTML parser\nhtml.entities\n— Definitions of HTML general entities\nXML Processing Modules\nxml.etree.ElementTree\n— The ElementTree XML API\nxml.dom\n— The Document Object Model API\nxml.dom.minidom\n— Minimal DOM implementation\nxml.dom.pulldom\n— Support for building partial DOM trees\nxml.sax\n— Support for SAX2 parsers\nxml.sax.handler\n— Base classes for SAX handlers\nxml.sax.saxutils\n— SAX Utilities\nxml.sax.xmlreader\n— Interface for XML parsers\nxml.parsers.expat\n— Fast XML parsing using Expat\nInternet Protocols and Support\nwebbrowser\n— Convenient web-browser controller\nwsgiref\n— WSGI Utilities and Reference Implementation\nurllib\n— URL handling modules\nurllib.request\n— Extensible library for opening URLs\nurllib.response\n— Response classes used by urllib\nurllib.parse\n— Parse URLs into components\nurllib.error\n— Exception classes raised by urllib.request\nurllib.robotparser\n—  Parser for robots.txt\nhttp\n— HTTP modules\nhttp.client\n— HTTP protocol client\nftplib\n— FTP protocol client\npoplib\n— POP3 protocol client\nimaplib\n— IMAP4 protocol client\nsmtplib\n— SMTP protocol client\nuuid\n— UUID objects according to\nRFC 4122\nsocketserver\n— A framework for network servers\nhttp.server\n— HTTP servers\nhttp.cookies\n— HTTP state management\nhttp.cookiejar\n— Cookie handling for HTTP clients\nxmlrpc\n— XMLRPC server and client modules\nxmlrpc.client\n— XML-RPC client access\nxmlrpc.server\n— Basic XML-RPC servers\nipaddress\n— IPv4/IPv6 manipulation library\nMultimedia Services\nwave\n— Read and write WAV files\ncolorsys\n— Conversions between color systems\nInternationalization\ngettext\n— Multilingual internationalization services\nlocale\n— Internationalization services\nProgram Frameworks\nturtle\n— Turtle graphics\ncmd\n— Support for line-oriented command interpreters\nshlex\n— Simple lexical analysis\nGraphical User Interfaces with Tk\ntkinter\n— Python interface to Tcl/Tk\ntkinter.colorchooser\n— Color choosing dialog\ntkinter.font\n— Tkinter font wrapper\nTkinter Dialogs\ntkinter.messagebox\n— Tkinter message prompts\ntkinter.scrolledtext\n— Scrolled Text Widget\ntkinter.dnd\n— Drag and drop support\ntkinter.ttk\n— Tk themed widgets\nIDLE — Python editor and shell\nDevelopment Tools\ntyping\n— Support for type hints\npydoc\n— Documentation generator and online help system\nPython Development Mode\ndoctest\n— Test interactive Python examples\nunittest\n— Unit testing framework\nunittest.mock\n— mock object library\nunittest.mock\n— getting started\ntest\n— Regression tests package for Python\ntest.support\n— Utilities for the Python test suite\ntest.support.socket_helper\n— Utilities for socket tests\ntest.support.script_helper\n— Utilities for the Python execution tests\ntest.support.bytecode_helper\n— Support tools for testing correct bytecode generation\ntest.support.threading_helper\n— Utilities for threading tests\ntest.support.os_helper\n— Utilities for os tests\ntest.support.import_helper\n— Utilities for import tests\ntest.support.warnings_helper\n— Utilities for warnings tests\nDebugging and Profiling\nAudit events table\nbdb\n— Debugger framework\nfaulthandler\n— Dump the Python traceback\npdb\n— The Python Debugger\nThe Python Profilers\ntimeit\n— Measure execution time of small code snippets\ntrace\n— Trace or track Python statement execution\ntracemalloc\n— Trace memory allocations\nSoftware Packaging and Distribution\nensurepip\n— Bootstrapping the\npip\ninstaller\nvenv\n— Creation of virtual environments\nzipapp\n— Manage executable Python zip archives\nPython Runtime Services\nsys\n— System-specific parameters and functions\nsys.monitoring\n— Execution event monitoring\nsysconfig\n— Provide access to Python’s configuration information\nbuiltins\n— Built-in objects\n__main__\n— Top-level code environment\nwarnings\n— Warning control\ndataclasses\n— Data Classes\ncontextlib\n— Utilities for\nwith\n-statement contexts\nabc\n— Abstract Base Classes\natexit\n— Exit handlers\ntraceback\n— Print or retrieve a stack traceback\n__future__\n— Future statement definitions\ngc\n— Garbage Collector interface\ninspect\n— Inspect live objects\nsite\n— Site-specific configuration hook\nCustom Python Interpreters\ncode\n— Interpreter base classes\ncodeop\n— Compile Python code\nImporting Modules\nzipimport\n— Import modules from Zip archives\npkgutil\n— Package extension utility\nmodulefinder\n— Find modules used by a script\nrunpy\n— Locating and executing Python modules\nimportlib\n— The implementation of\nimport\nimportlib.resources\n– Package resource reading, opening and access\nimportlib.resources.abc\n– Abstract base classes for resources\nimportlib.metadata\n– Accessing package metadata\nThe initialization of the\nsys.path\nmodule search path\nPython Language Services\nast\n— Abstract Syntax Trees\nsymtable\n— Access to the compiler’s symbol tables\ntoken\n— Constants used with Python parse trees\nkeyword\n— Testing for Python keywords\ntokenize\n— Tokenizer for Python source\ntabnanny\n— Detection of ambiguous indentation\npyclbr\n— Python module browser support\npy_compile\n— Compile Python source files\ncompileall\n— Byte-compile Python libraries\ndis\n— Disassembler for Python bytecode\npickletools\n— Tools for pickle developers\nMS Windows Specific Services\nmsvcrt\n— Useful routines from the MS VC++ runtime\nwinreg\n— Windows registry access\nwinsound\n— Sound-playing interface for Windows\nUnix Specific Services\nposix\n— The most common POSIX system calls\npwd\n— The password database\ngrp\n— The group database\ntermios\n— POSIX style tty control\ntty\n— Terminal control functions\npty\n— Pseudo-terminal utilities\nfcntl\n— The\nfcntl\nand\nioctl\nsystem calls\nresource\n— Resource usage information\nsyslog\n— Unix syslog library routines\nModules command-line interface (CLI)\nSuperseded Modules\ngetopt\n— C-style parser for command line options\nRemoved Modules\nSecurity Considerations\nPrevious topic\n10.\nFull Grammar specification\nNext topic\nIntroduction\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nThe Python Standard Library\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nThe Python Language Reference\n|\nTheme\nAuto\nLight\nDark\n|\nThe Python Language Reference\n¶\nThis reference manual describes the syntax and “core semantics” of the\nlanguage. It is terse, but attempts to be exact and complete. The semantics of\nnon-essential built-in object types and of the built-in functions and modules\nare described in\nThe Python Standard Library\n. For an informal introduction to the\nlanguage, see\nThe Python Tutorial\n. For C or C++ programmers, two additional\nmanuals exist:\nExtending and Embedding the Python Interpreter\ndescribes the high-level picture of how to\nwrite a Python extension module, and the\nPython/C API Reference Manual\ndescribes the\ninterfaces available to C/C++ programmers in detail.\n1. Introduction\n1.1. Alternate Implementations\n1.2. Notation\n2. Lexical analysis\n2.1. Line structure\n2.2. Other tokens\n2.3. Identifiers and keywords\n2.4. Literals\n2.5. Operators\n2.6. Delimiters\n3. Data model\n3.1. Objects, values and types\n3.2. The standard type hierarchy\n3.3. Special method names\n3.4. Coroutines\n4. Execution model\n4.1. Structure of a program\n4.2. Naming and binding\n4.3. Exceptions\n5. The import system\n5.1.\nimportlib\n5.2. Packages\n5.3. Searching\n5.4. Loading\n5.5. The Path Based Finder\n5.6. Replacing the standard import system\n5.7. Package Relative Imports\n5.8. Special considerations for __main__\n5.9. References\n6. Expressions\n6.1. Arithmetic conversions\n6.2. Atoms\n6.3. Primaries\n6.4. Await expression\n6.5. The power operator\n6.6. Unary arithmetic and bitwise operations\n6.7. Binary arithmetic operations\n6.8. Shifting operations\n6.9. Binary bitwise operations\n6.10. Comparisons\n6.11. Boolean operations\n6.12. Assignment expressions\n6.13. Conditional expressions\n6.14. Lambdas\n6.15. Expression lists\n6.16. Evaluation order\n6.17. Operator precedence\n7. Simple statements\n7.1. Expression statements\n7.2. Assignment statements\n7.3. The\nassert\nstatement\n7.4. The\npass\nstatement\n7.5. The\ndel\nstatement\n7.6. The\nreturn\nstatement\n7.7. The\nyield\nstatement\n7.8. The\nraise\nstatement\n7.9. The\nbreak\nstatement\n7.10. The\ncontinue\nstatement\n7.11. The\nimport\nstatement\n7.12. The\nglobal\nstatement\n7.13. The\nnonlocal\nstatement\n7.14. The\ntype\nstatement\n8. Compound statements\n8.1. The\nif\nstatement\n8.2. The\nwhile\nstatement\n8.3. The\nfor\nstatement\n8.4. The\ntry\nstatement\n8.5. The\nwith\nstatement\n8.6. The\nmatch\nstatement\n8.7. Function definitions\n8.8. Class definitions\n8.9. Coroutines\n8.10. Type parameter lists\n9. Top-level components\n9.1. Complete Python programs\n9.2. File input\n9.3. Interactive input\n9.4. Expression input\n10. Full Grammar specification\nPrevious topic\n8.\nEditors and IDEs\nNext topic\n1.\nIntroduction\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nThe Python Language Reference\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nPython HOWTOs\n|\nTheme\nAuto\nLight\nDark\n|\nPython HOWTOs\n¶\nPython HOWTOs are documents that cover a specific topic in-depth.\nModeled on the Linux Documentation Project’s HOWTO collection, this collection is an\neffort to foster documentation that’s more detailed than the\nPython Library Reference.\nGeneral:\nAnnotations Best Practices\nArgparse Tutorial\nDescriptor Guide\nEnum HOWTO\nFunctional Programming HOWTO\nAn introduction to the ipaddress module\nLogging HOWTO\nLogging Cookbook\nRegular Expression HOWTO\nSorting Techniques\nUnicode HOWTO\nHOWTO Fetch Internet Resources Using The urllib Package\nAdvanced development:\nCurses Programming with Python\nPython experimental support for free threading\nC API Extension Support for Free Threading\nIsolating Extension Modules\nThe Python 2.3 Method Resolution Order\nSocket Programming HOWTO\ntimer file descriptor HOWTO\nPorting Extension Modules to Python 3\nDebugging and profiling:\nDebugging C API extensions and CPython Internals with GDB\nInstrumenting CPython with DTrace and SystemTap\nPython support for the Linux perf profiler\nPrevious topic\nInstalling Python Modules\nNext topic\nPorting Extension Modules to Python 3\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nPython HOWTOs\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nPython\n»\n3.13.5 Documentation\n»\nDistributing Python Modules\n|\nTheme\nAuto\nLight\nDark\n|\nDistributing Python Modules\n¶\nNote\nInformation and guidance on distributing Python modules and packages\nhas been moved to the\nPython Packaging User Guide\n,\nand the tutorial on\npackaging Python projects\n.\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nPython\n»\n3.13.5 Documentation\n»\nDistributing Python Modules\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nExtending and Embedding the Python Interpreter\n|\nTheme\nAuto\nLight\nDark\n|\nExtending and Embedding the Python Interpreter\n¶\nThis document describes how to write modules in C or C++ to extend the Python\ninterpreter with new modules.  Those modules can not only define new functions\nbut also new object types and their methods.  The document also describes how\nto embed the Python interpreter in another application, for use as an extension\nlanguage.  Finally, it shows how to compile and link extension modules so that\nthey can be loaded dynamically (at run time) into the interpreter, if the\nunderlying operating system supports this feature.\nThis document assumes basic knowledge about Python.  For an informal\nintroduction to the language, see\nThe Python Tutorial\n.\nThe Python Language Reference\ngives a more formal definition of the language.\nThe Python Standard Library\ndocuments\nthe existing object types, functions and modules (both built-in and written in\nPython) that give the language its wide application range.\nFor a detailed description of the whole Python/C API, see the separate\nPython/C API Reference Manual\n.\nRecommended third party tools\n¶\nThis guide only covers the basic tools for creating extensions provided\nas part of this version of CPython. Some\nthird party tools\noffer both simpler and more sophisticated approaches to creating\nC and C++ extensions for Python.\nCreating extensions without third party tools\n¶\nThis section of the guide covers creating C and C++ extensions without\nassistance from third party tools. It is intended primarily for creators\nof those tools, rather than being a recommended way to create your own\nC extensions.\nSee also\nPEP 489\n– Multi-phase extension module initialization\n1. Extending Python with C or C++\n1.1. A Simple Example\n1.2. Intermezzo: Errors and Exceptions\n1.3. Back to the Example\n1.4. The Module’s Method Table and Initialization Function\n1.5. Compilation and Linkage\n1.6. Calling Python Functions from C\n1.7. Extracting Parameters in Extension Functions\n1.8. Keyword Parameters for Extension Functions\n1.9. Building Arbitrary Values\n1.10. Reference Counts\n1.11. Writing Extensions in C++\n1.12. Providing a C API for an Extension Module\n2. Defining Extension Types: Tutorial\n2.1. The Basics\n2.2. Adding data and methods to the Basic example\n2.3. Providing finer control over data attributes\n2.4. Supporting cyclic garbage collection\n2.5. Subclassing other types\n3. Defining Extension Types: Assorted Topics\n3.1. Finalization and De-allocation\n3.2. Object Presentation\n3.3. Attribute Management\n3.4. Object Comparison\n3.5. Abstract Protocol Support\n3.6. Weak Reference Support\n3.7. More Suggestions\n4. Building C and C++ Extensions\n4.1. Building C and C++ Extensions with setuptools\n5. Building C and C++ Extensions on Windows\n5.1. A Cookbook Approach\n5.2. Differences Between Unix and Windows\n5.3. Using DLLs in Practice\nEmbedding the CPython runtime in a larger application\n¶\nSometimes, rather than creating an extension that runs inside the Python\ninterpreter as the main application, it is desirable to instead embed\nthe CPython runtime inside a larger application. This section covers\nsome of the details involved in doing that successfully.\n1. Embedding Python in Another Application\n1.1. Very High Level Embedding\n1.2. Beyond Very High Level Embedding: An overview\n1.3. Pure Embedding\n1.4. Extending Embedded Python\n1.5. Embedding Python in C++\n1.6. Compiling and Linking under Unix-like systems\nTable of Contents\nExtending and Embedding the Python Interpreter\nRecommended third party tools\nCreating extensions without third party tools\nEmbedding the CPython runtime in a larger application\nPrevious topic\nSecurity Considerations\nNext topic\n1.\nExtending Python with C or C++\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nExtending and Embedding the Python Interpreter\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nPython/C API Reference Manual\n|\nTheme\nAuto\nLight\nDark\n|\nPython/C API Reference Manual\n¶\nThis manual documents the API used by C and C++ programmers who want to write\nextension modules or embed Python.  It is a companion to\nExtending and Embedding the Python Interpreter\n,\nwhich describes the general principles of extension writing but does not\ndocument the API functions in detail.\nIntroduction\nCoding standards\nInclude Files\nUseful macros\nObjects, Types and Reference Counts\nExceptions\nEmbedding Python\nDebugging Builds\nRecommended third party tools\nC API Stability\nUnstable C API\nStable Application Binary Interface\nPlatform Considerations\nContents of Limited API\nThe Very High Level Layer\nReference Counting\nException Handling\nPrinting and clearing\nRaising exceptions\nIssuing warnings\nQuerying the error indicator\nSignal Handling\nException Classes\nException Objects\nUnicode Exception Objects\nRecursion Control\nStandard Exceptions\nStandard Warning Categories\nUtilities\nOperating System Utilities\nSystem Functions\nProcess Control\nImporting Modules\nData marshalling support\nParsing arguments and building values\nString conversion and formatting\nPyHash API\nReflection\nCodec registry and support functions\nPyTime C API\nSupport for Perf Maps\nAbstract Objects Layer\nObject Protocol\nCall Protocol\nNumber Protocol\nSequence Protocol\nMapping Protocol\nIterator Protocol\nBuffer Protocol\nConcrete Objects Layer\nFundamental Objects\nNumeric Objects\nSequence Objects\nContainer Objects\nFunction Objects\nOther Objects\nInitialization, Finalization, and Threads\nBefore Python Initialization\nGlobal configuration variables\nInitializing and finalizing the interpreter\nProcess-wide parameters\nThread State and the Global Interpreter Lock\nSub-interpreter support\nAsynchronous Notifications\nProfiling and Tracing\nReference tracing\nAdvanced Debugger Support\nThread Local Storage Support\nSynchronization Primitives\nPython Initialization Configuration\nExample\nPyWideStringList\nPyStatus\nPyPreConfig\nPreinitialize Python with PyPreConfig\nPyConfig\nInitialization with PyConfig\nIsolated Configuration\nPython Configuration\nPython Path Configuration\nPy_GetArgcArgv()\nMulti-Phase Initialization Private Provisional API\nMemory Management\nOverview\nAllocator Domains\nRaw Memory Interface\nMemory Interface\nObject allocators\nDefault Memory Allocators\nCustomize Memory Allocators\nDebug hooks on the Python memory allocators\nThe pymalloc allocator\nThe mimalloc allocator\ntracemalloc C API\nExamples\nObject Implementation Support\nAllocating Objects on the Heap\nCommon Object Structures\nType Object Structures\nSupporting Cyclic Garbage Collection\nAPI and ABI Versioning\nMonitoring C API\nGenerating Execution Events\nManaging the Monitoring State\nPrevious topic\n1.\nEmbedding Python in Another Application\nNext topic\nIntroduction\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nPython/C API Reference Manual\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nPython Frequently Asked Questions\n|\nTheme\nAuto\nLight\nDark\n|\nPython Frequently Asked Questions\n¶\nGeneral Python FAQ\nProgramming FAQ\nDesign and History FAQ\nLibrary and Extension FAQ\nExtending/Embedding FAQ\nPython on Windows FAQ\nGraphic User Interface FAQ\n“Why is Python Installed on my Computer?” FAQ\nPrevious topic\nC API Extension Support for Free Threading\nNext topic\nGeneral Python FAQ\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nPython Frequently Asked Questions\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nWhat’s New in Python\n»\nWhat’s New In Python 3.13\n|\nTheme\nAuto\nLight\nDark\n|\nWhat’s New In Python 3.13\n¶\nEditors\n:\nAdam Turner and Thomas Wouters\nThis article explains the new features in Python 3.13, compared to 3.12.\nPython 3.13 was released on October 7, 2024.\nFor full details, see the\nchangelog\n.\nSee also\nPEP 719\n– Python 3.13 Release Schedule\nSummary – Release Highlights\n¶\nPython 3.13 is the latest stable release of the Python programming\nlanguage, with a mix of changes to the language, the implementation\nand the standard library.\nThe biggest changes include a new\ninteractive interpreter\n,\nexperimental support for running in a\nfree-threaded mode\n(\nPEP 703\n),\nand a\nJust-In-Time compiler\n(\nPEP 744\n).\nError messages continue to improve, with tracebacks now highlighted in color\nby default. The\nlocals()\nbuiltin now has\ndefined semantics\nfor changing the returned mapping,\nand type parameters now support default values.\nThe library changes contain removal of deprecated APIs and modules,\nas well as the usual improvements in user-friendliness and correctness.\nSeveral legacy standard library modules have now\nbeen removed\nfollowing their deprecation in Python 3.11 (\nPEP 594\n).\nThis article doesn’t attempt to provide a complete specification\nof all new features, but instead gives a convenient overview.\nFor full details refer to the documentation,\nsuch as the\nLibrary Reference\nand\nLanguage Reference\n.\nTo understand the complete implementation and design rationale for a change,\nrefer to the PEP for a particular new feature;\nbut note that PEPs usually are not kept up-to-date\nonce a feature has been fully implemented.\nSee\nPorting to Python 3.13\nfor guidance on upgrading from\nearlier versions of Python.\nInterpreter improvements:\nA greatly improved\ninteractive interpreter\nand\nimproved error messages\n.\nPEP 667\n: The\nlocals()\nbuiltin now has\ndefined semantics\nwhen mutating the\nreturned mapping. Python debuggers and similar tools may now more reliably\nupdate local variables in optimized scopes even during concurrent code\nexecution.\nPEP 703\n: CPython 3.13 has experimental support for running with the\nglobal interpreter lock\ndisabled. See\nFree-threaded CPython\nfor more details.\nPEP 744\n: A basic\nJIT compiler\nwas added.\nIt is currently disabled by default (though we may turn it on later).\nPerformance improvements are modest – we expect to improve this\nover the next few releases.\nColor support in the new\ninteractive interpreter\n,\nas well as in\ntracebacks\nand\ndoctest\noutput.\nThis can be disabled through the\nPYTHON_COLORS\nand\nNO_COLOR\nenvironment variables.\nPython data model improvements:\n__static_attributes__\nstores the names of attributes accessed\nthrough\nself.X\nin any function in a class body.\n__firstlineno__\nrecords the first line number of a class\ndefinition.\nSignificant improvements in the standard library:\nAdd a new\nPythonFinalizationError\nexception, raised when an operation\nis blocked during\nfinalization\n.\nThe\nargparse\nmodule now supports deprecating command-line options,\npositional arguments, and subcommands.\nThe new functions\nbase64.z85encode()\nand\nbase64.z85decode()\nsupport encoding and decoding\nZ85 data\n.\nThe\ncopy\nmodule now has a\ncopy.replace()\nfunction,\nwith support for many builtin types and any class defining\nthe\n__replace__()\nmethod.\nThe new\ndbm.sqlite3\nmodule is now the default\ndbm\nbackend.\nThe\nos\nmodule has a\nsuite of new functions\nfor working with Linux’s timer notification file descriptors.\nThe\nrandom\nmodule now has a\ncommand-line interface\n.\nSecurity improvements:\nssl.create_default_context()\nsets\nssl.VERIFY_X509_PARTIAL_CHAIN\nand\nssl.VERIFY_X509_STRICT\nas default flags.\nC API improvements:\nThe\nPy_mod_gil\nslot is now used to indicate that\nan extension module supports running with the\nGIL\ndisabled.\nThe\nPyTime C API\nhas been added,\nproviding access to system clocks.\nPyMutex\nis a new lightweight mutex that occupies a single byte.\nThere is a new\nsuite of functions\nfor generating\nPEP 669\nmonitoring events in the C API.\nNew typing features:\nPEP 696\n: Type parameters (\ntyping.TypeVar\n,\ntyping.ParamSpec\n,\nand\ntyping.TypeVarTuple\n) now support defaults.\nPEP 702\n: The new\nwarnings.deprecated()\ndecorator adds support\nfor marking deprecations in the type system and at runtime.\nPEP 705\n:\ntyping.ReadOnly\ncan be used to mark an item of a\ntyping.TypedDict\nas read-only for type checkers.\nPEP 742\n:\ntyping.TypeIs\nprovides more intuitive\ntype narrowing behavior, as an alternative to\ntyping.TypeGuard\n.\nPlatform support:\nPEP 730\n: Apple’s iOS is now an\nofficially supported platform\n, at\ntier 3\n.\nPEP 738\n: Android is now an\nofficially supported platform\n, at\ntier 3\n.\nwasm32-wasi\nis now supported as a\ntier 2\nplatform.\nwasm32-emscripten\nis no longer an officially supported platform.\nImportant removals:\nPEP 594\n: The remaining 19 “dead batteries”\n(legacy stdlib modules) have been removed from the standard library:\naifc\n,\naudioop\n,\ncgi\n,\ncgitb\n,\nchunk\n,\ncrypt\n,\nimghdr\n,\nmailcap\n,\nmsilib\n,\nnis\n,\nnntplib\n,\nossaudiodev\n,\npipes\n,\nsndhdr\n,\nspwd\n,\nsunau\n,\ntelnetlib\n,\nuu\nand\nxdrlib\n.\nRemove the\n2to3\ntool and\nlib2to3\nmodule\n(deprecated in Python 3.11).\nRemove the\ntkinter.tix\nmodule (deprecated in Python 3.6).\nRemove the\nlocale.resetlocale()\nfunction.\nRemove the\ntyping.io\nand\ntyping.re\nnamespaces.\nRemove chained\nclassmethod\ndescriptors.\nRelease schedule changes:\nPEP 602\n(“Annual Release Cycle for Python”) has been updated\nto extend the full support (‘bugfix’) period for new releases to two years.\nThis updated policy means that:\nPython 3.9–3.12 have one and a half years of full support,\nfollowed by three and a half years of security fixes.\nPython 3.13 and later have two years of full support,\nfollowed by three years of security fixes.\nNew Features\n¶\nA better interactive interpreter\n¶\nPython now uses a new\ninteractive\nshell by default, based on code\nfrom the\nPyPy project\n.\nWhen the user starts the\nREPL\nfrom an interactive terminal,\nthe following new features are now supported:\nMultiline editing with history preservation.\nDirect support for REPL-specific commands like\nhelp\n,\nexit\n,\nand\nquit\n, without the need to call them as functions.\nPrompts and tracebacks with\ncolor enabled by default\n.\nInteractive help browsing using\nF1\nwith a separate command\nhistory.\nHistory browsing using\nF2\nthat skips output as well as the\n>>>\nand\n…\nprompts.\n“Paste mode” with\nF3\nthat makes pasting larger blocks of code\neasier (press\nF3\nagain to return to the regular prompt).\nTo disable the new interactive shell,\nset the\nPYTHON_BASIC_REPL\nenvironment variable.\nFor more on interactive mode, see\nInteractive Mode\n.\n(Contributed by Pablo Galindo Salgado, Łukasz Langa, and\nLysandros Nikolaou in\ngh-111201\nbased on code from the PyPy project.\nWindows support contributed by Dino Viehland and Anthony Shaw.)\nImproved error messages\n¶\nThe interpreter now uses color by default when displaying tracebacks in the\nterminal. This feature\ncan be controlled\nvia the new\nPYTHON_COLORS\nenvironment variable as well as\nthe canonical\nNO_COLOR\nand\nFORCE_COLOR\nenvironment variables.\n(Contributed by Pablo Galindo Salgado in\ngh-112730\n.)\nA common mistake is to write a script with the same name as a\nstandard library module. When this results in errors, we now\ndisplay a more helpful error message:\n$ python random.py\nTraceback (most recent call last):\nFile\n\"/home/me/random.py\"\n, line\n1\n, in\n<module>\nimport\nrandom\nFile\n\"/home/me/random.py\"\n, line\n3\n, in\n<module>\nprint\n(\nrandom\n.\nrandint\n(\n5\n))\n^^^^^^^^^^^^^^\nAttributeError\n:\nmodule 'random' has no attribute 'randint' (consider renaming '/home/me/random.py' since it has the same name as the standard library module named 'random' and prevents importing that standard library module)\nSimilarly, if a script has the same name as a third-party\nmodule that it attempts to import and this results in errors,\nwe also display a more helpful error message:\n$ python numpy.py\nTraceback (most recent call last):\nFile\n\"/home/me/numpy.py\"\n, line\n1\n, in\n<module>\nimport\nnumpy\nas\nnp\nFile\n\"/home/me/numpy.py\"\n, line\n3\n, in\n<module>\nnp\n.\narray\n([\n1\n,\n2\n,\n3\n])\n^^^^^^^^\nAttributeError\n:\nmodule 'numpy' has no attribute 'array' (consider renaming '/home/me/numpy.py' if it has the same name as a library you intended to import)\n(Contributed by Shantanu Jain in\ngh-95754\n.)\nThe error message now tries to suggest the correct keyword argument\nwhen an incorrect keyword argument is passed to a function.\n>>>\n\"Better error messages!\"\n.\nsplit\n(\nmax_split\n=\n1\n)\nTraceback (most recent call last):\nFile\n\"<python-input-0>\"\n, line\n1\n, in\n<module>\n\"Better error messages!\"\n.\nsplit\n(\nmax_split\n=\n1\n)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\nTypeError\n:\nsplit() got an unexpected keyword argument 'max_split'. Did you mean 'maxsplit'?\n(Contributed by Pablo Galindo Salgado and Shantanu Jain in\ngh-107944\n.)\nFree-threaded CPython\n¶\nCPython now has experimental support for running in a free-threaded mode,\nwith the\nglobal interpreter lock\n(GIL) disabled.\nThis is an experimental feature and therefore is not enabled by default.\nThe free-threaded mode requires a different executable,\nusually called\npython3.13t\nor\npython3.13t.exe\n.\nPre-built binaries marked as\nfree-threaded\ncan be installed as part of\nthe official\nWindows\nand\nmacOS\ninstallers,\nor CPython can be built from source with the\n--disable-gil\noption.\nFree-threaded execution allows for full utilization of the available\nprocessing power by running threads in parallel on available CPU cores.\nWhile not all software will benefit from this automatically, programs\ndesigned with threading in mind will run faster on multi-core hardware.\nThe free-threaded mode is experimental\nand work is ongoing to improve it:\nexpect some bugs and a substantial single-threaded performance hit.\nFree-threaded builds of CPython support optionally running with the GIL\nenabled at runtime using the environment variable\nPYTHON_GIL\nor\nthe command-line option\n-X\ngil=1\n.\nTo check if the current interpreter supports free-threading,\npython\n-VV\nand\nsys.version\ncontain “experimental free-threading build”.\nThe new\nsys._is_gil_enabled()\nfunction can be used to check whether\nthe GIL is actually disabled in the running process.\nC-API extension modules need to be built specifically for the free-threaded\nbuild. Extensions that support running with the\nGIL\ndisabled should\nuse the\nPy_mod_gil\nslot. Extensions using single-phase init should\nuse\nPyUnstable_Module_SetGIL()\nto indicate whether they support\nrunning with the GIL disabled. Importing C extensions that don’t use these\nmechanisms will cause the GIL to be enabled, unless the GIL was explicitly\ndisabled with the\nPYTHON_GIL\nenvironment variable or the\n-X\ngil=0\noption.\npip 24.1 or newer is required to install packages with C extensions in the\nfree-threaded build.\nThis work was made possible thanks to many individuals and\norganizations, including the large community of contributors to Python\nand third-party projects to test and enable free-threading support.\nNotable contributors include:\nSam Gross, Ken Jin, Donghee Na, Itamar Oren, Matt Page, Brett Simmers,\nDino Viehland, Carl Meyer, Nathan Goldbaum, Ralf Gommers,\nLysandros Nikolaou, and many others.\nMany of these contributors are employed by Meta, which has\nprovided significant engineering resources to support this project.\nSee also\nPEP 703\n“Making the Global Interpreter Lock Optional in CPython”\ncontains rationale and information surrounding this work.\nPorting Extension Modules to Support Free-Threading\n: A community-maintained\nporting guide for extension authors.\nAn experimental just-in-time (JIT) compiler\n¶\nWhen CPython is configured and built using\nthe\n--enable-experimental-jit\noption,\na just-in-time (JIT) compiler is added which may speed up some Python programs.\nOn Windows, use\nPCbuild/build.bat\n--experimental-jit\nto enable the JIT\nor\n--experimental-jit-interpreter\nto enable the Tier 2 interpreter.\nBuild requirements and further supporting information\nare contained at\nTools/jit/README.md\n.\nThe\n--enable-experimental-jit\noption takes these (optional) values,\ndefaulting to\nyes\nif\n--enable-experimental-jit\nis present\nwithout the optional value.\nno\n: Disable the entire Tier 2 and JIT pipeline.\nyes\n: Enable the JIT.\nTo disable the JIT at runtime, pass the environment variable\nPYTHON_JIT=0\n.\nyes-off\n: Build the JIT but disable it by default.\nTo enable the JIT at runtime, pass the environment variable\nPYTHON_JIT=1\n.\ninterpreter\n: Enable the Tier 2 interpreter but disable the JIT.\nThe interpreter can be disabled by running with\nPYTHON_JIT=0\n.\nThe internal architecture is roughly as follows:\nWe start with specialized\nTier 1 bytecode\n.\nSee\nWhat’s new in 3.11\nfor details.\nWhen the Tier 1 bytecode gets hot enough, it gets translated\nto a new purely internal intermediate representation (IR),\ncalled the\nTier 2 IR\n, and sometimes referred to as micro-ops (“uops”).\nThe Tier 2 IR uses the same stack-based virtual machine as Tier 1,\nbut the instruction format is better suited to translation to machine code.\nWe have several optimization passes for Tier 2 IR, which are applied\nbefore it is interpreted or translated to machine code.\nThere is a Tier 2 interpreter, but it is mostly intended for debugging\nthe earlier stages of the optimization pipeline.\nThe Tier 2 interpreter can be enabled by configuring Python\nwith\n--enable-experimental-jit=interpreter\n.\nWhen the JIT is enabled, the optimized\nTier 2 IR is translated to machine code, which is then executed.\nThe machine code translation process uses a technique called\ncopy-and-patch\n. It has no runtime dependencies, but there is a new\nbuild-time dependency on LLVM.\nSee also\nPEP 744\n(JIT by Brandt Bucher, inspired by a paper by Haoran Xu and Fredrik Kjolstad.\nTier 2 IR by Mark Shannon and Guido van Rossum.\nTier 2 optimizer by Ken Jin.)\nDefined mutation semantics for\nlocals()\n¶\nHistorically, the expected result of mutating the return value of\nlocals()\nhas been left to individual Python implementations to define.\nStarting from Python 3.13,\nPEP 667\nstandardises\nthe historical behavior of CPython for most code execution scopes,\nbut changes\noptimized scopes\n(functions, generators, coroutines, comprehensions, and generator expressions)\nto explicitly return independent snapshots of the currently assigned local\nvariables, including locally referenced nonlocal variables captured in closures.\nThis change to the semantics of\nlocals()\nin optimized scopes also\naffects the default behavior of code execution functions that implicitly\ntarget\nlocals()\nif no explicit namespace is provided\n(such as\nexec()\nand\neval()\n).\nIn previous versions, whether or not changes could be accessed by calling\nlocals()\nafter calling the code execution function was\nimplementation-dependent. In CPython specifically, such code would typically\nappear to work as desired, but could sometimes fail in optimized scopes based\non other code (including debuggers and code execution tracing tools)\npotentially resetting the shared snapshot in that scope.\nNow, the code will always run against an independent snapshot of\nthe local variables in optimized scopes, and hence the changes will never\nbe visible in subsequent calls to\nlocals()\n.\nTo access the changes made in these cases, an explicit namespace reference\nmust now be passed to the relevant function.\nAlternatively, it may make sense to update affected code to use a higher level\ncode execution API that returns the resulting code execution namespace\n(e.g.\nrunpy.run_path()\nwhen executing Python files from disk).\nTo ensure debuggers and similar tools can reliably update local variables in\nscopes affected by this change,\nFrameType.f_locals\nnow\nreturns a write-through proxy to the frame’s local and locally referenced\nnonlocal variables in these scopes, rather than returning an inconsistently\nupdated shared\ndict\ninstance with undefined runtime semantics.\nSee\nPEP 667\nfor more details, including related C API changes\nand deprecations. Porting notes are also provided below for the affected\nPython APIs\nand\nC APIs\n.\n(PEP and implementation contributed by Mark Shannon and Tian Gao in\ngh-74929\n. Documentation updates provided by Guido van Rossum and\nAlyssa Coghlan.)\nSupport for mobile platforms\n¶\nPEP 730\n: iOS is now a\nPEP 11\nsupported platform, with the\narm64-apple-ios\nand\narm64-apple-ios-simulator\ntargets at tier 3\n(iPhone and iPad devices released after 2013 and the Xcode iOS simulator\nrunning on Apple silicon hardware, respectively).\nx86_64-apple-ios-simulator\n(the Xcode iOS simulator running on older\nx86_64\nhardware)\nis not a tier 3 supported platform, but will have best-effort support.\n(PEP written and implementation contributed by Russell Keith-Magee in\ngh-114099\n.)\nPEP 738\n: Android is now a\nPEP 11\nsupported platform, with the\naarch64-linux-android\nand\nx86_64-linux-android\ntargets at tier 3.\nThe 32-bit targets\narm-linux-androideabi\nand\ni686-linux-android\nare not tier 3 supported platforms, but will have best-effort support.\n(PEP written and implementation contributed by Malcolm Smith in\ngh-116622\n.)\nSee also\nPEP 730\n,\nPEP 738\nOther Language Changes\n¶\nThe compiler now strips common leading whitespace\nfrom every line in a docstring.\nThis reduces the size of the\nbytecode cache\n(such as\n.pyc\nfiles), with reductions in file size of around 5%,\nfor example in\nsqlalchemy.orm.session\nfrom SQLAlchemy 2.0.\nThis change affects tools that use docstrings, such as\ndoctest\n.\n>>>\ndef\nspam\n():\n...\n\"\"\"\n...\nThis is a docstring with\n...\nleading whitespace.\n...\n...\nIt even has multiple paragraphs!\n...\n\"\"\"\n...\n>>>\nspam\n.\n__doc__\n'\\nThis is a docstring with\\n  leading whitespace.\\n\\nIt even has multiple paragraphs!\\n'\n(Contributed by Inada Naoki in\ngh-81283\n.)\nAnnotation scopes\nwithin class scopes\ncan now contain lambdas and comprehensions.\nComprehensions that are located within class scopes\nare not inlined into their parent scope.\nclass\nC\n[\nT\n]:\ntype\nAlias\n=\nlambda\n:\nT\n(Contributed by Jelle Zijlstra in\ngh-109118\nand\ngh-118160\n.)\nFuture statements\nare no longer triggered by\nrelative imports of the\n__future__\nmodule,\nmeaning that statements of the form\nfrom\n.__future__\nimport\n...\nare now simply standard relative imports, with no special features activated.\n(Contributed by Jeremiah Gabriel Pascual in\ngh-118216\n.)\nglobal\ndeclarations are now permitted in\nexcept\nblocks\nwhen that global is used in the\nelse\nblock.\nPreviously this raised an erroneous\nSyntaxError\n.\n(Contributed by Irit Katriel in\ngh-111123\n.)\nAdd\nPYTHON_FROZEN_MODULES\n, a new environment variable that\ndetermines whether frozen modules are ignored by the import machinery,\nequivalent to the\n-X\nfrozen_modules\ncommand-line option.\n(Contributed by Yilei Yang in\ngh-111374\n.)\nAdd\nsupport for the perf profiler\nworking\nwithout\nframe pointers\nthrough\nthe new environment variable\nPYTHON_PERF_JIT_SUPPORT\nand command-line option\n-X\nperf_jit\n.\n(Contributed by Pablo Galindo in\ngh-118518\n.)\nThe location of a\n.python_history\nfile can be changed via the\nnew\nPYTHON_HISTORY\nenvironment variable.\n(Contributed by Levi Sabah, Zackery Spytz and Hugo van Kemenade\nin\ngh-73965\n.)\nClasses have a new\n__static_attributes__\nattribute.\nThis is populated by the compiler with a tuple of the class’s attribute names\nwhich are assigned through\nself.<name>\nfrom any function in its body.\n(Contributed by Irit Katriel in\ngh-115775\n.)\nThe compiler now creates a\n__firstlineno__\nattribute on classes\nwith the line number of the first line of the class definition.\n(Contributed by Serhiy Storchaka in\ngh-118465\n.)\nThe\nexec()\nand\neval()\nbuiltins now accept\nthe\nglobals\nand\nlocals\narguments as keywords.\n(Contributed by Raphael Gaschignard in\ngh-105879\n)\nThe\ncompile()\nbuiltin now accepts a new flag,\nast.PyCF_OPTIMIZED_AST\n, which is similar to\nast.PyCF_ONLY_AST\nexcept that the returned AST is optimized according to\nthe value of the\noptimize\nargument.\n(Contributed by Irit Katriel in\ngh-108113\n).\nAdd a\n__name__\nattribute on\nproperty\nobjects.\n(Contributed by Eugene Toder in\ngh-101860\n.)\nAdd\nPythonFinalizationError\n, a new exception derived from\nRuntimeError\nand used to signal when operations are blocked\nduring\nfinalization\n.\nThe following callables now raise\nPythonFinalizationError\n,\ninstead of\nRuntimeError\n:\n_thread.start_new_thread()\nos.fork()\nos.forkpty()\nsubprocess.Popen\n(Contributed by Victor Stinner in\ngh-114570\n.)\nAllow the\ncount\nargument of\nstr.replace()\nto be a keyword.\n(Contributed by Hugo van Kemenade in\ngh-106487\n.)\nMany functions now emit a warning if a boolean value is passed as\na file descriptor argument.\nThis can help catch some errors earlier.\n(Contributed by Serhiy Storchaka in\ngh-82626\n.)\nAdded\nname\nand\nmode\nattributes\nfor compressed and archived file-like objects in\nthe\nbz2\n,\nlzma\n,\ntarfile\n, and\nzipfile\nmodules.\n(Contributed by Serhiy Storchaka in\ngh-115961\n.)\nNew Modules\n¶\ndbm.sqlite3\n: An SQLite backend for\ndbm\n.\n(Contributed by Raymond Hettinger and Erlend E. Aasland in\ngh-100414\n.)\nImproved Modules\n¶\nargparse\n¶\nAdd the\ndeprecated\nparameter to the\nadd_argument()\nand\nadd_parser()\nmethods, to enable deprecating\ncommand-line options, positional arguments, and subcommands.\n(Contributed by Serhiy Storchaka in\ngh-83648\n.)\narray\n¶\nAdd the\n'w'\ntype code (\nPy_UCS4\n) for Unicode characters.\nIt should be used instead of the deprecated\n'u'\ntype code.\n(Contributed by Inada Naoki in\ngh-80480\n.)\nRegister\narray.array\nas a\nMutableSequence\nby implementing the\nclear()\nmethod.\n(Contributed by Mike Zimin in\ngh-114894\n.)\nast\n¶\nThe constructors of node types in the\nast\nmodule are now\nstricter in the arguments they accept,\nwith more intuitive behavior when arguments are omitted.\nIf an optional field on an AST node is not included as an argument when\nconstructing an instance, the field will now be set to\nNone\n. Similarly,\nif a list field is omitted, that field will now be set to an empty list,\nand if an\nexpr_context\nfield is omitted, it defaults to\nLoad()\n.\n(Previously, in all cases, the attribute would be missing on the newly\nconstructed AST node instance.)\nIn all other cases, where a required argument is omitted,\nthe node constructor will emit a\nDeprecationWarning\n.\nThis will raise an exception in Python 3.15.\nSimilarly, passing a keyword argument to the constructor\nthat does not map to a field on the AST node is now deprecated,\nand will raise an exception in Python 3.15.\nThese changes do not apply to user-defined subclasses of\nast.AST\nunless the class opts in to the new behavior\nby defining the\nAST._field_types\nmapping.\n(Contributed by Jelle Zijlstra in\ngh-105858\n,\ngh-117486\n, and\ngh-118851\n.)\nast.parse()\nnow accepts an optional argument\noptimize\nwhich is passed on to\ncompile()\n.\nThis makes it possible to obtain an optimized AST.\n(Contributed by Irit Katriel in\ngh-108113\n.)\nasyncio\n¶\nasyncio.as_completed()\nnow returns an object that is both an\nasynchronous iterator\nand a plain\niterator\nof\nawaitables\n.\nThe awaitables yielded by asynchronous iteration include original task\nor future objects that were passed in,\nmaking it easier to associate results with the tasks being completed.\n(Contributed by Justin Arthur in\ngh-77714\n.)\nasyncio.loop.create_unix_server()\nwill now automatically remove\nthe Unix socket when the server is closed.\n(Contributed by Pierre Ossman in\ngh-111246\n.)\nDatagramTransport.sendto()\nwill now send zero-length\ndatagrams if called with an empty bytes object.\nThe transport flow control also now accounts for the datagram header\nwhen calculating the buffer size.\n(Contributed by Jamie Phan in\ngh-115199\n.)\nAdd\nQueue.shutdown\nand\nQueueShutDown\nto manage queue termination.\n(Contributed by Laurie Opperman and Yves Duprat in\ngh-104228\n.)\nAdd the\nServer.close_clients()\nand\nServer.abort_clients()\nmethods, which more forcefully close an asyncio server.\n(Contributed by Pierre Ossman in\ngh-113538\n.)\nAccept a tuple of separators in\nStreamReader.readuntil()\n,\nstopping when any one of them is encountered.\n(Contributed by Bruce Merry in\ngh-81322\n.)\nImprove the behavior of\nTaskGroup\nwhen\nan external cancellation collides with an internal cancellation.\nFor example, when two task groups are nested\nand both experience an exception in a child task simultaneously,\nit was possible that the outer task group would hang,\nbecause its internal cancellation was swallowed by the inner task group.\nIn the case where a task group is cancelled externally\nand also must raise an\nExceptionGroup\n,\nit will now call the parent task’s\ncancel()\nmethod.\nThis ensures that a\nCancelledError\nwill be raised\nat the next\nawait\n, so the cancellation is not lost.\nAn added benefit of these changes is that task groups now preserve\nthe cancellation count (\ncancelling()\n).\nIn order to handle some corner cases,\nuncancel()\nmay now\nreset the undocumented\n_must_cancel\nflag\nwhen the cancellation count reaches zero.\n(Inspired by an issue reported by Arthur Tacca in\ngh-116720\n.)\nWhen\nTaskGroup.create_task()\nis called on an inactive\nTaskGroup\n, the given coroutine will be closed (which\nprevents a\nRuntimeWarning\nabout the given coroutine being\nnever awaited).\n(Contributed by Arthur Tacca and Jason Zhang in\ngh-115957\n.)\nThe function and methods named\ncreate_task\nhave received a new\n**kwargs\nargument that is passed through to the task constructor.\nThis change was accidentally added in 3.13.3,\nand broke the API contract for custom task factories.\nSeveral third-party task factories implemented workarounds for this.\nIn 3.13.4 and later releases the old factory contract is honored\nonce again (until 3.14).\nTo keep the workarounds working, the extra\n**kwargs\nargument still\nallows passing additional keyword arguments to\nTask\nand to custom task factories.\nThis affects the following function and methods:\nasyncio.create_task()\n,\nasyncio.loop.create_task()\n,\nasyncio.TaskGroup.create_task()\n.\n(Contributed by Thomas Grainger in\ngh-128307\n.)\nbase64\n¶\nAdd\nz85encode()\nand\nz85decode()\nfunctions\nfor encoding\nbytes\nas\nZ85 data\nand decoding Z85-encoded data to\nbytes\n.\n(Contributed by Matan Perelman in\ngh-75299\n.)\ncompileall\n¶\nThe default number of worker threads and processes is now selected using\nos.process_cpu_count()\ninstead of\nos.cpu_count()\n.\n(Contributed by Victor Stinner in\ngh-109649\n.)\nconcurrent.futures\n¶\nThe default number of worker threads and processes is now selected using\nos.process_cpu_count()\ninstead of\nos.cpu_count()\n.\n(Contributed by Victor Stinner in\ngh-109649\n.)\nconfigparser\n¶\nConfigParser\nnow has support for unnamed sections,\nwhich allows for top-level key-value pairs.\nThis can be enabled with the new\nallow_unnamed_section\nparameter.\n(Contributed by Pedro Sousa Lacerda in\ngh-66449\n.)\ncopy\n¶\nThe new\nreplace()\nfunction and the\nreplace\nprotocol\nmake creating modified copies of objects much simpler.\nThis is especially useful when working with immutable objects.\nThe following types support the\nreplace()\nfunction\nand implement the replace protocol:\ncollections.namedtuple()\ndataclasses.dataclass\ndatetime.datetime\n,\ndatetime.date\n,\ndatetime.time\ninspect.Signature\n,\ninspect.Parameter\ntypes.SimpleNamespace\ncode objects\nAny user-defined class can also support\ncopy.replace()\nby defining\nthe\n__replace__()\nmethod.\n(Contributed by Serhiy Storchaka in\ngh-108751\n.)\nctypes\n¶\nAs a consequence of necessary internal refactoring, initialization of\ninternal metaclasses now happens in\n__init__\nrather\nthan in\n__new__\n. This affects projects that subclass these internal\nmetaclasses to provide custom initialization.\nGenerally:\nCustom logic that was done in\n__new__\nafter calling\nsuper().__new__\nshould be moved to\n__init__\n.\nTo create a class, call the metaclass, not only the metaclass’s\n__new__\nmethod.\nSee\ngh-124520\nfor discussion and links to changes in some affected\nprojects.\nctypes.Structure\nobjects have a new\n_align_\nattribute which allows the alignment of the structure being packed to/from\nmemory to be specified explicitly.\n(Contributed by Matt Sanderson in\ngh-112433\n)\ndbm\n¶\nAdd\ndbm.sqlite3\n, a new module which implements an SQLite backend,\nand make it the default\ndbm\nbackend.\n(Contributed by Raymond Hettinger and Erlend E. Aasland in\ngh-100414\n.)\nAllow removing all items from the database through\nthe new\ngdbm.clear()\nand\nndbm.clear()\nmethods.\n(Contributed by Donghee Na in\ngh-107122\n.)\ndis\n¶\nChange the output of\ndis\nmodule functions to show logical\nlabels for jump targets and exception handlers, rather than offsets.\nThe offsets can be added with the new\n-O\ncommand-line option\nor the\nshow_offsets\nargument.\n(Contributed by Irit Katriel in\ngh-112137\n.)\nget_instructions()\nno longer represents cache entries\nas separate instructions.\nInstead, it returns them as part of the\nInstruction\n,\nin the new\ncache_info\nfield.\nThe\nshow_caches\nargument to\nget_instructions()\nis deprecated\nand no longer has any effect.\n(Contributed by Irit Katriel in\ngh-112962\n.)\ndoctest\n¶\ndoctest\noutput is now colored by default.\nThis can be controlled via the new\nPYTHON_COLORS\nenvironment\nvariable as well as the canonical\nNO_COLOR\nand\nFORCE_COLOR\nenvironment variables.\nSee also\nControlling color\n.\n(Contributed by Hugo van Kemenade in\ngh-117225\n.)\nThe\nDocTestRunner.run()\nmethod now counts the number of skipped tests.\nAdd the\nDocTestRunner.skips\nand\nTestResults.skipped\nattributes.\n(Contributed by Victor Stinner in\ngh-108794\n.)\nemail\n¶\nHeaders with embedded newlines are now quoted on output.\nThe\ngenerator\nwill now refuse to serialize (write) headers\nthat are improperly folded or delimited, such that they would be parsed as\nmultiple headers or joined with adjacent data.\nIf you need to turn this safety feature off,\nset\nverify_generated_headers\n.\n(Contributed by Bas Bloemsaat and Petr Viktorin in\ngh-121650\n.)\ngetaddresses()\nand\nparseaddr()\nnow\nreturn\n('',\n'')\npairs in more situations where invalid email addresses\nare encountered instead of potentially inaccurate values.\nThe two functions have a new optional\nstrict\nparameter (default\nTrue\n).\nTo get the old behavior (accepting malformed input), use\nstrict=False\n.\ngetattr(email.utils,\n'supports_strict_parsing',\nFalse)\ncan be used\nto check if the\nstrict\nparameter is available.\n(Contributed by Thomas Dwyer and Victor Stinner for\ngh-102988\nto improve\nthe\nCVE 2023-27043\nfix.)\nenum\n¶\nEnumDict\nhas been made public to better support subclassing\nEnumType\n.\nfractions\n¶\nFraction\nobjects now support the standard\nformat specification mini-language\nrules\nfor fill, alignment, sign handling, minimum width, and grouping.\n(Contributed by Mark Dickinson in\ngh-111320\n.)\nglob\n¶\nAdd\ntranslate()\n, a function to convert a path specification\nwith shell-style wildcards to a regular expression.\n(Contributed by Barney Gale in\ngh-72904\n.)\nimportlib\n¶\nThe following functions in\nimportlib.resources\nnow allow accessing\na directory (or tree) of resources, using multiple positional arguments\n(the\nencoding\nand\nerrors\narguments in the text-reading functions\nare now keyword-only):\nis_resource()\nopen_binary()\nopen_text()\npath()\nread_binary()\nread_text()\nThese functions are no longer deprecated and are not scheduled for removal.\n(Contributed by Petr Viktorin in\ngh-116608\n.)\ncontents()\nremains deprecated in favor of\nthe fully-featured\nTraversable\nAPI.\nHowever, there is now no plan to remove it.\n(Contributed by Petr Viktorin in\ngh-116608\n.)\nio\n¶\nThe\nIOBase\nfinalizer now logs any errors raised by\nthe\nclose()\nmethod with\nsys.unraisablehook\n.\nPreviously, errors were ignored silently by default,\nand only logged in\nPython Development Mode\nor when using a\nPython debug build\n.\n(Contributed by Victor Stinner in\ngh-62948\n.)\nipaddress\n¶\nAdd the\nIPv4Address.ipv6_mapped\nproperty,\nwhich returns the IPv4-mapped IPv6 address.\n(Contributed by Charles Machalow in\ngh-109466\n.)\nFix\nis_global\nand\nis_private\nbehavior in\nIPv4Address\n,\nIPv6Address\n,\nIPv4Network\n, and\nIPv6Network\n.\n(Contributed by Jakub Stasiak in\ngh-113171\n.)\nitertools\n¶\nbatched()\nhas a new\nstrict\nparameter,\nwhich raises a\nValueError\nif the final batch is shorter\nthan the specified batch size.\n(Contributed by Raymond Hettinger in\ngh-113202\n.)\nmarshal\n¶\nAdd the\nallow_code\nparameter in module functions.\nPassing\nallow_code=False\nprevents serialization and de-serialization\nof code objects which are incompatible between Python versions.\n(Contributed by Serhiy Storchaka in\ngh-113626\n.)\nmath\n¶\nThe new function\nfma()\nperforms fused multiply-add operations.\nThis computes\nx\n*\ny\n+\nz\nwith only a single round,\nand so avoids any intermediate loss of precision.\nIt wraps the\nfma()\nfunction provided by C99,\nand follows the specification of the IEEE 754 “fusedMultiplyAdd” operation\nfor special cases.\n(Contributed by Mark Dickinson and Victor Stinner in\ngh-73468\n.)\nmimetypes\n¶\nAdd the\nguess_file_type()\nfunction to guess a MIME type\nfrom a filesystem path.\nUsing paths with\nguess_type()\nis now\nsoft deprecated\n.\n(Contributed by Serhiy Storchaka in\ngh-66543\n.)\nmmap\n¶\nmmap\nis now protected from crashing on Windows when the\nmapped memory is inaccessible due to file system errors or access violations.\n(Contributed by Jannis Weigend in\ngh-118209\n.)\nmmap\nhas a new\nseekable()\nmethod\nthat can be used when a seekable file-like object is required.\nThe\nseek()\nmethod now returns the new absolute position.\n(Contributed by Donghee Na and Sylvie Liberman in\ngh-111835\n.)\nThe new UNIX-only\ntrackfd\nparameter for\nmmap\ncontrols\nfile descriptor duplication;\nif false, the file descriptor specified by\nfileno\nwill not be duplicated.\n(Contributed by Zackery Spytz and Petr Viktorin in\ngh-78502\n.)\nmultiprocessing\n¶\nThe default number of worker threads and processes is now selected using\nos.process_cpu_count()\ninstead of\nos.cpu_count()\n.\n(Contributed by Victor Stinner in\ngh-109649\n.)\nos\n¶\nAdd\nprocess_cpu_count()\nfunction to get the number\nof logical CPU cores usable by the calling thread of the current process.\n(Contributed by Victor Stinner in\ngh-109649\n.)\ncpu_count()\nand\nprocess_cpu_count()\ncan be overridden\nthrough the new environment variable\nPYTHON_CPU_COUNT\nor the new command-line option\n-X\ncpu_count\n.\nThis option is useful for users who need to limit CPU resources\nof a container system without having to modify application code\nor the container itself.\n(Contributed by Donghee Na in\ngh-109595\n.)\nAdd a\nlow level interface\nto Linux’s\ntimer file descriptors\nvia\ntimerfd_create()\n,\ntimerfd_settime()\n,\ntimerfd_settime_ns()\n,\ntimerfd_gettime()\n,\ntimerfd_gettime_ns()\n,\nTFD_NONBLOCK\n,\nTFD_CLOEXEC\n,\nTFD_TIMER_ABSTIME\n, and\nTFD_TIMER_CANCEL_ON_SET\n(Contributed by Masaru Tsuchiyama in\ngh-108277\n.)\nlchmod()\nand the\nfollow_symlinks\nargument of\nchmod()\nare both now available on Windows.\nNote that the default value of\nfollow_symlinks\nin\nlchmod()\nis\nFalse\non Windows.\n(Contributed by Serhiy Storchaka in\ngh-59616\n.)\nfchmod()\nand support for file descriptors in\nchmod()\nare both now available on Windows.\n(Contributed by Serhiy Storchaka in\ngh-113191\n.)\nOn Windows,\nmkdir()\nand\nmakedirs()\nnow support passing\na\nmode\nvalue of\n0o700\nto apply access control to the new directory.\nThis implicitly affects\ntempfile.mkdtemp()\nand is a mitigation for\nCVE 2024-4030\n.\nOther values for\nmode\ncontinue to be ignored.\n(Contributed by Steve Dower in\ngh-118486\n.)\nposix_spawn()\nnow accepts\nNone\nfor the\nenv\nargument,\nwhich makes the newly spawned process use the current process environment.\n(Contributed by Jakub Kulik in\ngh-113119\n.)\nposix_spawn()\ncan now use the\nPOSIX_SPAWN_CLOSEFROM\nattribute in the\nfile_actions\nparameter on platforms that support\nposix_spawn_file_actions_addclosefrom_np()\n.\n(Contributed by Jakub Kulik in\ngh-113117\n.)\nos.path\n¶\nAdd\nisreserved()\nto check if a path is reserved\non the current system.\nThis function is only available on Windows.\n(Contributed by Barney Gale in\ngh-88569\n.)\nOn Windows,\nisabs()\nno longer considers paths\nstarting with exactly one slash (\n\\\nor\n/\n) to be absolute.\n(Contributed by Barney Gale and Jon Foster in\ngh-44626\n.)\nrealpath()\nnow resolves MS-DOS style file names\neven if the file is not accessible.\n(Contributed by Moonsik Park in\ngh-82367\n.)\npathlib\n¶\nAdd\nUnsupportedOperation\n, which is raised instead of\nNotImplementedError\nwhen a path operation isn’t supported.\n(Contributed by Barney Gale in\ngh-89812\n.)\nAdd a new constructor for creating\nPath\nobjects\nfrom ‘file’ URIs (\nfile:///\n),\nPath.from_uri()\n.\n(Contributed by Barney Gale in\ngh-107465\n.)\nAdd\nPurePath.full_match()\nfor matching paths with\nshell-style wildcards, including the recursive wildcard “\n**\n”.\n(Contributed by Barney Gale in\ngh-73435\n.)\nAdd the\nPurePath.parser\nclass attribute to store the\nimplementation of\nos.path\nused\nfor low-level path parsing and joining.\nThis will be either\nposixpath\nor\nntpath\n.\nAdd\nrecurse_symlinks\nkeyword-only argument to\nPath.glob()\nand\nrglob()\n.\n(Contributed by Barney Gale in\ngh-77609\n.)\nPath.glob()\nand\nrglob()\nnow return files and directories when given a pattern that ends with “\n**\n”.\nPreviously, only directories were returned.\n(Contributed by Barney Gale in\ngh-70303\n.)\nAdd the\nfollow_symlinks\nkeyword-only argument to\nPath.is_file\n,\nPath.is_dir\n,\nPath.owner()\n, and\nPath.group()\n.\n(Contributed by Barney Gale in\ngh-105793\nand Kamil Turek in\ngh-107962\n.)\npdb\n¶\nbreakpoint()\nand\nset_trace()\nnow enter the debugger immediately\nrather than on the next line of code to be executed. This change prevents the\ndebugger from breaking outside of the context when\nbreakpoint()\nis positioned\nat the end of the context.\n(Contributed by Tian Gao in\ngh-118579\n.)\nsys.path[0]\nis no longer replaced by the directory of the script\nbeing debugged when\nsys.flags.safe_path\nis set.\n(Contributed by Tian Gao and Christian Walther in\ngh-111762\n.)\nzipapp\nis now supported as a debugging target.\n(Contributed by Tian Gao in\ngh-118501\n.)\nAdd ability to move between chained exceptions during\npost-mortem debugging in\npm()\nusing\nthe new\nexceptions\n[exc_number]\ncommand for Pdb.\n(Contributed by Matthias Bussonnier in\ngh-106676\n.)\nExpressions and statements whose prefix is a pdb command are now correctly\nidentified and executed.\n(Contributed by Tian Gao in\ngh-108464\n.)\nqueue\n¶\nAdd\nQueue.shutdown\nand\nShutDown\nto manage queue termination.\n(Contributed by Laurie Opperman and Yves Duprat in\ngh-104750\n.)\nrandom\n¶\nAdd a\ncommand-line interface\n.\n(Contributed by Hugo van Kemenade in\ngh-118131\n.)\nre\n¶\nRename\nre.error\nto\nPatternError\nfor improved clarity.\nre.error\nis kept for backward compatibility.\nshutil\n¶\nSupport the\ndir_fd\nand\nfollow_symlinks\nkeyword arguments\nin\nchown()\n.\n(Contributed by Berker Peksag and Tahia K in\ngh-62308\n)\nsite\n¶\n.pth\nfiles are now decoded using UTF-8 first,\nand then with the\nlocale encoding\nif UTF-8 decoding fails.\n(Contributed by Inada Naoki in\ngh-117802\n.)\nsqlite3\n¶\nA\nResourceWarning\nis now emitted if a\nConnection\nobject is not\nclosed\nexplicitly.\n(Contributed by Erlend E. Aasland in\ngh-105539\n.)\nAdd the\nfilter\nkeyword-only parameter to\nConnection.iterdump()\nfor filtering database objects to dump.\n(Contributed by Mariusz Felisiak in\ngh-91602\n.)\nssl\n¶\nThe\ncreate_default_context()\nAPI now includes\nVERIFY_X509_PARTIAL_CHAIN\nand\nVERIFY_X509_STRICT\nin its default flags.\nNote\nVERIFY_X509_STRICT\nmay reject pre-\nRFC 5280\nor malformed certificates that the underlying OpenSSL implementation\nmight otherwise accept.\nWhilst disabling this is not recommended, you can do so using:\nimport\nssl\nctx\n=\nssl\n.\ncreate_default_context\n()\nctx\n.\nverify_flags\n&=\n~\nssl\n.\nVERIFY_X509_STRICT\n(Contributed by William Woodruff in\ngh-112389\n.)\nstatistics\n¶\nAdd\nkde()\nfor kernel density estimation.\nThis makes it possible to estimate a continuous probability density function\nfrom a fixed number of discrete samples.\n(Contributed by Raymond Hettinger in\ngh-115863\n.)\nAdd\nkde_random()\nfor sampling from an\nestimated probability density function created by\nkde()\n.\n(Contributed by Raymond Hettinger in\ngh-115863\n.)\nsubprocess\n¶\nThe\nsubprocess\nmodule now uses the\nposix_spawn()\nfunction in\nmore situations.\nNotably, when\nclose_fds\nis\nTrue\n(the default),\nposix_spawn()\nwill be used when the C library provides\nposix_spawn_file_actions_addclosefrom_np()\n,\nwhich includes recent versions of Linux, FreeBSD, and Solaris.\nOn Linux, this should perform similarly to the existing\nLinux\nvfork()\nbased code.\nA private control knob\nsubprocess._USE_POSIX_SPAWN\ncan\nbe set to\nFalse\nif you need to force\nsubprocess\nto never use\nposix_spawn()\n.\nPlease report your reason and platform details in\nthe\nissue tracker\nif you set this\nso that we can improve our API selection logic for everyone.\n(Contributed by Jakub Kulik in\ngh-113117\n.)\nsys\n¶\nAdd the\n_is_interned()\nfunction to test if a string was interned.\nThis function is not guaranteed to exist in all implementations of Python.\n(Contributed by Serhiy Storchaka in\ngh-78573\n.)\ntempfile\n¶\nOn Windows, the default mode\n0o700\nused by\ntempfile.mkdtemp()\nnow\nlimits access to the new directory due to changes to\nos.mkdir()\n.\nThis is a mitigation for\nCVE 2024-4030\n.\n(Contributed by Steve Dower in\ngh-118486\n.)\ntime\n¶\nOn Windows,\nmonotonic()\nnow uses the\nQueryPerformanceCounter()\nclock for a resolution of 1 microsecond,\ninstead of the\nGetTickCount64()\nclock which has\na resolution of 15.6 milliseconds.\n(Contributed by Victor Stinner in\ngh-88494\n.)\nOn Windows,\ntime()\nnow uses the\nGetSystemTimePreciseAsFileTime()\nclock for a resolution of 1 microsecond,\ninstead of the\nGetSystemTimeAsFileTime()\nclock which has\na resolution of 15.6 milliseconds.\n(Contributed by Victor Stinner in\ngh-63207\n.)\ntkinter\n¶\nAdd\ntkinter\nwidget methods:\ntk_busy_hold()\n,\ntk_busy_configure()\n,\ntk_busy_cget()\n,\ntk_busy_forget()\n,\ntk_busy_current()\n, and\ntk_busy_status()\n.\n(Contributed by Miguel, klappnase and Serhiy Storchaka in\ngh-72684\n.)\nThe\ntkinter\nwidget method\nwm_attributes()\nnow accepts\nthe attribute name without the minus prefix to get window attributes,\nfor example\nw.wm_attributes('alpha')\nand allows specifying attributes and values to set as keyword arguments,\nfor example\nw.wm_attributes(alpha=0.5)\n.\n(Contributed by Serhiy Storchaka in\ngh-43457\n.)\nwm_attributes()\ncan now return attributes as a\ndict\n,\nby using the new optional keyword-only parameter\nreturn_python_dict\n.\n(Contributed by Serhiy Storchaka in\ngh-43457\n.)\nText.count()\ncan now return a simple\nint\nwhen the new optional keyword-only parameter\nreturn_ints\nis used.\nOtherwise, the single count is returned as a 1-tuple or\nNone\n.\n(Contributed by Serhiy Storchaka in\ngh-97928\n.)\nSupport the “vsapi” element type in\nthe\nelement_create()\nmethod of\ntkinter.ttk.Style\n.\n(Contributed by Serhiy Storchaka in\ngh-68166\n.)\nAdd the\nafter_info()\nmethod for Tkinter widgets.\n(Contributed by Cheryl Sabella in\ngh-77020\n.)\nAdd a new\ncopy_replace()\nmethod to\nPhotoImage\nto copy a region from one image to another,\npossibly with pixel zooming, subsampling, or both.\n(Contributed by Serhiy Storchaka in\ngh-118225\n.)\nAdd\nfrom_coords\nparameter to the\nPhotoImage\nmethods\ncopy()\n,\nzoom()\nand\nsubsample()\n.\nAdd\nzoom\nand\nsubsample\nparameters to the\nPhotoImage\nmethod\ncopy()\n.\n(Contributed by Serhiy Storchaka in\ngh-118225\n.)\nAdd the\nPhotoImage\nmethods\nread()\nto read an image from a file\nand\ndata()\nto get the image data.\nAdd\nbackground\nand\ngrayscale\nparameters to the\nwrite()\nmethod.\n(Contributed by Serhiy Storchaka in\ngh-118271\n.)\ntraceback\n¶\nAdd the\nexc_type_str\nattribute\nto\nTracebackException\n,\nwhich holds a string display of the\nexc_type\n.\nDeprecate the\nexc_type\nattribute,\nwhich holds the type object itself.\nAdd parameter\nsave_exc_type\n(default\nTrue\n)\nto indicate whether\nexc_type\nshould be saved.\n(Contributed by Irit Katriel in\ngh-112332\n.)\nAdd a new\nshow_group\nkeyword-only parameter to\nTracebackException.format_exception_only()\nto (recursively) format\nthe nested exceptions of a\nBaseExceptionGroup\ninstance.\n(Contributed by Irit Katriel in\ngh-105292\n.)\ntypes\n¶\nSimpleNamespace\ncan now take a single positional argument\nto initialise the namespace’s arguments.\nThis argument must either be a mapping or an iterable of key-value pairs.\n(Contributed by Serhiy Storchaka in\ngh-108191\n.)\ntyping\n¶\nPEP 705\n: Add\nReadOnly\n, a special typing construct\nto mark a\nTypedDict\nitem as read-only for type checkers.\nPEP 742\n: Add\nTypeIs\n, a typing construct\nthat can be used to instruct a type checker how to narrow a type.\nAdd\nNoDefault\n, a sentinel object used to represent\nthe defaults of some parameters in the\ntyping\nmodule.\n(Contributed by Jelle Zijlstra in\ngh-116126\n.)\nAdd\nget_protocol_members()\nto return the set of members\ndefining a\ntyping.Protocol\n.\n(Contributed by Jelle Zijlstra in\ngh-104873\n.)\nAdd\nis_protocol()\nto check whether a class\nis a\nProtocol\n.\n(Contributed by Jelle Zijlstra in\ngh-104873\n.)\nClassVar\ncan now be nested in\nFinal\n,\nand vice versa.\n(Contributed by Mehdi Drissi in\ngh-89547\n.)\nunicodedata\n¶\nUpdate the Unicode database to\nversion 15.1.0\n.\n(Contributed by James Gerity in\ngh-109559\n.)\nvenv\n¶\nAdd support for creating source control management (SCM) ignore files\nin a virtual environment’s directory.\nBy default, Git is supported.\nThis is implemented as opt-in via the API,\nwhich can be extended to support other SCMs\n(\nEnvBuilder\nand\ncreate()\n),\nand opt-out via the CLI, using\n--without-scm-ignore-files\n.\n(Contributed by Brett Cannon in\ngh-108125\n.)\nwarnings\n¶\nPEP 702\n: The new\nwarnings.deprecated()\ndecorator provides a way to\ncommunicate deprecations to a\nstatic type checker\nand to warn on usage of deprecated classes and functions.\nA\nDeprecationWarning\nmay also be emitted when\na decorated function or class is used at runtime.\n(Contributed by Jelle Zijlstra in\ngh-104003\n.)\nxml\n¶\nAllow controlling Expat >=2.6.0 reparse deferral (\nCVE 2023-52425\n)\nby adding five new methods:\nxml.etree.ElementTree.XMLParser.flush()\nxml.etree.ElementTree.XMLPullParser.flush()\nxml.parsers.expat.xmlparser.GetReparseDeferralEnabled()\nxml.parsers.expat.xmlparser.SetReparseDeferralEnabled()\nxml.sax.expatreader.ExpatParser.flush()\n(Contributed by Sebastian Pipping in\ngh-115623\n.)\nAdd the\nclose()\nmethod for the iterator returned by\niterparse()\nfor explicit cleanup.\n(Contributed by Serhiy Storchaka in\ngh-69893\n.)\nzipimport\n¶\nAdd support for\nZIP64\nformat files.\nEverybody loves huge data, right?\n(Contributed by Tim Hatch in\ngh-94146\n.)\nOptimizations\n¶\nSeveral standard library modules have had\ntheir import times significantly improved.\nFor example, the import time of the\ntyping\nmodule\nhas been reduced by around a third by removing dependencies\non\nre\nand\ncontextlib\n.\nOther modules to enjoy import-time speedups include\nemail.utils\n,\nenum\n,\nfunctools\n,\nimportlib.metadata\n, and\nthreading\n.\n(Contributed by Alex Waygood, Shantanu Jain, Adam Turner, Daniel Hollas,\nand others in\ngh-109653\n.)\ntextwrap.indent()\nis now around 30% faster than before for large input.\n(Contributed by Inada Naoki in\ngh-107369\n.)\nThe\nsubprocess\nmodule now uses the\nposix_spawn()\nfunction in\nmore situations, including when\nclose_fds\nis\nTrue\n(the default)\non many modern platforms.\nThis should provide a notable performance increase\nwhen launching processes on FreeBSD and Solaris.\nSee the\nsubprocess\nsection above for details.\n(Contributed by Jakub Kulik in\ngh-113117\n.)\nRemoved Modules And APIs\n¶\nPEP 594: Remove “dead batteries” from the standard library\n¶\nPEP 594\nproposed removing 19 modules from the standard library,\ncolloquially referred to as ‘dead batteries’ due to their\nhistoric, obsolete, or insecure status.\nAll of the following modules were deprecated in Python 3.11,\nand are now removed:\naifc\nstandard-aifc\n:\nUse the redistribution of\naifc\nlibrary from PyPI.\naudioop\naudioop-lts\n:\nUse\naudioop-lts\nlibrary from PyPI.\nchunk\nstandard-chunk\n:\nUse the redistribution of\nchunk\nlibrary from PyPI.\ncgi\nand\ncgitb\ncgi.FieldStorage\ncan typically be replaced with\nurllib.parse.parse_qsl()\nfor\nGET\nand\nHEAD\nrequests,\nand the\nemail.message\nmodule or the\nmultipart\nlibrary\nfor\nPOST\nand\nPUT\nrequests.\ncgi.parse()\ncan be replaced by calling\nurllib.parse.parse_qs()\ndirectly on the desired query string,\nunless the input is\nmultipart/form-data\n,\nwhich should be replaced as described below for\ncgi.parse_multipart()\n.\ncgi.parse_header()\ncan be replaced with the functionality\nin the\nemail\npackage, which implements the same MIME RFCs.\nFor example, with\nemail.message.EmailMessage\n:\nfrom\nemail.message\nimport\nEmailMessage\nmsg\n=\nEmailMessage\n()\nmsg\n[\n'content-type'\n]\n=\n'application/json; charset=\"utf8\"'\nmain\n,\nparams\n=\nmsg\n.\nget_content_type\n(),\nmsg\n[\n'content-type'\n]\n.\nparams\ncgi.parse_multipart()\ncan be replaced with the functionality\nin the\nemail\npackage, which implements the same MIME RFCs,\nor with the\nmultipart\nlibrary.\nFor example, the\nemail.message.EmailMessage\nand\nemail.message.Message\nclasses.\nstandard-cgi\n: and\nstandard-cgitb\n:\nUse the redistribution of\ncgi\nand\ncgitb\nlibrary from PyPI.\ncrypt\nand the private\n_crypt\nextension.\nThe\nhashlib\nmodule may be an appropriate replacement\nwhen simply hashing a value is required.\nOtherwise, various third-party libraries on PyPI are available:\nbcrypt\n:\nModern password hashing for your software and your servers.\npasslib\n:\nComprehensive password hashing framework supporting over 30 schemes.\nargon2-cffi\n:\nThe secure Argon2 password hashing algorithm.\nlegacycrypt\n:\nctypes\nwrapper to the POSIX crypt library call\nand associated functionality.\ncrypt_r\n:\nFork of the\ncrypt\nmodule,\nwrapper to the\ncrypt_r(3)\nlibrary call\nand associated functionality.\nstandard-crypt\nand\ndeprecated-crypt-alternative\n:\nUse the redistribution of\ncrypt\nand reimplementation of\n_crypt\nlibraries from PyPI.\nimghdr\n:\nThe\nfiletype\n,\npuremagic\n, or\npython-magic\nlibraries\nshould be used as replacements.\nFor example, the\npuremagic.what()\nfunction can be used\nto replace the\nimghdr.what()\nfunction for all file formats\nthat were supported by\nimghdr\n.\nstandard-imghdr\n:\nUse the redistribution of\nimghdr\nlibrary from PyPI.\nmailcap\n:\nUse the\nmimetypes\nmodule instead.\nstandard-mailcap\n:\nUse the redistribution of\nmailcap\nlibrary from PyPI.\nmsilib\nnis\nnntplib\n:\nUse the\npynntp\nlibrary from PyPI instead.\nstandard-nntplib\n:\nUse the redistribution of\nnntplib\nlibrary from PyPI.\nossaudiodev\n:\nFor audio playback, use the\npygame\nlibrary from PyPI instead.\npipes\n:\nUse the\nsubprocess\nmodule instead.\nUse\nshlex.quote()\nto replace the undocumented\npipes.quote\nfunction.\nstandard-pipes\n:\nUse the redistribution of\npipes\nlibrary from PyPI.\nsndhdr\n:\nThe\nfiletype\n,\npuremagic\n, or\npython-magic\nlibraries\nshould be used as replacements.\nstandard-sndhdr\n:\nUse the redistribution of\nsndhdr\nlibrary from PyPI.\nspwd\n:\nUse the\npython-pam\nlibrary from PyPI instead.\nsunau\nstandard-sunau\n:\nUse the redistribution of\nsunau\nlibrary from PyPI.\ntelnetlib\n,\nUse the\ntelnetlib3\nor\nExscript\nlibraries from PyPI instead.\nstandard-telnetlib\n:\nUse the redistribution of\ntelnetlib\nlibrary from PyPI.\nuu\n:\nUse the\nbase64\nmodule instead, as a modern alternative.\nstandard-uu\n:\nUse the redistribution of\nuu\nlibrary from PyPI.\nxdrlib\nstandard-xdrlib\n:\nUse the redistribution of\nxdrlib\nlibrary from PyPI.\n(Contributed by Victor Stinner and Zachary Ware in\ngh-104773\nand\ngh-104780\n.)\n2to3\n¶\nRemove the\n2to3\nprogram and the\nlib2to3\nmodule,\npreviously deprecated in Python 3.11.\n(Contributed by Victor Stinner in\ngh-104780\n.)\nbuiltins\n¶\nRemove support for chained\nclassmethod\ndescriptors\n(introduced in\ngh-63272\n).\nThese can no longer be used to wrap other descriptors,\nsuch as\nproperty\n.\nThe core design of this feature was flawed and led to several problems.\nTo “pass-through” a\nclassmethod\n, consider using\nthe\n__wrapped__\nattribute that was added in Python 3.10.\n(Contributed by Raymond Hettinger in\ngh-89519\n.)\nRaise a\nRuntimeError\nwhen calling\nframe.clear()\non a suspended frame (as has always been the case for an executing frame).\n(Contributed by Irit Katriel in\ngh-79932\n.)\nconfigparser\n¶\nRemove the undocumented\nLegacyInterpolation\nclass,\ndeprecated in the docstring since Python 3.2,\nand at runtime since Python 3.11.\n(Contributed by Hugo van Kemenade in\ngh-104886\n.)\nimportlib.metadata\n¶\nRemove deprecated subscript (\n__getitem__()\n) access for\nEntryPoint\nobjects.\n(Contributed by Jason R. Coombs in\ngh-113175\n.)\nlocale\n¶\nRemove the\nlocale.resetlocale()\nfunction, deprecated in Python 3.11.\nUse\nlocale.setlocale(locale.LC_ALL,\n\"\")\ninstead.\n(Contributed by Victor Stinner in\ngh-104783\n.)\nopcode\n¶\nMove\nopcode.ENABLE_SPECIALIZATION\nto\n_opcode.ENABLE_SPECIALIZATION\n.\nThis field was added in 3.12, it was never documented,\nand is not intended for external use.\n(Contributed by Irit Katriel in\ngh-105481\n.)\nRemove\nopcode.is_pseudo()\n,\nopcode.MIN_PSEUDO_OPCODE\n,\nand\nopcode.MAX_PSEUDO_OPCODE\n, which were added in Python 3.12,\nbut were neither documented nor exposed through\ndis\n,\nand were not intended to be used externally.\n(Contributed by Irit Katriel in\ngh-105481\n.)\noptparse\n¶\nThis module is no longer considered\nsoft deprecated\n.\nWhile\nargparse\nremains preferred for new projects that\naren’t using a third party command line argument processing\nlibrary, there are aspects of the way\nargparse\nworks that\nmean the lower level\noptparse\nmodule may provide a better\nfoundation for\nwriting\nargument processing libraries, and\nfor implementing command line applications which adhere more\nstrictly than\nargparse\ndoes to various Unix command line\nprocessing conventions that originate in the behaviour of the\nC\ngetopt()\nfunction .\n(Contributed by Alyssa Coghlan and Serhiy Storchaka in\ngh-126180\n.)\npathlib\n¶\nRemove the ability to use\nPath\nobjects as context managers.\nThis functionality was deprecated and has had no effect since Python 3.9.\n(Contributed by Barney Gale in\ngh-83863\n.)\nre\n¶\nRemove the undocumented, deprecated, and broken\nre.template()\nfunction and\nre.TEMPLATE\n/\nre.T\nflag.\n(Contributed by Serhiy Storchaka and Nikita Sobolev in\ngh-105687\n.)\ntkinter.tix\n¶\nRemove the\ntkinter.tix\nmodule, deprecated in Python 3.6.\nThe third-party Tix library which the module wrapped is unmaintained.\n(Contributed by Zachary Ware in\ngh-75552\n.)\nturtle\n¶\nRemove the\nRawTurtle.settiltangle()\nmethod,\ndeprecated in the documentation since Python 3.1\nand at runtime since Python 3.11.\n(Contributed by Hugo van Kemenade in\ngh-104876\n.)\ntyping\n¶\nRemove the\ntyping.io\nand\ntyping.re\nnamespaces,\ndeprecated since Python 3.8.\nThe items in those namespaces can be imported directly\nfrom the\ntyping\nmodule.\n(Contributed by Sebastian Rittau in\ngh-92871\n.)\nRemove the keyword-argument method of creating\nTypedDict\ntypes, deprecated in Python 3.11.\n(Contributed by Tomas Roun in\ngh-104786\n.)\nunittest\n¶\nRemove the following\nunittest\nfunctions, deprecated in Python 3.11:\nunittest.findTestCases()\nunittest.makeSuite()\nunittest.getTestCaseNames()\nUse\nTestLoader\nmethods instead:\nloadTestsFromModule()\nloadTestsFromTestCase()\ngetTestCaseNames()\n(Contributed by Hugo van Kemenade in\ngh-104835\n.)\nRemove the untested and undocumented\nTestProgram.usageExit()\nmethod, deprecated in Python 3.11.\n(Contributed by Hugo van Kemenade in\ngh-104992\n.)\nurllib\n¶\nRemove the\ncafile\n,\ncapath\n, and\ncadefault\nparameters of the\nurllib.request.urlopen()\nfunction, deprecated in Python 3.6.\nUse the\ncontext\nparameter instead with an\nSSLContext\ninstance.\nThe\nssl.SSLContext.load_cert_chain()\nfunction\ncan be used to load specific certificates,\nor let\nssl.create_default_context()\nselect\nthe operating system’s trusted certificate authority (CA) certificates.\n(Contributed by Victor Stinner in\ngh-105382\n.)\nwebbrowser\n¶\nRemove the untested and undocumented\nMacOSX\nclass,\ndeprecated in Python 3.11.\nUse the\nMacOSXOSAScript\nclass (introduced in Python 3.2) instead.\n(Contributed by Hugo van Kemenade in\ngh-104804\n.)\nRemove the deprecated\nMacOSXOSAScript._name\nattribute.\nUse the\nMacOSXOSAScript.name\nattribute instead.\n(Contributed by Nikita Sobolev in\ngh-105546\n.)\nNew Deprecations\n¶\nUser-defined functions\n:\nDeprecate assignment to a function’s\n__code__\nattribute,\nwhere the new code object’s type does not match the function’s type.\nThe different types are:\nplain function, generator, async generator, and coroutine.\n(Contributed by Irit Katriel in\ngh-81137\n.)\narray\n:\nDeprecate the\n'u'\nformat code (\nwchar_t\n) at runtime.\nThis format code has been deprecated in documentation since Python 3.3,\nand will be removed in Python 3.16.\nUse the\n'w'\nformat code (\nPy_UCS4\n)\nfor Unicode characters instead.\n(Contributed by Hugo van Kemenade in\ngh-80480\n.)\nctypes\n:\nDeprecate the undocumented\nSetPointerType()\nfunction,\nto be removed in Python 3.15.\n(Contributed by Victor Stinner in\ngh-105733\n.)\nSoft-deprecate\nthe\nARRAY()\nfunction in favour of\ntype\n*\nlength\nmultiplication.\n(Contributed by Victor Stinner in\ngh-105733\n.)\ndecimal\n:\nDeprecate the non-standard and undocumented\nDecimal\nformat specifier\n'N'\n,\nwhich is only supported in the\ndecimal\nmodule’s C implementation.\n(Contributed by Serhiy Storchaka in\ngh-89902\n.)\ndis\n:\nDeprecate the\nHAVE_ARGUMENT\nseparator.\nCheck membership in\nhasarg\ninstead.\n(Contributed by Irit Katriel in\ngh-109319\n.)\ngettext\n:\nDeprecate non-integer numbers as arguments to functions and methods\nthat consider plural forms in the\ngettext\nmodule,\neven if no translation was found.\n(Contributed by Serhiy Storchaka in\ngh-88434\n.)\nglob\n:\nDeprecate the undocumented\nglob0()\nand\nglob1()\nfunctions.\nUse\nglob()\nand pass a\npath-like object\nspecifying\nthe root directory to the\nroot_dir\nparameter instead.\n(Contributed by Barney Gale in\ngh-117337\n.)\nhttp.server\n:\nDeprecate\nCGIHTTPRequestHandler\n,\nto be removed in Python 3.15.\nProcess-based CGI HTTP servers have been out of favor for a very long time.\nThis code was outdated, unmaintained, and rarely used.\nIt has a high potential for both security and functionality bugs.\n(Contributed by Gregory P. Smith in\ngh-109096\n.)\nDeprecate the\n--cgi\nflag to\nthe\npython -m http.server\ncommand-line interface,\nto be removed in Python 3.15.\n(Contributed by Gregory P. Smith in\ngh-109096\n.)\nmimetypes\n:\nSoft-deprecate\nfile path arguments\nto\nguess_type()\n,\nuse\nguess_file_type()\ninstead.\n(Contributed by Serhiy Storchaka in\ngh-66543\n.)\nre\n:\nDeprecate passing the optional\nmaxsplit\n,\ncount\n, or\nflags\narguments\nas positional arguments to the module-level\nsplit()\n,\nsub()\n, and\nsubn()\nfunctions.\nThese parameters will become\nkeyword-only\nin a future version of Python.\n(Contributed by Serhiy Storchaka in\ngh-56166\n.)\npathlib\n:\nDeprecate\nPurePath.is_reserved()\n,\nto be removed in Python 3.15.\nUse\nos.path.isreserved()\nto detect reserved paths on Windows.\n(Contributed by Barney Gale in\ngh-88569\n.)\nplatform\n:\nDeprecate\njava_ver()\n,\nto be removed in Python 3.15.\nThis function is only useful for Jython support, has a confusing API,\nand is largely untested.\n(Contributed by Nikita Sobolev in\ngh-116349\n.)\npydoc\n:\nDeprecate the undocumented\nispackage()\nfunction.\n(Contributed by Zackery Spytz in\ngh-64020\n.)\nsqlite3\n:\nDeprecate passing more than one positional argument to\nthe\nconnect()\nfunction\nand the\nConnection\nconstructor.\nThe remaining parameters will become keyword-only in Python 3.15.\n(Contributed by Erlend E. Aasland in\ngh-107948\n.)\nDeprecate passing name, number of arguments, and the callable as keyword\narguments for\nConnection.create_function()\nand\nConnection.create_aggregate()\nThese parameters will become positional-only in Python 3.15.\n(Contributed by Erlend E. Aasland in\ngh-108278\n.)\nDeprecate passing the callback callable by keyword for the\nset_authorizer()\n,\nset_progress_handler()\n, and\nset_trace_callback()\nConnection\nmethods.\nThe callback callables will become positional-only in Python 3.15.\n(Contributed by Erlend E. Aasland in\ngh-108278\n.)\nsys\n:\nDeprecate the\n_enablelegacywindowsfsencoding()\nfunction,\nto be removed in Python 3.16.\nUse the\nPYTHONLEGACYWINDOWSFSENCODING\nenvironment variable instead.\n(Contributed by Inada Naoki in\ngh-73427\n.)\ntarfile\n:\nDeprecate the undocumented and unused\nTarFile.tarfile\nattribute,\nto be removed in Python 3.16.\n(Contributed in\ngh-115256\n.)\ntraceback\n:\nDeprecate the\nTracebackException.exc_type\nattribute.\nUse\nTracebackException.exc_type_str\ninstead.\n(Contributed by Irit Katriel in\ngh-112332\n.)\ntyping\n:\nDeprecate the undocumented keyword argument syntax for creating\nNamedTuple\nclasses\n(e.g.\nPoint\n=\nNamedTuple(\"Point\",\nx=int,\ny=int)\n),\nto be removed in Python 3.15.\nUse the class-based syntax or the functional syntax instead.\n(Contributed by Alex Waygood in\ngh-105566\n.)\nDeprecate omitting the\nfields\nparameter when creating\na\nNamedTuple\nor\ntyping.TypedDict\nclass,\nand deprecate passing\nNone\nto the\nfields\nparameter of both types.\nPython 3.15 will require a valid sequence for the\nfields\nparameter.\nTo create a NamedTuple class with zero fields,\nuse\nclass\nNT(NamedTuple):\npass\nor\nNT\n=\nNamedTuple(\"NT\",\n())\n.\nTo create a TypedDict class with zero fields,\nuse\nclass\nTD(TypedDict):\npass\nor\nTD\n=\nTypedDict(\"TD\",\n{})\n.\n(Contributed by Alex Waygood in\ngh-105566\nand\ngh-105570\n.)\nDeprecate the\ntyping.no_type_check_decorator()\ndecorator function,\nto be removed in in Python 3.15.\nAfter eight years in the\ntyping\nmodule,\nit has yet to be supported by any major type checker.\n(Contributed by Alex Waygood in\ngh-106309\n.)\nDeprecate\ntyping.AnyStr\n.\nIn Python 3.16, it will be removed from\ntyping.__all__\n,\nand a\nDeprecationWarning\nwill be emitted at runtime\nwhen it is imported or accessed.\nIt will be removed entirely in Python 3.18.\nUse the new\ntype parameter syntax\ninstead.\n(Contributed by Michael The in\ngh-107116\n.)\nwave\n:\nDeprecate the\ngetmark()\n,\nsetmark()\n,\nand\ngetmarkers()\nmethods of\nthe\nWave_read\nand\nWave_write\nclasses,\nto be removed in Python 3.15.\n(Contributed by Victor Stinner in\ngh-105096\n.)\nPending Removal in Python 3.14\n¶\nargparse\n: The\ntype\n,\nchoices\n, and\nmetavar\nparameters\nof\nargparse.BooleanOptionalAction\nare deprecated\nand will be removed in 3.14.\n(Contributed by Nikita Sobolev in\ngh-92248\n.)\nast\n: The following features have been deprecated in documentation\nsince Python 3.8, now cause a\nDeprecationWarning\nto be emitted at\nruntime when they are accessed or used, and will be removed in Python 3.14:\nast.Num\nast.Str\nast.Bytes\nast.NameConstant\nast.Ellipsis\nUse\nast.Constant\ninstead.\n(Contributed by Serhiy Storchaka in\ngh-90953\n.)\nasyncio\n:\nThe child watcher classes\nMultiLoopChildWatcher\n,\nFastChildWatcher\n,\nAbstractChildWatcher\nand\nSafeChildWatcher\nare deprecated and\nwill be removed in Python 3.14.\n(Contributed by Kumar Aditya in\ngh-94597\n.)\nasyncio.set_child_watcher()\n,\nasyncio.get_child_watcher()\n,\nasyncio.AbstractEventLoopPolicy.set_child_watcher()\nand\nasyncio.AbstractEventLoopPolicy.get_child_watcher()\nare deprecated\nand will be removed in Python 3.14.\n(Contributed by Kumar Aditya in\ngh-94597\n.)\nThe\nget_event_loop()\nmethod of the\ndefault event loop policy now emits a\nDeprecationWarning\nif there\nis no current event loop set and it decides to create one.\n(Contributed by Serhiy Storchaka and Guido van Rossum in\ngh-100160\n.)\ncollections.abc\n: Deprecated\nByteString\n.\nPrefer\nSequence\nor\nBuffer\n.\nFor use in typing, prefer a union, like\nbytes\n|\nbytearray\n,\nor\ncollections.abc.Buffer\n.\n(Contributed by Shantanu Jain in\ngh-91896\n.)\nemail\n: Deprecated the\nisdst\nparameter in\nemail.utils.localtime()\n.\n(Contributed by Alan Williams in\ngh-72346\n.)\nimportlib.abc\ndeprecated classes:\nimportlib.abc.ResourceReader\nimportlib.abc.Traversable\nimportlib.abc.TraversableResources\nUse\nimportlib.resources.abc\nclasses instead:\nimportlib.resources.abc.Traversable\nimportlib.resources.abc.TraversableResources\n(Contributed by Jason R. Coombs and Hugo van Kemenade in\ngh-93963\n.)\nitertools\nhad undocumented, inefficient, historically buggy,\nand inconsistent support for copy, deepcopy, and pickle operations.\nThis will be removed in 3.14 for a significant reduction in code\nvolume and maintenance burden.\n(Contributed by Raymond Hettinger in\ngh-101588\n.)\nmultiprocessing\n: The default start method will change to a safer one on\nLinux, BSDs, and other non-macOS POSIX platforms where\n'fork'\nis currently\nthe default (\ngh-84559\n). Adding a runtime warning about this was deemed too\ndisruptive as the majority of code is not expected to care. Use the\nget_context()\nor\nset_start_method()\nAPIs to explicitly specify when\nyour code\nrequires\n'fork'\n.  See\nContexts and start methods\n.\npathlib\n:\nis_relative_to()\nand\nrelative_to()\n: passing additional arguments is\ndeprecated.\npkgutil\n:\nfind_loader()\nand\nget_loader()\nnow raise\nDeprecationWarning\n;\nuse\nimportlib.util.find_spec()\ninstead.\n(Contributed by Nikita Sobolev in\ngh-97850\n.)\npty\n:\nmaster_open()\n: use\npty.openpty()\n.\nslave_open()\n: use\npty.openpty()\n.\nsqlite3\n:\nversion\nand\nversion_info\n.\nexecute()\nand\nexecutemany()\nif\nnamed placeholders\nare used and\nparameters\nis a sequence instead of a\ndict\n.\ntyping\n:\nByteString\n, deprecated since Python 3.9,\nnow causes a\nDeprecationWarning\nto be emitted when it is used.\nurllib\n:\nurllib.parse.Quoter\nis deprecated: it was not intended to be a\npublic API.\n(Contributed by Gregory P. Smith in\ngh-88168\n.)\nPending Removal in Python 3.15\n¶\nThe import system:\nSetting\n__cached__\non a module while\nfailing to set\n__spec__.cached\nis deprecated. In Python 3.15,\n__cached__\nwill cease to be set or\ntake into consideration by the import system or standard library. (\ngh-97879\n)\nSetting\n__package__\non a module while\nfailing to set\n__spec__.parent\nis deprecated. In Python 3.15,\n__package__\nwill cease to be set or\ntake into consideration by the import system or standard library. (\ngh-97879\n)\nctypes\n:\nThe undocumented\nctypes.SetPointerType()\nfunction\nhas been deprecated since Python 3.13.\nhttp.server\n:\nThe obsolete and rarely used\nCGIHTTPRequestHandler\nhas been deprecated since Python 3.13.\nNo direct replacement exists.\nAnything\nis better than CGI to interface\na web server with a request handler.\nThe\n--cgi\nflag to the\npython -m http.server\ncommand-line interface has been deprecated since Python 3.13.\nimportlib\n:\nload_module()\nmethod: use\nexec_module()\ninstead.\nlocale\n:\nThe\ngetdefaultlocale()\nfunction\nhas been deprecated since Python 3.11.\nIts removal was originally planned for Python 3.13 (\ngh-90817\n),\nbut has been postponed to Python 3.15.\nUse\ngetlocale()\n,\nsetlocale()\n,\nand\ngetencoding()\ninstead.\n(Contributed by Hugo van Kemenade in\ngh-111187\n.)\npathlib\n:\nPurePath.is_reserved()\nhas been deprecated since Python 3.13.\nUse\nos.path.isreserved()\nto detect reserved paths on Windows.\nplatform\n:\njava_ver()\nhas been deprecated since Python 3.13.\nThis function is only useful for Jython support, has a confusing API,\nand is largely untested.\nsysconfig\n:\nThe\ncheck_home\nargument of\nsysconfig.is_python_build()\nhas been\ndeprecated since Python 3.12.\nthreading\n:\nRLock()\nwill take no arguments in Python 3.15.\nPassing any arguments has been deprecated since Python 3.14,\nas the  Python version does not permit any arguments,\nbut the C version allows any number of positional or keyword arguments,\nignoring every argument.\ntypes\n:\ntypes.CodeType\n: Accessing\nco_lnotab\nwas\ndeprecated in\nPEP 626\nsince 3.10 and was planned to be removed in 3.12,\nbut it only got a proper\nDeprecationWarning\nin 3.12.\nMay be removed in 3.15.\n(Contributed by Nikita Sobolev in\ngh-101866\n.)\ntyping\n:\nThe undocumented keyword argument syntax for creating\nNamedTuple\nclasses\n(e.g.\nPoint\n=\nNamedTuple(\"Point\",\nx=int,\ny=int)\n)\nhas been deprecated since Python 3.13.\nUse the class-based syntax or the functional syntax instead.\nWhen using the functional syntax of\nTypedDict\ns, failing\nto pass a value to the\nfields\nparameter (\nTD\n=\nTypedDict(\"TD\")\n) or\npassing\nNone\n(\nTD\n=\nTypedDict(\"TD\",\nNone)\n) has been deprecated\nsince Python 3.13.\nUse\nclass\nTD(TypedDict):\npass\nor\nTD\n=\nTypedDict(\"TD\",\n{})\nto create a TypedDict with zero field.\nThe\ntyping.no_type_check_decorator()\ndecorator function\nhas been deprecated since Python 3.13.\nAfter eight years in the\ntyping\nmodule,\nit has yet to be supported by any major type checker.\nwave\n:\nThe\ngetmark()\n,\nsetmark()\n,\nand\ngetmarkers()\nmethods of\nthe\nWave_read\nand\nWave_write\nclasses\nhave been deprecated since Python 3.13.\nPending removal in Python 3.16\n¶\nThe import system:\nSetting\n__loader__\non a module while\nfailing to set\n__spec__.loader\nis deprecated. In Python 3.16,\n__loader__\nwill cease to be set or\ntaken into consideration by the import system or the standard library.\narray\n:\nThe\n'u'\nformat code (\nwchar_t\n)\nhas been deprecated in documentation since Python 3.3\nand at runtime since Python 3.13.\nUse the\n'w'\nformat code (\nPy_UCS4\n)\nfor Unicode characters instead.\nasyncio\n:\nasyncio.iscoroutinefunction()\nis deprecated\nand will be removed in Python 3.16,\nuse\ninspect.iscoroutinefunction()\ninstead.\n(Contributed by Jiahao Li and Kumar Aditya in\ngh-122875\n.)\nbuiltins\n:\nBitwise inversion on boolean types,\n~True\nor\n~False\nhas been deprecated since Python 3.12,\nas it produces surprising and unintuitive results (\n-2\nand\n-1\n).\nUse\nnot\nx\ninstead for the logical negation of a Boolean.\nIn the rare case that you need the bitwise inversion of\nthe underlying integer, convert to\nint\nexplicitly (\n~int(x)\n).\nshutil\n:\nThe\nExecError\nexception\nhas been deprecated since Python 3.14.\nIt has not been used by any function in\nshutil\nsince Python 3.4,\nand is now an alias of\nRuntimeError\n.\nsymtable\n:\nThe\nClass.get_methods\nmethod\nhas been deprecated since Python 3.14.\nsys\n:\nThe\n_enablelegacywindowsfsencoding()\nfunction\nhas been deprecated since Python 3.13.\nUse the\nPYTHONLEGACYWINDOWSFSENCODING\nenvironment variable instead.\ntarfile\n:\nThe undocumented and unused\nTarFile.tarfile\nattribute\nhas been deprecated since Python 3.13.\nPending Removal in Future Versions\n¶\nThe following APIs will be removed in the future,\nalthough there is currently no date scheduled for their removal.\nargparse\n: Nesting argument groups and nesting mutually exclusive\ngroups are deprecated.\nbuiltins\n:\nbool(NotImplemented)\n.\nGenerators:\nthrow(type,\nexc,\ntb)\nand\nathrow(type,\nexc,\ntb)\nsignature is deprecated: use\nthrow(exc)\nand\nathrow(exc)\ninstead,\nthe single argument signature.\nCurrently Python accepts numeric literals immediately followed by keywords,\nfor example\n0in\nx\n,\n1or\nx\n,\n0if\n1else\n2\n.  It allows confusing and\nambiguous expressions like\n[0x1for\nx\nin\ny]\n(which can be interpreted as\n[0x1\nfor\nx\nin\ny]\nor\n[0x1f\nor\nx\nin\ny]\n).  A syntax warning is raised\nif the numeric literal is immediately followed by one of keywords\nand\n,\nelse\n,\nfor\n,\nif\n,\nin\n,\nis\nand\nor\n.  In a future release it\nwill be changed to a syntax error. (\ngh-87999\n)\nSupport for\n__index__()\nand\n__int__()\nmethod returning non-int type:\nthese methods will be required to return an instance of a strict subclass of\nint\n.\nSupport for\n__float__()\nmethod returning a strict subclass of\nfloat\n: these methods will be required to return an instance of\nfloat\n.\nSupport for\n__complex__()\nmethod returning a strict subclass of\ncomplex\n: these methods will be required to return an instance of\ncomplex\n.\nDelegation of\nint()\nto\n__trunc__()\nmethod.\nPassing a complex number as the\nreal\nor\nimag\nargument in the\ncomplex()\nconstructor is now deprecated; it should only be passed\nas a single positional argument.\n(Contributed by Serhiy Storchaka in\ngh-109218\n.)\ncalendar\n:\ncalendar.January\nand\ncalendar.February\nconstants are\ndeprecated and replaced by\ncalendar.JANUARY\nand\ncalendar.FEBRUARY\n.\n(Contributed by Prince Roshan in\ngh-103636\n.)\ncodeobject.co_lnotab\n: use the\ncodeobject.co_lines()\nmethod\ninstead.\ndatetime\n:\nutcnow()\n:\nuse\ndatetime.datetime.now(tz=datetime.UTC)\n.\nutcfromtimestamp()\n:\nuse\ndatetime.datetime.fromtimestamp(timestamp,\ntz=datetime.UTC)\n.\ngettext\n: Plural value must be an integer.\nimportlib\n:\ncache_from_source()\ndebug_override\nparameter is\ndeprecated: use the\noptimization\nparameter instead.\nimportlib.metadata\n:\nEntryPoints\ntuple interface.\nImplicit\nNone\non return values.\nlogging\n: the\nwarn()\nmethod has been deprecated\nsince Python 3.3, use\nwarning()\ninstead.\nmailbox\n: Use of StringIO input and text mode is deprecated, use\nBytesIO and binary mode instead.\nos\n: Calling\nos.register_at_fork()\nin multi-threaded process.\npydoc.ErrorDuringImport\n: A tuple value for\nexc_info\nparameter is\ndeprecated, use an exception instance.\nre\n: More strict rules are now applied for numerical group references\nand group names in regular expressions.  Only sequence of ASCII digits is now\naccepted as a numerical reference.  The group name in bytes patterns and\nreplacement strings can now only contain ASCII letters and digits and\nunderscore.\n(Contributed by Serhiy Storchaka in\ngh-91760\n.)\nsre_compile\n,\nsre_constants\nand\nsre_parse\nmodules.\nshutil\n:\nrmtree()\n’s\nonerror\nparameter is deprecated in\nPython 3.12; use the\nonexc\nparameter instead.\nssl\noptions and protocols:\nssl.SSLContext\nwithout protocol argument is deprecated.\nssl.SSLContext\n:\nset_npn_protocols()\nand\nselected_npn_protocol()\nare deprecated: use ALPN\ninstead.\nssl.OP_NO_SSL*\noptions\nssl.OP_NO_TLS*\noptions\nssl.PROTOCOL_SSLv3\nssl.PROTOCOL_TLS\nssl.PROTOCOL_TLSv1\nssl.PROTOCOL_TLSv1_1\nssl.PROTOCOL_TLSv1_2\nssl.TLSVersion.SSLv3\nssl.TLSVersion.TLSv1\nssl.TLSVersion.TLSv1_1\nthreading\nmethods:\nthreading.Condition.notifyAll()\n: use\nnotify_all()\n.\nthreading.Event.isSet()\n: use\nis_set()\n.\nthreading.Thread.isDaemon()\n,\nthreading.Thread.setDaemon()\n:\nuse\nthreading.Thread.daemon\nattribute.\nthreading.Thread.getName()\n,\nthreading.Thread.setName()\n:\nuse\nthreading.Thread.name\nattribute.\nthreading.currentThread()\n: use\nthreading.current_thread()\n.\nthreading.activeCount()\n: use\nthreading.active_count()\n.\ntyping.Text\n(\ngh-92332\n).\nunittest.IsolatedAsyncioTestCase\n: it is deprecated to return a value\nthat is not\nNone\nfrom a test case.\nurllib.parse\ndeprecated functions:\nurlparse()\ninstead\nsplitattr()\nsplithost()\nsplitnport()\nsplitpasswd()\nsplitport()\nsplitquery()\nsplittag()\nsplittype()\nsplituser()\nsplitvalue()\nto_bytes()\nurllib.request\n:\nURLopener\nand\nFancyURLopener\nstyle of invoking requests is\ndeprecated. Use newer\nurlopen()\nfunctions and methods.\nwsgiref\n:\nSimpleHandler.stdout.write()\nshould not do partial\nwrites.\nxml.etree.ElementTree\n: Testing the truth value of an\nElement\nis deprecated. In a future release it\nwill always return\nTrue\n. Prefer explicit\nlen(elem)\nor\nelem\nis\nnot\nNone\ntests instead.\nzipimport.zipimporter.load_module()\nis deprecated:\nuse\nexec_module()\ninstead.\nCPython Bytecode Changes\n¶\nThe oparg of\nYIELD_VALUE\nis now\n1\nif the yield is part of a yield-from or await, and\n0\notherwise.\nThe oparg of\nRESUME\nwas changed to add a bit indicating\nif the except-depth is 1, which is needed to optimize closing of generators.\n(Contributed by Irit Katriel in\ngh-111354\n.)\nC API Changes\n¶\nNew Features\n¶\nAdd the\nPyMonitoring C API\nfor generating\nPEP 669\nmonitoring events:\nPyMonitoringState\nPyMonitoring_FirePyStartEvent()\nPyMonitoring_FirePyResumeEvent()\nPyMonitoring_FirePyReturnEvent()\nPyMonitoring_FirePyYieldEvent()\nPyMonitoring_FireCallEvent()\nPyMonitoring_FireLineEvent()\nPyMonitoring_FireJumpEvent()\nPyMonitoring_FireBranchEvent()\nPyMonitoring_FireCReturnEvent()\nPyMonitoring_FirePyThrowEvent()\nPyMonitoring_FireRaiseEvent()\nPyMonitoring_FireCRaiseEvent()\nPyMonitoring_FireReraiseEvent()\nPyMonitoring_FireExceptionHandledEvent()\nPyMonitoring_FirePyUnwindEvent()\nPyMonitoring_FireStopIterationEvent()\nPyMonitoring_EnterScope()\nPyMonitoring_ExitScope()\n(Contributed by Irit Katriel in\ngh-111997\n).\nAdd\nPyMutex\n, a lightweight mutex that occupies a single byte,\nand the new\nPyMutex_Lock()\nand\nPyMutex_Unlock()\nfunctions.\nPyMutex_Lock()\nwill release the\nGIL\n(if currently held)\nif the operation needs to block.\n(Contributed by Sam Gross in\ngh-108724\n.)\nAdd the\nPyTime C API\nto provide access to system clocks:\nPyTime_t\n.\nPyTime_MIN\nand\nPyTime_MAX\n.\nPyTime_AsSecondsDouble()\n.\nPyTime_Monotonic()\n.\nPyTime_MonotonicRaw()\n.\nPyTime_PerfCounter()\n.\nPyTime_PerfCounterRaw()\n.\nPyTime_Time()\n.\nPyTime_TimeRaw()\n.\n(Contributed by Victor Stinner and Petr Viktorin in\ngh-110850\n.)\nAdd the\nPyDict_ContainsString()\nfunction\nwith the same behavior as\nPyDict_Contains()\n,\nbut\nkey\nis specified as a\nconst\nchar\n*\nUTF-8 encoded bytes string,\nrather than a\nPyObject\n*\n.\n(Contributed by Victor Stinner in\ngh-108314\n.)\nAdd the\nPyDict_GetItemRef()\nand\nPyDict_GetItemStringRef()\nfunctions,\nwhich behave similarly to\nPyDict_GetItemWithError()\n,\nbut return a\nstrong reference\ninstead of a\nborrowed reference\n.\nMoreover, these functions return\n-1\non error,\nremoving the need to check\nPyErr_Occurred()\n.\n(Contributed by Victor Stinner in\ngh-106004\n.)\nAdd the\nPyDict_SetDefaultRef()\nfunction,\nwhich behaves similarly to\nPyDict_SetDefault()\n,\nbut returns a\nstrong reference\ninstead of a\nborrowed reference\n.\nThis function returns\n-1\non error,\n0\non insertion,\nand\n1\nif the key was already present in the dictionary.\n(Contributed by Sam Gross in\ngh-112066\n.)\nAdd the\nPyDict_Pop()\nand\nPyDict_PopString()\nfunctions\nto remove a key from a dictionary and optionally return the removed value.\nThis is similar to\ndict.pop()\n,\nthough there is no default value,\nand\nKeyError\nis not raised for missing keys.\n(Contributed by Stefan Behnel and Victor Stinner in\ngh-111262\n.)\nAdd the\nPyMapping_GetOptionalItem()\nand\nPyMapping_GetOptionalItemString()\nfunctions\nas alternatives to\nPyObject_GetItem()\nand\nPyMapping_GetItemString()\nrespectively.\nThe new functions do not raise\nKeyError\nif the requested key is missing from the mapping.\nThese variants are more convenient and faster\nif a missing key should not be treated as a failure.\n(Contributed by Serhiy Storchaka in\ngh-106307\n.)\nAdd the\nPyObject_GetOptionalAttr()\nand\nPyObject_GetOptionalAttrString()\nfunctions\nas alternatives to\nPyObject_GetAttr()\nand\nPyObject_GetAttrString()\nrespectively.\nThe new functions do not raise\nAttributeError\nif the requested attribute is not found on the object.\nThese variants are more convenient and faster\nif the missing attribute should not be treated as a failure.\n(Contributed by Serhiy Storchaka in\ngh-106521\n.)\nAdd the\nPyErr_FormatUnraisable()\nfunction\nas an extension to\nPyErr_WriteUnraisable()\nthat allows customizing the warning message.\n(Contributed by Serhiy Storchaka in\ngh-108082\n.)\nAdd new functions that return a\nstrong reference\ninstead of\na\nborrowed reference\nfor frame locals, globals, and builtins,\nas part of\nPEP 667\n:\nPyEval_GetFrameBuiltins()\nreplaces\nPyEval_GetBuiltins()\nPyEval_GetFrameGlobals()\nreplaces\nPyEval_GetGlobals()\nPyEval_GetFrameLocals()\nreplaces\nPyEval_GetLocals()\n(Contributed by Mark Shannon and Tian Gao in\ngh-74929\n.)\nAdd the\nPy_GetConstant()\nand\nPy_GetConstantBorrowed()\nfunctions to get\nstrong\nor\nborrowed\nreferences to constants.\nFor example,\nPy_GetConstant(Py_CONSTANT_ZERO)\nreturns a strong reference\nto the constant zero.\n(Contributed by Victor  Stinner in\ngh-115754\n.)\nAdd the\nPyImport_AddModuleRef()\nfunction\nas a replacement for\nPyImport_AddModule()\nthat returns a\nstrong reference\ninstead of a\nborrowed reference\n.\n(Contributed by Victor Stinner in\ngh-105922\n.)\nAdd the\nPy_IsFinalizing()\nfunction to check\nwhether the main Python interpreter is\nshutting down\n.\n(Contributed by Victor Stinner in\ngh-108014\n.)\nAdd the\nPyList_GetItemRef()\nfunction\nas a replacement for\nPyList_GetItem()\nthat returns a\nstrong reference\ninstead of a\nborrowed reference\n.\n(Contributed by Sam Gross in\ngh-114329\n.)\nAdd the\nPyList_Extend()\nand\nPyList_Clear()\nfunctions,\nmirroring the Python\nlist.extend()\nand\nlist.clear()\nmethods.\n(Contributed by Victor Stinner in\ngh-111138\n.)\nAdd the\nPyLong_AsInt()\nfunction.\nIt behaves similarly to\nPyLong_AsLong()\n,\nbut stores the result in a C\nint\ninstead of a C\nlong\n.\n(Contributed by Victor Stinner in\ngh-108014\n.)\nAdd the\nPyLong_AsNativeBytes()\n,\nPyLong_FromNativeBytes()\n,\nand\nPyLong_FromUnsignedNativeBytes()\nfunctions\nto simplify converting between native integer types\nand Python\nint\nobjects.\n(Contributed by Steve Dower in\ngh-111140\n.)\nAdd\nPyModule_Add()\nfunction, which is similar to\nPyModule_AddObjectRef()\nand\nPyModule_AddObject()\n,\nbut always steals a reference to the value.\n(Contributed by Serhiy Storchaka in\ngh-86493\n.)\nAdd the\nPyObject_GenericHash()\nfunction\nthat implements the default hashing function of a Python object.\n(Contributed by Serhiy Storchaka in\ngh-113024\n.)\nAdd the\nPy_HashPointer()\nfunction to hash a raw pointer.\n(Contributed by Victor Stinner in\ngh-111545\n.)\nAdd the\nPyObject_VisitManagedDict()\nand\nPyObject_ClearManagedDict()\nfunctions.\nwhich must be called by the traverse and clear functions of a type using\nthe\nPy_TPFLAGS_MANAGED_DICT\nflag.\nThe\npythoncapi-compat project\ncan be used to\nuse these functions with Python 3.11 and 3.12.\n(Contributed by Victor Stinner in\ngh-107073\n.)\nAdd the\nPyRefTracer_SetTracer()\nand\nPyRefTracer_GetTracer()\nfunctions,\nwhich enable tracking object creation and destruction\nin the same way that the\ntracemalloc\nmodule does.\n(Contributed by Pablo Galindo in\ngh-93502\n.)\nAdd the\nPySys_AuditTuple()\nfunction\nas an alternative to\nPySys_Audit()\nthat takes event arguments as a Python\ntuple\nobject.\n(Contributed by Victor Stinner in\ngh-85283\n.)\nAdd the\nPyThreadState_GetUnchecked()\nfunction\nas an alternative to\nPyThreadState_Get()\nthat doesn’t kill the process with a fatal error if it is\nNULL\n.\nThe caller is responsible for checking if the result is\nNULL\n.\n(Contributed by Victor Stinner in\ngh-108867\n.)\nAdd the\nPyType_GetFullyQualifiedName()\nfunction\nto get the type’s fully qualified name.\nThe module name is prepended if\ntype.__module__\nis\na string and is not equal to either\n'builtins'\nor\n'__main__'\n.\n(Contributed by Victor Stinner in\ngh-111696\n.)\nAdd the\nPyType_GetModuleName()\nfunction\nto get the type’s module name. This is equivalent to getting the\ntype.__module__\nattribute.\n(Contributed by Eric Snow and Victor Stinner in\ngh-111696\n.)\nAdd the\nPyUnicode_EqualToUTF8AndSize()\nand\nPyUnicode_EqualToUTF8()\nfunctions\nto compare a Unicode object with a\nconst\nchar\n*\nUTF-8 encoded string\nand\n1\nif they are equal or\n0\notherwise.\nThese functions do not raise exceptions.\n(Contributed by Serhiy Storchaka in\ngh-110289\n.)\nAdd the\nPyWeakref_GetRef()\nfunction\nas an alternative to\nPyWeakref_GetObject()\nthat returns a\nstrong reference\nor\nNULL\nif the referent is no longer live.\n(Contributed by Victor Stinner in\ngh-105927\n.)\nAdd fixed variants of functions which silently ignore errors:\nPyObject_HasAttrWithError()\nreplaces\nPyObject_HasAttr()\n.\nPyObject_HasAttrStringWithError()\nreplaces\nPyObject_HasAttrString()\n.\nPyMapping_HasKeyWithError()\nreplaces\nPyMapping_HasKey()\n.\nPyMapping_HasKeyStringWithError()\nreplaces\nPyMapping_HasKeyString()\n.\nThe new functions return\n-1\nfor errors\nand the standard\n1\nfor true and\n0\nfor false.\n(Contributed by Serhiy Storchaka in\ngh-108511\n.)\nChanged C APIs\n¶\nThe\nkeywords\nparameter of\nPyArg_ParseTupleAndKeywords()\nand\nPyArg_VaParseTupleAndKeywords()\nnow has type\nchar\n*\nconst\n*\nin C\nand\nconst\nchar\n*\nconst\n*\nin C++,\ninstead of\nchar\n*\n*\n.\nIn C++, this makes these functions compatible with arguments\nof type\nconst\nchar\n*\nconst\n*\n,\nconst\nchar\n*\n*\n,\nor\nchar\n*\nconst\n*\nwithout an explicit type cast.\nIn C, the functions only support arguments of type\nchar\n*\nconst\n*\n.\nThis can be overridden with the\nPY_CXX_CONST\nmacro.\n(Contributed by Serhiy Storchaka in\ngh-65210\n.)\nPyArg_ParseTupleAndKeywords()\nnow supports\nnon-ASCII keyword parameter names.\n(Contributed by Serhiy Storchaka in\ngh-110815\n.)\nThe\nPyCode_GetFirstFree()\nfunction is now unstable API\nand is now named\nPyUnstable_Code_GetFirstFree()\n.\n(Contributed by Bogdan Romanyuk in\ngh-115781\n.)\nThe\nPyDict_GetItem()\n,\nPyDict_GetItemString()\n,\nPyMapping_HasKey()\n,\nPyMapping_HasKeyString()\n,\nPyObject_HasAttr()\n,\nPyObject_HasAttrString()\n,\nand\nPySys_GetObject()\nfunctions,\neach of which clears all errors which occurred when calling them\nnow reports these errors using\nsys.unraisablehook()\n.\nYou may replace them with other functions as recommended in the documentation.\n(Contributed by Serhiy Storchaka in\ngh-106672\n.)\nAdd support for the\n%T\n,\n%#T\n,\n%N\nand\n%#N\nformats\nto\nPyUnicode_FromFormat()\n:\n%T\n: Get the fully qualified name of an object type\n%#T\n: As above, but use a colon as the separator\n%N\n: Get the fully qualified name of a type\n%#N\n: As above, but use a colon as the separator\nSee\nPEP 737\nfor more information.\n(Contributed by Victor Stinner in\ngh-111696\n.)\nYou no longer have to define the\nPY_SSIZE_T_CLEAN\nmacro before\nincluding\nPython.h\nwhen using\n#\nformats in\nformat codes\n.\nAPIs accepting the format codes always use\nPy_ssize_t\nfor\n#\nformats.\n(Contributed by Inada Naoki in\ngh-104922\n.)\nIf Python is built in\ndebug mode\nor\nwith\nassertions\n,\nPyTuple_SET_ITEM()\nand\nPyList_SET_ITEM()\nnow check the index argument with an assertion.\n(Contributed by Victor Stinner in\ngh-106168\n.)\nLimited C API Changes\n¶\nThe following functions are now included in the Limited C API:\nPyMem_RawMalloc()\nPyMem_RawCalloc()\nPyMem_RawRealloc()\nPyMem_RawFree()\nPySys_Audit()\nPySys_AuditTuple()\nPyType_GetModuleByDef()\n(Contributed by Victor Stinner in\ngh-85283\n,\ngh-85283\n, and\ngh-116936\n.)\nPython built with\n--with-trace-refs\n(tracing references)\nnow supports the\nLimited API\n.\n(Contributed by Victor Stinner in\ngh-108634\n.)\nRemoved C APIs\n¶\nRemove several functions, macros, variables, etc\nwith names prefixed by\n_Py\nor\n_PY\n(which are considered private).\nIf your project is affected  by one of these removals\nand you believe that the removed API should remain available,\nplease\nopen a new issue\nto request a public C API\nand add\ncc:\n@vstinner\nto the issue to notify Victor Stinner.\n(Contributed by Victor Stinner in\ngh-106320\n.)\nRemove old buffer protocols deprecated in Python 3.0.\nUse\nBuffer Protocol\ninstead.\nPyObject_CheckReadBuffer()\n:\nUse\nPyObject_CheckBuffer()\nto test\nwhether the object supports the buffer protocol.\nNote that\nPyObject_CheckBuffer()\ndoesn’t guarantee\nthat\nPyObject_GetBuffer()\nwill succeed.\nTo test if the object is actually readable,\nsee the next example of\nPyObject_GetBuffer()\n.\nPyObject_AsCharBuffer()\n,\nPyObject_AsReadBuffer()\n:\nUse\nPyObject_GetBuffer()\nand\nPyBuffer_Release()\ninstead:\nPy_buffer\nview\n;\nif\n(\nPyObject_GetBuffer\n(\nobj\n,\n&\nview\n,\nPyBUF_SIMPLE\n)\n<\n0\n)\n{\nreturn\nNULL\n;\n}\n// Use `view.buf` and `view.len` to read from the buffer.\n// You may need to cast buf as `(const char*)view.buf`.\nPyBuffer_Release\n(\n&\nview\n);\nPyObject_AsWriteBuffer()\n:\nUse\nPyObject_GetBuffer()\nand\nPyBuffer_Release()\ninstead:\nPy_buffer\nview\n;\nif\n(\nPyObject_GetBuffer\n(\nobj\n,\n&\nview\n,\nPyBUF_WRITABLE\n)\n<\n0\n)\n{\nreturn\nNULL\n;\n}\n// Use `view.buf` and `view.len` to write to the buffer.\nPyBuffer_Release\n(\n&\nview\n);\n(Contributed by Inada Naoki in\ngh-85275\n.)\nRemove various functions deprecated in Python 3.9:\nPyEval_CallObject()\n,\nPyEval_CallObjectWithKeywords()\n:\nUse\nPyObject_CallNoArgs()\nor\nPyObject_Call()\ninstead.\nWarning\nIn\nPyObject_Call()\n, positional arguments must be a\ntuple\nand must not be\nNULL\n,\nand keyword arguments must be a\ndict\nor\nNULL\n,\nwhereas the removed functions checked argument types\nand accepted\nNULL\npositional and keyword arguments.\nTo replace\nPyEval_CallObjectWithKeywords(func,\nNULL,\nkwargs)\nwith\nPyObject_Call()\n,\npass an empty tuple as positional arguments using\nPyTuple_New(0)\n.\nPyEval_CallFunction()\n:\nUse\nPyObject_CallFunction()\ninstead.\nPyEval_CallMethod()\n:\nUse\nPyObject_CallMethod()\ninstead.\nPyCFunction_Call()\n:\nUse\nPyObject_Call()\ninstead.\n(Contributed by Victor Stinner in\ngh-105107\n.)\nRemove the following old functions to configure the Python initialization,\ndeprecated in Python 3.11:\nPySys_AddWarnOptionUnicode()\n:\nUse\nPyConfig.warnoptions\ninstead.\nPySys_AddWarnOption()\n:\nUse\nPyConfig.warnoptions\ninstead.\nPySys_AddXOption()\n:\nUse\nPyConfig.xoptions\ninstead.\nPySys_HasWarnOptions()\n:\nUse\nPyConfig.xoptions\ninstead.\nPySys_SetPath()\n:\nSet\nPyConfig.module_search_paths\ninstead.\nPy_SetPath()\n:\nSet\nPyConfig.module_search_paths\ninstead.\nPy_SetStandardStreamEncoding()\n:\nSet\nPyConfig.stdio_encoding\ninstead,\nand set also maybe\nPyConfig.legacy_windows_stdio\n(on Windows).\n_Py_SetProgramFullPath()\n:\nSet\nPyConfig.executable\ninstead.\nUse the new\nPyConfig\nAPI of the\nPython Initialization\nConfiguration\ninstead (\nPEP 587\n), added to Python 3.8.\n(Contributed by Victor Stinner in\ngh-105145\n.)\nRemove\nPyEval_AcquireLock()\nand\nPyEval_ReleaseLock()\nfunctions,\ndeprecated in Python 3.2.\nThey didn’t update the current thread state.\nThey can be replaced with:\nPyEval_SaveThread()\nand\nPyEval_RestoreThread()\n;\nlow-level\nPyEval_AcquireThread()\nand\nPyEval_RestoreThread()\n;\nor\nPyGILState_Ensure()\nand\nPyGILState_Release()\n.\n(Contributed by Victor Stinner in\ngh-105182\n.)\nRemove the\nPyEval_ThreadsInitialized()\nfunction,\ndeprecated in Python 3.9.\nSince Python 3.7,\nPy_Initialize()\nalways creates the GIL:\ncalling\nPyEval_InitThreads()\ndoes nothing and\nPyEval_ThreadsInitialized()\nalways returns non-zero.\n(Contributed by Victor Stinner in\ngh-105182\n.)\nRemove the\n_PyInterpreterState_Get()\nalias to\nPyInterpreterState_Get()\nwhich was kept for backward compatibility with Python 3.8.\nThe\npythoncapi-compat project\ncan be used to get\nPyInterpreterState_Get()\non Python 3.8 and older.\n(Contributed by Victor Stinner in\ngh-106320\n.)\nRemove the private\n_PyObject_FastCall()\nfunction:\nuse\nPyObject_Vectorcall()\nwhich is available since Python 3.8\n(\nPEP 590\n).\n(Contributed by Victor Stinner in\ngh-106023\n.)\nRemove the\ncpython/pytime.h\nheader file,\nwhich only contained private functions.\n(Contributed by Victor Stinner in\ngh-106316\n.)\nRemove the undocumented\nPY_TIMEOUT_MAX\nconstant from the limited C API.\n(Contributed by Victor Stinner in\ngh-110014\n.)\nRemove the old trashcan macros\nPy_TRASHCAN_SAFE_BEGIN\nand\nPy_TRASHCAN_SAFE_END\n.\nReplace both with the new macros\nPy_TRASHCAN_BEGIN\nand\nPy_TRASHCAN_END\n.\n(Contributed by Irit Katriel in\ngh-105111\n.)\nDeprecated C APIs\n¶\nDeprecate old Python initialization functions:\nPySys_ResetWarnOptions()\n:\nClear\nsys.warnoptions\nand\nwarnings.filters\ninstead.\nPy_GetExecPrefix()\n:\nGet\nsys.exec_prefix\ninstead.\nPy_GetPath()\n:\nGet\nsys.path\ninstead.\nPy_GetPrefix()\n:\nGet\nsys.prefix\ninstead.\nPy_GetProgramFullPath()\n:\nGet\nsys.executable\ninstead.\nPy_GetProgramName()\n:\nGet\nsys.executable\ninstead.\nPy_GetPythonHome()\n:\nGet\nPyConfig.home\nor the\nPYTHONHOME\nenvironment variable instead.\n(Contributed by Victor Stinner in\ngh-105145\n.)\nSoft deprecate\nthe\nPyEval_GetBuiltins()\n,\nPyEval_GetGlobals()\n,\nand\nPyEval_GetLocals()\nfunctions,\nwhich return a\nborrowed reference\n.\n(Soft deprecated as part of\nPEP 667\n.)\nDeprecate the\nPyImport_ImportModuleNoBlock()\nfunction,\nwhich is just an alias to\nPyImport_ImportModule()\nsince Python 3.3.\n(Contributed by Victor Stinner in\ngh-105396\n.)\nSoft deprecate\nthe\nPyModule_AddObject()\nfunction.\nIt should be replaced with\nPyModule_Add()\nor\nPyModule_AddObjectRef()\n.\n(Contributed by Serhiy Storchaka in\ngh-86493\n.)\nDeprecate the old\nPy_UNICODE\nand\nPY_UNICODE_TYPE\ntypes\nand the\nPy_UNICODE_WIDE\ndefine.\nUse the\nwchar_t\ntype directly instead.\nSince Python 3.3,\nPy_UNICODE\nand\nPY_UNICODE_TYPE\nare just aliases to\nwchar_t\n.\n(Contributed by Victor Stinner in\ngh-105156\n.)\nDeprecate the\nPyWeakref_GetObject()\nand\nPyWeakref_GET_OBJECT()\nfunctions,\nwhich return a\nborrowed reference\n.\nReplace them with the new\nPyWeakref_GetRef()\nfunction,\nwhich returns a\nstrong reference\n.\nThe\npythoncapi-compat project\ncan be used to get\nPyWeakref_GetRef()\non Python 3.12 and older.\n(Contributed by Victor Stinner in\ngh-105927\n.)\nPending Removal in Python 3.14\n¶\nThe\nma_version_tag\nfield in\nPyDictObject\nfor extension modules\n(\nPEP 699\n;\ngh-101193\n).\nCreating\nimmutable\ntypes\nwith mutable\nbases (\ngh-95388\n).\nFunctions to configure Python’s initialization, deprecated in Python 3.11:\nPySys_SetArgvEx()\n:\nSet\nPyConfig.argv\ninstead.\nPySys_SetArgv()\n:\nSet\nPyConfig.argv\ninstead.\nPy_SetProgramName()\n:\nSet\nPyConfig.program_name\ninstead.\nPy_SetPythonHome()\n:\nSet\nPyConfig.home\ninstead.\nThe\nPy_InitializeFromConfig()\nAPI should be used with\nPyConfig\ninstead.\nGlobal configuration variables:\nPy_DebugFlag\n:\nUse\nPyConfig.parser_debug\ninstead.\nPy_VerboseFlag\n:\nUse\nPyConfig.verbose\ninstead.\nPy_QuietFlag\n:\nUse\nPyConfig.quiet\ninstead.\nPy_InteractiveFlag\n:\nUse\nPyConfig.interactive\ninstead.\nPy_InspectFlag\n:\nUse\nPyConfig.inspect\ninstead.\nPy_OptimizeFlag\n:\nUse\nPyConfig.optimization_level\ninstead.\nPy_NoSiteFlag\n:\nUse\nPyConfig.site_import\ninstead.\nPy_BytesWarningFlag\n:\nUse\nPyConfig.bytes_warning\ninstead.\nPy_FrozenFlag\n:\nUse\nPyConfig.pathconfig_warnings\ninstead.\nPy_IgnoreEnvironmentFlag\n:\nUse\nPyConfig.use_environment\ninstead.\nPy_DontWriteBytecodeFlag\n:\nUse\nPyConfig.write_bytecode\ninstead.\nPy_NoUserSiteDirectory\n:\nUse\nPyConfig.user_site_directory\ninstead.\nPy_UnbufferedStdioFlag\n:\nUse\nPyConfig.buffered_stdio\ninstead.\nPy_HashRandomizationFlag\n:\nUse\nPyConfig.use_hash_seed\nand\nPyConfig.hash_seed\ninstead.\nPy_IsolatedFlag\n:\nUse\nPyConfig.isolated\ninstead.\nPy_LegacyWindowsFSEncodingFlag\n:\nUse\nPyPreConfig.legacy_windows_fs_encoding\ninstead.\nPy_LegacyWindowsStdioFlag\n:\nUse\nPyConfig.legacy_windows_stdio\ninstead.\nPy_FileSystemDefaultEncoding\n:\nUse\nPyConfig.filesystem_encoding\ninstead.\nPy_HasFileSystemDefaultEncoding\n:\nUse\nPyConfig.filesystem_encoding\ninstead.\nPy_FileSystemDefaultEncodeErrors\n:\nUse\nPyConfig.filesystem_errors\ninstead.\nPy_UTF8Mode\n:\nUse\nPyPreConfig.utf8_mode\ninstead.\n(see\nPy_PreInitialize()\n)\nThe\nPy_InitializeFromConfig()\nAPI should be used with\nPyConfig\ninstead.\nPending Removal in Python 3.15\n¶\nThe bundled copy of\nlibmpdecimal\n.\nThe\nPyImport_ImportModuleNoBlock()\n:\nUse\nPyImport_ImportModule()\ninstead.\nPyWeakref_GetObject()\nand\nPyWeakref_GET_OBJECT()\n:\nUse\nPyWeakref_GetRef()\ninstead.\nPy_UNICODE\ntype and the\nPy_UNICODE_WIDE\nmacro:\nUse\nwchar_t\ninstead.\nPython initialization functions:\nPySys_ResetWarnOptions()\n:\nClear\nsys.warnoptions\nand\nwarnings.filters\ninstead.\nPy_GetExecPrefix()\n:\nGet\nsys.base_exec_prefix\nand\nsys.exec_prefix\ninstead.\nPy_GetPath()\n:\nGet\nsys.path\ninstead.\nPy_GetPrefix()\n:\nGet\nsys.base_prefix\nand\nsys.prefix\ninstead.\nPy_GetProgramFullPath()\n:\nGet\nsys.executable\ninstead.\nPy_GetProgramName()\n:\nGet\nsys.executable\ninstead.\nPy_GetPythonHome()\n:\nGet\nPyConfig.home\nor the\nPYTHONHOME\nenvironment variable instead.\nPending Removal in Future Versions\n¶\nThe following APIs are deprecated and will be removed,\nalthough there is currently no date scheduled for their removal.\nPy_TPFLAGS_HAVE_FINALIZE\n:\nUnneeded since Python 3.8.\nPyErr_Fetch()\n:\nUse\nPyErr_GetRaisedException()\ninstead.\nPyErr_NormalizeException()\n:\nUse\nPyErr_GetRaisedException()\ninstead.\nPyErr_Restore()\n:\nUse\nPyErr_SetRaisedException()\ninstead.\nPyModule_GetFilename()\n:\nUse\nPyModule_GetFilenameObject()\ninstead.\nPyOS_AfterFork()\n:\nUse\nPyOS_AfterFork_Child()\ninstead.\nPySlice_GetIndicesEx()\n:\nUse\nPySlice_Unpack()\nand\nPySlice_AdjustIndices()\ninstead.\nPyUnicode_AsDecodedObject()\n:\nUse\nPyCodec_Decode()\ninstead.\nPyUnicode_AsDecodedUnicode()\n:\nUse\nPyCodec_Decode()\ninstead.\nPyUnicode_AsEncodedObject()\n:\nUse\nPyCodec_Encode()\ninstead.\nPyUnicode_AsEncodedUnicode()\n:\nUse\nPyCodec_Encode()\ninstead.\nPyUnicode_READY()\n:\nUnneeded since Python 3.12\nPyErr_Display()\n:\nUse\nPyErr_DisplayException()\ninstead.\n_PyErr_ChainExceptions()\n:\nUse\n_PyErr_ChainExceptions1()\ninstead.\nPyBytesObject.ob_shash\nmember:\ncall\nPyObject_Hash()\ninstead.\nPyDictObject.ma_version_tag\nmember.\nThread Local Storage (TLS) API:\nPyThread_create_key()\n:\nUse\nPyThread_tss_alloc()\ninstead.\nPyThread_delete_key()\n:\nUse\nPyThread_tss_free()\ninstead.\nPyThread_set_key_value()\n:\nUse\nPyThread_tss_set()\ninstead.\nPyThread_get_key_value()\n:\nUse\nPyThread_tss_get()\ninstead.\nPyThread_delete_key_value()\n:\nUse\nPyThread_tss_delete()\ninstead.\nPyThread_ReInitTLS()\n:\nUnneeded since Python 3.7.\nBuild Changes\n¶\narm64-apple-ios\nand\narm64-apple-ios-simulator\nare both\nnow\nPEP 11\ntier 3 platforms.\n(\nPEP 730\nwritten\nand implementation contributed by Russell Keith-Magee in\ngh-114099\n.)\naarch64-linux-android\nand\nx86_64-linux-android\nare both\nnow\nPEP 11\ntier 3 platforms.\n(\nPEP 738\nwritten\nand implementation contributed by Malcolm Smith in\ngh-116622\n.)\nwasm32-wasi\nis now a\nPEP 11\ntier 2 platform.\n(Contributed by Brett Cannon in\ngh-115192\n.)\nwasm32-emscripten\nis no longer a\nPEP 11\nsupported platform.\n(Contributed by Brett Cannon in\ngh-115192\n.)\nBuilding CPython now requires a compiler with support for the C11 atomic\nlibrary, GCC built-in atomic functions, or MSVC interlocked intrinsics.\nAutoconf 2.71 and aclocal 1.16.5 are now required to regenerate\nthe\nconfigure\nscript.\n(Contributed by Christian Heimes in\ngh-89886\nand by Victor Stinner in\ngh-112090\n.)\nSQLite 3.15.2 or newer is required to build\nthe\nsqlite3\nextension module.\n(Contributed by Erlend Aasland in\ngh-105875\n.)\nCPython now bundles the\nmimalloc library\nby default.\nIt is licensed under the MIT license;\nsee\nmimalloc license\n.\nThe bundled mimalloc has custom changes, see\ngh-113141\nfor details.\n(Contributed by Dino Viehland in\ngh-109914\n.)\nThe\nconfigure\noption\n--with-system-libmpdec\nnow defaults to\nyes\n.\nThe bundled copy of\nlibmpdecimal\nwill be removed in Python 3.15.\nPython built with\nconfigure\n--with-trace-refs\n(tracing references) is now ABI compatible with the Python release build\nand\ndebug build\n.\n(Contributed by Victor Stinner in\ngh-108634\n.)\nOn POSIX systems, the pkg-config (\n.pc\n) filenames now include the ABI\nflags.  For example, the free-threaded build generates\npython-3.13t.pc\nand the debug build generates\npython-3.13d.pc\n.\nThe\nerrno\n,\nfcntl\n,\ngrp\n,\nmd5\n,\npwd\n,\nresource\n,\ntermios\n,\nwinsound\n,\n_ctypes_test\n,\n_multiprocessing.posixshmem\n,\n_scproxy\n,\n_stat\n,\n_statistics\n,\n_testconsole\n,\n_testimportmultiple\nand\n_uuid\nC extensions are now built with the\nlimited C API\n.\n(Contributed by Victor Stinner in\ngh-85283\n.)\nPorting to Python 3.13\n¶\nThis section lists previously described changes and other bugfixes\nthat may require changes to your code.\nChanges in the Python API\n¶\nPEP 667\nintroduces several changes\nto the semantics of\nlocals()\nand\nf_locals\n:\nCalling\nlocals()\nin an\noptimized scope\nnow produces an\nindependent snapshot on each call, and hence no longer implicitly updates\npreviously returned references. Obtaining the legacy CPython behavior now\nrequires explicit calls to update the initially returned dictionary with the\nresults of subsequent calls to\nlocals()\n. Code execution functions that\nimplicitly target\nlocals()\n(such as\nexec\nand\neval\n) must be\npassed an explicit namespace to access their results in an optimized scope.\n(Changed as part of\nPEP 667\n.)\nCalling\nlocals()\nfrom a comprehension at module or class scope\n(including via\nexec\nor\neval\n) once more behaves as if the comprehension\nwere running as an independent nested function (i.e. the local variables from\nthe containing scope are not included). In Python 3.12, this had changed\nto include the local variables from the containing scope when implementing\nPEP 709\n. (Changed as part of\nPEP 667\n.)\nAccessing\nFrameType.f_locals\nin an\noptimized scope\nnow returns a write-through proxy rather than a\nsnapshot that gets updated at ill-specified times. If a snapshot is desired,\nit must be created explicitly with\ndict\nor the proxy’s\n.copy()\nmethod.\n(Changed as part of\nPEP 667\n.)\nfunctools.partial\nnow emits a\nFutureWarning\nwhen used as a method.\nThe behavior will change in future Python versions.\nWrap it in\nstaticmethod()\nif you want to preserve the old behavior.\n(Contributed by Serhiy Storchaka in\ngh-121027\n.)\nAn\nOSError\nis now raised by\ngetpass.getuser()\nfor any failure to retrieve a username,\ninstead of\nImportError\non non-Unix platforms\nor\nKeyError\non Unix platforms where the password database is empty.\nThe value of the\nmode\nattribute of\ngzip.GzipFile\nis now a string (\n'rb'\nor\n'wb'\n) instead of an integer (\n1\nor\n2\n).\nThe value of the\nmode\nattribute of the readable file-like object\nreturned by\nzipfile.ZipFile.open()\nis now\n'rb'\ninstead of\n'r'\n.\n(Contributed by Serhiy Storchaka in\ngh-115961\n.)\nmailbox.Maildir\nnow ignores files with a leading dot (\n.\n).\n(Contributed by Zackery Spytz in\ngh-65559\n.)\npathlib.Path.glob()\nand\nrglob()\nnow return both\nfiles and directories if a pattern that ends with “\n**\n” is given,\nrather than directories only.\nAdd a trailing slash to keep the previous behavior and only match directories.\nThe\nthreading\nmodule now expects the\n_thread\nmodule\nto have an\n_is_main_interpreter()\nfunction.\nThis function takes no arguments and returns\nTrue\nif the current interpreter is the main interpreter.\nAny library or application that provides a custom\n_thread\nmodule\nmust provide\n_is_main_interpreter()\n,\njust like the module’s other “private” attributes.\n(\ngh-112826\n.)\nChanges in the C API\n¶\nPython.h\nno longer includes the\n<ieeefp.h>\nstandard header. It was\nincluded for the\nfinite()\nfunction which is now provided by the\n<math.h>\nheader. It should now be included explicitly if needed. Remove\nalso the\nHAVE_IEEEFP_H\nmacro.\n(Contributed by Victor Stinner in\ngh-108765\n.)\nPython.h\nno longer includes these standard header files:\n<time.h>\n,\n<sys/select.h>\nand\n<sys/time.h>\n. If needed, they should now be\nincluded explicitly. For example,\n<time.h>\nprovides the\nclock()\nand\ngmtime()\nfunctions,\n<sys/select.h>\nprovides the\nselect()\nfunction, and\n<sys/time.h>\nprovides the\nfutimes()\n,\ngettimeofday()\nand\nsetitimer()\nfunctions.\n(Contributed by Victor Stinner in\ngh-108765\n.)\nOn Windows,\nPython.h\nno longer includes the\n<stddef.h>\nstandard\nheader file. If needed, it should now be included explicitly. For example, it\nprovides\noffsetof()\nfunction, and\nsize_t\nand\nptrdiff_t\ntypes.\nIncluding\n<stddef.h>\nexplicitly was already needed by all other\nplatforms, the\nHAVE_STDDEF_H\nmacro is only defined on Windows.\n(Contributed by Victor Stinner in\ngh-108765\n.)\nIf the\nPy_LIMITED_API\nmacro is defined,\nPy_BUILD_CORE\n,\nPy_BUILD_CORE_BUILTIN\nand\nPy_BUILD_CORE_MODULE\nmacros\nare now undefined by\n<Python.h>\n.\n(Contributed by Victor Stinner in\ngh-85283\n.)\nThe old trashcan macros\nPy_TRASHCAN_SAFE_BEGIN\nand\nPy_TRASHCAN_SAFE_END\nwere removed. They should be replaced by the new macros\nPy_TRASHCAN_BEGIN\nand\nPy_TRASHCAN_END\n.\nA\ntp_dealloc\nfunction that has the old macros, such as:\nstatic\nvoid\nmytype_dealloc\n(\nmytype\n*\np\n)\n{\nPyObject_GC_UnTrack\n(\np\n);\nPy_TRASHCAN_SAFE_BEGIN\n(\np\n);\n...\nPy_TRASHCAN_SAFE_END\n}\nshould migrate to the new macros as follows:\nstatic\nvoid\nmytype_dealloc\n(\nmytype\n*\np\n)\n{\nPyObject_GC_UnTrack\n(\np\n);\nPy_TRASHCAN_BEGIN\n(\np\n,\nmytype_dealloc\n)\n...\nPy_TRASHCAN_END\n}\nNote that\nPy_TRASHCAN_BEGIN\nhas a second argument which\nshould be the deallocation function it is in. The new macros were\nadded in Python 3.8 and the old macros were deprecated in Python 3.11.\n(Contributed by Irit Katriel in\ngh-105111\n.)\nPEP 667\nintroduces several changes\nto frame-related functions:\nThe effects of mutating the dictionary returned from\nPyEval_GetLocals()\nin an\noptimized scope\nhave changed.\nNew dict entries added this way will now\nonly\nbe visible to\nsubsequent\nPyEval_GetLocals()\ncalls in that frame,\nas\nPyFrame_GetLocals()\n,\nlocals()\n,\nand\nFrameType.f_locals\nno longer access\nthe same underlying cached dictionary.\nChanges made to entries for actual variable names and names added via\nthe write-through proxy interfaces will be overwritten on subsequent calls\nto\nPyEval_GetLocals()\nin that frame.\nThe recommended code update depends on how the function was being used,\nso refer to the deprecation notice on the function for details.\nCalling\nPyFrame_GetLocals()\nin an\noptimized scope\nnow returns a write-through proxy rather than a snapshot\nthat gets updated at ill-specified times.\nIf a snapshot is desired, it must be created explicitly\n(e.g. with\nPyDict_Copy()\n),\nor by calling the new\nPyEval_GetFrameLocals()\nAPI.\nPyFrame_FastToLocals()\nand\nPyFrame_FastToLocalsWithError()\nno longer have any effect.\nCalling these functions has been redundant since Python 3.11,\nwhen\nPyFrame_GetLocals()\nwas first introduced.\nPyFrame_LocalsToFast()\nno longer has any effect.\nCalling this function is redundant now that\nPyFrame_GetLocals()\nreturns a write-through proxy for\noptimized scopes\n.\nPython 3.13 removed many private functions. Some of them can be replaced using these\nalternatives:\n_PyDict_Pop()\n:\nPyDict_Pop()\nor\nPyDict_PopString()\n;\n_PyDict_GetItemWithError()\n:\nPyDict_GetItemRef()\n;\n_PyErr_WriteUnraisableMsg()\n:\nPyErr_FormatUnraisable()\n;\n_PyEval_SetTrace()\n:\nPyEval_SetTrace()\nor\nPyEval_SetTraceAllThreads()\n;\n_PyList_Extend()\n:\nPyList_Extend()\n;\n_PyLong_AsInt()\n:\nPyLong_AsInt()\n;\n_PyMem_RawStrdup()\n:\nstrdup()\n;\n_PyMem_Strdup()\n:\nstrdup()\n;\n_PyObject_ClearManagedDict()\n:\nPyObject_ClearManagedDict()\n;\n_PyObject_VisitManagedDict()\n:\nPyObject_VisitManagedDict()\n;\n_PyThreadState_UncheckedGet()\n:\nPyThreadState_GetUnchecked()\n;\n_PyTime_AsSecondsDouble()\n:\nPyTime_AsSecondsDouble()\n;\n_PyTime_GetMonotonicClock()\n:\nPyTime_Monotonic()\nor\nPyTime_MonotonicRaw()\n;\n_PyTime_GetPerfCounter()\n:\nPyTime_PerfCounter()\nor\nPyTime_PerfCounterRaw()\n;\n_PyTime_GetSystemClock()\n:\nPyTime_Time()\nor\nPyTime_TimeRaw()\n;\n_PyTime_MAX\n:\nPyTime_MAX\n;\n_PyTime_MIN\n:\nPyTime_MIN\n;\n_PyTime_t\n:\nPyTime_t\n;\n_Py_HashPointer()\n:\nPy_HashPointer()\n;\n_Py_IsFinalizing()\n:\nPy_IsFinalizing()\n.\nThe\npythoncapi-compat project\ncan be used to get most of these new\nfunctions on Python 3.12 and older.\nRegression Test Changes\n¶\nPython built with\nconfigure\n--with-pydebug\nnow\nsupports a\n-X\npresite=package.module\ncommand-line\noption. If used, it specifies a module that should be imported early\nin the lifecycle of the interpreter, before\nsite.py\nis executed.\n(Contributed by Łukasz Langa in\ngh-110769\n.)\nNotable changes in 3.13.1\n¶\nsys\n¶\nThe previously undocumented special function\nsys.getobjects()\n,\nwhich only exists in specialized builds of Python, may now return objects\nfrom other interpreters than the one it’s called in.\nNotable changes in 3.13.4\n¶\nos.path\n¶\nThe\nstrict\nparameter to\nos.path.realpath()\naccepts a new value,\nos.path.ALLOW_MISSING\n.\nIf used, errors other than\nFileNotFoundError\nwill be re-raised;\nthe resulting path can be missing but it will be free of symlinks.\n(Contributed by Petr Viktorin for\nCVE 2025-4517\n.)\ntarfile\n¶\ndata_filter()\nnow normalizes symbolic link targets in order to\navoid path traversal attacks.Add commentMore actions\n(Contributed by Petr Viktorin in\ngh-127987\nand\nCVE 2025-4138\n.)\nextractall()\nnow skips fixing up directory attributes\nwhen a directory was removed or replaced by another kind of file.\n(Contributed by Petr Viktorin in\ngh-127987\nand\nCVE 2024-12718\n.)\nextract()\nand\nextractall()\nnow (re-)apply the extraction filter when substituting a link (hard or\nsymbolic) with a copy of another archive member, and when fixing up\ndirectory attributes.\nThe former raises a new exception,\nLinkFallbackError\n.\n(Contributed by Petr Viktorin for\nCVE 2025-4330\nand\nCVE 2024-12718\n.)\nextract()\nand\nextractall()\nno longer extract rejected members when\nerrorlevel()\nis zero.\n(Contributed by Matt Prodani and Petr Viktorin in\ngh-112887\nand\nCVE 2025-4435\n.)\nTable of Contents\nWhat’s New In Python 3.13\nSummary – Release Highlights\nNew Features\nA better interactive interpreter\nImproved error messages\nFree-threaded CPython\nAn experimental just-in-time (JIT) compiler\nDefined mutation semantics for\nlocals()\nSupport for mobile platforms\nOther Language Changes\nNew Modules\nImproved Modules\nargparse\narray\nast\nasyncio\nbase64\ncompileall\nconcurrent.futures\nconfigparser\ncopy\nctypes\ndbm\ndis\ndoctest\nemail\nenum\nfractions\nglob\nimportlib\nio\nipaddress\nitertools\nmarshal\nmath\nmimetypes\nmmap\nmultiprocessing\nos\nos.path\npathlib\npdb\nqueue\nrandom\nre\nshutil\nsite\nsqlite3\nssl\nstatistics\nsubprocess\nsys\ntempfile\ntime\ntkinter\ntraceback\ntypes\ntyping\nunicodedata\nvenv\nwarnings\nxml\nzipimport\nOptimizations\nRemoved Modules And APIs\nPEP 594: Remove “dead batteries” from the standard library\n2to3\nbuiltins\nconfigparser\nimportlib.metadata\nlocale\nopcode\noptparse\npathlib\nre\ntkinter.tix\nturtle\ntyping\nunittest\nurllib\nwebbrowser\nNew Deprecations\nPending Removal in Python 3.14\nPending Removal in Python 3.15\nPending removal in Python 3.16\nPending Removal in Future Versions\nCPython Bytecode Changes\nC API Changes\nNew Features\nChanged C APIs\nLimited C API Changes\nRemoved C APIs\nDeprecated C APIs\nPending Removal in Python 3.14\nPending Removal in Python 3.15\nPending Removal in Future Versions\nBuild Changes\nPorting to Python 3.13\nChanges in the Python API\nChanges in the C API\nRegression Test Changes\nNotable changes in 3.13.1\nsys\nNotable changes in 3.13.4\nos.path\ntarfile\nPrevious topic\nWhat’s New in Python\nNext topic\nWhat’s New In Python 3.12\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nWhat’s New in Python\n»\nWhat’s New In Python 3.13\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nWhat’s New in Python\n|\nTheme\nAuto\nLight\nDark\n|\nWhat’s New in Python\n¶\nThe “What’s New in Python” series of essays takes tours through the most\nimportant changes between major Python versions.  They are a “must read” for\nanyone wishing to stay up-to-date after a new release.\nWhat’s New In Python 3.13\nSummary – Release Highlights\nNew Features\nOther Language Changes\nNew Modules\nImproved Modules\nOptimizations\nRemoved Modules And APIs\nNew Deprecations\nCPython Bytecode Changes\nC API Changes\nBuild Changes\nPorting to Python 3.13\nRegression Test Changes\nNotable changes in 3.13.1\nNotable changes in 3.13.4\nWhat’s New In Python 3.12\nSummary – Release highlights\nNew Features\nNew Features Related to Type Hints\nOther Language Changes\nNew Modules\nImproved Modules\nOptimizations\nCPython bytecode changes\nDemos and Tools\nDeprecated\nRemoved\nPorting to Python 3.12\nBuild Changes\nC API Changes\nWhat’s New In Python 3.11\nSummary – Release highlights\nNew Features\nNew Features Related to Type Hints\nOther Language Changes\nOther CPython Implementation Changes\nNew Modules\nImproved Modules\nOptimizations\nFaster CPython\nCPython bytecode changes\nDeprecated\nPending Removal in Python 3.12\nRemoved\nPorting to Python 3.11\nBuild Changes\nC API Changes\nNotable changes in 3.11.4\nNotable changes in 3.11.5\nWhat’s New In Python 3.10\nSummary – Release highlights\nNew Features\nNew Features Related to Type Hints\nOther Language Changes\nNew Modules\nImproved Modules\nOptimizations\nDeprecated\nRemoved\nPorting to Python 3.10\nCPython bytecode changes\nBuild Changes\nC API Changes\nNotable security feature in 3.10.7\nNotable security feature in 3.10.8\nNotable changes in 3.10.12\nWhat’s New In Python 3.9\nSummary – Release highlights\nYou should check for DeprecationWarning in your code\nNew Features\nOther Language Changes\nNew Modules\nImproved Modules\nOptimizations\nDeprecated\nRemoved\nPorting to Python 3.9\nBuild Changes\nC API Changes\nNotable changes in Python 3.9.1\nNotable changes in Python 3.9.2\nNotable changes in Python 3.9.3\nNotable changes in Python 3.9.5\nNotable security feature in 3.9.14\nNotable changes in 3.9.17\nWhat’s New In Python 3.8\nSummary – Release highlights\nNew Features\nOther Language Changes\nNew Modules\nImproved Modules\nOptimizations\nBuild and C API Changes\nDeprecated\nAPI and Feature Removals\nPorting to Python 3.8\nNotable changes in Python 3.8.1\nNotable changes in Python 3.8.2\nNotable changes in Python 3.8.3\nNotable changes in Python 3.8.8\nNotable changes in Python 3.8.9\nNotable changes in Python 3.8.10\nNotable changes in Python 3.8.10\nNotable changes in Python 3.8.12\nNotable security feature in 3.8.14\nNotable changes in 3.8.17\nWhat’s New In Python 3.7\nSummary – Release Highlights\nNew Features\nOther Language Changes\nNew Modules\nImproved Modules\nC API Changes\nBuild Changes\nOptimizations\nOther CPython Implementation Changes\nDeprecated Python Behavior\nDeprecated Python modules, functions and methods\nDeprecated functions and types of the C API\nPlatform Support Removals\nAPI and Feature Removals\nModule Removals\nWindows-only Changes\nPorting to Python 3.7\nNotable changes in Python 3.7.1\nNotable changes in Python 3.7.2\nNotable changes in Python 3.7.6\nNotable changes in Python 3.7.10\nNotable changes in Python 3.7.11\nNotable security feature in 3.7.14\nWhat’s New In Python 3.6\nSummary – Release highlights\nNew Features\nOther Language Changes\nNew Modules\nImproved Modules\nOptimizations\nBuild and C API Changes\nOther Improvements\nDeprecated\nRemoved\nPorting to Python 3.6\nNotable changes in Python 3.6.2\nNotable changes in Python 3.6.4\nNotable changes in Python 3.6.5\nNotable changes in Python 3.6.7\nNotable changes in Python 3.6.10\nNotable changes in Python 3.6.13\nNotable changes in Python 3.6.14\nWhat’s New In Python 3.5\nSummary – Release highlights\nNew Features\nOther Language Changes\nNew Modules\nImproved Modules\nOther module-level changes\nOptimizations\nBuild and C API Changes\nDeprecated\nRemoved\nPorting to Python 3.5\nNotable changes in Python 3.5.4\nWhat’s New In Python 3.4\nSummary – Release Highlights\nNew Features\nNew Modules\nImproved Modules\nCPython Implementation Changes\nDeprecated\nRemoved\nPorting to Python 3.4\nChanged in 3.4.3\nWhat’s New In Python 3.3\nSummary – Release highlights\nPEP 405: Virtual Environments\nPEP 420: Implicit Namespace Packages\nPEP 3118: New memoryview implementation and buffer protocol documentation\nPEP 393: Flexible String Representation\nPEP 397: Python Launcher for Windows\nPEP 3151: Reworking the OS and IO exception hierarchy\nPEP 380: Syntax for Delegating to a Subgenerator\nPEP 409: Suppressing exception context\nPEP 414: Explicit Unicode literals\nPEP 3155: Qualified name for classes and functions\nPEP 412: Key-Sharing Dictionary\nPEP 362: Function Signature Object\nPEP 421: Adding sys.implementation\nUsing importlib as the Implementation of Import\nOther Language Changes\nA Finer-Grained Import Lock\nBuiltin functions and types\nNew Modules\nImproved Modules\nOptimizations\nBuild and C API Changes\nDeprecated\nPorting to Python 3.3\nWhat’s New In Python 3.2\nPEP 384: Defining a Stable ABI\nPEP 389: Argparse Command Line Parsing Module\nPEP 391:  Dictionary Based Configuration for Logging\nPEP 3148:  The\nconcurrent.futures\nmodule\nPEP 3147:  PYC Repository Directories\nPEP 3149: ABI Version Tagged .so Files\nPEP 3333: Python Web Server Gateway Interface v1.0.1\nOther Language Changes\nNew, Improved, and Deprecated Modules\nMulti-threading\nOptimizations\nUnicode\nCodecs\nDocumentation\nIDLE\nCode Repository\nBuild and C API Changes\nPorting to Python 3.2\nWhat’s New In Python 3.1\nPEP 372: Ordered Dictionaries\nPEP 378: Format Specifier for Thousands Separator\nOther Language Changes\nNew, Improved, and Deprecated Modules\nOptimizations\nIDLE\nBuild and C API Changes\nPorting to Python 3.1\nWhat’s New In Python 3.0\nCommon Stumbling Blocks\nOverview Of Syntax Changes\nChanges Already Present In Python 2.6\nLibrary Changes\nPEP 3101\n: A New Approach To String Formatting\nChanges To Exceptions\nMiscellaneous Other Changes\nBuild and C API Changes\nPerformance\nPorting To Python 3.0\nWhat’s New in Python 2.7\nThe Future for Python 2.x\nChanges to the Handling of Deprecation Warnings\nPython 3.1 Features\nPEP 372: Adding an Ordered Dictionary to collections\nPEP 378: Format Specifier for Thousands Separator\nPEP 389: The argparse Module for Parsing Command Lines\nPEP 391: Dictionary-Based Configuration For Logging\nPEP 3106: Dictionary Views\nPEP 3137: The memoryview Object\nOther Language Changes\nNew and Improved Modules\nBuild and C API Changes\nOther Changes and Fixes\nPorting to Python 2.7\nNew Features Added to Python 2.7 Maintenance Releases\nAcknowledgements\nWhat’s New in Python 2.6\nPython 3.0\nChanges to the Development Process\nPEP 343: The ‘with’ statement\nPEP 366: Explicit Relative Imports From a Main Module\nPEP 370: Per-user\nsite-packages\nDirectory\nPEP 371: The\nmultiprocessing\nPackage\nPEP 3101: Advanced String Formatting\nPEP 3105:\nprint\nAs a Function\nPEP 3110: Exception-Handling Changes\nPEP 3112: Byte Literals\nPEP 3116: New I/O Library\nPEP 3118: Revised Buffer Protocol\nPEP 3119: Abstract Base Classes\nPEP 3127: Integer Literal Support and Syntax\nPEP 3129: Class Decorators\nPEP 3141: A Type Hierarchy for Numbers\nOther Language Changes\nNew and Improved Modules\nDeprecations and Removals\nBuild and C API Changes\nPorting to Python 2.6\nAcknowledgements\nWhat’s New in Python 2.5\nPEP 308: Conditional Expressions\nPEP 309: Partial Function Application\nPEP 314: Metadata for Python Software Packages v1.1\nPEP 328: Absolute and Relative Imports\nPEP 338: Executing Modules as Scripts\nPEP 341: Unified try/except/finally\nPEP 342: New Generator Features\nPEP 343: The ‘with’ statement\nPEP 352: Exceptions as New-Style Classes\nPEP 353: Using ssize_t as the index type\nPEP 357: The ‘__index__’ method\nOther Language Changes\nNew, Improved, and Removed Modules\nBuild and C API Changes\nPorting to Python 2.5\nAcknowledgements\nWhat’s New in Python 2.4\nPEP 218: Built-In Set Objects\nPEP 237: Unifying Long Integers and Integers\nPEP 289: Generator Expressions\nPEP 292: Simpler String Substitutions\nPEP 318: Decorators for Functions and Methods\nPEP 322: Reverse Iteration\nPEP 324: New subprocess Module\nPEP 327: Decimal Data Type\nPEP 328: Multi-line Imports\nPEP 331: Locale-Independent Float/String Conversions\nOther Language Changes\nNew, Improved, and Deprecated Modules\nBuild and C API Changes\nPorting to Python 2.4\nAcknowledgements\nWhat’s New in Python 2.3\nPEP 218: A Standard Set Datatype\nPEP 255: Simple Generators\nPEP 263: Source Code Encodings\nPEP 273: Importing Modules from ZIP Archives\nPEP 277: Unicode file name support for Windows NT\nPEP 278: Universal Newline Support\nPEP 279: enumerate()\nPEP 282: The logging Package\nPEP 285: A Boolean Type\nPEP 293: Codec Error Handling Callbacks\nPEP 301: Package Index and Metadata for Distutils\nPEP 302: New Import Hooks\nPEP 305: Comma-separated Files\nPEP 307: Pickle Enhancements\nExtended Slices\nOther Language Changes\nNew, Improved, and Deprecated Modules\nPymalloc: A Specialized Object Allocator\nBuild and C API Changes\nOther Changes and Fixes\nPorting to Python 2.3\nAcknowledgements\nWhat’s New in Python 2.2\nIntroduction\nPEPs 252 and 253: Type and Class Changes\nPEP 234: Iterators\nPEP 255: Simple Generators\nPEP 237: Unifying Long Integers and Integers\nPEP 238: Changing the Division Operator\nUnicode Changes\nPEP 227: Nested Scopes\nNew and Improved Modules\nInterpreter Changes and Fixes\nOther Changes and Fixes\nAcknowledgements\nWhat’s New in Python 2.1\nIntroduction\nPEP 227: Nested Scopes\nPEP 236: __future__ Directives\nPEP 207: Rich Comparisons\nPEP 230: Warning Framework\nPEP 229: New Build System\nPEP 205: Weak References\nPEP 232: Function Attributes\nPEP 235: Importing Modules on Case-Insensitive Platforms\nPEP 217: Interactive Display Hook\nPEP 208: New Coercion Model\nPEP 241: Metadata in Python Packages\nNew and Improved Modules\nOther Changes and Fixes\nAcknowledgements\nWhat’s New in Python 2.0\nIntroduction\nWhat About Python 1.6?\nNew Development Process\nUnicode\nList Comprehensions\nAugmented Assignment\nString Methods\nGarbage Collection of Cycles\nOther Core Changes\nPorting to 2.0\nExtending/Embedding Changes\nDistutils: Making Modules Easy to Install\nXML Modules\nModule changes\nNew modules\nIDLE Improvements\nDeleted and Deprecated Modules\nAcknowledgements\nThe “Changelog” is an HTML version of the\nfile built\nfrom the contents of the\nMisc/NEWS.d\ndirectory tree, which contains\nall\nnontrivial changes\nto Python for the current version.\nChangelog\nPython next\nPython 3.13.5 final\nPython 3.13.4 final\nPython 3.13.3 final\nPython 3.13.2 final\nPython 3.13.1 final\nPython 3.13.0 final\nPython 3.13.0 release candidate 3\nPython 3.13.0 release candidate 2\nPython 3.13.0 release candidate 1\nPython 3.13.0 beta 4\nPython 3.13.0 beta 3\nPython 3.13.0 beta 2\nPython 3.13.0 beta 1\nPython 3.13.0 alpha 6\nPython 3.13.0 alpha 5\nPython 3.13.0 alpha 4\nPython 3.13.0 alpha 3\nPython 3.13.0 alpha 2\nPython 3.13.0 alpha 1\nPython 3.12.0 beta 1\nPython 3.12.0 alpha 7\nPython 3.12.0 alpha 6\nPython 3.12.0 alpha 5\nPython 3.12.0 alpha 4\nPython 3.12.0 alpha 3\nPython 3.12.0 alpha 2\nPython 3.12.0 alpha 1\nPython 3.11.0 beta 1\nPython 3.11.0 alpha 7\nPython 3.11.0 alpha 6\nPython 3.11.0 alpha 5\nPython 3.11.0 alpha 4\nPython 3.11.0 alpha 3\nPython 3.11.0 alpha 2\nPython 3.11.0 alpha 1\nPython 3.10.0 beta 1\nPython 3.10.0 alpha 7\nPython 3.10.0 alpha 6\nPython 3.10.0 alpha 5\nPython 3.10.0 alpha 4\nPython 3.10.0 alpha 3\nPython 3.10.0 alpha 2\nPython 3.10.0 alpha 1\nPython 3.9.0 beta 1\nPython 3.9.0 alpha 6\nPython 3.9.0 alpha 5\nPython 3.9.0 alpha 4\nPython 3.9.0 alpha 3\nPython 3.9.0 alpha 2\nPython 3.9.0 alpha 1\nPython 3.8.0 beta 1\nPython 3.8.0 alpha 4\nPython 3.8.0 alpha 3\nPython 3.8.0 alpha 2\nPython 3.8.0 alpha 1\nPython 3.7.0 final\nPython 3.7.0 release candidate 1\nPython 3.7.0 beta 5\nPython 3.7.0 beta 4\nPython 3.7.0 beta 3\nPython 3.7.0 beta 2\nPython 3.7.0 beta 1\nPython 3.7.0 alpha 4\nPython 3.7.0 alpha 3\nPython 3.7.0 alpha 2\nPython 3.7.0 alpha 1\nPython 3.6.6 final\nPython 3.6.6 release candidate 1\nPython 3.6.5 final\nPython 3.6.5 release candidate 1\nPython 3.6.4 final\nPython 3.6.4 release candidate 1\nPython 3.6.3 final\nPython 3.6.3 release candidate 1\nPython 3.6.2 final\nPython 3.6.2 release candidate 2\nPython 3.6.2 release candidate 1\nPython 3.6.1 final\nPython 3.6.1 release candidate 1\nPython 3.6.0 final\nPython 3.6.0 release candidate 2\nPython 3.6.0 release candidate 1\nPython 3.6.0 beta 4\nPython 3.6.0 beta 3\nPython 3.6.0 beta 2\nPython 3.6.0 beta 1\nPython 3.6.0 alpha 4\nPython 3.6.0 alpha 3\nPython 3.6.0 alpha 2\nPython 3.6.0 alpha 1\nPython 3.5.5 final\nPython 3.5.5 release candidate 1\nPython 3.5.4 final\nPython 3.5.4 release candidate 1\nPython 3.5.3 final\nPython 3.5.3 release candidate 1\nPython 3.5.2 final\nPython 3.5.2 release candidate 1\nPython 3.5.1 final\nPython 3.5.1 release candidate 1\nPython 3.5.0 final\nPython 3.5.0 release candidate 4\nPython 3.5.0 release candidate 3\nPython 3.5.0 release candidate 2\nPython 3.5.0 release candidate 1\nPython 3.5.0 beta 4\nPython 3.5.0 beta 3\nPython 3.5.0 beta 2\nPython 3.5.0 beta 1\nPython 3.5.0 alpha 4\nPython 3.5.0 alpha 3\nPython 3.5.0 alpha 2\nPython 3.5.0 alpha 1\nPrevious topic\nPython Documentation contents\nNext topic\nWhat’s New In Python 3.13\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nWhat’s New in Python\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of PYTHON.\nStart by understanding the core concepts from this content.\n\nDOC START:\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nPython Setup and Usage\n|\nTheme\nAuto\nLight\nDark\n|\nPython Setup and Usage\n¶\nThis part of the documentation is devoted to general information on the setup\nof the Python environment on different platforms, the invocation of the\ninterpreter and things that make working with Python easier.\n1. Command line and environment\n1.1. Command line\n1.2. Environment variables\n2. Using Python on Unix platforms\n2.1. Getting and installing the latest version of Python\n2.2. Building Python\n2.3. Python-related paths and files\n2.4. Miscellaneous\n2.5. Custom OpenSSL\n3. Configure Python\n3.1. Build Requirements\n3.2. Generated files\n3.3. Configure Options\n3.4. Python Build System\n3.5. Compiler and linker flags\n4. Using Python on Windows\n4.1. The full installer\n4.2. The Microsoft Store package\n4.3. The nuget.org packages\n4.4. The embeddable package\n4.5. Alternative bundles\n4.6. Configuring Python\n4.7. UTF-8 mode\n4.8. Python Launcher for Windows\n4.9. Finding modules\n4.10. Additional modules\n4.11. Compiling Python on Windows\n4.12. Other Platforms\n5. Using Python on macOS\n5.1. Using Python for macOS from\npython.org\n5.2. Alternative Distributions\n5.3. Installing Additional Python Packages\n5.4. GUI Programming\n5.5. Advanced Topics\n5.6. Other Resources\n6. Using Python on Android\n6.1. Adding Python to an Android app\n6.2. Building a Python package for Android\n7. Using Python on iOS\n7.1. Python at runtime on iOS\n7.2. Installing Python on iOS\n7.3. App Store Compliance\n8. Editors and IDEs\n8.1. IDLE — Python editor and shell\n8.2. Other Editors and IDEs\nPrevious topic\n16.\nAppendix\nNext topic\n1.\nCommand line and environment\nThis page\nReport a bug\nShow source\n«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.13.5 Documentation\n»\nPython Setup and Usage\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001-2025, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Jun 23, 2025 (23:21 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the PYTHON official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "python", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nTerraform integration program\nThe Terraform Integration Program facilitates prospect partners in creating and publishing Terraform integrations validated by HashiCorp.\nTypes of Terraform Integrations\nThe Terraform ecosystem enables users to apply Terraform across different use cases and environments. The Terraform Integration Program currently supports both workflow and integration partners. Some partners can be both, depending on their use cases.\nWorkflow Partners\nbuild integrations for HCP Terraform, Terraform Enterprise, or both. Typically, these partners seek to enable customers to use their existing platform within a Terraform Run.\nInfrastructure Partners\nbuild Terraform providers (plugins) to empower customers and users to leverage Terraform to manage resources exposed by their platform APIs. These providers are accessible to users of all Terraform editions.\nOur Workflow Partners typically have the following use cases:\nCode Scanning:\nPartners that provide tooling to review IaC configurations to prevent errors or security issues.\nCost Management:\nPartners that analyze the impact of new infrastructure to cost and apply cost governance.\nObservability/Monitoring:\nPartners focused on providing visibility of performance or automatically detect infrastructure changes and ensure optimal observability in place.\nSecurity:\nPartners that detect Terraform configuration errors that do not align with security and compliance policies.\nAudit:\nPartners that help manage multi-cloud infrastructure resources to discover, prevent service disruptions, improve governance, and increase efficiency.\nNo code/low code: Partners focused on implementing, deployment and delivery of IT, Supply Chain, Operations Management, Business Management and other workflows\nSSO (Single Sign On):\nPartners focused on authentication for end users to securely sign on.\nCI/CD:\nPartners focused on continuous integration and continuous delivery/deployment.\nVCS:\nPartners focused on tracking and managing software code changes.\nMost workflow partners create\nrun tasks\nthat let them integrate their services directly into the Terraform workflow.\nRun tasks\nlet HCP Terraform execute tasks in external systems at certain points in the HCP Terraform run lifecycle. Specifically, HCP Terraform users can configure run tasks to execute during the pre-plan, post-plan and the pre-apply run stages. Refer to the\nHCP Terraform Integrations\nor\nTerraform Enterprise Integrations\ndocumentation for more details.\nMost workflow partners create run tasks that let them integrate their services directly into the Terraform workflow. Most workflow partners integrate with the Terraform workflow itself. Run tasks allow HCP Terraform to execute tasks in external systems at specific points in the HCP Terraform run lifecycle. This integration offers much more extensibility to HCP Terraform customers, enabling them to integrate your services into the HCP Terraform workflow. This feature allows users to add and execute these tasks during the pre-plan, post-plan and pre-apply stages. Eventually, we will open the third phase of this workflow to HCP Terraform users, the post-apply stage. Refer to the\nHCP Terraform Integrations\nor\nTerraform Enterprise Integrations\ndocumentation for more details.\nOur Infrastructure Partners typically have the following use cases:\nPublic Cloud:\nThese are large-scale, global cloud providers that offer a range of services including IaaS, SaaS, and PaaS.\nContainer Orchestration:\nThese partners help with container provisioning and deployment.\nIaaS (Infrastructure-as-a-Service):\nThese are infrastructure and IaaS providers that offer solutions such as storage, networking, and virtualization.\nSecurity & Authentication:\nThese are partners with authentication and security monitoring platforms.\nAsset Management:\nThese partners offer asset management of key organization and IT resources, including software licenses, hardware assets, and cloud resources.\nCI/CD:\nThese partners focus on continuous integration and continuous delivery/deployment.\nLogging & Monitoring:\nThese partners offer the capability to configure and manage services such as loggers, metric tools, and monitoring services.\nUtility:\nThese partners offer helper functionality, such as random value generation, file creation, http interactions, and time-based resources.\nCloud Automation:\nThese partners offer specialized cloud infrastructure automation management capabilities such as configuration management.\nData Management:\nThese partners focus on data center storage, backup, and recovery solutions.\nNetworking:\nThese partners integrate with network-specific hardware and virtualized products such as routing, switching, firewalls, and SD-WAN solutions.\nVCS (Version Control Systems):\nThese partners focus on VCS (Version Control System) projects, teams, and repositories from within Terraform.\nComms & Messaging:\nThese partners integrate with communication, email, and messaging platforms.\nDatabase:\nThese partners offer capabilities to provision and configure your database resources.\nPaaS (Platform-as-a-Service):\nThese are platform and PaaS providers that offer a range of hardware, software, and application development tools. This category includes smaller-scale providers and those with more specialized offerings.\nWeb Services:\nThese partners focus on web hosting, web performance, CDN and DNS services.\nInfrastructure partners integrate by building and publishing a plugin called a Terraform\nprovider\n. Providers are executable binaries written in Go that communicate with Terraform Core over an RPC interface. The provider acts as a translation layer for transactions with external APIs, such as a public cloud service (AWS, GCP, Azure), a PaaS service (Heroku), a SaaS service (DNSimple, CloudFlare), or on-prem resources (vSphere). Providers work across Terraform OSS, HCP Terraform and Terraform Enterprise. Refer to the\nTerraform Provider Integrations documentation\nfor more detail.\nTerraform Provider Integrations\nYou can follow the five steps. below to develop your provider alongside HashiCorp. This ensures that you can publish new versions with Terraform quickly and efficiently.\nPrepare\n: Develop integration using included resources\nPublish\n: Publish provider to the Registry or plugin documentation\nApply\n: Apply to Technology Partnership Program\nVerify\n: Verify integration with HashiCorp Alliances team\nSupport\n: Vendor provides ongoing maintenance and support\nWe encourage you to follow the tasks associated with each step fully to streamline the development process and minimize rework.\nAll providers integrate into and operate with Terraform exactly the same way. The table below is intended to help users understand who develops, and maintains a particular provider.\nTier\nDescription\nNamespace\nOfficial\nOfficial providers are owned and maintained by HashiCorp\nhashicorp\nPartner\nPartner providers are written, maintained, validated and published by third-party companies against their own APIs. To earn a partner provider badge the partner must participate in the\nHashiCorp Technology Partner Program\n.\nThird-party organization, e.g.\nmongodb/mongodbatlas\nCommunity\nCommunity providers are published to the Terraform Registry by individual maintainers, groups of maintainers, or other members of the Terraform community.\nMaintainer’s individual or organization account, e.g.\nDeviaVir/gsuite\nArchived\nArchived Providers are Official or Partner Providers that are no longer maintained by HashiCorp or the community. This may occur if an API is deprecated or interest was low.\nhashicorp\nor third-party\n1. Prepare\nTo get started with the Terraform provider development, we recommend reviewing and following the resources listed below.\nProvider Development Kit\na) Write your own custom provider with our\ntutorials\n.\nb) Get started with the plugin framework\nrepository\n, and reference the\ndocumentation\n.\nc) Follow the below video to create a Terraform provider with the plugin framework:\nd) Watch our overview of the new plugin framework and its improvements to the Terraform provider ecosystem:\ne) Reference our example\nHTTP provider\nf) Contribute to Terraform's\nguidelines\ng) Discuss providers in the HashiCorp developer discuss\nforum\n.\nPlease submit questions or suggestions about the provider tooling and development to the\nHashiCorp Terraform providers forum\n. If you are new to provider development and would like assistance, you can leverage one of the following development agencies that have developed Terraform providers in the past:\nPartner\nEmail\nWebsite\nCrest Data Systems\nmalhar@crestdatasys.com\nwww.crestdatasys.com\nTech Mahindra\nhashicorp@digitalonus.com\nwww.digitalonus.com\nAkava\nbd@akava.io\nwww.akava.io\nOpenCredo\nhashicorp@opencredo.com\nwww.opencredo.com\nProvider License\nAll Terraform providers listed as Partner must contain one of the following open source licenses:\nCDDL 1.0, 2.0\nCPL 1.0\nEclipse Public License (EPL) 1.0\nMPL 1.0, 1.1, 2.0\nPSL 2.0\nRuby's Licensing\nAFL 2.1, 3.0\nApache License 2.0\nArtistic License 1.0, 2.0\nApache Software License (ASL) 1.1\nBoost Software License\nBSD, BSD 3-clause, \"BSD-new\"\nCC-BY\nMicrosoft Public License (MS-PL)\nMIT\n2. Publish\nAfter your provider is complete and ready to release, you can publish it the integration to the\nTerraform Registry\n. This makes it publicly available for all Terraform users.\nFollow the\nTerraform Registry publishing documentation\nand review the\nprovider publishing learn guide\n. If your company has multiple products with separate providers, we recommend publishing them under the same Github organization to help with discoverability.\nOnce completed, your provider will be visible in the Terraform Registry and available to use in Terraform. Please confirm that everything looks correct and that documentation is rendering properly.\n3. Apply\nAfter your provider is published, connect with HashiCorp Alliances to onboard your integration to the HashiCorp technology ecosystem or\napply to become a technology partner\n.\n4. Verify\nWork with your HashiCorp Alliances representative to verify the plugin within the Registry and become listed as an HashiCorp technology partner integration on HashiCorp website.\n5. Support\nBuilding and publishing a new provider to the Terraform Registry is just the first step toward a quality Terraform integration. Integrations require consistent maintenance throughout their lifecycle.\nHashiCorp Terraform has an extremely wide community of users and contributors, and we encourage the community to report issues and help resolve them. However, we expect that all partner provider publishers continue maintaining the provider and that they address user-reported issues in a timely manner. This includes resolving all critical issues within 48 hours and all other issues within 5 business days. HashiCorp reserves the right to remove partner status from any integration that does not meet our maintenance requirements.\nIf a vendor chooses not to support their provider, the provider's tag may change to Community, rather than Partner.\nTerraform modules published by Partner: We now have the ability to tag partner published modules in the Terraform Registry. If you have a module that you want to receive the “Partner” tag, please contact\ntechnologypartners@hashicorp.com\nHCP Terraform Integrations\nRun tasks allow HCP Terraform to execute tasks in external systems at certain points in the HCP Terraform run lifecycle. Specifically, HCP Terraform users can add and execute these tasks during the pre-plan, post-plan and the pre-apply run stages. To execute a run task, HCP Terraform sends an API payload to the external system. This payload contains a collection of run-related information and a callback URL that the external system can use to send updates back to HCP Terraform.\nThe external system can then use this run information and respond back to HCP Terraform with a passed or failed status. HCP Terraform uses this status response to determine if a run should proceed, based on the task's enforcement settings within a workspace.\nPartners who successfully complete the HCP Terraform integration checklist obtain an HCP Terraform badge. This badge signifies HashiCorp validated the integration under ideal conditions and the partner is a member of the HashiCorp Technology Partner Program.\nThe above badge will help drive visibility for the partner as well as provide better differentiation for joint customers. This badge will be available for partners to use at their digital properties (as per guidelines in the technology partner guide that partners receive when they join HashiCorp’s technology partner program).\nThe HCP Terraform Integration program has the following five steps.\nEngage\n: Sign up for the Technology Partner\nProgram\nDevelop & Test\n: Understand and build using the API integration for Run Tasks\nReview\n: Review integration with HashiCorp Alliances team\nRelease\n: Provide documentation for your Integration\nSupport\n: Vendor provides ongoing maintenance and support\n1. Engage\nFor partners who are new to working with Hashicorp, we recommend\nsigning up for our Technology Partner Program\n. To understand more about the program, check out our “\nBecome a Partner\n” page.\n2. Develop and Test\nPartners should build an integration using the\nrun tasks APIs in HCP Terraform\n. To learn more about how post-plan run tasks can enhance the HCP Terraform workflow, refer to the following diagram and read our\nrun tasks announcement\n. For a more hands-on experience with run tasks follow the\nHCP Packer run tasks learn guide here\n.\nFor the run tasks API documentation\nclick here\n.\nTo better understand how pre-plan run task enhances the Terraform workflow, refer to the diagram of pre-plan tasks. For additional API resources,\nclick here\n.\n3. Review\nSchedule time with your Partner Alliance manager to review your integration. The review should include enabling the integration on the partner’s platform and HCP Terraform, explaining the use case for the integration, and a live demonstration of the functionality. If you are unable to engage with your Partner Alliances manager, you can also reach out to\ntechnologypartners@hashicorp.com\n.\n4. Release\nWe add new partners to the\nTerraform Run Task page\nafter the integration review and documentation is complete. On this page, you will provide a two-line summary about your integration(s). If you have multiple integrations, we highly recommend creating a summary that highlights all potential integration options.\nYou must provide documentation that helps users get started with your integration. You also need to provide documentation for our support team, including points of contact, email address, FAQ and/or best practices. We want to ensure end users are able to reach the right contacts for internal HashiCorp support when working with customers.\n5. Support\nAt HashiCorp, we view the release step to be the beginning of the journey. Getting the integration built is just the first step in enabling users to leverage it against their infrastructure. On-going effort is required to support and maintain it after you complete the initial development.\nWe expect that partners will create a mechanism to track and resolve all critical issues as soon as possible (ideally within 48 hours) and all other issues within 5 business days. This is a requirement given the critical nature of HCP Terraform to a customer’s operation. If you choose not to support your integration, we cannot consider it verified and will not list it on the Terraform documentation website.\nTerraform Enterprise Integrations\nTerraform Enterprise integrations are workflow integrations that are validated against Terraform Enterprise. Partners with a verified Terraform Enterprise integration will be given a Terraform Enterprise verified badge. This signifies HashiCorp has verified the integration and the partner is a member of the HashiCorp Technology Partner Program. At this time we are working to release an official program around integrating with Terraform Enterprise. Until then, if you are interested in publishing a verified Terraform Enterprise integration, please reach out to us at\ntechnologypartners@hashicorp.com\n.\nContact us at\ntechnologypartners@hashicorp.com\nwith any questions or feedback.\nUpon successfully building a run task integration with HCP Terraform, partners can contact their alliance manager to validate it against Terraform Enterprise. Customers commonly deploy Terraform Enterprise in a private cloud or in an air-gapped environment, in which case partners need to ensure that their product can communicate bi-directionally with Terraform Enterprise inside the customer’s network.\nEdit this page on GitHub\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nTerraform glossary\nThis page collects brief definitions of some of the technical terms used in the documentation for Terraform, as well as some terms that come up frequently in conversations throughout the Terraform community.\nAPI\nApply (noun)\nApply (verb)\nArgument\nAttribute\nBackend\nBlob Storage\nBlock\nBranch\nCI/CD\nCLI\nCommit\n(Terraform) Configuration\nConfiguration Version\nCost Estimation\nData Source\nDeposed\nExpression\nFork\nGit\nHCL\nID\nInfrastructure as Code\nIngress\nInput Variables\nInterpolation\nJSON\nLocking\nLog\nModule\nOAuth\nOAuth Client\nOAuth Token\nOrganization\nOutput Values\nOSS\nPermissions\nPlan (verb)\nPlan (noun, 1)\nPlan File\nPolicy\nPolicy Check\nPolicy Set\nPrivate Terraform Registry\nPrivate Terraform Enterprise (PTFE)\n(Terraform) Provider\nPull Request (PR)\nQueue\n(Terraform) Registry\nRemote Operations\nRemote Backend\nRepository\nResource\nRoot Module\nRoot Outputs\nRun\nRun Triggers\nS3\nSAML\nSentinel\nSite Admin\nSpeculative Plan\nSSH Key\nState\nState Version\nTeam\nTerraform\nHCP Terraform\nTerraform Enterprise\nTerraform Version\nTFE\nTFE Provider / HCP Terraform Provider\n(API) Token\nTrigger\nVariables\nVCS\nVCS Provider\nWebhook\nWorking Directory\nWorkspace\nAPI\nAcronym for \"Application Programming Interface\". Any interface designed to allow programmatic manipulation of some kind of software system. For most software developers today, the most common kinds of APIs are based on HTTP requests.\nTerraform relies on cloud service provider APIs to manage resources; each service's Terraform provider is responsible for mapping Terraform's resource model to the series of actual API calls necessary to create, check, modify, or delete a real infrastructure resource in that cloud service.\nHCP Terraform also offers its own API, for managing resources like team membership, policies, and workspaces. That API, in turn, is used by the\ntfe\nTerraform provider, so you can use Terraform to manage the system that runs Terraform for you.\nHCP Terraform: API\nTerraform providers:\ntfe\nApply (noun)\nOne of the stages of a\nrun\n, in which changes are made to real infrastructure resources in order to make them match their desired state. The counterpart of a\nplan\n.\nIn Terraform's CLI, applies are performed with the\nterraform apply\ncommand. HCP Terraform runs\nterraform apply\nusing a\nplan file\nas its input.\nTerraform: The\nterraform apply\ncommand\nHCP Terraform: About Runs\nApply (verb)\nTo make changes to real infrastructure in order to make it match the desired state (as specified by a Terraform\nconfig\nand set of\nvariables\n).\nIn conversation, it's common to refer to \"applying a\nplan\n\" (usually in the context of HCP Terraform's workflow) or \"applying a\nconfiguration\n\" (usually in the context of the Terraform CLI workflow).\nTerraform: The\nterraform apply\ncommand\nHCP Terraform: About Runs\nArgument\nIn Terraform's\nconfiguration\nlanguage: a syntax construct that assigns a value to a name. Arguments have the form\n<IDENTIFIER> = <EXPRESSION>\n, and they appear within blocks.\nMost of a Terraform configuration consists of using arguments to configure Terraform\nresources\n. Each resource type defines the arguments its resources can use, the allowed values for each argument, and which arguments are required or optional. Information about a given resource type can be found in the docs for that resource's\nprovider\n.\nTerraform Language docs: Syntax — Arguments and Blocks\nAttribute\nIn Terraform's\nconfiguration\nlanguage: a named piece of data that belongs to some kind of object. The value of an attribute can be referenced in\nexpressions\nusing a dot-separated notation, like\naws_instance.example.id\n.\nTerraform\nresources\nand\ndata sources\nmake all of their\narguments\navailable as readable attributes, and also typically export additional read-only attributes.\nTerraform docs: Expressions — Indices and Attributes\nBackend\nThe part of Terraform's core that determines how Terraform stores\nstate\nand performs\noperations\n(like\nplan\n,\napply\n, import, etc.). Terraform has multiple backends to choose from, which can be configured in a variety of ways. Backends are not plugins, so it is not possible to install additional backends.\nIn a general computer science sense, a backend is any lower-level implementation that enables a higher-level feature. But in the context of Terraform, \"backend\" always means the built-in code that handles state and operations.\nTerraform docs: Backends\nBlob Storage\nThe blob storage API service for HCP Terraform stores and retrieves arbitrary chunks of data using opaque addresses. The opaque addresses are then indexed by a directory of some kind. The most notable example is AWS's\nS3\n.\nYou do not need to be familiar with the properties and advantages of blob storage services in order to work with Terraform or HCP Terraform. However, you might need to administer or configure a blob storage service if you are responsible for administering a\nTerraform Enterprise\ninstance.\nBlock\nIn Terraform's\nconfiguration\nlanguage: a container for other content which usually represents the configuration of some kind of object, like a\nresource\n. Blocks have a\nblock type,\ncan have zero or more\nlabels,\nand have a\nbody\nthat contains any number of\narguments\nand nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file.\nresource\n\"aws_vpc\"\n\"main\"\n{\ncidr_block\n=\nvar.base_cidr_block\n}\n<\nBLOCK\nTYPE\n>\n\"<BLOCK LABEL>\"\n\"<BLOCK LABEL>\"\n{\n# Block body\n<\nIDENTIFIER\n>\n=\n<\nEXPRESSION\n>\n# Argument\n}\nTerraform Language docs: Syntax — Arguments and Blocks\nBranch\nIn some\nversion control systems\n: a semi-independent history of changes to content in a repository. A branch generally shares some history with other branches in the same repository, but eventually diverges to include changes that aren't yet present elsewhere.\nA repository usually has a default branch (whose name, in\nGit\n, defaults to\nmaster\n), which successful changes are eventually merged into. Most modern development workflows also include topic branches (where a specific set of changes is explored, iterated on, and verified), and some workflows include long-lived branches other than the default branch.\nCI/CD\nCI/CD (Continuous Integration/Continuous Delivery) platforms perform pre-defined tasks (like tests or builds) in response to trigger events (like new commits in a project). This helps make new releases more automated, predictable, and understandable. Infrastructure management, typically handled by operations teams, can also fit into an automated release workflow using infrastructure as code with Terraform.\nCLI\nCommand-line interface. The\nterraform\ncommand expects to be run in a CLI (a Unix shell or the Windows command prompt), which it uses to accept instructions and return text output.\nWe often use \"Terraform CLI\" to refer to the core Terraform Community edition binary when we need to distinguish it from other parts of the Terraform ecosystem (like HCP Terraform or the Terraform GitHub Actions).\nWikipedia: Command-line Interface\nTerraform docs: Commands (CLI)\nCommit\nIn a\nversion control system\n: A coherent set of changes saved to a repository's version history.\nIn Git, commits act like a complete snapshot of the contents of a repo, on a specific\nbranch\n(or group of branches with shared history) and at a specific moment in time. Each commit also records the identity of its parent(s), which enables viewing the entire history of the repo up to any specific moment. Additionally, comparing a commit to its parent(s) can reveal the exact changes introduced by that commit; if those changes are applied as a diff, they can be added to a different branch in the repo without merging in the entire history of the commit in question.\n(Terraform) Configuration\nAlso \"config\".\nCode written in\nTerraform's configuration language\nthat declaratively describes the desired state of your infrastructure. A complete config consists of a\nroot module\n, which can optionally call any number of child\nmodules\n.\nTerraform docs: Configuration Language\nIntroduction to Terraform\nConfiguration Version\nHCP Terraform\nAlso \"config version\".\nThe contents of a Terraform\nconfig\nat a specific moment in time. This concept only applies to HCP Terraform, since the Terraform CLI doesn't have visibility into repeated runs for a specific configuration over a period of time.\nEvery stage of a given run uses one specific configuration version.\nConfig versions can be automatically imported when new commits are merged to a workspace's repo, uploaded via the API, or uploaded by running\nterraform plan\nor\nterraform apply\nas a\nremote operation\n. Adding a new config version is sometimes called \"\ningressing\n.\"\nCost Estimation\nHCP Terraform\nPart of a\nrun\n. After gathering the\nplan file\nfor a run, HCP Terraform uses plan data to estimate costs for each resource found in the\nplan\n. Previous and proposed resources are given estimated costs, providing an estimated delta cost between the two. The results can be used in\nSentinel policies\n.\nHCP Terraform docs: Run States and Stages\nData Source\nA\nresource\n-like object that can be configured in Terraform's\nconfiguration\nlanguage.\nUnlike resources, data sources do not create or manage infrastructure. Instead, they return information about some kind of external object in the form of readable\nattributes\n. This allows a Terraform configuration to make use of information defined outside of Terraform, or defined by another separate Terraform configuration.\nData sources are implemented by\nproviders\n.\nTerraform docs: Data Sources\nDeposed\nThis status tracks a\nresource\nthat was marked for deletion, but still remains in the Terraform\nstate\nand infrastructure due an error from a previous\napply\n. Terraform expected to replace the existing resource by creating a new resource, then destroying the existing resource, but an error occurred in the apply before the destruction. Existing references to the resource refer to the new resource. Terraform will destroy the\ndeposed\nresource on the next apply. This only can occur in resource\nconfigurations\nthat have the\nlifecycle\nconfiguration block\ncreate_before_destroy\nargument\nset to\ntrue\n.\nTerraform docs: Resources\nExpression\nIn Terraform's\nconfiguration\nlanguage: a piece of syntax that represents a value, either literally or by referencing and combining other values. Expressions appear as values for\narguments\n, or within other expressions.\nPrior to Terraform 0.12,\n[\ninterpolation]\n[\n] was the only way to use non-literal expressions in Terraform configurations; in 0.12 and later, expressions can be used independently.\nTerraform docs: Expressions\nFork\nAlso \"forked repository\" or \"forked repo\".\nA\nVCS\nrepository\nthat was created by copying content and history from another repository.\nDifferent VCS providers handle forks differently, but a fork is usually owned by a different person or organization than the original repo, and a fork usually does not inherit all of the original repo's access permissions.\nHCP Terraform makes extensive use of VCS repos, and assumes that forks of a trusted repo are not necessarily trusted. As such, HCP Terraform avoids evaluating any code from external forks, which prevents HCP Terraform from running\nspeculative plans\nfor\npull requests\nfrom forks.\nOn\nTerraform Enterprise\n,\nspeculative plans\non\npull requests\nfrom forks can be enabled by an administrator.\nGit\nA distributed\nversion control system\nfor tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files.\nWikipedia: Git\nHCL\nHashiCorp Configuration Language. The structured configuration syntax that serves as the basis for Terraform's\nconfiguration\nlanguage, as well as the configuration layer for several other HashiCorp products.\nHCL establishes the syntax Terraform uses for things like\narguments\n,\nblocks\n, literal values, and\nexpressions\n. But what most people think of as the Terraform language extends beyond just the syntax; the built-in functions, Terraform-specific block types (like\nresource\nand\nvariable\n), and Terraform-specific named values available in expressions are all implementation details of Terraform itself.\nGitHub: HCL\nTerraform docs: Configuration Language\nID\nAn identifier; an opaque string permanently associated with a specific object, which doesn't contain any information about the object but which can be used to retrieve information about it or perform actions on it via an\nAPI\n.\nIn Terraform, many\nresource\ntypes have an ID\nattribute\nthat helps link Terraform's\nstate\nto the real infrastructure resource being managed.\nIn HCP Terraform, most internal application objects (like\nworkspaces\n, users,\npolicies\n, etc.) can be identified by both a name and by an opaque, permanent ID. Most API endpoints use IDs instead of names, since names sometimes change. IDs for HCP Terraform's application objects are sometimes called \"external IDs.\"\nYou can usually copy an external ID from the URL bar when viewing an object in HCP Terraform's UI. Workspaces don't display an ID in the URL bar, but their general settings page includes a UI control for viewing and copying the ID.\nInfrastructure as Code\nInfrastructure-as-Code (IaC) is the practice of managing infrastructure in a file or files rather than manually configuring it via a user interface. Infrastructure resource types managed with IaC can include virtual machines, security groups, network interfaces, and many others.\nIngress\nHCP Terraform\nThe process of bringing content into HCP Terraform. Usually that content is a\nconfiguration version\n, but it can also be a\nprivate module\nversion or some other kind of content.\nThis term comes from HCP Terraform's internal subsystems. Most documentation and UI avoids using \"ingress,\" but it can sometimes appear in API contexts or support conversations.\nInput Variables\nSee\nVariables\n.\nInterpolation\nUsing a special placeholder to insert a computed value into a string.\nTerraform's configuration language\nsupports interpolation in strings using\n${<EXPRESSION>}\nplaceholders. For example:\n\"Hello,\n${\nvar\n.\nname\n}\n!\"\nPrior to Terraform 0.12, interpolation was the only way to use non-literal\nexpressions\nin Terraform configurations; in 0.12 and later, expressions can be used independently.\nInterpolation is a very common feature in programming languages; for example, Ruby uses\n#{<EXPRESSION>}\nplaceholders in double-quoted strings, and JavaScript (ES6 and up) uses\n${<EXPRESSION>}\nplaceholders in backtick-quoted strings.\nTerraform docs: Expressions - String Templates\nJSON\nAcronym for \"JavaScript Object Notation\". A popular text-based data interchange format, which can represent strings, numbers, booleans, null, arrays, and objects/maps.\nTerraform and HCP Terraform often interact with JSON data in order to consume or provide APIs. Terraform also supports JSON as an alternate format for\nconfigurations\n.\nWikipedia: JSON\nTerraform docs: JSON Configuration Syntax\nLocking\nHCP Terraform\nThe ability to prevent new\nruns\nfrom starting in a given\nworkspace\n. Workspaces are automatically locked while a run is in progress, and can also be manually locked.\nThe\nremote\nbackend\nrespects the lock status in HCP Terraform workspaces. Some other Terraform\nbackends\ncan also lock state during runs.\nLog\nThe text-based output of actions taken within a\nrun\n. For example, the output of running\nterraform plan\n.\nModule\nAlso \"Terraform module\".\nA self-contained collection of Terraform\nconfigurations\nthat manages a collection of related infrastructure resources.\nOther Terraform configurations can\ncall\na module, which tells Terraform to manage any resources described by that module.\nModules define\ninput variables\n(which the calling module can set values for) and\noutput values\n(which the calling module can reference in\nexpressions\n).\nTerraform docs: Modules\nOAuth\nAn open standard for token-based authorization between applications on the internet.\nHCP Terraform uses OAuth to connect your organization to your\nVCS provider\n. Generally takes an\nid\nand\nsecret\nfrom your VCS provider to give access to HCP Terraform and allow it to pull in configuration from the provider.\nHCP Terraform docs: VCS Integration\nOAuth Client\nHCP Terraform\nAn entity collecting the configuration information that an HCP Terraform organization needs in order to connect to a specific\nVCS provider\n.\nAn OAuth client needs an\nOAuth token\nin order to actually access data belonging to a user or organization in that VCS provider. The client can be created with an existing token for that VCS provider (API-only, and not supported for some VCS providers), or it can be created with the details HCP Terraform needs in order to request a token. Requesting a token requires a user to click through and approve access with their VCS provider account.\nHCP Terraform docs: VCS Integration\nHCP Terraform API docs: OAuth Clients\nOAuth Token\nIn general:\nA secret string that allows an application to authenticate itself to another application. The token is generated ahead of time by the application being accessed (either during the OAuth authorization exchange or in response to a direct request by a user), and allows access with whatever permissions that application assigns to the token. The\nVCS providers\nsupported by HCP Terraform allow access via OAuth tokens; that access is generally restricted to data belonging to a given user or organization within that provider.\nWithin HCP Terraform:\nAn entity in the HCP Terraform application that associates an OAuth token (in the \"secret string\" sense) with a permanent ID and with metadata about which VCS provider it applies to and which VCS user approved the token. When documentation refers specifically to this kind of entity, the name is often styled as\noauth-token\nto indicate that it's an entity you can interact with via the\nAPI\n.\nAn\noauth-token\nhas a one-to-one relationship with an\nOAuth client\n, but the client can outlive a specific token, to allow revoking and re-requesting VCS access.\nWorkspaces\nthat are linked to a VCS\nrepo\nhave a relationship with one\noauth-token\n.\nHCP Terraform docs: VCS Integration\nHCP Terraform API docs: OAuth Tokens\nOrganization\nHCP Terraform\nHCP Terraform's fundamental unit for controlling access and grouping things together; meant to represent a company or a business unit within a company. An organization contains a group of\nworkspaces\n, a group of\nteams\n, a group of\nSentinel policies\n, and a variety of settings. Adding users to an organization is done via teams.\nHCP Terraform docs: Organizations\nOutput Values\nAlso \"outputs\".\nData exported by a Terraform\nmodule\n, which can be displayed to a user and/or programmatically used by other Terraform code.\nOSS\nAcronym for \"Open-Source Software\". Publicly available Terraform providers are open-source.\nTerraform and all other HashiCorp products are accessible through a source-available license that allows broad copying, modification, and redistribution. HCP Terraform and Terraform Enterprise are closed-source commercial software, that make use of Terraform and the available Terraform providers.\nWikipedia: Open-Source Software\nPermissions\nHCP Terraform\nSpecific levels of access allowed within HCP Terraform. Can be managed at the\nworkspace\nand/or\norganization\nlevel. For example, a user with \"read\" permissions for a workspace can see a list of runs but cannot approve a run like a user with \"write\" permissions can.\nHCP Terraform docs: Permissions\nPlan (verb)\nAlso \"queue plan\".\nTo start a new\nrun\n, which begins by running a Terraform\nplan (noun)\n.\nHCP Terraform docs: About Runs\nPlan (noun, 1)\nOne of the stages of a\nrun\n, in which Terraform compares the managed infrastructure's real state to the\nconfiguration\nand\nvariables\n, determines which changes are necessary to make the real state match the desired state, and presents a human-readable summary to the user. The counterpart of an\napply\n.\nIn Terraform's CLI, plans are performed by all of the following commands:\nterraform plan\n, which only performs a plan. It can optionally output a\nplan file\n, which\nterraform apply\ncan use to perform that exact set of planned changes.\nterraform apply\n, which performs a plan and then, if a user approves, immediately applies it.\nterraform destroy\n, which is similar to\nterraform apply\nbut uses a desired state in which none of the managed resources exist; if the plan is approved, those resources are destroyed.\nIn HCP Terraform, plans are performed by committing changes to a workspace's configuration, running\nterraform plan\nor\nterraform apply\nwith the remote backend enabled, manually queueing a plan, or uploading a configuration via the API.\nHCP Terraform's workflow always creates a\nplan file\n, which can be auto-applied or can wait for a user's approval. HCP Terraform also supports\nspeculative plans\n, which are for informational purposes only and cannot be applied.\nTerraform docs: The\nterraform plan\ncommand\nHCP Terraform docs: About Runs\nPlan File\nAlso \"\n.tfplan\n\", \"saved plan\", or simply \"plan\" in contexts where it's clearly referring to an artifact.\nA binary artifact optionally produced by the\nterraform plan\ncommand, which\nterraform apply\ncan use to carry out the exact changes that were decided at the time of the\nplan\n.\nHCP Terraform always uses a saved plan as the input to an\napply\n, so that applies never make changes that weren't shown to the user after the plan (in cases where the config or the variables changed in the meantime).\nPolicy\nHCP Terraform\nSentinel\ncode that can be enforced on runs. Combined into\npolicy sets\n.\nHCP Terraform docs: Managing Sentinel Policies\nPolicy Check\nHCP Terraform\nPart of a\nrun\n. After gathering the\nconfiguration\n,\nstate\n, and\nplan file\nfor a run, HCP Terraform runs\nSentinel\nto check that data against the active\npolicies\n. Policy checks end in success or failure. If a failure occurs in a required policy, this can prevent the run from proceeding to the\napply\nstage.\nHCP Terraform docs: Run States and Stages\nPolicy Set\nHCP Terraform\nA list of\nSentinel\npolicies\nto enforce globally or on specific workspaces.\nHCP Terraform docs: Managing Sentinel Policies\nPrivate Terraform Registry\nHCP Terraform\nAlso \"private module registry\".\nA version of the\nTerraform Registry\nthat is built-in to HCP Terraform, to enable code sharing within an organization. It includes a configuration designer, which lets you combine and configure modules to generate a Terraform\nconfiguration\nthat uses them.\nHCP Terraform docs: Private Registry\nPrivate Terraform Enterprise (PTFE)\nTerraform Enterprise\n.\nIn April 2024, we renamed Terraform Cloud as HCP Terraform. Terraform Enterprise is the self-managed distribution of HCP Terraform.\nTerraform Enterprise docs\n(Terraform) Provider\nA plugin for Terraform that makes a collection of related resources available. A provider plugin is responsible for understanding\nAPI\ninteractions with some kind of service and exposing\nresources\nbased on that API.\nTerraform providers are generally tied to a specific\ninfrastructure provider,\nwhich might be an infrastructure as a service (IaaS) provider (like AWS, GCP, Microsoft Azure, OpenStack), a platform as a service (PaaS) provider (like Heroku), or a software as a service (SaaS) provider (like HCP Terraform, DNSimple, Cloudflare).\nThere are many existing providers available, but providers can also be custom-built to work with any API.\nTerraform docs: Provider Use\nTerraform docs: Plugin Development\nPull Request (PR)\nA mechanism created by GitHub to review and discuss changes made to a\nGit\nrepository\nbranch\nthat a user wants to merge into another branch. Other collaborators can request changes, approve, or reject these changes.\nConversationally, people often say \"pull request\" to refer to this review-before-merging workflow even when working with a VCS provider that calls it something else. (For example, GitLab calls this a Merge Request.)\nQueue\nHCP Terraform\nThe list of\nruns\nwaiting to be processed. HCP Terraform uses several queues of runs, including a per-workspace queue (since only one run at a time is allowed in a given\nworkspace\n, to avoid conflicts and state corruption) and a global per-instance queue (since compute resources are finite and HCP Terraform only has access to so many VMs).\n(Terraform) Registry\nA repository of\nmodules\nwritten by the Terraform community, which can be used as-is within a Terraform\nconfiguration\nor\nforked\nand modified. The registry can help you get started with Terraform more quickly, see examples of how Terraform is written, and find pre-made modules for infrastructure components you require.\nTerraform docs: The Terraform Registry\nRemote Operations\nHCP Terraform\nThe ability to start a\nrun\n(or perform a few other tasks) from your local\nCLI\nand view output inline, while still allowing HCP Terraform to perform the actual operation.\nHCP Terraform performs Terraform runs in its own disposable VMs, where it can capture information, control access to secrets, etc., but many users are accustomed to running Terraform on their local machines. Remote operations exist to help split the difference.\nHCP Terraform Docs: Terraform Runs and Remote Operations\nRemote Backend\nA Terraform CLI feature that lets Terraform connect to HCP Terraform, by specifying in the Terraform\nconfiguration\nwhich organization and workspace(s) to use. Used for\nremote operations\nin HCP Terraform workspaces, and used for\nstate\nstorage in HCP Terraform's free edition.\nSee also\nbackend\n. Older documentation sometimes refers to backends like\ns3\nor\nconsul\nas \"remote backends,\" since they store Terraform's state in a remote service instead of the local filesystem, but today this term usually means the specific backend whose name is\nremote\n.\nTerraform docs: The\nremote\nBackend\nRepository\nAlso \"repo\".\nA collection of files and a history of the changes to them, managed by a\nversion control system\n. In HCP Terraform, a \"repo\" is generally a Git repository that contains a Terraform configuration, although\nprivate modules\nare also based on Git repos.\nWikipedia: Repository (version control)\nResource\nAlso \"infrastructure resource\".\nIn Terraform's\nconfiguration\nlanguage: A\nblock\nthat describes one or more infrastructure objects. Resources can be things like virtual networks, compute instances, or higher-level components such as DNS records.\nIn other Terraform-related contexts: An infrastructure object of a type that\ncould\nbe managed by Terraform.\nA resource block in a configuration instructs Terraform to\nmanage\nthe described resource — during a run, Terraform will create a matching real infrastructure object if one doesn't already exist, and will modify the existing object if it doesn't match the desired configuration. Terraform uses\nstate\nto keep track of which real infrastructure objects correspond to resources in a configuration.\nTerraform uses cloud provider\nAPIs\nto create, edit, and destroy resources. Terraform\nproviders\nare responsible for defining resource types and mapping transitions in a resource's state to a specific series of API calls that will carry out the necessary changes.\nTerraform docs: Resources\nRoot Module\nThe place where Terraform begins evaluating a\nconfiguration\n. The root module consists of all of the configuration files in Terraform's\nworking directory\n.\nThe root module's\nvariables\nand\noutputs\nare handled directly by Terraform (unlike child\nmodules\n, whose variables and outputs are handled by the calling module). Root variable values can be provided with HCP Terraform,\n.tfvars\nfiles, CLI options, or environment variables. Root outputs are printed after a run and stored in the state.\nRoot Outputs\nAlso \"root-level outputs\".\nThe\noutput values\nof a configuration's\nroot module\n. A\nconfiguration\ncan access the root outputs of another configuration with a\nterraform_remote_state\ndata source.\nRun\nAlso \"Terraform Run\".\nThe process of using Terraform to make real infrastructure match the desired state (as specified by the contents of the config and variables at a specific moment in time).\nIn HCP Terraform, runs are performed in a series of stages (\nplan\n,\npolicy check\n, and\napply\n), though not every stage occurs in every run. HCP Terraform saves information about historical runs.\nLearn Terraform: Get Started\nHCP Terraform docs: About Runs\nRun Triggers\nHCP Terraform\nHCP Terraform provides a way to connect a given workspace to one or more workspaces within an organization, known as \"source workspaces\". These connections, called run triggers, allow runs to queue automatically in your workspace on successful apply of runs in any of the source workspaces.\nHCP Terraform docs: Runs Triggers\nS3\nAmazon Web Services' \"Simple Storage Service\", a service for storing and retrieving arbitrary blobs of data.\nMany other cloud or self-hosted services provide\nAPIs\nthat are compatible with S3's API, which allows them to be used with software that was designed to work with S3.\nTerraform's\naws\nprovider\ncan manage S3 resources.\nTerraform Enterprise can use an S3-compatible\nblob storage\nservice when configured to use\nExternal Services\nfor storage.\nAWS: S3\nSAML\nTerraform Enterprise\nSAML is an XML-based standard for authentication and authorization. Terraform Enterprise can act as a service provider (SP) (or Relying Party, RP) with your internal SAML identity provider (IdP). The SAML Single Sign On feature is only available on\nTerraform Enterprise\n; the HCP Terraform SaaS does not support it.\nTerraform Enterprise docs: SAML Single Sign-On\nSentinel\nHCP Terraform\nA language and runtime for managing policy as code. Allows you to define rules around operations in HCP Terraform.\nHCP Terraform docs: Sentinel\nSite Admin\nTerraform Enterprise\nAn administrator of a\nTerraform Enterprise\ninstance, who has access to application settings that affect all\norganizations\nusing the instance.\nTerraform Enterprise docs: Administration\nSpeculative Plan\nHCP Terraform\nA\nrun\nthat is only intended to show possible changes to infrastructure, when using a given\nconfig version\nand set of\nvariables\n. Speculative plans can never be\napplied\n, and are usually started as a result of\npull requests\nto a\nworkspace\n's repo, or by running\nterraform plan\non the command line with\nremote operations\nconfigured.\nSSH Key\nA type of access credential based on public key cryptography, used to log into servers.\nHCP Terraform uses SSH private keys for two kinds of operations:\nDownloading private Terraform\nmodules\nwith\nGit\n-based sources during a Terraform run. Keys for downloading modules are assigned per-workspace.\nBringing content from a connected\nVCS provider\ninto HCP Terraform, usually when pulling in a Terraform\nconfiguration\nfor a\nworkspace\nor importing a module into the\nprivate module registry\n. Only some VCS providers require an SSH key, but other providers only need an SSH key if your repositories include submodules that are only accessible via SSH (instead of your VCS provider's API).\nWikipedia: SSH\nHCP Terraform docs: SSH Keys for Cloning Modules\nState\nTerraform's cached information about your managed infrastructure and\nconfiguration\n. This state is used to persistently map the same real world\nresources\nto your configuration from run-to-run, keep track of metadata, and improve performance for large infrastructures.\nWithout state, Terraform has no way to identify the real resources it created during previous runs. Thus, when multiple people are collaborating on shared infrastructure, it's important to store state in a shared location, like a free HCP Terraform organization.\nTerraform docs: State\nState Version\nHCP Terraform\nA snapshot of your infrastructure's\nstate\nat a point in time. Can be manually uploaded to HCP Terraform or created as the result of an\napply\n.\nTerraform CLI doesn't have any concept of historical state data; it only interacts with the most recent state, and writes a snapshot of the new state to the configured\nbackend\nafter making changes. Whatever system that backend writes to can choose to retain previous state; HCP Terraform does this, and some other backends (like S3) can also be configured to keep historical snapshots. The\nlocal\nbackend does not.\nTeam\nHCP Terraform\nA group of HCP Terraform users, which can be given permission to access and/or edit various objects (\nworkspaces\n,\npolicies\n, etc.) within an\norganization\n. A team belongs to a single organization. Users can belong to any number of teams in any number of organizations.\nHCP Terraform docs: Teams\nTerraform\nA tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions.\nIntro to Terraform\nHCP Terraform\nAn application that helps teams use Terraform together. HCP Terraform provides a consistent environment for Terraform runs, as well as hosting Terraform\nstate\nand other shared information.\nHCP Terraform is available as a hosted service at\nhttps://app.terraform.io\n. It is also available in an on-premises distribution called\nTerraform Enterprise\n.\nPrior to 2024, HCP Terraform was previously referred to as \"Terraform Cloud\". Prior to 2019, \"Terraform Enterprise\" (TFE) was the name of Terraform's cloud application (now HCP Terraform), and \"Private Terraform Enterprise\" (PTFE) was the name of the on-premises distribution.\nHCP Terraform docs\nTerraform Enterprise\nAn on-premises distribution of HCP Terraform, which enables large enterprises to host their own private instance of the application.\nPrior to 2019, \"Terraform Enterprise\" (TFE) was the name of Terraform's cloud application (now HCP Terraform), and \"Private Terraform Enterprise\" (PTFE) was the name of the on-premises distribution. Prior to 2024, HCP Terraform was previously referred to as \"Terraform Cloud\".\nTerraform Enterprise docs\nTerraform Version\nHCP Terraform\nA particular version of the\nterraform\nbinary available for use in HCP Terraform workspaces. Specifies a URL, a SHA256 checksum and enabled/beta flags.\nSite admins\ncan configure the available Terraform versions for a\nTerraform Enterprise\ninstance.\nTerraform Enterprise docs: Managing Terraform Versions\nTFE\nTerraform Enterprise\n.\nTerraform Enterprise docs\nTFE Provider / HCP Terraform Provider\nA Terraform provider that manages HCP Terraform. Allows you to manage HCP Terraform using a Terraform\nconfiguration\n.\nProvider docs: tfe\n(API) Token\nHCP Terraform\nA revocable secret string that authenticates a user, to allow use of HCP Terraform's\nAPI\nor the\nremote backend\n.\nDifferent kinds of tokens grant different permissions. Tokens can be granted by user,\nteam\n, or\norganization\n.\nMany applications other than HCP Terraform use token-based authentication, but within HCP Terraform's documentation and UI, \"token\" without any other qualification generally means an HCP Terraform API token.\nHCP Terraform docs: User Tokens\nHCP Terraform docs: Team Tokens\nHCP Terraform docs: Organization Tokens\nTrigger\nHCP Terraform\nSomething that causes a new\nrun\nto queue. Runs can be UI/VCS-driven (in which case the trigger is a new VCS commit or a UI action), API-driven (in which case the trigger is an API call) or CLI-driven (in which case the trigger is a CLI command).\nHCP Terraform docs: UI/VCS-based Run Workflow\nVariables\nAlso \"input variables\".\nIn general-purpose programming, a variable is a symbolic name associated with a value.\nIn Terraform, \"variables\" almost always refers to\ninput variables,\nwhich are key/value pairs used in a\nrun\n. Terraform\nmodules\ncan declare variables to allow customization. For child modules, the parent module provides a value when calling the module; for the\nroot module\n, values are provided at run time.\nHCP Terraform lets you set values for root input variables in a\nworkspace\n, so all collaborators can use the same values. Variable values marked as \"sensitive\" become unreadable in the UI and API, and all variable values are protected by Vault.\nTerraform docs: Input Variables\nHCP Terraform docs: Variables\nVCS\nVersion Control System, like\nGit\n. Software that tracks changes over time to a collection of files, making it possible to keep track of changes, undo changes, and combine changes made in parallel by different users. Usually used for any non-trivial code effort, including infrastructure as code.\nDifferent VCSes use different models for history; Git models changes as a directed acyclic graph of\ncommits\n, and parallel changes that begin from the same parent commit become diverging\nbranches\n(which might later be merged together).\nWikipedia: Version Control\nVCS Provider\nHCP Terraform\nA specific service that provides\nVCS\nfeatures, with the goal of enabling teams to collaborate on code. HCP Terraform can integrate with VCS providers to access your Terraform\nconfigurations\nand\nmodules\n, and currently supports GitHub, GitHub Enterprise, GitLab.com, GitLab EE and CE, Bitbucket Cloud, Bitbucket Data Center, Azure DevOps Server, and Azure DevOps Services.\nHCP Terraform docs: Connecting VCS Providers\nWebhook\nA server-to-server HTTP request, in which one system responds to a change in its internal state by sending information to another system.\nThe recipient of a webhook might return information to the requesting system, call other webhooks in response, perform its action silently, or ignore the request entirely.\nHCP Terraform uses webhooks in multiple ways. Most notably:\nHCP Terraform creates webhook configurations on\nVCS providers\n, so that they send webhook requests to HCP Terraform whenever linked\nrepositories\nreceive\npull requests\nor new\ncommits\n. (\nHCP Terraform docs: UI/VCS-driven Runs\n)\nUsers can create webhook configurations on HCP Terraform\nworkspaces\n, so that HCP Terraform will send run notification webhooks to Slack or other external services. (\nHCP Terraform docs: Run Notifications\n)\nWorking Directory\nThe directory where\nterraform init\nruns, creating a\n.terraform\nsubdirectory. All subsequent commands for the same\nconfiguration\nshould then be run from the same working directory.\nThe\nroot module\nconsists of all of the configuration files in the top level of Terraform's working directory. Subdirectories of the working directory can contain child\nmodules\n.\nWorkspace\nIn Terraform CLI, a workspace is an isolated instance of\nstate\ndata. Using multiple workspaces lets you manage multiple non-overlapping sets of infrastructure from a single\nconfiguration\nin a single\nworking directory\n.\nIn HCP Terraform, a workspace is a complex object that represents everything needed to manage a specific collection of infrastructure resources over time. In particular, this includes:\nA Terraform\nconfiguration\n(which has multiple versions over time, and which can come from a repo or from manual uploads)\nA set of\nvariables\nState\ndata that represents the current and historical condition of the infrastructure\nHistorical information about\nruns\n.\nAll HCP Terraform runs occur in the context of a workspace — they use that workspace's config data, use that workspace's state to identify the real infrastructure being managed, and edit that workspace's state to match any infrastructure changes during the run. A workspace belongs to an\norganization\n.\nHCP Terraform docs: Workspaces\nTerraform docs: Workspaces\nEdit this page on GitHub\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nRelease History\nThis page summarizes the changes between official stable releases of Go.\nThe\nchange log\nhas the full details.\nTo update to a specific release, use:\ngit fetch --tags\ngit checkout\ngoX.Y.Z\nRelease Policy\nEach major Go release is supported until there are two newer major releases.\nFor example, Go 1.5 was supported until the Go 1.7 release, and Go 1.6 was\nsupported until the Go 1.8 release.\nWe fix critical problems, including\ncritical security problems\n,\nin supported releases as needed by issuing minor revisions\n(for example, Go 1.6.1, Go 1.6.2, and so on).\ngo1.24.0 (released 2025-02-11)\nGo 1.24.0 is a major release of Go.\n\tRead the\nGo 1.24 Release Notes\nfor more information.\nMinor revisions\ngo1.24.1\n\t\t(released 2025-03-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to cgo, the compiler, the\ngo\ncommand, and the\nreflect\n,\nruntime\n, and\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.24.1 milestone\non our issue tracker for details.\ngo1.24.2\n\t\t(released 2025-04-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the runtime, the\ngo\ncommand, and the\ncrypto/tls\n,\ngo/types\n,\nnet/http\n, and\ntesting\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.24.2 milestone\non our issue tracker for details.\ngo1.24.3\n\t\t(released 2025-05-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nos\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the runtime, the compiler, the linker, the\ngo\ncommand, and the\ncrypto/tls\nand\nos\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.24.3 milestone\non our issue tracker for details.\ngo1.24.4\n\t\t(released 2025-06-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/x509\n,\nnet/http\n, and\nos\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker, the\ngo\ncommand, and the\nhash/maphash\nand\nos\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.24.4 milestone\non our issue tracker for details.\ngo1.23.0 (released 2024-08-13)\nGo 1.23.0 is a major release of Go.\n\tRead the\nGo 1.23 Release Notes\nfor more information.\nMinor revisions\ngo1.23.1\n\t\t(released 2024-09-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nencoding/gob\n,\ngo/build/constraint\n, and\ngo/parser\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the runtime, and the\ndatabase/sql\n,\ngo/types\n,\nos\n,\nruntime/trace\n, and\nunique\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.23.1 milestone\non our issue tracker for details.\ngo1.23.2\n\t\t(released 2024-10-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, cgo, the runtime, and the\nmaps\n,\nos\n,\nos/exec\n,\ntime\n, and\nunique\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.23.2 milestone\non our issue tracker for details.\ngo1.23.3\n\t\t(released 2024-11-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the linker, the runtime, and the\nnet/http\n,\nos\n, and\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.23.3 milestone\non our issue tracker for details.\ngo1.23.4\n\t\t(released 2024-12-03)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, the runtime, the\ntrace\ncommand, and the\nsyscall\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.23.4 milestone\non our issue tracker for details.\ngo1.23.5\n\t\t(released 2025-01-16)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/x509\nand\nnet/http\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the runtime, and the\nnet\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.23.5 milestone\non our issue tracker for details.\ngo1.23.6\n\t\t(released 2025-02-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/elliptic\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler and the\ngo\ncommand.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.23.6 milestone\non our issue tracker for details.\ngo1.23.7\n\t\t(released 2025-03-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to cgo, the compiler, and the\nreflect\n,\nruntime\n, and\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.23.7 milestone\non our issue tracker for details.\ngo1.23.8\n\t\t(released 2025-04-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the runtime and the\ngo\ncommand.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.23.8 milestone\non our issue tracker for details.\ngo1.23.9\n\t\t(released 2025-05-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the runtime and the linker.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.23.9 milestone\non our issue tracker for details.\ngo1.23.10\n\t\t(released 2025-06-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\nand\nos\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.23.10 milestone\non our issue tracker for details.\ngo1.22.0 (released 2024-02-06)\nGo 1.22.0 is a major release of Go.\n\tRead the\nGo 1.22 Release Notes\nfor more information.\nMinor revisions\ngo1.22.1\n\t\t(released 2024-03-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/x509\n,\nhtml/template\n,\nnet/http\n,\nnet/http/cookiejar\n, and\nnet/mail\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the runtime, the\ntrace\ncommand, and the\ngo/types\nand\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.1 milestone\non our issue tracker for details.\ngo1.22.2\n\t\t(released 2024-04-03)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the linker, and the\nencoding/gob\n,\ngo/types\n,\nnet/http\n, and\nruntime/trace\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.2 milestone\non our issue tracker for details.\ngo1.22.3\n\t\t(released 2024-05-07)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo\ncommand and the\nnet\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the runtime, and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.3 milestone\non our issue tracker for details.\ngo1.22.4\n\t\t(released 2024-06-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\narchive/zip\nand\nnet/netip\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the linker, the runtime, and the\nos\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.4 milestone\non our issue tracker for details.\ngo1.22.5\n\t\t(released 2024-07-02)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, cgo, the\ngo\ncommand, the linker, the runtime, and the\ncrypto/tls\n,\ngo/types\n,\nnet\n,\nnet/http\n, and\nos/exec\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.5 milestone\non our issue tracker for details.\ngo1.22.6\n\t\t(released 2024-08-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the\ngo\ncommand, the compiler, the linker, the\ntrace\ncommand, the\ncovdata\ncommand, and the\nbytes\n,\ngo/types\n, and\nos/exec\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.6 milestone\non our issue tracker for details.\ngo1.22.7\n\t\t(released 2024-09-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nencoding/gob\n,\ngo/build/constraint\n, and\ngo/parser\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the\nfix\ncommand and the runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.7 milestone\non our issue tracker for details.\ngo1.22.8\n\t\t(released 2024-10-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, and the\nmaps\nand\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.8 milestone\non our issue tracker for details.\ngo1.22.9\n\t\t(released 2024-11-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the linker.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.9 milestone\non our issue tracker for details.\ngo1.22.10\n\t\t(released 2024-12-03)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the runtime and the\nsyscall\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.10 milestone\non our issue tracker for details.\ngo1.22.11\n\t\t(released 2025-01-16)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/x509\nand\nnet/http\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.11 milestone\non our issue tracker for details.\ngo1.22.12\n\t\t(released 2025-02-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/elliptic\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler and the\ngo\ncommand.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.22.12 milestone\non our issue tracker for details.\ngo1.21.0 (released 2023-08-08)\nGo 1.21.0 is a major release of Go.\n\tRead the\nGo 1.21 Release Notes\nfor more information.\nMinor revisions\ngo1.21.1\n\t\t(released 2023-09-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\tfour\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncmd/go\n,\ncrypto/tls\n, and\nhtml/template\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the linker, the runtime, and the\ncontext\n,\ncrypto/tls\n,\nencoding/gob\n,\nencoding/xml\n,\ngo/types\n,\nnet/http\n,\nos\n, and\npath/filepath\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.1 milestone\non our issue tracker for details.\ngo1.21.2\n\t\t(released 2023-10-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\tone\n\t\t\t\tsecurity\n\t\t\t\tfix to the\ncmd/go\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the linker, the runtime, and the\nruntime/metrics\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.2 milestone\non our issue tracker for details.\ngo1.21.3\n\t\t(released 2023-10-10)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.3 milestone\non our issue tracker for details.\ngo1.21.4\n\t\t(released 2023-11-07)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\npath/filepath\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker, the runtime, the compiler, and the\ngo/types\n,\nnet/http\n, and\nruntime/cgo\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.4 milestone\non our issue tracker for details.\ngo1.21.5\n\t\t(released 2023-12-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo\ncommand, and the\nnet/http\nand\npath/filepath\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the runtime, and the\ncrypto/rand\n,\nnet\n,\nos\n, and\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.5 milestone\non our issue tracker for details.\ngo1.21.6\n\t\t(released 2024-01-09)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, the runtime, and the\ncrypto/tls\n,\nmaps\n, and\nruntime/pprof\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.6 milestone\non our issue tracker for details.\ngo1.21.7\n\t\t(released 2024-02-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the runtime, and the\ncrypto/x509\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.7 milestone\non our issue tracker for details.\ngo1.21.8\n\t\t(released 2024-03-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/x509\n,\nhtml/template\n,\nnet/http\n,\nnet/http/cookiejar\n, and\nnet/mail\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the\ngo\ncommand and the runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.8 milestone\non our issue tracker for details.\ngo1.21.9\n\t\t(released 2024-04-03)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker, and the\ngo/types\nand\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.9 milestone\non our issue tracker for details.\ngo1.21.10\n\t\t(released 2024-05-07)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo\ncommand, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.10 milestone\non our issue tracker for details.\ngo1.21.11\n\t\t(released 2024-06-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\narchive/zip\nand\nnet/netip\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the runtime, and the\nos\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.11 milestone\non our issue tracker for details.\ngo1.21.12\n\t\t(released 2024-07-02)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the runtime, and the\ncrypto/x509\n,\nnet/http\n,\nnet/netip\n, and\nos\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.12 milestone\non our issue tracker for details.\ngo1.21.13\n\t\t(released 2024-08-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the\ngo\ncommand, the\ncovdata\ncommand, and the\nbytes\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.21.13 milestone\non our issue tracker for details.\ngo1.20 (released 2023-02-01)\nGo 1.20 is a major release of Go.\n\tRead the\nGo 1.20 Release Notes\nfor more information.\nMinor revisions\ngo1.20.1\n\t\t(released 2023-02-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/tls\n,\nmime/multipart\n,\nnet/http\n, and\npath/filepath\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the linker, the runtime, and the\ntime\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.1 milestone\non our issue tracker for details.\ngo1.20.2\n\t\t(released 2023-03-07)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\ncrypto/elliptic\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ncovdata\ncommand, the linker, the runtime, and the\ncrypto/ecdh\n,\ncrypto/rsa\n,\ncrypto/x509\n,\nos\n, and\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.2 milestone\non our issue tracker for details.\ngo1.20.3\n\t\t(released 2023-04-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo/parser\n,\nhtml/template\n,\nmime/multipart\n,\nnet/http\n, and\nnet/textproto\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the linker, the runtime, and the\ntime\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.3 milestone\non our issue tracker for details.\ngo1.20.4\n\t\t(released 2023-05-02)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\tthree\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nhtml/template\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the runtime, and the\ncrypto/subtle\n,\ncrypto/tls\n,\nnet/http\n, and\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.4 milestone\non our issue tracker for details.\ngo1.20.5\n\t\t(released 2023-06-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\tfour\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncmd/go\nand\nruntime\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the runtime, and the\ncrypto/rsa\n,\nnet\n, and\nos\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.5 milestone\non our issue tracker for details.\ngo1.20.6\n\t\t(released 2023-07-11)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, cgo, the\ncover\ntool, the\ngo\ncommand, the runtime, and the\ncrypto/ecdsa\n,\ngo/build\n,\ngo/printer\n,\nnet/mail\n, and\ntext/template\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.6 milestone\non our issue tracker for details.\ngo1.20.7\n\t\t(released 2023-08-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\ncrypto/tls\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the assembler and the compiler.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.7 milestone\non our issue tracker for details.\ngo1.20.8\n\t\t(released 2023-09-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ttwo\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nhtml/template\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the runtime, and the\ncrypto/tls\n,\ngo/types\n,\nnet/http\n, and\npath/filepath\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.8 milestone\non our issue tracker for details.\ngo1.20.9\n\t\t(released 2023-10-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\tone\n\t\t\t\tsecurity\n\t\t\t\tfix to the\ncmd/go\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the\ngo\ncommand and the linker.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.9 milestone\non our issue tracker for details.\ngo1.20.10\n\t\t(released 2023-10-10)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.10 milestone\non our issue tracker for details.\ngo1.20.11\n\t\t(released 2023-11-07)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\npath/filepath\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.11 milestone\non our issue tracker for details.\ngo1.20.12\n\t\t(released 2023-12-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo\ncommand, and the\nnet/http\nand\npath/filepath\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler and the\ngo\ncommand.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.12 milestone\non our issue tracker for details.\ngo1.20.13\n\t\t(released 2024-01-09)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the runtime and the\ncrypto/tls\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.13 milestone\non our issue tracker for details.\ngo1.20.14\n\t\t(released 2024-02-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the\ncrypto/x509\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.20.14 milestone\non our issue tracker for details.\ngo1.19 (released 2022-08-02)\nGo 1.19 is a major release of Go.\n\tRead the\nGo 1.19 Release Notes\nfor more information.\nMinor revisions\ngo1.19.1\n\t\t(released 2022-09-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\nand\nnet/url\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the\npprof\ncommand, the linker, the runtime, and the\ncrypto/tls\nand\ncrypto/x509\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.1 milestone\non our issue tracker for details.\ngo1.19.2\n\t\t(released 2022-10-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\narchive/tar\n,\nnet/http/httputil\n, and\nregexp\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the linker, the runtime, and the\ngo/types\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.2 milestone\non our issue tracker for details.\ngo1.19.3\n\t\t(released 2022-11-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nos/exec\nand\nsyscall\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler and the runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.3 milestone\non our issue tracker for details.\ngo1.19.4\n\t\t(released 2022-12-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\nand\nos\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the runtime, and the\ncrypto/x509\n,\nos/exec\n, and\nsync/atomic\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.4 milestone\non our issue tracker for details.\ngo1.19.5\n\t\t(released 2023-01-10)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, the linker, and the\ncrypto/x509\n,\nnet/http\n,\nsync/atomic\n, and\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.5 milestone\non our issue tracker for details.\ngo1.19.6\n\t\t(released 2023-02-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/tls\n,\nmime/multipart\n,\nnet/http\n, and\npath/filepath\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the\ngo\ncommand, the linker, the runtime, and the\ncrypto/x509\n,\nnet/http\n, and\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.6 milestone\non our issue tracker for details.\ngo1.19.7\n\t\t(released 2023-03-07)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\ncrypto/elliptic\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker, the runtime, and the\ncrypto/x509\nand\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.7 milestone\non our issue tracker for details.\ngo1.19.8\n\t\t(released 2023-04-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo/parser\n,\nhtml/template\n,\nmime/multipart\n,\nnet/http\n, and\nnet/textproto\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker, the runtime, and the\ntime\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.8 milestone\non our issue tracker for details.\ngo1.19.9\n\t\t(released 2023-05-02)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\tthree\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nhtml/template\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the runtime, and the\ncrypto/tls\nand\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.9 milestone\non our issue tracker for details.\ngo1.19.10\n\t\t(released 2023-06-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\tfour\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncmd/go\nand\nruntime\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, and the runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.10 milestone\non our issue tracker for details.\ngo1.19.11\n\t\t(released 2023-07-11)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to cgo, the\ncover\ntool, the\ngo\ncommand, the runtime, and the\ngo/printer\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.11 milestone\non our issue tracker for details.\ngo1.19.12\n\t\t(released 2023-08-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\ncrypto/tls\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the assembler and the compiler.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.12 milestone\non our issue tracker for details.\ngo1.19.13\n\t\t(released 2023-09-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the\ngo\ncommand, and the\ncrypto/tls\nand\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.19.13 milestone\non our issue tracker for details.\ngo1.18 (released 2022-03-15)\nGo 1.18 is a major release of Go.\n\tRead the\nGo 1.18 Release Notes\nfor more information.\nMinor revisions\ngo1.18.1\n\t\t(released 2022-04-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/elliptic\n,\ncrypto/x509\n, and\nencoding/pem\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, linker, runtime, the\ngo\ncommand, vet, and the\nbytes\n,\ncrypto/x509\n, and\ngo/types\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.18.1 milestone\non our issue tracker for details.\ngo1.18.2\n\t\t(released 2022-05-10)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nsyscall\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, runtime, the\ngo\ncommand, and the\ncrypto/x509\n,\ngo/types\n,\nnet/http/httptest\n,\nreflect\n, and\nsync/atomic\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.18.2 milestone\non our issue tracker for details.\ngo1.18.3\n\t\t(released 2022-06-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/rand\n,\ncrypto/tls\n,\nos/exec\n, and\npath/filepath\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, and the\ncrypto/tls\nand\ntext/template/parse\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.18.3 milestone\non our issue tracker for details.\ngo1.18.4\n\t\t(released 2022-07-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncompress/gzip\n,\nencoding/gob\n,\nencoding/xml\n,\ngo/parser\n,\nio/fs\n,\nnet/http\n, and\npath/filepath\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the linker, the runtime, and the\nruntime/metrics\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.18.4 milestone\non our issue tracker for details.\ngo1.18.5\n\t\t(released 2022-08-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nencoding/gob\nand\nmath/big\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the runtime, and the\ntesting\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.18.5 milestone\non our issue tracker for details.\ngo1.18.6\n\t\t(released 2022-09-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the\npprof\ncommand, the runtime, and the\ncrypto/tls\n,\nencoding/xml\n, and\nnet\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.18.6 milestone\non our issue tracker for details.\ngo1.18.7\n\t\t(released 2022-10-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\narchive/tar\n,\nnet/http/httputil\n, and\nregexp\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the linker, and the\ngo/types\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.18.7 milestone\non our issue tracker for details.\ngo1.18.8\n\t\t(released 2022-11-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nos/exec\nand\nsyscall\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.18.8 milestone\non our issue tracker for details.\ngo1.18.9\n\t\t(released 2022-12-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\nand\nos\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to cgo, the compiler, the runtime, and the\ncrypto/x509\nand\nos/exec\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.18.9 milestone\non our issue tracker for details.\ngo1.18.10\n\t\t(released 2023-01-10)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the compiler, the linker, and the\ncrypto/x509\n,\nnet/http\n, and\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.18.10 milestone\non our issue tracker for details.\ngo1.17 (released 2021-08-16)\nGo 1.17 is a major release of Go.\n\tRead the\nGo 1.17 Release Notes\nfor more information.\nMinor revisions\ngo1.17.1\n\t\t(released 2021-09-09)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\narchive/zip\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, linker, the\ngo\ncommand, and the\ncrypto/rand\n,\nembed\n,\ngo/types\n,\nhtml/template\n, and\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.1 milestone\non our issue tracker for details.\ngo1.17.2\n\t\t(released 2021-10-07)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to linker and the\nmisc/wasm\ndirectory, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, runtime, the\ngo\ncommand, and the\ntext/template\nand\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.2 milestone\non our issue tracker for details.\ngo1.17.3\n\t\t(released 2021-11-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\narchive/zip\nand\ndebug/macho\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, linker, runtime, the\ngo\ncommand, the\nmisc/wasm\ndirectory, and the\nnet/http\nand\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.3 milestone\non our issue tracker for details.\ngo1.17.4\n\t\t(released 2021-12-02)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, linker, runtime, and the\ngo/types\n,\nnet/http\n, and\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.4 milestone\non our issue tracker for details.\ngo1.17.5\n\t\t(released 2021-12-09)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\nand\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.5 milestone\non our issue tracker for details.\ngo1.17.6\n\t\t(released 2022-01-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, linker, runtime, and the\ncrypto/x509\n,\nnet/http\n, and\nreflect\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.6 milestone\non our issue tracker for details.\ngo1.17.7\n\t\t(released 2022-02-10)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo\ncommand, and the\ncrypto/elliptic\nand\nmath/big\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, linker, runtime, the\ngo\ncommand, and the\ndebug/macho\n,\ndebug/pe\n, and\nnet/http/httptest\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.7 milestone\non our issue tracker for details.\ngo1.17.8\n\t\t(released 2022-03-03)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nregexp/syntax\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, runtime, the\ngo\ncommand, and the\ncrypto/x509\nand\nnet\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.8 milestone\non our issue tracker for details.\ngo1.17.9\n\t\t(released 2022-04-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/elliptic\nand\nencoding/pem\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker and runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.9 milestone\non our issue tracker for details.\ngo1.17.10\n\t\t(released 2022-05-10)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nsyscall\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, runtime, and the\ncrypto/x509\nand\nnet/http/httptest\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.10 milestone\non our issue tracker for details.\ngo1.17.11\n\t\t(released 2022-06-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/rand\n,\ncrypto/tls\n,\nos/exec\n, and\npath/filepath\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the\ncrypto/tls\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.11 milestone\non our issue tracker for details.\ngo1.17.12\n\t\t(released 2022-07-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncompress/gzip\n,\nencoding/gob\n,\nencoding/xml\n,\ngo/parser\n,\nio/fs\n,\nnet/http\n, and\npath/filepath\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, the runtime, and the\nruntime/metrics\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.12 milestone\non our issue tracker for details.\ngo1.17.13\n\t\t(released 2022-08-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nencoding/gob\nand\nmath/big\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler and the runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.17.13 milestone\non our issue tracker for details.\ngo1.16 (released 2021-02-16)\nGo 1.16 is a major release of Go.\n\tRead the\nGo 1.16 Release Notes\nfor more information.\nMinor revisions\ngo1.16.1\n\t\t(released 2021-03-10)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\narchive/zip\nand\nencoding/xml\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.1 milestone\non our issue tracker for details.\ngo1.16.2\n\t\t(released 2021-03-11)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the compiler, linker, the\ngo\ncommand, and the\nsyscall\nand\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.2 milestone\non our issue tracker for details.\ngo1.16.3\n\t\t(released 2021-04-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, linker, runtime, the\ngo\ncommand, and the\ntesting\nand\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.3 milestone\non our issue tracker for details.\ngo1.16.4\n\t\t(released 2021-05-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, runtime, and the\narchive/zip\n,\nsyscall\n, and\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.4 milestone\non our issue tracker for details.\ngo1.16.5\n\t\t(released 2021-06-03)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\narchive/zip\n,\nmath/big\n,\nnet\n, and\nnet/http/httputil\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker, the\ngo\ncommand, and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.5 milestone\non our issue tracker for details.\ngo1.16.6\n\t\t(released 2021-07-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\ncrypto/tls\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, and the\nnet\nand\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.6 milestone\non our issue tracker for details.\ngo1.16.7\n\t\t(released 2021-08-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nnet/http/httputil\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, linker, runtime, the\ngo\ncommand, and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.7 milestone\non our issue tracker for details.\ngo1.16.8\n\t\t(released 2021-09-09)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\narchive/zip\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the\narchive/zip\n,\ngo/internal/gccgoimporter\n,\nhtml/template\n,\nnet/http\n, and\nruntime/pprof\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.8 milestone\non our issue tracker for details.\ngo1.16.9\n\t\t(released 2021-10-07)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to linker and the\nmisc/wasm\ndirectory, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to runtime and the\ntext/template\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.9 milestone\non our issue tracker for details.\ngo1.16.10\n\t\t(released 2021-11-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\narchive/zip\nand\ndebug/macho\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, linker, runtime, the\nmisc/wasm\ndirectory, and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.10 milestone\non our issue tracker for details.\ngo1.16.11\n\t\t(released 2021-12-02)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, runtime, and the\nnet/http\n,\nnet/http/httptest\n, and\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.11 milestone\non our issue tracker for details.\ngo1.16.12\n\t\t(released 2021-12-09)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\nand\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.12 milestone\non our issue tracker for details.\ngo1.16.13\n\t\t(released 2022-01-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, linker, runtime, and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.13 milestone\non our issue tracker for details.\ngo1.16.14\n\t\t(released 2022-02-10)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo\ncommand, and the\ncrypto/elliptic\nand\nmath/big\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, linker, runtime, the\ngo\ncommand, and the\ndebug/macho\n,\ndebug/pe\n,\nnet/http/httptest\n, and\ntesting\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.14 milestone\non our issue tracker for details.\ngo1.16.15\n\t\t(released 2022-03-03)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nregexp/syntax\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, runtime, the\ngo\ncommand, and the\nnet\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.16.15 milestone\non our issue tracker for details.\ngo1.15 (released 2020-08-11)\nGo 1.15 is a major release of Go.\n\tRead the\nGo 1.15 Release Notes\nfor more information.\nMinor revisions\ngo1.15.1\n\t\t(released 2020-09-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http/cgi\nand\nnet/http/fcgi\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.1 milestone\non our issue tracker for details.\ngo1.15.2\n\t\t(released 2020-09-09)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, runtime, documentation, the\ngo\ncommand, and the\nnet/mail\n,\nos\n,\nsync\n, and\ntesting\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.2 milestone\non our issue tracker for details.\ngo1.15.3\n\t\t(released 2020-10-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the compiler, runtime, the\ngo\ncommand, and the\nbytes\n,\nplugin\n, and\ntesting\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.3 milestone\non our issue tracker for details.\ngo1.15.4\n\t\t(released 2020-11-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the compiler, linker, runtime, and the\ncompress/flate\n,\nnet/http\n,\nreflect\n, and\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.4 milestone\non our issue tracker for details.\ngo1.15.5\n\t\t(released 2020-11-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo\ncommand and the\nmath/big\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.5 milestone\non our issue tracker for details.\ngo1.15.6\n\t\t(released 2020-12-03)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, linker, runtime, the\ngo\ncommand, and the\nio\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.6 milestone\non our issue tracker for details.\ngo1.15.7\n\t\t(released 2021-01-19)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo\ncommand and the\ncrypto/elliptic\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.7 milestone\non our issue tracker for details.\ngo1.15.8\n\t\t(released 2021-02-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, linker, runtime, the\ngo\ncommand, and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.8 milestone\non our issue tracker for details.\ngo1.15.9\n\t\t(released 2021-03-10)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nencoding/xml\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.9 milestone\non our issue tracker for details.\ngo1.15.10\n\t\t(released 2021-03-11)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, the\ngo\ncommand, and the\nnet/http\n,\nos\n,\nsyscall\n, and\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.10 milestone\non our issue tracker for details.\ngo1.15.11\n\t\t(released 2021-04-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the compiler, linker, runtime, the\ngo\ncommand, and the\ndatabase/sql\nand\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.11 milestone\non our issue tracker for details.\ngo1.15.12\n\t\t(released 2021-05-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nnet/http\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, runtime, and the\narchive/zip\n,\nsyscall\n, and\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.12 milestone\non our issue tracker for details.\ngo1.15.13\n\t\t(released 2021-06-03)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\narchive/zip\n,\nmath/big\n,\nnet\n, and\nnet/http/httputil\npackages, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker, the\ngo\ncommand, and the\nmath/big\nand\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.13 milestone\non our issue tracker for details.\ngo1.15.14\n\t\t(released 2021-07-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\ncrypto/tls\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker and the\nnet\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.14 milestone\non our issue tracker for details.\ngo1.15.15\n\t\t(released 2021-08-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\nnet/http/httputil\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, runtime, the\ngo\ncommand, and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.15.15 milestone\non our issue tracker for details.\ngo1.14 (released 2020-02-25)\nGo 1.14 is a major release of Go.\n\tRead the\nGo 1.14 Release Notes\nfor more information.\nMinor revisions\ngo1.14.1\n\t\t(released 2020-03-19)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the go command, tools, and the runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.1 milestone\non our issue tracker for details.\ngo1.14.2\n\t\t(released 2020-04-08)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the go command, the runtime, and the\nos/exec\nand\ntesting\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.2 milestone\non our issue tracker for details.\ngo1.14.3\n\t\t(released 2020-05-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the compiler, the runtime, and the\ngo/doc\nand\nmath/big\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.3 milestone\non our issue tracker for details.\ngo1.14.4\n\t\t(released 2020-06-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the\ngo\ndoc\ncommand, the runtime, and the\nencoding/json\nand\nos\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.4 milestone\non our issue tracker for details.\ngo1.14.5\n\t\t(released 2020-07-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/x509\nand\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.5 milestone\non our issue tracker for details.\ngo1.14.6\n\t\t(released 2020-07-16)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the\ngo\ncommand, the compiler, the linker, vet, and the\ndatabase/sql\n,\nencoding/json\n,\nnet/http\n,\nreflect\n, and\ntesting\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.6 milestone\non our issue tracker for details.\ngo1.14.7\n\t\t(released 2020-08-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nencoding/binary\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.7 milestone\non our issue tracker for details.\ngo1.14.8\n\t\t(released 2020-09-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http/cgi\nand\nnet/http/fcgi\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.8 milestone\non our issue tracker for details.\ngo1.14.9\n\t\t(released 2020-09-09)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, linker, runtime, documentation, and the\nnet/http\nand\ntesting\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.9 milestone\non our issue tracker for details.\ngo1.14.10\n\t\t(released 2020-10-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, runtime, and the\nplugin\nand\ntesting\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.10 milestone\non our issue tracker for details.\ngo1.14.11\n\t\t(released 2020-11-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the runtime, and the\nnet/http\nand\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.11 milestone\non our issue tracker for details.\ngo1.14.12\n\t\t(released 2020-11-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo\ncommand and the\nmath/big\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.12 milestone\non our issue tracker for details.\ngo1.14.13\n\t\t(released 2020-12-03)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, runtime, and the\ngo\ncommand.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.13 milestone\non our issue tracker for details.\ngo1.14.14\n\t\t(released 2021-01-19)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ngo\ncommand and the\ncrypto/elliptic\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.14 milestone\non our issue tracker for details.\ngo1.14.15\n\t\t(released 2021-02-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, runtime, the\ngo\ncommand, and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.14.15 milestone\non our issue tracker for details.\ngo1.13 (released 2019-09-03)\nGo 1.13 is a major release of Go.\n\tRead the\nGo 1.13 Release Notes\nfor more information.\nMinor revisions\ngo1.13.1\n\t\t(released 2019-09-25)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\nand\nnet/textproto\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.1 milestone\non our issue tracker for details.\ngo1.13.2\n\t\t(released 2019-10-17)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the compiler and the\ncrypto/dsa\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.2 milestone\non our issue tracker for details.\ngo1.13.3\n\t\t(released 2019-10-17)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the go command, the toolchain, the runtime, and the\ncrypto/ecdsa\n,\nnet\n,\nnet/http\n, and\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.3 milestone\non our issue tracker for details.\ngo1.13.4\n\t\t(released 2019-10-31)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the\nnet/http\nand\nsyscall\npackages.\n\t\t\tIt also fixes an issue on macOS 10.15 Catalina\nwhere the non-notarized installer and binaries were being\nrejected by Gatekeeper\n.\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.4 milestone\non our issue tracker for details.\ngo1.13.5\n\t\t(released 2019-12-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the go command, the runtime, the linker, and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.5 milestone\non our issue tracker for details.\ngo1.13.6\n\t\t(released 2020-01-09)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the runtime and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.6 milestone\non our issue tracker for details.\ngo1.13.7\n\t\t(released 2020-01-28)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ttwo\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/x509\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.7 milestone\non our issue tracker for details.\ngo1.13.8\n\t\t(released 2020-02-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the runtime, and the\ncrypto/x509\nand\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.8 milestone\non our issue tracker for details.\ngo1.13.9\n\t\t(released 2020-03-19)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the go command, tools, the runtime, the toolchain, and the\ncrypto/cypher\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.9 milestone\non our issue tracker for details.\ngo1.13.10\n\t\t(released 2020-04-08)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the go command, the runtime, and the\nos/exec\nand\ntime\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.10 milestone\non our issue tracker for details.\ngo1.13.11\n\t\t(released 2020-05-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.11 milestone\non our issue tracker for details.\ngo1.13.12\n\t\t(released 2020-06-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the runtime, and the\ngo/types\nand\nmath/big\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.12 milestone\non our issue tracker for details.\ngo1.13.13\n\t\t(released 2020-07-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/x509\nand\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.13 milestone\non our issue tracker for details.\ngo1.13.14\n\t\t(released 2020-07-16)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, vet, and the\ndatabase/sql\n,\nnet/http\n, and\nreflect\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.14 milestone\non our issue tracker for details.\ngo1.13.15\n\t\t(released 2020-08-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nencoding/binary\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.13.15 milestone\non our issue tracker for details.\ngo1.12 (released 2019-02-25)\nGo 1.12 is a major release of Go.\n\tRead the\nGo 1.12 Release Notes\nfor more information.\nMinor revisions\ngo1.12.1\n\t\t(released 2019-03-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the compiler, the go command, and the\nfmt\n,\nnet/smtp\n,\nos\n,\npath/filepath\n,\nsync\n, and\ntext/template\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.1 milestone\non our issue tracker for details.\ngo1.12.2\n\t\t(released 2019-04-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the runtime, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, the go command, and the\ndoc\n,\nnet\n,\nnet/http/httputil\n, and\nos\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.2 milestone\non our issue tracker for details.\ngo1.12.3\n\t\t(released 2019-04-08)\n\t\t\n\t\t\twas accidentally released without its\nintended fix. It is identical to go1.12.2, except for its version\nnumber. The intended fix is in go1.12.4.\ngo1.12.4\n\t\t(released 2019-04-11)\n\t\t\n\t\t\tfixes an issue where using the prebuilt binary\nreleases on older versions of GNU/Linux\nled to failures\nwhen linking programs that used cgo.\nOnly Linux users who hit this issue need to update.\ngo1.12.5\n\t\t(released 2019-05-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, the linker, the go command, the runtime, and the\nos\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.5 milestone\non our issue tracker for details.\ngo1.12.6\n\t\t(released 2019-06-11)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, the linker, the go command, and the\ncrypto/x509\n,\nnet/http\n, and\nos\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.6 milestone\non our issue tracker for details.\ngo1.12.7\n\t\t(released 2019-07-08)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the compiler, and the linker.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.7 milestone\non our issue tracker for details.\ngo1.12.8\n\t\t(released 2019-08-13)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\nand\nnet/url\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.8 milestone\non our issue tracker for details.\ngo1.12.9\n\t\t(released 2019-08-15)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the linker, and the\nmath/big\nand\nos\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.9 milestone\non our issue tracker for details.\ngo1.12.10\n\t\t(released 2019-09-25)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\nand\nnet/textproto\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.10 milestone\non our issue tracker for details.\ngo1.12.11\n\t\t(released 2019-10-17)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/dsa\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.11 milestone\non our issue tracker for details.\ngo1.12.12\n\t\t(released 2019-10-17)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the go command, runtime, and the\nnet\nand\nsyscall\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.12 milestone\non our issue tracker for details.\ngo1.12.13\n\t\t(released 2019-10-31)\n\t\t\n\t\t\tfixes an issue on macOS 10.15 Catalina\nwhere the non-notarized installer and binaries were being\nrejected by Gatekeeper\n.\nOnly macOS users who hit this issue need to update.\ngo1.12.14\n\t\t(released 2019-12-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\t \n\t\t\t\tfix to the runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.14 milestone\non our issue tracker for details.\ngo1.12.15\n\t\t(released 2020-01-09)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the runtime and the\nnet/http\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.15 milestone\non our issue tracker for details.\ngo1.12.16\n\t\t(released 2020-01-28)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ttwo\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\ncrypto/x509\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.16 milestone\non our issue tracker for details.\ngo1.12.17\n\t\t(released 2020-02-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\t \n\t\t\t\tfix to the runtime.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.12.17 milestone\non our issue tracker for details.\ngo1.11 (released 2018-08-24)\nGo 1.11 is a major release of Go.\n\tRead the\nGo 1.11 Release Notes\nfor more information.\nMinor revisions\ngo1.11.1\n\t\t(released 2018-10-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, documentation, go command, runtime, and the\ncrypto/x509\n,\nencoding/json\n,\ngo/types\n,\nnet\n,\nnet/http\n, and\nreflect\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.1 milestone\non our issue tracker for details.\ngo1.11.2\n\t\t(released 2018-11-02)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, linker, documentation, go command, and the\ndatabase/sql\nand\ngo/types\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.2 milestone\non our issue tracker for details.\ngo1.11.3\n\t\t(released 2018-12-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\tthree\n\t\t\t\tsecurity\n\t\t\t\tfixes to \"go get\" and the\ncrypto/x509\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.3 milestone\non our issue tracker for details.\ngo1.11.4\n\t\t(released 2018-12-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the compiler, linker, runtime, documentation, go command, and the\ngo/types\nand\nnet/http\npackages.\n\t\t\tIt includes a fix to a bug introduced in Go 1.11.3 that broke\ngo\nget\nfor import path patterns containing \"\n...\n\".\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.4 milestone\non our issue tracker for details.\ngo1.11.5\n\t\t(released 2019-01-23)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\ncrypto/elliptic\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.5 milestone\non our issue tracker for details.\ngo1.11.6\n\t\t(released 2019-03-14)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to cgo, the compiler, linker, runtime, go command, and the\ncrypto/x509\n,\nencoding/json\n,\nnet\n, and\nnet/url\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.6 milestone\non our issue tracker for details.\ngo1.11.7\n\t\t(released 2019-04-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the runtime and the\nnet\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.7 milestone\non our issue tracker for details.\ngo1.11.8\n\t\t(released 2019-04-08)\n\t\t\n\t\t\twas accidentally released without its\nintended fix. It is identical to go1.11.7, except for its version\nnumber. The intended fix is in go1.11.9.\ngo1.11.9\n\t\t(released 2019-04-11)\n\t\t\n\t\t\tfixes an issue where using the prebuilt binary\nreleases on older versions of GNU/Linux\nled to failures\nwhen linking programs that used cgo.\nOnly Linux users who hit this issue need to update.\ngo1.11.10\n\t\t(released 2019-05-06)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the runtime, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the linker.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.10 milestone\non our issue tracker for details.\ngo1.11.11\n\t\t(released 2019-06-11)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\t \n\t\t\t\tfix to the\ncrypto/x509\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.11 milestone\non our issue tracker for details.\ngo1.11.12\n\t\t(released 2019-07-08)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler and the linker.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.12 milestone\non our issue tracker for details.\ngo1.11.13\n\t\t(released 2019-08-13)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/http\nand\nnet/url\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.11.13 milestone\non our issue tracker for details.\ngo1.10 (released 2018-02-16)\nGo 1.10 is a major release of Go.\n\tRead the\nGo 1.10 Release Notes\nfor more information.\nMinor revisions\ngo1.10.1\n\t\t(released 2018-03-28)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the go command, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, runtime, and the\narchive/zip\n,\ncrypto/tls\n,\ncrypto/x509\n,\nencoding/json\n,\nnet\n,\nnet/http\n, and\nnet/http/pprof\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.10.1 milestone\non our issue tracker for details.\ngo1.10.2\n\t\t(released 2018-05-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, linker, and go command.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.10.2 milestone\non our issue tracker for details.\ngo1.10.3\n\t\t(released 2018-06-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the go command, and the\ncrypto/tls\n,\ncrypto/x509\n, and\nstrings\npackages.\n\t\t\tIn particular, it adds\nminimal support to the go command for the vgo transition\n.\n\n\t\t\t\n\t\t\tSee the\nGo 1.10.3 milestone\non our issue tracker for details.\ngo1.10.4\n\t\t(released 2018-08-24)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the go command, linker, and the\nbytes\n,\nmime/multipart\n,\nnet/http\n, and\nstrings\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.10.4 milestone\non our issue tracker for details.\ngo1.10.5\n\t\t(released 2018-11-02)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the go command, linker, runtime, and the\ndatabase/sql\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.10.5 milestone\non our issue tracker for details.\ngo1.10.6\n\t\t(released 2018-12-12)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\tthree\n\t\t\t\tsecurity\n\t\t\t\tfixes to \"go get\" and the\ncrypto/x509\npackage.\n\t\t\tIt contains the same fixes as Go 1.11.3 and was released at the same time.\n\n\t\t\t\n\t\t\tSee the\nGo 1.10.6 milestone\non our issue tracker for details.\ngo1.10.7\n\t\t(released 2018-12-14)\n\t\t\n\t\t\tincludes a fix to a bug introduced in Go 1.10.6\nthat broke\ngo\nget\nfor import path patterns containing\n\"\n...\n\".\nSee the\nGo 1.10.7 milestone\non our issue tracker for details.\ngo1.10.8\n\t\t(released 2019-01-23)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to the\ncrypto/elliptic\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.10.8 milestone\non our issue tracker for details.\ngo1.9 (released 2017-08-24)\nGo 1.9 is a major release of Go.\n\tRead the\nGo 1.9 Release Notes\nfor more information.\nMinor revisions\ngo1.9.1\n\t\t(released 2017-10-04)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ttwo\n\t\t\t\tsecurity\n\t\t\t\tfixes.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.9.1 milestone\non our issue tracker for details.\ngo1.9.2\n\t\t(released 2017-10-25)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler, linker, runtime, documentation,\ngo\ncommand, and the\ncrypto/x509\n,\ndatabase/sql\n,\nlog\n, and\nnet/smtp\npackages.\n\t\t\tIt includes a fix to a bug introduced in Go 1.9.1 that broke\ngo\nget\nof non-Git repositories under certain conditions.\n\n\t\t\t\n\t\t\tSee the\nGo 1.9.2 milestone\non our issue tracker for details.\ngo1.9.3\n\t\t(released 2018-01-22)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the\nnet/url\npackage, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, runtime, and the\ndatabase/sql\n,\nmath/big\n, and\nnet/http\npackages.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.9.3 milestone\non our issue tracker for details.\ngo1.9.4\n\t\t(released 2018-02-07)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\ta\n\t\t\t\tsecurity\n\t\t\t\tfix to \"go get\".\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.9.4 milestone\non our issue tracker for details.\ngo1.9.5\n\t\t(released 2018-03-28)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\tsecurity\n\t\t\t\tfixes to the go command, as well as\n\t\t\t\t\n\t\t\t\tbug \n\t\t\t\tfixes to the compiler, go command, and the\nnet/http/pprof\npackage.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.9.5 milestone\non our issue tracker for details.\ngo1.9.6\n\t\t(released 2018-05-01)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the compiler and go command.\n\t\t\t\n\n\t\t\t\n\t\t\tSee the\nGo 1.9.6 milestone\non our issue tracker for details.\ngo1.9.7\n\t\t(released 2018-06-05)\n\t\t\n\t\t\tincludes\n\t\t\t\n\t\t\t\t\n\t\t\t\t \n\t\t\t\tfixes to the go command, and the\ncrypto/x509\nand\nstrings\npackages.\n\t\t\tIn particular, it adds\nminimal support to the go command for the vgo transition\n.\n\n\t\t\t\n\t\t\tSee the\nGo 1.9.7 milestone\non our issue tracker for details.\ngo1.8 (released 2017-02-16)\nGo 1.8 is a major release of Go.\nRead the\nGo 1.8 Release Notes\nfor more information.\nMinor revisions\ngo1.8.1 (released 2017-04-07) includes fixes to the compiler, linker, runtime,\ndocumentation,\ngo\ncommand and the\ncrypto/tls\n,\nencoding/xml\n,\nimage/png\n,\nnet\n,\nnet/http\n,\nreflect\n,\ntext/template\n,\nand\ntime\npackages.\nSee the\nGo\n1.8.1 milestone\non our issue tracker for details.\ngo1.8.2 (released 2017-05-23) includes a security fix to the\ncrypto/elliptic\npackage.\nSee the\nGo\n1.8.2 milestone\non our issue tracker for details.\ngo1.8.3 (released 2017-05-24) includes fixes to the compiler, runtime,\ndocumentation, and the\ndatabase/sql\npackage.\nSee the\nGo\n1.8.3 milestone\non our issue tracker for details.\ngo1.8.4 (released 2017-10-04) includes two security fixes.\nIt contains the same fixes as Go 1.9.1 and was released at the same time.\nSee the\nGo\n1.8.4 milestone\non our issue tracker for details.\ngo1.8.5 (released 2017-10-25) includes fixes to the compiler, linker, runtime,\ndocumentation,\ngo\ncommand,\nand the\ncrypto/x509\nand\nnet/smtp\npackages.\nIt includes a fix to a bug introduced in Go 1.8.4 that broke\ngo\nget\nof non-Git repositories under certain conditions.\nSee the\nGo\n1.8.5 milestone\non our issue tracker for details.\ngo1.8.6 (released 2018-01-22) includes the same fix in\nmath/big\nas Go 1.9.3 and was released at the same time.\nSee the\nGo\n1.8.6 milestone\non our issue tracker for details.\ngo1.8.7 (released 2018-02-07) includes a security fix to \"go get\".\nIt contains the same fix as Go 1.9.4 and was released at the same time.\nSee the\nGo\n1.8.7\nmilestone on our issue tracker for details.\ngo1.7 (released 2016-08-15)\nGo 1.7 is a major release of Go.\nRead the\nGo 1.7 Release Notes\nfor more information.\nMinor revisions\ngo1.7.1 (released 2016-09-07) includes fixes to the compiler, runtime,\ndocumentation, and the\ncompress/flate\n,\nhash/crc32\n,\nio\n,\nnet\n,\nnet/http\n,\npath/filepath\n,\nreflect\n, and\nsyscall\npackages.\nSee the\nGo\n1.7.1 milestone\non our issue tracker for details.\ngo1.7.2 should not be used. It was tagged but not fully released.\nThe release was deferred due to a last minute bug report.\nUse go1.7.3 instead, and refer to the summary of changes below.\ngo1.7.3 (released 2016-10-19) includes fixes to the compiler, runtime,\nand the\ncrypto/cipher\n,\ncrypto/tls\n,\nnet/http\n, and\nstrings\npackages.\nSee the\nGo\n1.7.3 milestone\non our issue tracker for details.\ngo1.7.4 (released 2016-12-01) includes two security fixes.\nSee the\nGo\n1.7.4 milestone\non our issue tracker for details.\ngo1.7.5 (released 2017-01-26) includes fixes to the compiler, runtime,\nand the\ncrypto/x509\nand\ntime\npackages.\nSee the\nGo\n1.7.5 milestone\non our issue tracker for details.\ngo1.7.6 (released 2017-05-23) includes the same security fix as Go 1.8.2 and\nwas released at the same time.\nSee the\nGo\n1.8.2 milestone\non our issue tracker for details.\ngo1.6 (released 2016-02-17)\nGo 1.6 is a major release of Go.\nRead the\nGo 1.6 Release Notes\nfor more information.\nMinor revisions\ngo1.6.1 (released 2016-04-12) includes two security fixes.\nSee the\nGo\n1.6.1 milestone\non our issue tracker for details.\ngo1.6.2 (released 2016-04-20) includes fixes to the compiler, runtime, tools,\ndocumentation, and the\nmime/multipart\n,\nnet/http\n, and\nsort\npackages.\nSee the\nGo\n1.6.2 milestone\non our issue tracker for details.\ngo1.6.3 (released 2016-07-17) includes security fixes to the\nnet/http/cgi\npackage and\nnet/http\npackage when used in\na CGI environment.\nSee the\nGo\n1.6.3 milestone\non our issue tracker for details.\ngo1.6.4 (released 2016-12-01) includes two security fixes.\nIt contains the same fixes as Go 1.7.4 and was released at the same time.\nSee the\nGo\n1.7.4 milestone\non our issue tracker for details.\ngo1.5 (released 2015-08-19)\nGo 1.5 is a major release of Go.\nRead the\nGo 1.5 Release Notes\nfor more information.\nMinor revisions\ngo1.5.1 (released 2015-09-08) includes bug fixes to the compiler, assembler, and\nthe\nfmt\n,\nnet/textproto\n,\nnet/http\n, and\nruntime\npackages.\nSee the\nGo\n1.5.1 milestone\non our issue tracker for details.\ngo1.5.2 (released 2015-12-02) includes bug fixes to the compiler, linker, and\nthe\nmime/multipart\n,\nnet\n, and\nruntime\npackages.\nSee the\nGo\n1.5.2 milestone\non our issue tracker for details.\ngo1.5.3 (released 2016-01-13) includes a security fix to the\nmath/big\npackage\naffecting the\ncrypto/tls\npackage.\nSee the\nrelease announcement\nfor details.\ngo1.5.4 (released 2016-04-12) includes two security fixes.\nIt contains the same fixes as Go 1.6.1 and was released at the same time.\nSee the\nGo\n1.6.1 milestone\non our issue tracker for details.\ngo1.4 (released 2014-12-10)\nGo 1.4 is a major release of Go.\nRead the\nGo 1.4 Release Notes\nfor more information.\nMinor revisions\ngo1.4.1 (released 2015-01-15) includes bug fixes to the linker and the\nlog\n,\nsyscall\n, and\nruntime\npackages.\nSee the\nGo 1.4.1 milestone on our issue tracker\nfor details.\ngo1.4.2 (released 2015-02-17) includes security fixes to the compiler, and bug fixes to the\ngo\ncommand, the compiler and linker, and the\nruntime\n,\nsyscall\n,\nreflect\n, and\nmath/big\npackages.\nSee the\nGo 1.4.2 milestone on our issue tracker\nfor details.\ngo1.4.3 (released 2015-09-22) includes security fixes to the\nnet/http\npackage and bug fixes to the\nruntime\npackage.\nSee the\nGo 1.4.3 milestone on our issue tracker\nfor details.\ngo1.3 (released 2014-06-18)\nGo 1.3 is a major release of Go.\nRead the\nGo 1.3 Release Notes\nfor more information.\nMinor revisions\ngo1.3.1 (released 2014-08-13) includes bug fixes to the compiler and the\nruntime\n,\nnet\n, and\ncrypto/rsa\npackages.\nSee the\nchange history\nfor details.\ngo1.3.2 (released 2014-09-25) includes security fixes to the\ncrypto/tls\npackage and bug fixes to cgo.\nSee the\nchange history\nfor details.\ngo1.3.3 (released 2014-09-30) includes further bug fixes to cgo, the runtime package, and the nacl port.\nSee the\nchange history\nfor details.\ngo1.2 (released 2013-12-01)\nGo 1.2 is a major release of Go.\nRead the\nGo 1.2 Release Notes\nfor more information.\nMinor revisions\ngo1.2.1 (released 2014-03-02) includes bug fixes to the\nruntime\n,\nnet\n, and\ndatabase/sql\npackages.\nSee the\nchange history\nfor details.\ngo1.2.2 (released 2014-05-05) includes a\nsecurity fix\nthat affects the tour binary included in the binary distributions (thanks to Guillaume T).\ngo1.1 (released 2013-05-13)\nGo 1.1 is a major release of Go.\nRead the\nGo 1.1 Release Notes\nfor more information.\nMinor revisions\ngo1.1.1 (released 2013-06-13) includes a security fix to the compiler and several bug fixes to the compiler and runtime.\nSee the\nchange history\nfor details.\ngo1.1.2 (released 2013-08-13) includes fixes to the\ngc\ncompiler\nand\ncgo\n, and the\nbufio\n,\nruntime\n,\nsyscall\n, and\ntime\npackages.\nSee the\nchange history\nfor details.\nIf you use package syscall's\nGetrlimit\nand\nSetrlimit\nfunctions under Linux on the ARM or 386 architectures, please note change\n11803043\nthat fixes\nissue 5949\n.\ngo1 (released 2012-03-28)\nGo 1 is a major release of Go that will be stable in the long term.\nRead the\nGo 1 Release Notes\nfor more information.\nIt is intended that programs written for Go 1 will continue to compile and run\ncorrectly, unchanged, under future versions of Go 1.\nRead the\nGo 1 compatibility document\nfor more\nabout the future of Go 1.\nThe go1 release corresponds to\nweekly.2012-03-27\n.\nMinor revisions\ngo1.0.1 (released 2012-04-25) was issued to\nfix\nan\nescape analysis bug\nthat can lead to memory corruption.\nIt also includes several minor code and documentation fixes.\ngo1.0.2 (released 2012-06-13) was issued to fix two bugs in the implementation\nof maps using struct or array keys:\nissue 3695\nand\nissue 3573\n.\nIt also includes many minor code and documentation fixes.\ngo1.0.3 (released 2012-09-21) includes minor code and documentation fixes.\nSee the\ngo1 release branch history\nfor the complete list of changes.\nOlder releases\nSee the\nPre-Go 1 Release History\npage for notes\non earlier releases.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nEffective Go\nEffective Go\nIntroduction\nGo is a new language.  Although it borrows ideas from\nexisting languages,\nit has unusual properties that make effective Go programs\ndifferent in character from programs written in its relatives.\nA straightforward translation of a C++ or Java program into Go\nis unlikely to produce a satisfactory result—Java programs\nare written in Java, not Go.\nOn the other hand, thinking about the problem from a Go\nperspective could produce a successful but quite different\nprogram.\nIn other words,\nto write Go well, it's important to understand its properties\nand idioms.\nIt's also important to know the established conventions for\nprogramming in Go, such as naming, formatting, program\nconstruction, and so on, so that programs you write\nwill be easy for other Go programmers to understand.\nThis document gives tips for writing clear, idiomatic Go code.\nIt augments the\nlanguage specification\n,\nthe\nTour of Go\n,\nand\nHow to Write Go Code\n,\nall of which you\nshould read first.\nNote added January, 2022:\nThis document was written for Go's\nrelease in 2009, and has not been updated significantly since.\nAlthough it is a good guide to understand how to use the language\nitself, thanks to the stability of the language, it says little\nabout the libraries and nothing about significant changes to the\nGo ecosystem since it was written, such as the build system, testing,\nmodules, and polymorphism.\nThere are no plans to update it, as so much has happened and a large\nand growing set of documents, blogs, and books do a fine job of\ndescribing modern Go usage.\nEffective Go continues to be useful, but the reader should\nunderstand it is far from a complete guide.\nSee\nissue\n28782\nfor context.\nExamples\nThe\nGo package sources\nare intended to serve not\nonly as the core library but also as examples of how to\nuse the language.\nMoreover, many of the packages contain working, self-contained\nexecutable examples you can run directly from the\ngo.dev\nweb site, such as\nthis one\n(if\nnecessary, click on the word \"Example\" to open it up).\nIf you have a question about how to approach a problem or how something\nmight be implemented, the documentation, code and examples in the\nlibrary can provide answers, ideas and\nbackground.\nFormatting\nFormatting issues are the most contentious\nbut the least consequential.\nPeople can adapt to different formatting styles\nbut it's better if they don't have to, and\nless time is devoted to the topic\nif everyone adheres to the same style.\nThe problem is how to approach this Utopia without a long\nprescriptive style guide.\nWith Go we take an unusual\napproach and let the machine\ntake care of most formatting issues.\nThe\ngofmt\nprogram\n(also available as\ngo fmt\n, which\noperates at the package level rather than source file level)\nreads a Go program\nand emits the source in a standard style of indentation\nand vertical alignment, retaining and if necessary\nreformatting comments.\nIf you want to know how to handle some new layout\nsituation, run\ngofmt\n; if the answer doesn't\nseem right, rearrange your program (or file a bug about\ngofmt\n),\ndon't work around it.\nAs an example, there's no need to spend time lining up\nthe comments on the fields of a structure.\nGofmt\nwill do that for you.  Given the\ndeclaration\ntype T struct {\n    name string // name of the object\n    value int // its value\n}\ngofmt\nwill line up the columns:\ntype T struct {\n    name    string // name of the object\n    value   int    // its value\n}\nAll Go code in the standard packages has been formatted with\ngofmt\n.\nSome formatting details remain.  Very briefly:\nIndentation\nWe use tabs for indentation and\ngofmt\nemits them by default.\n    Use spaces only if you must.\nLine length\nGo has no line length limit.  Don't worry about overflowing a punched card.\n    If a line feels too long, wrap it and indent with an extra tab.\nParentheses\nGo needs fewer parentheses than C and Java: control structures (\nif\n,\nfor\n,\nswitch\n) do not have parentheses in\n    their syntax.\n    Also, the operator precedence hierarchy is shorter and clearer, so\nx<<8 + y<<16\nmeans what the spacing implies, unlike in the other languages.\nCommentary\nGo provides C-style\n/* */\nblock comments\nand C++-style\n//\nline comments.\nLine comments are the norm;\nblock comments appear mostly as package comments, but\nare useful within an expression or to disable large swaths of code.\nComments that appear before top-level declarations, with no intervening newlines,\nare considered to document the declaration itself.\nThese “doc comments” are the primary documentation for a given Go package or command.\nFor more about doc comments, see “\nGo Doc Comments\n”.\nNames\nNames are as important in Go as in any other language.\nThey even have semantic effect:\nthe visibility of a name outside a package is determined by whether its\nfirst character is upper case.\nIt's therefore worth spending a little time talking about naming conventions\nin Go programs.\nPackage names\nWhen a package is imported, the package name becomes an accessor for the\ncontents.  After\nimport \"bytes\"\nthe importing package can talk about\nbytes.Buffer\n.  It's\nhelpful if everyone using the package can use the same name to refer to\nits contents, which implies that the package name should be good:\nshort, concise, evocative.  By convention, packages are given\nlower case, single-word names; there should be no need for underscores\nor mixedCaps.\nErr on the side of brevity, since everyone using your\npackage will be typing that name.\nAnd don't worry about collisions\na priori\n.\nThe package name is only the default name for imports; it need not be unique\nacross all source code, and in the rare case of a collision the\nimporting package can choose a different name to use locally.\nIn any case, confusion is rare because the file name in the import\ndetermines just which package is being used.\nAnother convention is that the package name is the base name of\nits source directory;\nthe package in\nsrc/encoding/base64\nis imported as\n\"encoding/base64\"\nbut has name\nbase64\n,\nnot\nencoding_base64\nand not\nencodingBase64\n.\nThe importer of a package will use the name to refer to its contents,\nso exported names in the package can use that fact\nto avoid repetition.\n(Don't use the\nimport .\nnotation, which can simplify\ntests that must run outside the package they are testing, but should otherwise be avoided.)\nFor instance, the buffered reader type in the\nbufio\npackage is called\nReader\n,\nnot\nBufReader\n, because users see it as\nbufio.Reader\n,\nwhich is a clear, concise name.\nMoreover,\nbecause imported entities are always addressed with their package name,\nbufio.Reader\ndoes not conflict with\nio.Reader\n.\nSimilarly, the function to make new instances of\nring.Ring\n—which\nis the definition of a\nconstructor\nin Go—would\nnormally be called\nNewRing\n, but since\nRing\nis the only type exported by the package, and since the\npackage is called\nring\n, it's called just\nNew\n,\nwhich clients of the package see as\nring.New\n.\nUse the package structure to help you choose good names.\nAnother short example is\nonce.Do\n;\nonce.Do(setup)\nreads well and would not be improved by\nwriting\nonce.DoOrWaitUntilDone(setup)\n.\nLong names don't automatically make things more readable.\nA helpful doc comment can often be more valuable than an extra long name.\nGetters\nGo doesn't provide automatic support for getters and setters.\nThere's nothing wrong with providing getters and setters yourself,\nand it's often appropriate to do so, but it's neither idiomatic nor necessary\nto put\nGet\ninto the getter's name.  If you have a field called\nowner\n(lower case, unexported), the getter method should be\ncalled\nOwner\n(upper case, exported), not\nGetOwner\n.\nThe use of upper-case names for export provides the hook to discriminate\nthe field from the method.\nA setter function, if needed, will likely be called\nSetOwner\n.\nBoth names read well in practice:\nowner := obj.Owner()\nif owner != user {\n    obj.SetOwner(user)\n}\nInterface names\nBy convention, one-method interfaces are named by\nthe method name plus an -er suffix or similar modification\nto construct an agent noun:\nReader\n,\nWriter\n,\nFormatter\n,\nCloseNotifier\netc.\nThere are a number of such names and it's productive to honor them and the function\nnames they capture.\nRead\n,\nWrite\n,\nClose\n,\nFlush\n,\nString\nand so on have\ncanonical signatures and meanings.  To avoid confusion,\ndon't give your method one of those names unless it\nhas the same signature and meaning.\nConversely, if your type implements a method with the\nsame meaning as a method on a well-known type,\ngive it the same name and signature;\ncall your string-converter method\nString\nnot\nToString\n.\nMixedCaps\nFinally, the convention in Go is to use\nMixedCaps\nor\nmixedCaps\nrather than underscores to write\nmultiword names.\nSemicolons\nLike C, Go's formal grammar uses semicolons to terminate statements,\nbut unlike in C, those semicolons do not appear in the source.\nInstead the lexer uses a simple rule to insert semicolons automatically\nas it scans, so the input text is mostly free of them.\nThe rule is this. If the last token before a newline is an identifier\n(which includes words like\nint\nand\nfloat64\n),\na basic literal such as a number or string constant, or one of the\ntokens\nbreak continue fallthrough return ++ -- ) }\nthe lexer always inserts a semicolon after the token.\nThis could be summarized as, “if the newline comes\nafter a token that could end a statement, insert a semicolon”.\nA semicolon can also be omitted immediately before a closing brace,\nso a statement such as\ngo func() { for { dst <- <-src } }()\nneeds no semicolons.\nIdiomatic Go programs have semicolons only in places such as\nfor\nloop clauses, to separate the initializer, condition, and\ncontinuation elements.  They are also necessary to separate multiple\nstatements on a line, should you write code that way.\nOne consequence of the semicolon insertion rules\nis that you cannot put the opening brace of a\ncontrol structure (\nif\n,\nfor\n,\nswitch\n,\nor\nselect\n) on the next line.  If you do, a semicolon\nwill be inserted before the brace, which could cause unwanted\neffects.  Write them like this\nif i < f() {\n    g()\n}\nnot like this\nif i < f()  // wrong!\n{           // wrong!\n    g()\n}\nControl structures\nThe control structures of Go are related to those of C but differ\nin important ways.\nThere is no\ndo\nor\nwhile\nloop, only a\nslightly generalized\nfor\n;\nswitch\nis more flexible;\nif\nand\nswitch\naccept an optional\ninitialization statement like that of\nfor\n;\nbreak\nand\ncontinue\nstatements\ntake an optional label to identify what to break or continue;\nand there are new control structures including a type switch and a\nmultiway communications multiplexer,\nselect\n.\nThe syntax is also slightly different:\nthere are no parentheses\nand the bodies must always be brace-delimited.\nIf\nIn Go a simple\nif\nlooks like this:\nif x > 0 {\n    return y\n}\nMandatory braces encourage writing simple\nif\nstatements\non multiple lines.  It's good style to do so anyway,\nespecially when the body contains a control statement such as a\nreturn\nor\nbreak\n.\nSince\nif\nand\nswitch\naccept an initialization\nstatement, it's common to see one used to set up a local variable.\nif err := file.Chmod(0664); err != nil {\n    log.Print(err)\n    return err\n}\nIn the Go libraries, you'll find that\nwhen an\nif\nstatement doesn't flow into the next statement—that is,\nthe body ends in\nbreak\n,\ncontinue\n,\ngoto\n, or\nreturn\n—the unnecessary\nelse\nis omitted.\nf, err := os.Open(name)\nif err != nil {\n    return err\n}\ncodeUsing(f)\nThis is an example of a common situation where code must guard against a\nsequence of error conditions.  The code reads well if the\nsuccessful flow of control runs down the page, eliminating error cases\nas they arise.  Since error cases tend to end in\nreturn\nstatements, the resulting code needs no\nelse\nstatements.\nf, err := os.Open(name)\nif err != nil {\n    return err\n}\nd, err := f.Stat()\nif err != nil {\n    f.Close()\n    return err\n}\ncodeUsing(f, d)\nRedeclaration and reassignment\nAn aside: The last example in the previous section demonstrates a detail of how the\n:=\nshort declaration form works.\nThe declaration that calls\nos.Open\nreads,\nf, err := os.Open(name)\nThis statement declares two variables,\nf\nand\nerr\n.\nA few lines later, the call to\nf.Stat\nreads,\nd, err := f.Stat()\nwhich looks as if it declares\nd\nand\nerr\n.\nNotice, though, that\nerr\nappears in both statements.\nThis duplication is legal:\nerr\nis declared by the first statement,\nbut only\nre-assigned\nin the second.\nThis means that the call to\nf.Stat\nuses the existing\nerr\nvariable declared above, and just gives it a new value.\nIn a\n:=\ndeclaration a variable\nv\nmay appear even\nif it has already been declared, provided:\nthis declaration is in the same scope as the existing declaration of\nv\n(if\nv\nis already declared in an outer scope, the declaration will create a new variable §),\nthe corresponding value in the initialization is assignable to\nv\n, and\nthere is at least one other variable that is created by the declaration.\nThis unusual property is pure pragmatism,\nmaking it easy to use a single\nerr\nvalue, for example,\nin a long\nif-else\nchain.\nYou'll see it used often.\n§ It's worth noting here that in Go the scope of function parameters and return values\nis the same as the function body, even though they appear lexically outside the braces\nthat enclose the body.\nFor\nThe Go\nfor\nloop is similar to—but not the same as—C's.\nIt unifies\nfor\nand\nwhile\nand there is no\ndo-while\n.\nThere are three forms, only one of which has semicolons.\n// Like a C for\nfor init; condition; post { }\n\n// Like a C while\nfor condition { }\n\n// Like a C for(;;)\nfor { }\nShort declarations make it easy to declare the index variable right in the loop.\nsum := 0\nfor i := 0; i < 10; i++ {\n    sum += i\n}\nIf you're looping over an array, slice, string, or map,\nor reading from a channel, a\nrange\nclause can\nmanage the loop.\nfor key, value := range oldMap {\n    newMap[key] = value\n}\nIf you only need the first item in the range (the key or index), drop the second:\nfor key := range m {\n    if key.expired() {\n        delete(m, key)\n    }\n}\nIf you only need the second item in the range (the value), use the\nblank identifier\n, an underscore, to discard the first:\nsum := 0\nfor _, value := range array {\n    sum += value\n}\nThe blank identifier has many uses, as described in\na later section\n.\nFor strings, the\nrange\ndoes more work for you, breaking out individual\nUnicode code points by parsing the UTF-8.\nErroneous encodings consume one byte and produce the\nreplacement rune U+FFFD.\n(The name (with associated builtin type)\nrune\nis Go terminology for a\nsingle Unicode code point.\nSee\nthe language specification\nfor details.)\nThe loop\nfor pos, char := range \"日本\\x80語\" { // \\x80 is an illegal UTF-8 encoding\n    fmt.Printf(\"character %#U starts at byte position %d\\n\", char, pos)\n}\nprints\ncharacter U+65E5 '日' starts at byte position 0\ncharacter U+672C '本' starts at byte position 3\ncharacter U+FFFD '�' starts at byte position 6\ncharacter U+8A9E '語' starts at byte position 7\nFinally, Go has no comma operator and\n++\nand\n--\nare statements not expressions.\nThus if you want to run multiple variables in a\nfor\nyou should use parallel assignment (although that precludes\n++\nand\n--\n).\n// Reverse a\nfor i, j := 0, len(a)-1; i < j; i, j = i+1, j-1 {\n    a[i], a[j] = a[j], a[i]\n}\nSwitch\nGo's\nswitch\nis more general than C's.\nThe expressions need not be constants or even integers,\nthe cases are evaluated top to bottom until a match is found,\nand if the\nswitch\nhas no expression it switches on\ntrue\n.\nIt's therefore possible—and idiomatic—to write an\nif\n-\nelse\n-\nif\n-\nelse\nchain as a\nswitch\n.\nfunc unhex(c byte) byte {\n    switch {\n    case '0' <= c && c <= '9':\n        return c - '0'\n    case 'a' <= c && c <= 'f':\n        return c - 'a' + 10\n    case 'A' <= c && c <= 'F':\n        return c - 'A' + 10\n    }\n    return 0\n}\nThere is no automatic fall through, but cases can be presented\nin comma-separated lists.\nfunc shouldEscape(c byte) bool {\n    switch c {\n    case ' ', '?', '&', '=', '#', '+', '%':\n        return true\n    }\n    return false\n}\nAlthough they are not nearly as common in Go as some other C-like\nlanguages,\nbreak\nstatements can be used to terminate\na\nswitch\nearly.\nSometimes, though, it's necessary to break out of a surrounding loop,\nnot the switch, and in Go that can be accomplished by putting a label\non the loop and \"breaking\" to that label.\nThis example shows both uses.\nLoop:\n    for n := 0; n < len(src); n += size {\n        switch {\n        case src[n] < sizeOne:\n            if validateOnly {\n                break\n            }\n            size = 1\n            update(src[n])\n\n        case src[n] < sizeTwo:\n            if n+1 >= len(src) {\n                err = errShortInput\n                break Loop\n            }\n            if validateOnly {\n                break\n            }\n            size = 2\n            update(src[n] + src[n+1]<<shift)\n        }\n    }\nOf course, the\ncontinue\nstatement also accepts an optional label\nbut it applies only to loops.\nTo close this section, here's a comparison routine for byte slices that uses two\nswitch\nstatements:\n// Compare returns an integer comparing the two byte slices,\n// lexicographically.\n// The result will be 0 if a == b, -1 if a < b, and +1 if a > b\nfunc Compare(a, b []byte) int {\n    for i := 0; i < len(a) && i < len(b); i++ {\n        switch {\n        case a[i] > b[i]:\n            return 1\n        case a[i] < b[i]:\n            return -1\n        }\n    }\n    switch {\n    case len(a) > len(b):\n        return 1\n    case len(a) < len(b):\n        return -1\n    }\n    return 0\n}\nType switch\nA switch can also be used to discover the dynamic type of an interface\nvariable.  Such a\ntype switch\nuses the syntax of a type\nassertion with the keyword\ntype\ninside the parentheses.\nIf the switch declares a variable in the expression, the variable will\nhave the corresponding type in each clause.\nIt's also idiomatic to reuse the name in such cases, in effect declaring\na new variable with the same name but a different type in each case.\nvar t interface{}\nt = functionOfSomeType()\nswitch t := t.(type) {\ndefault:\n    fmt.Printf(\"unexpected type %T\\n\", t)     // %T prints whatever type t has\ncase bool:\n    fmt.Printf(\"boolean %t\\n\", t)             // t has type bool\ncase int:\n    fmt.Printf(\"integer %d\\n\", t)             // t has type int\ncase *bool:\n    fmt.Printf(\"pointer to boolean %t\\n\", *t) // t has type *bool\ncase *int:\n    fmt.Printf(\"pointer to integer %d\\n\", *t) // t has type *int\n}\nFunctions\nMultiple return values\nOne of Go's unusual features is that functions and methods\ncan return multiple values.  This form can be used to\nimprove on a couple of clumsy idioms in C programs: in-band\nerror returns such as\n-1\nfor\nEOF\nand modifying an argument passed by address.\nIn C, a write error is signaled by a negative count with the\nerror code secreted away in a volatile location.\nIn Go,\nWrite\ncan return a count\nand\nan error: “Yes, you wrote some\nbytes but not all of them because you filled the device”.\nThe signature of the\nWrite\nmethod on files from\npackage\nos\nis:\nfunc (file *File) Write(b []byte) (n int, err error)\nand as the documentation says, it returns the number of bytes\nwritten and a non-nil\nerror\nwhen\nn\n!=\nlen(b)\n.\nThis is a common style; see the section on error handling for more examples.\nA similar approach obviates the need to pass a pointer to a return\nvalue to simulate a reference parameter.\nHere's a simple-minded function to\ngrab a number from a position in a byte slice, returning the number\nand the next position.\nfunc nextInt(b []byte, i int) (int, int) {\n    for ; i < len(b) && !isDigit(b[i]); i++ {\n    }\n    x := 0\n    for ; i < len(b) && isDigit(b[i]); i++ {\n        x = x*10 + int(b[i]) - '0'\n    }\n    return x, i\n}\nYou could use it to scan the numbers in an input slice\nb\nlike this:\nfor i := 0; i < len(b); {\n        x, i = nextInt(b, i)\n        fmt.Println(x)\n    }\nNamed result parameters\nThe return or result \"parameters\" of a Go function can be given names and\nused as regular variables, just like the incoming parameters.\nWhen named, they are initialized to the zero values for their types when\nthe function begins; if the function executes a\nreturn\nstatement\nwith no arguments, the current values of the result parameters are\nused as the returned values.\nThe names are not mandatory but they can make code shorter and clearer:\nthey're documentation.\nIf we name the results of\nnextInt\nit becomes\nobvious which returned\nint\nis which.\nfunc nextInt(b []byte, pos int) (value, nextPos int) {\nBecause named results are initialized and tied to an unadorned return, they can simplify\nas well as clarify.  Here's a version\nof\nio.ReadFull\nthat uses them well:\nfunc ReadFull(r Reader, buf []byte) (n int, err error) {\n    for len(buf) > 0 && err == nil {\n        var nr int\n        nr, err = r.Read(buf)\n        n += nr\n        buf = buf[nr:]\n    }\n    return\n}\nDefer\nGo's\ndefer\nstatement schedules a function call (the\ndeferred\nfunction) to be run immediately before the function\nexecuting the\ndefer\nreturns.  It's an unusual but\neffective way to deal with situations such as resources that must be\nreleased regardless of which path a function takes to return.  The\ncanonical examples are unlocking a mutex or closing a file.\n// Contents returns the file's contents as a string.\nfunc Contents(filename string) (string, error) {\n    f, err := os.Open(filename)\n    if err != nil {\n        return \"\", err\n    }\n    defer f.Close()  // f.Close will run when we're finished.\n\n    var result []byte\n    buf := make([]byte, 100)\n    for {\n        n, err := f.Read(buf[0:])\n        result = append(result, buf[0:n]...) // append is discussed later.\n        if err != nil {\n            if err == io.EOF {\n                break\n            }\n            return \"\", err  // f will be closed if we return here.\n        }\n    }\n    return string(result), nil // f will be closed if we return here.\n}\nDeferring a call to a function such as\nClose\nhas two advantages.  First, it\nguarantees that you will never forget to close the file, a mistake\nthat's easy to make if you later edit the function to add a new return\npath.  Second, it means that the close sits near the open,\nwhich is much clearer than placing it at the end of the function.\nThe arguments to the deferred function (which include the receiver if\nthe function is a method) are evaluated when the\ndefer\nexecutes, not when the\ncall\nexecutes.  Besides avoiding worries\nabout variables changing values as the function executes, this means\nthat a single deferred call site can defer multiple function\nexecutions.  Here's a silly example.\nfor i := 0; i < 5; i++ {\n    defer fmt.Printf(\"%d \", i)\n}\nDeferred functions are executed in LIFO order, so this code will cause\n4 3 2 1 0\nto be printed when the function returns.  A\nmore plausible example is a simple way to trace function execution\nthrough the program.  We could write a couple of simple tracing\nroutines like this:\nfunc trace(s string)   { fmt.Println(\"entering:\", s) }\nfunc untrace(s string) { fmt.Println(\"leaving:\", s) }\n\n// Use them like this:\nfunc a() {\n    trace(\"a\")\n    defer untrace(\"a\")\n    // do something....\n}\nWe can do better by exploiting the fact that arguments to deferred\nfunctions are evaluated when the\ndefer\nexecutes.  The\ntracing routine can set up the argument to the untracing routine.\nThis example:\nfunc trace(s string) string {\n    fmt.Println(\"entering:\", s)\n    return s\n}\n\nfunc un(s string) {\n    fmt.Println(\"leaving:\", s)\n}\n\nfunc a() {\n    defer un(trace(\"a\"))\n    fmt.Println(\"in a\")\n}\n\nfunc b() {\n    defer un(trace(\"b\"))\n    fmt.Println(\"in b\")\n    a()\n}\n\nfunc main() {\n    b()\n}\nprints\nentering: b\nin b\nentering: a\nin a\nleaving: a\nleaving: b\nFor programmers accustomed to block-level resource management from\nother languages,\ndefer\nmay seem peculiar, but its most\ninteresting and powerful applications come precisely from the fact\nthat it's not block-based but function-based.  In the section on\npanic\nand\nrecover\nwe'll see another\nexample of its possibilities.\nData\nAllocation with\nnew\nGo has two allocation primitives, the built-in functions\nnew\nand\nmake\n.\nThey do different things and apply to different types, which can be confusing,\nbut the rules are simple.\nLet's talk about\nnew\nfirst.\nIt's a built-in function that allocates memory, but unlike its namesakes\nin some other languages it does not\ninitialize\nthe memory,\nit only\nzeros\nit.\nThat is,\nnew(T)\nallocates zeroed storage for a new item of type\nT\nand returns its address, a value of type\n*T\n.\nIn Go terminology, it returns a pointer to a newly allocated zero value of type\nT\n.\nSince the memory returned by\nnew\nis zeroed, it's helpful to arrange\nwhen designing your data structures that the\nzero value of each type can be used without further initialization.  This means a user of\nthe data structure can create one with\nnew\nand get right to\nwork.\nFor example, the documentation for\nbytes.Buffer\nstates that\n\"the zero value for\nBuffer\nis an empty buffer ready to use.\"\nSimilarly,\nsync.Mutex\ndoes not\nhave an explicit constructor or\nInit\nmethod.\nInstead, the zero value for a\nsync.Mutex\nis defined to be an unlocked mutex.\nThe zero-value-is-useful property works transitively. Consider this type declaration.\ntype SyncedBuffer struct {\n    lock    sync.Mutex\n    buffer  bytes.Buffer\n}\nValues of type\nSyncedBuffer\nare also ready to use immediately upon allocation\nor just declaration.  In the next snippet, both\np\nand\nv\nwill work\ncorrectly without further arrangement.\np := new(SyncedBuffer)  // type *SyncedBuffer\nvar v SyncedBuffer      // type  SyncedBuffer\nConstructors and composite literals\nSometimes the zero value isn't good enough and an initializing\nconstructor is necessary, as in this example derived from\npackage\nos\n.\nfunc NewFile(fd int, name string) *File {\n    if fd < 0 {\n        return nil\n    }\n    f := new(File)\n    f.fd = fd\n    f.name = name\n    f.dirinfo = nil\n    f.nepipe = 0\n    return f\n}\nThere's a lot of boilerplate in there.  We can simplify it\nusing a\ncomposite literal\n, which is\nan expression that creates a\nnew instance each time it is evaluated.\nfunc NewFile(fd int, name string) *File {\n    if fd < 0 {\n        return nil\n    }\n    f := File{fd, name, nil, 0}\n    return &f\n}\nNote that, unlike in C, it's perfectly OK to return the address of a local variable;\nthe storage associated with the variable survives after the function\nreturns.\nIn fact, taking the address of a composite literal\nallocates a fresh instance each time it is evaluated,\nso we can combine these last two lines.\nreturn &File{fd, name, nil, 0}\nThe fields of a composite literal are laid out in order and must all be present.\nHowever, by labeling the elements explicitly as\nfield\n:\nvalue\npairs, the initializers can appear in any\norder, with the missing ones left as their respective zero values.  Thus we could say\nreturn &File{fd: fd, name: name}\nAs a limiting case, if a composite literal contains no fields at all, it creates\na zero value for the type.  The expressions\nnew(File)\nand\n&File{}\nare equivalent.\nComposite literals can also be created for arrays, slices, and maps,\nwith the field labels being indices or map keys as appropriate.\nIn these examples, the initializations work regardless of the values of\nEnone\n,\nEio\n, and\nEinval\n, as long as they are distinct.\na := [...]string   {Enone: \"no error\", Eio: \"Eio\", Einval: \"invalid argument\"}\ns := []string      {Enone: \"no error\", Eio: \"Eio\", Einval: \"invalid argument\"}\nm := map[int]string{Enone: \"no error\", Eio: \"Eio\", Einval: \"invalid argument\"}\nAllocation with\nmake\nBack to allocation.\nThe built-in function\nmake(T,\nargs\n)\nserves\na purpose different from\nnew(T)\n.\nIt creates slices, maps, and channels only, and it returns an\ninitialized\n(not\nzeroed\n)\nvalue of type\nT\n(not\n*T\n).\nThe reason for the distinction\nis that these three types represent, under the covers, references to data structures that\nmust be initialized before use.\nA slice, for example, is a three-item descriptor\ncontaining a pointer to the data (inside an array), the length, and the\ncapacity, and until those items are initialized, the slice is\nnil\n.\nFor slices, maps, and channels,\nmake\ninitializes the internal data structure and prepares\nthe value for use.\nFor instance,\nmake([]int, 10, 100)\nallocates an array of 100 ints and then creates a slice\nstructure with length 10 and a capacity of 100 pointing at the first\n10 elements of the array.\n(When making a slice, the capacity can be omitted; see the section on slices\nfor more information.)\nIn contrast,\nnew([]int)\nreturns a pointer to a newly allocated, zeroed slice\nstructure, that is, a pointer to a\nnil\nslice value.\nThese examples illustrate the difference between\nnew\nand\nmake\n.\nvar p *[]int = new([]int)       // allocates slice structure; *p == nil; rarely useful\nvar v  []int = make([]int, 100) // the slice v now refers to a new array of 100 ints\n\n// Unnecessarily complex:\nvar p *[]int = new([]int)\n*p = make([]int, 100, 100)\n\n// Idiomatic:\nv := make([]int, 100)\nRemember that\nmake\napplies only to maps, slices and channels\nand does not return a pointer.\nTo obtain an explicit pointer allocate with\nnew\nor take the address\nof a variable explicitly.\nArrays\nArrays are useful when planning the detailed layout of memory and sometimes\ncan help avoid allocation, but primarily\nthey are a building block for slices, the subject of the next section.\nTo lay the foundation for that topic, here are a few words about arrays.\nThere are major differences between the ways arrays work in Go and C.\nIn Go,\nArrays are values. Assigning one array to another copies all the elements.\nIn particular, if you pass an array to a function, it\nwill receive a\ncopy\nof the array, not a pointer to it.\nThe size of an array is part of its type.  The types\n[10]int\nand\n[20]int\nare distinct.\nThe value property can be useful but also expensive; if you want C-like behavior and efficiency,\nyou can pass a pointer to the array.\nfunc Sum(a *[3]float64) (sum float64) {\n    for _, v := range *a {\n        sum += v\n    }\n    return\n}\n\narray := [...]float64{7.0, 8.5, 9.1}\nx := Sum(&array)  // Note the explicit address-of operator\nBut even this style isn't idiomatic Go.\nUse slices instead.\nSlices\nSlices wrap arrays to give a more general, powerful, and convenient\ninterface to sequences of data.  Except for items with explicit\ndimension such as transformation matrices, most array programming in\nGo is done with slices rather than simple arrays.\nSlices hold references to an underlying array, and if you assign one\nslice to another, both refer to the same array.\nIf a function takes a slice argument, changes it makes to\nthe elements of the slice will be visible to the caller, analogous to\npassing a pointer to the underlying array.  A\nRead\nfunction can therefore accept a slice argument rather than a pointer\nand a count; the length within the slice sets an upper\nlimit of how much data to read.  Here is the signature of the\nRead\nmethod of the\nFile\ntype in package\nos\n:\nfunc (f *File) Read(buf []byte) (n int, err error)\nThe method returns the number of bytes read and an error value, if\nany.\nTo read into the first 32 bytes of a larger buffer\nbuf\n,\nslice\n(here used as a verb) the buffer.\nn, err := f.Read(buf[0:32])\nSuch slicing is common and efficient.  In fact, leaving efficiency aside for\nthe moment, the following snippet would also read the first 32 bytes of the buffer.\nvar n int\n    var err error\n    for i := 0; i < 32; i++ {\n        nbytes, e := f.Read(buf[i:i+1])  // Read one byte.\n        n += nbytes\n        if nbytes == 0 || e != nil {\n            err = e\n            break\n        }\n    }\nThe length of a slice may be changed as long as it still fits within\nthe limits of the underlying array; just assign it to a slice of\nitself.  The\ncapacity\nof a slice, accessible by the built-in\nfunction\ncap\n, reports the maximum length the slice may\nassume.  Here is a function to append data to a slice.  If the data\nexceeds the capacity, the slice is reallocated.  The\nresulting slice is returned.  The function uses the fact that\nlen\nand\ncap\nare legal when applied to the\nnil\nslice, and return 0.\nfunc Append(slice, data []byte) []byte {\n    l := len(slice)\n    if l + len(data) > cap(slice) {  // reallocate\n        // Allocate double what's needed, for future growth.\n        newSlice := make([]byte, (l+len(data))*2)\n        // The copy function is predeclared and works for any slice type.\n        copy(newSlice, slice)\n        slice = newSlice\n    }\n    slice = slice[0:l+len(data)]\n    copy(slice[l:], data)\n    return slice\n}\nWe must return the slice afterwards because, although\nAppend\ncan modify the elements of\nslice\n, the slice itself (the run-time data\nstructure holding the pointer, length, and capacity) is passed by value.\nThe idea of appending to a slice is so useful it's captured by the\nappend\nbuilt-in function.  To understand that function's\ndesign, though, we need a little more information, so we'll return\nto it later.\nTwo-dimensional slices\nGo's arrays and slices are one-dimensional.\nTo create the equivalent of a 2D array or slice, it is necessary to define an array-of-arrays\nor slice-of-slices, like this:\ntype Transform [3][3]float64  // A 3x3 array, really an array of arrays.\ntype LinesOfText [][]byte     // A slice of byte slices.\nBecause slices are variable-length, it is possible to have each inner\nslice be a different length.\nThat can be a common situation, as in our\nLinesOfText\nexample: each line has an independent length.\ntext := LinesOfText{\n    []byte(\"Now is the time\"),\n    []byte(\"for all good gophers\"),\n    []byte(\"to bring some fun to the party.\"),\n}\nSometimes it's necessary to allocate a 2D slice, a situation that can arise when\nprocessing scan lines of pixels, for instance.\nThere are two ways to achieve this.\nOne is to allocate each slice independently; the other\nis to allocate a single array and point the individual slices into it.\nWhich to use depends on your application.\nIf the slices might grow or shrink, they should be allocated independently\nto avoid overwriting the next line; if not, it can be more efficient to construct\nthe object with a single allocation.\nFor reference, here are sketches of the two methods.\nFirst, a line at a time:\n// Allocate the top-level slice.\npicture := make([][]uint8, YSize) // One row per unit of y.\n// Loop over the rows, allocating the slice for each row.\nfor i := range picture {\n    picture[i] = make([]uint8, XSize)\n}\nAnd now as one allocation, sliced into lines:\n// Allocate the top-level slice, the same as before.\npicture := make([][]uint8, YSize) // One row per unit of y.\n// Allocate one large slice to hold all the pixels.\npixels := make([]uint8, XSize*YSize) // Has type []uint8 even though picture is [][]uint8.\n// Loop over the rows, slicing each row from the front of the remaining pixels slice.\nfor i := range picture {\n    picture[i], pixels = pixels[:XSize], pixels[XSize:]\n}\nMaps\nMaps are a convenient and powerful built-in data structure that associate\nvalues of one type (the\nkey\n) with values of another type\n(the\nelement\nor\nvalue\n).\nThe key can be of any type for which the equality operator is defined,\nsuch as integers,\nfloating point and complex numbers,\nstrings, pointers, interfaces (as long as the dynamic type\nsupports equality), structs and arrays.\nSlices cannot be used as map keys,\nbecause equality is not defined on them.\nLike slices, maps hold references to an underlying data structure.\nIf you pass a map to a function\nthat changes the contents of the map, the changes will be visible\nin the caller.\nMaps can be constructed using the usual composite literal syntax\nwith colon-separated key-value pairs,\nso it's easy to build them during initialization.\nvar timeZone = map[string]int{\n    \"UTC\":  0*60*60,\n    \"EST\": -5*60*60,\n    \"CST\": -6*60*60,\n    \"MST\": -7*60*60,\n    \"PST\": -8*60*60,\n}\nAssigning and fetching map values looks syntactically just like\ndoing the same for arrays and slices except that the index doesn't\nneed to be an integer.\noffset := timeZone[\"EST\"]\nAn attempt to fetch a map value with a key that\nis not present in the map will return the zero value for the type\nof the entries\nin the map.  For instance, if the map contains integers, looking\nup a non-existent key will return\n0\n.\nA set can be implemented as a map with value type\nbool\n.\nSet the map entry to\ntrue\nto put the value in the set, and then\ntest it by simple indexing.\nattended := map[string]bool{\n    \"Ann\": true,\n    \"Joe\": true,\n    ...\n}\n\nif attended[person] { // will be false if person is not in the map\n    fmt.Println(person, \"was at the meeting\")\n}\nSometimes you need to distinguish a missing entry from\na zero value.  Is there an entry for\n\"UTC\"\nor is that 0 because it's not in the map at all?\nYou can discriminate with a form of multiple assignment.\nvar seconds int\nvar ok bool\nseconds, ok = timeZone[tz]\nFor obvious reasons this is called the “comma ok” idiom.\nIn this example, if\ntz\nis present,\nseconds\nwill be set appropriately and\nok\nwill be true; if not,\nseconds\nwill be set to zero and\nok\nwill\nbe false.\nHere's a function that puts it together with a nice error report:\nfunc offset(tz string) int {\n    if seconds, ok := timeZone[tz]; ok {\n        return seconds\n    }\n    log.Println(\"unknown time zone:\", tz)\n    return 0\n}\nTo test for presence in the map without worrying about the actual value,\nyou can use the\nblank identifier\n(\n_\n)\nin place of the usual variable for the value.\n_, present := timeZone[tz]\nTo delete a map entry, use the\ndelete\nbuilt-in function, whose arguments are the map and the key to be deleted.\nIt's safe to do this even if the key is already absent\nfrom the map.\ndelete(timeZone, \"PDT\")  // Now on Standard Time\nPrinting\nFormatted printing in Go uses a style similar to C's\nprintf\nfamily but is richer and more general. The functions live in the\nfmt\npackage and have capitalized names:\nfmt.Printf\n,\nfmt.Fprintf\n,\nfmt.Sprintf\nand so on.  The string functions (\nSprintf\netc.)\nreturn a string rather than filling in a provided buffer.\nYou don't need to provide a format string.  For each of\nPrintf\n,\nFprintf\nand\nSprintf\nthere is another pair\nof functions, for instance\nPrint\nand\nPrintln\n.\nThese functions do not take a format string but instead generate a default\nformat for each argument. The\nPrintln\nversions also insert a blank\nbetween arguments and append a newline to the output while\nthe\nPrint\nversions add blanks only if the operand on neither side is a string.\nIn this example each line produces the same output.\nfmt.Printf(\"Hello %d\\n\", 23)\nfmt.Fprint(os.Stdout, \"Hello \", 23, \"\\n\")\nfmt.Println(\"Hello\", 23)\nfmt.Println(fmt.Sprint(\"Hello \", 23))\nThe formatted print functions\nfmt.Fprint\nand friends take as a first argument any object\nthat implements the\nio.Writer\ninterface; the variables\nos.Stdout\nand\nos.Stderr\nare familiar instances.\nHere things start to diverge from C.  First, the numeric formats such as\n%d\ndo not take flags for signedness or size; instead, the printing routines use the\ntype of the argument to decide these properties.\nvar x uint64 = 1<<64 - 1\nfmt.Printf(\"%d %x; %d %x\\n\", x, x, int64(x), int64(x))\nprints\n18446744073709551615 ffffffffffffffff; -1 -1\nIf you just want the default conversion, such as decimal for integers, you can use\nthe catchall format\n%v\n(for “value”); the result is exactly\nwhat\nPrint\nand\nPrintln\nwould produce.\nMoreover, that format can print\nany\nvalue, even arrays, slices, structs, and\nmaps.  Here is a print statement for the time zone map defined in the previous section.\nfmt.Printf(\"%v\\n\", timeZone)  // or just fmt.Println(timeZone)\nwhich gives output:\nmap[CST:-21600 EST:-18000 MST:-25200 PST:-28800 UTC:0]\nFor maps,\nPrintf\nand friends sort the output lexicographically by key.\nWhen printing a struct, the modified format\n%+v\nannotates the\nfields of the structure with their names, and for any value the alternate\nformat\n%#v\nprints the value in full Go syntax.\ntype T struct {\n    a int\n    b float64\n    c string\n}\nt := &T{ 7, -2.35, \"abc\\tdef\" }\nfmt.Printf(\"%v\\n\", t)\nfmt.Printf(\"%+v\\n\", t)\nfmt.Printf(\"%#v\\n\", t)\nfmt.Printf(\"%#v\\n\", timeZone)\nprints\n&{7 -2.35 abc   def}\n&{a:7 b:-2.35 c:abc     def}\n&main.T{a:7, b:-2.35, c:\"abc\\tdef\"}\nmap[string]int{\"CST\":-21600, \"EST\":-18000, \"MST\":-25200, \"PST\":-28800, \"UTC\":0}\n(Note the ampersands.)\nThat quoted string format is also available through\n%q\nwhen\napplied to a value of type\nstring\nor\n[]byte\n.\nThe alternate format\n%#q\nwill use backquotes instead if possible.\n(The\n%q\nformat also applies to integers and runes, producing a\nsingle-quoted rune constant.)\nAlso,\n%x\nworks on strings, byte arrays and byte slices as well as\non integers, generating a long hexadecimal string, and with\na space in the format (\n% x\n) it puts spaces between the bytes.\nAnother handy format is\n%T\n, which prints the\ntype\nof a value.\nfmt.Printf(\"%T\\n\", timeZone)\nprints\nmap[string]int\nIf you want to control the default format for a custom type, all that's required is to define\na method with the signature\nString() string\non the type.\nFor our simple type\nT\n, that might look like this.\nfunc (t *T) String() string {\n    return fmt.Sprintf(\"%d/%g/%q\", t.a, t.b, t.c)\n}\nfmt.Printf(\"%v\\n\", t)\nto print in the format\n7/-2.35/\"abc\\tdef\"\n(If you need to print\nvalues\nof type\nT\nas well as pointers to\nT\n,\nthe receiver for\nString\nmust be of value type; this example used a pointer because\nthat's more efficient and idiomatic for struct types.\nSee the section below on\npointers vs. value receivers\nfor more information.)\nOur\nString\nmethod is able to call\nSprintf\nbecause the\nprint routines are fully reentrant and can be wrapped this way.\nThere is one important detail to understand about this approach,\nhowever: don't construct a\nString\nmethod by calling\nSprintf\nin a way that will recur into your\nString\nmethod indefinitely.  This can happen if the\nSprintf\ncall attempts to print the receiver directly as a string, which in\nturn will invoke the method again.  It's a common and easy mistake\nto make, as this example shows.\ntype MyString string\n\nfunc (m MyString) String() string {\n    return fmt.Sprintf(\"MyString=%s\", m) // Error: will recur forever.\n}\nIt's also easy to fix: convert the argument to the basic string type, which does not have the\nmethod.\ntype MyString string\nfunc (m MyString) String() string {\n    return fmt.Sprintf(\"MyString=%s\", string(m)) // OK: note conversion.\n}\nIn the\ninitialization section\nwe'll see another technique that avoids this recursion.\nAnother printing technique is to pass a print routine's arguments directly to another such routine.\nThe signature of\nPrintf\nuses the type\n...interface{}\nfor its final argument to specify that an arbitrary number of parameters (of arbitrary type)\ncan appear after the format.\nfunc Printf(format string, v ...interface{}) (n int, err error) {\nWithin the function\nPrintf\n,\nv\nacts like a variable of type\n[]interface{}\nbut if it is passed to another variadic function, it acts like\na regular list of arguments.\nHere is the implementation of the\nfunction\nlog.Println\nwe used above. It passes its arguments directly to\nfmt.Sprintln\nfor the actual formatting.\n// Println prints to the standard logger in the manner of fmt.Println.\nfunc Println(v ...interface{}) {\n    std.Output(2, fmt.Sprintln(v...))  // Output takes parameters (int, string)\n}\nWe write\n...\nafter\nv\nin the nested call to\nSprintln\nto tell the\ncompiler to treat\nv\nas a list of arguments; otherwise it would just pass\nv\nas a single slice argument.\nThere's even more to printing than we've covered here.  See the\ngodoc\ndocumentation\nfor package\nfmt\nfor the details.\nBy the way, a\n...\nparameter can be of a specific type, for instance\n...int\nfor a min function that chooses the least of a list of integers:\nfunc Min(a ...int) int {\n    min := int(^uint(0) >> 1)  // largest int\n    for _, i := range a {\n        if i < min {\n            min = i\n        }\n    }\n    return min\n}\nAppend\nNow we have the missing piece we needed to explain the design of\nthe\nappend\nbuilt-in function.  The signature of\nappend\nis different from our custom\nAppend\nfunction above.\nSchematically, it's like this:\nfunc append(slice []\nT\n, elements ...\nT\n) []\nT\nwhere\nT\nis a placeholder for any given type.  You can't\nactually write a function in Go where the type\nT\nis determined by the caller.\nThat's why\nappend\nis built in: it needs support from the\ncompiler.\nWhat\nappend\ndoes is append the elements to the end of\nthe slice and return the result.  The result needs to be returned\nbecause, as with our hand-written\nAppend\n, the underlying\narray may change.  This simple example\nx := []int{1,2,3}\nx = append(x, 4, 5, 6)\nfmt.Println(x)\nprints\n[1 2 3 4 5 6]\n.  So\nappend\nworks a\nlittle like\nPrintf\n, collecting an arbitrary number of\narguments.\nBut what if we wanted to do what our\nAppend\ndoes and\nappend a slice to a slice?  Easy: use\n...\nat the call\nsite, just as we did in the call to\nOutput\nabove.  This\nsnippet produces identical output to the one above.\nx := []int{1,2,3}\ny := []int{4,5,6}\nx = append(x, y...)\nfmt.Println(x)\nWithout that\n...\n, it wouldn't compile because the types\nwould be wrong;\ny\nis not of type\nint\n.\nInitialization\nAlthough it doesn't look superficially very different from\ninitialization in C or C++, initialization in Go is more powerful.\nComplex structures can be built during initialization and the ordering\nissues among initialized objects, even among different packages, are handled\ncorrectly.\nConstants\nConstants in Go are just that—constant.\nThey are created at compile time, even when defined as\nlocals in functions,\nand can only be numbers, characters (runes), strings or booleans.\nBecause of the compile-time restriction, the expressions\nthat define them must be constant expressions,\nevaluatable by the compiler.  For instance,\n1<<3\nis a constant expression, while\nmath.Sin(math.Pi/4)\nis not because\nthe function call to\nmath.Sin\nneeds\nto happen at run time.\nIn Go, enumerated constants are created using the\niota\nenumerator.  Since\niota\ncan be part of an expression and\nexpressions can be implicitly repeated, it is easy to build intricate\nsets of values.\ntype ByteSize float64\n\nconst (\n    _           = iota\n// ignore first value by assigning to blank identifier\nKB ByteSize = 1 << (10 * iota)\n    MB\n    GB\n    TB\n    PB\n    EB\n    ZB\n    YB\n)\nThe ability to attach a method such as\nString\nto any\nuser-defined type makes it possible for arbitrary values to format themselves\nautomatically for printing.\nAlthough you'll see it most often applied to structs, this technique is also useful for\nscalar types such as floating-point types like\nByteSize\n.\nfunc (b ByteSize) String() string {\n    switch {\n    case b >= YB:\n        return fmt.Sprintf(\"%.2fYB\", b/YB)\n    case b >= ZB:\n        return fmt.Sprintf(\"%.2fZB\", b/ZB)\n    case b >= EB:\n        return fmt.Sprintf(\"%.2fEB\", b/EB)\n    case b >= PB:\n        return fmt.Sprintf(\"%.2fPB\", b/PB)\n    case b >= TB:\n        return fmt.Sprintf(\"%.2fTB\", b/TB)\n    case b >= GB:\n        return fmt.Sprintf(\"%.2fGB\", b/GB)\n    case b >= MB:\n        return fmt.Sprintf(\"%.2fMB\", b/MB)\n    case b >= KB:\n        return fmt.Sprintf(\"%.2fKB\", b/KB)\n    }\n    return fmt.Sprintf(\"%.2fB\", b)\n}\nThe expression\nYB\nprints as\n1.00YB\n,\nwhile\nByteSize(1e13)\nprints as\n9.09TB\n.\nThe use here of\nSprintf\nto implement\nByteSize\n's\nString\nmethod is safe\n(avoids recurring indefinitely) not because of a conversion but\nbecause it calls\nSprintf\nwith\n%f\n,\nwhich is not a string format:\nSprintf\nwill only call\nthe\nString\nmethod when it wants a string, and\n%f\nwants a floating-point value.\nVariables\nVariables can be initialized just like constants but the\ninitializer can be a general expression computed at run time.\nvar (\n    home   = os.Getenv(\"HOME\")\n    user   = os.Getenv(\"USER\")\n    gopath = os.Getenv(\"GOPATH\")\n)\nThe init function\nFinally, each source file can define its own niladic\ninit\nfunction to\nset up whatever state is required.  (Actually each file can have multiple\ninit\nfunctions.)\nAnd finally means finally:\ninit\nis called after all the\nvariable declarations in the package have evaluated their initializers,\nand those are evaluated only after all the imported packages have been\ninitialized.\nBesides initializations that cannot be expressed as declarations,\na common use of\ninit\nfunctions is to verify or repair\ncorrectness of the program state before real execution begins.\nfunc init() {\n    if user == \"\" {\n        log.Fatal(\"$USER not set\")\n    }\n    if home == \"\" {\n        home = \"/home/\" + user\n    }\n    if gopath == \"\" {\n        gopath = home + \"/go\"\n    }\n    // gopath may be overridden by --gopath flag on command line.\n    flag.StringVar(&gopath, \"gopath\", gopath, \"override default GOPATH\")\n}\nMethods\nPointers vs. Values\nAs we saw with\nByteSize\n,\nmethods can be defined for any named type (except a pointer or an interface);\nthe receiver does not have to be a struct.\nIn the discussion of slices above, we wrote an\nAppend\nfunction.  We can define it as a method on slices instead.  To do\nthis, we first declare a named type to which we can bind the method, and\nthen make the receiver for the method a value of that type.\ntype ByteSlice []byte\n\nfunc (slice ByteSlice) Append(data []byte) []byte {\n    // Body exactly the same as the Append function defined above.\n}\nThis still requires the method to return the updated slice.  We can\neliminate that clumsiness by redefining the method to take a\npointer\nto a\nByteSlice\nas its receiver, so the\nmethod can overwrite the caller's slice.\nfunc (p *ByteSlice) Append(data []byte) {\n    slice := *p\n    // Body as above, without the return.\n    *p = slice\n}\nIn fact, we can do even better.  If we modify our function so it looks\nlike a standard\nWrite\nmethod, like this,\nfunc (p *ByteSlice) Write(data []byte) (n int, err error) {\n    slice := *p\n    // Again as above.\n    *p = slice\n    return len(data), nil\n}\nthen the type\n*ByteSlice\nsatisfies the standard interface\nio.Writer\n, which is handy.  For instance, we can\nprint into one.\nvar b ByteSlice\n    fmt.Fprintf(&b, \"This hour has %d days\\n\", 7)\nWe pass the address of a\nByteSlice\nbecause only\n*ByteSlice\nsatisfies\nio.Writer\n.\nThe rule about pointers vs. values for receivers is that value methods\ncan be invoked on pointers and values, but pointer methods can only be\ninvoked on pointers.\nThis rule arises because pointer methods can modify the receiver; invoking\nthem on a value would cause the method to receive a copy of the value, so\nany modifications would be discarded.\nThe language therefore disallows this mistake.\nThere is a handy exception, though. When the value is addressable, the\nlanguage takes care of the common case of invoking a pointer method on a\nvalue by inserting the address operator automatically.\nIn our example, the variable\nb\nis addressable, so we can call\nits\nWrite\nmethod with just\nb.Write\n. The compiler\nwill rewrite that to\n(&b).Write\nfor us.\nBy the way, the idea of using\nWrite\non a slice of bytes\nis central to the implementation of\nbytes.Buffer\n.\nInterfaces and other types\nInterfaces\nInterfaces in Go provide a way to specify the behavior of an\nobject: if something can do\nthis\n, then it can be used\nhere\n.  We've seen a couple of simple examples already;\ncustom printers can be implemented by a\nString\nmethod\nwhile\nFprintf\ncan generate output to anything\nwith a\nWrite\nmethod.\nInterfaces with only one or two methods are common in Go code, and are\nusually given a name derived from the method, such as\nio.Writer\nfor something that implements\nWrite\n.\nA type can implement multiple interfaces.\nFor instance, a collection can be sorted\nby the routines in package\nsort\nif it implements\nsort.Interface\n, which contains\nLen()\n,\nLess(i, j int) bool\n, and\nSwap(i, j int)\n,\nand it could also have a custom formatter.\nIn this contrived example\nSequence\nsatisfies both.\ntype Sequence []int\n// Methods required by sort.Interface.\nfunc (s Sequence) Len() int {\n    return len(s)\n}\nfunc (s Sequence) Less(i, j int) bool {\n    return s[i] < s[j]\n}\nfunc (s Sequence) Swap(i, j int) {\n    s[i], s[j] = s[j], s[i]\n}\n// Copy returns a copy of the Sequence.\nfunc (s Sequence) Copy() Sequence {\n    copy := make(Sequence, 0, len(s))\n    return append(copy, s...)\n}\n// Method for printing - sorts the elements before printing.\nfunc (s Sequence) String() string {\n    s = s.Copy()\n// Make a copy; don't overwrite argument.\nsort.Sort(s)\n    str := \"[\"\n    for i, elem := range s {\n// Loop is O(N²); will fix that in next example.\nif i > 0 {\n            str += \" \"\n        }\n        str += fmt.Sprint(elem)\n    }\n    return str + \"]\"\n}\nConversions\nThe\nString\nmethod of\nSequence\nis recreating the\nwork that\nSprint\nalready does for slices.\n(It also has complexity O(N²), which is poor.) We can share the\neffort (and also speed it up) if we convert the\nSequence\nto a plain\n[]int\nbefore calling\nSprint\n.\nfunc (s Sequence) String() string {\n    s = s.Copy()\n    sort.Sort(s)\n    return fmt.Sprint([]int(s))\n}\nThis method is another example of the conversion technique for calling\nSprintf\nsafely from a\nString\nmethod.\nBecause the two types (\nSequence\nand\n[]int\n)\nare the same if we ignore the type name, it's legal to convert between them.\nThe conversion doesn't create a new value, it just temporarily acts\nas though the existing value has a new type.\n(There are other legal conversions, such as from integer to floating point, that\ndo create a new value.)\nIt's an idiom in Go programs to convert the\ntype of an expression to access a different\nset of methods. As an example, we could use the existing\ntype\nsort.IntSlice\nto reduce the entire example\nto this:\ntype Sequence []int\n\n// Method for printing - sorts the elements before printing\nfunc (s Sequence) String() string {\n    s = s.Copy()\n    sort.IntSlice(s).Sort()\n    return fmt.Sprint([]int(s))\n}\nNow, instead of having\nSequence\nimplement multiple\ninterfaces (sorting and printing), we're using the ability of a data item to be\nconverted to multiple types (\nSequence\n,\nsort.IntSlice\nand\n[]int\n), each of which does some part of the job.\nThat's more unusual in practice but can be effective.\nInterface conversions and type assertions\nType switches\nare a form of conversion: they take an interface and, for\neach case in the switch, in a sense convert it to the type of that case.\nHere's a simplified version of how the code under\nfmt.Printf\nturns a value into\na string using a type switch.\nIf it's already a string, we want the actual string value held by the interface, while if it has a\nString\nmethod we want the result of calling the method.\ntype Stringer interface {\n    String() string\n}\n\nvar value interface{} // Value provided by caller.\nswitch str := value.(type) {\ncase string:\n    return str\ncase Stringer:\n    return str.String()\n}\nThe first case finds a concrete value; the second converts the interface into another interface.\nIt's perfectly fine to mix types this way.\nWhat if there's only one type we care about? If we know the value holds a\nstring\nand we just want to extract it?\nA one-case type switch would do, but so would a\ntype assertion\n.\nA type assertion takes an interface value and extracts from it a value of the specified explicit type.\nThe syntax borrows from the clause opening a type switch, but with an explicit\ntype rather than the\ntype\nkeyword:\nvalue.(typeName)\nand the result is a new value with the static type\ntypeName\n.\nThat type must either be the concrete type held by the interface, or a second interface\ntype that the value can be converted to.\nTo extract the string we know is in the value, we could write:\nstr := value.(string)\nBut if it turns out that the value does not contain a string, the program will crash with a run-time error.\nTo guard against that, use the \"comma, ok\" idiom to test, safely, whether the value is a string:\nstr, ok := value.(string)\nif ok {\n    fmt.Printf(\"string value is: %q\\n\", str)\n} else {\n    fmt.Printf(\"value is not a string\\n\")\n}\nIf the type assertion fails,\nstr\nwill still exist and be of type string, but it will have\nthe zero value, an empty string.\nAs an illustration of the capability, here's an\nif\n-\nelse\nstatement that's equivalent to the type switch that opened this section.\nif str, ok := value.(string); ok {\n    return str\n} else if str, ok := value.(Stringer); ok {\n    return str.String()\n}\nGenerality\nIf a type exists only to implement an interface and will\nnever have exported methods beyond that interface, there is\nno need to export the type itself.\nExporting just the interface makes it clear the value has no\ninteresting behavior beyond what is described in the\ninterface.\nIt also avoids the need to repeat the documentation\non every instance of a common method.\nIn such cases, the constructor should return an interface value\nrather than the implementing type.\nAs an example, in the hash libraries\nboth\ncrc32.NewIEEE\nand\nadler32.New\nreturn the interface type\nhash.Hash32\n.\nSubstituting the CRC-32 algorithm for Adler-32 in a Go program\nrequires only changing the constructor call;\nthe rest of the code is unaffected by the change of algorithm.\nA similar approach allows the streaming cipher algorithms\nin the various\ncrypto\npackages to be\nseparated from the block ciphers they chain together.\nThe\nBlock\ninterface\nin the\ncrypto/cipher\npackage specifies the\nbehavior of a block cipher, which provides encryption\nof a single block of data.\nThen, by analogy with the\nbufio\npackage,\ncipher packages that implement this interface\ncan be used to construct streaming ciphers, represented\nby the\nStream\ninterface, without\nknowing the details of the block encryption.\nThe\ncrypto/cipher\ninterfaces look like this:\ntype Block interface {\n    BlockSize() int\n    Encrypt(dst, src []byte)\n    Decrypt(dst, src []byte)\n}\n\ntype Stream interface {\n    XORKeyStream(dst, src []byte)\n}\nHere's the definition of the counter mode (CTR) stream,\nwhich turns a block cipher into a streaming cipher; notice\nthat the block cipher's details are abstracted away:\n// NewCTR returns a Stream that encrypts/decrypts using the given Block in\n// counter mode. The length of iv must be the same as the Block's block size.\nfunc NewCTR(block Block, iv []byte) Stream\nNewCTR\napplies not\njust to one specific encryption algorithm and data source but to any\nimplementation of the\nBlock\ninterface and any\nStream\n.  Because they return\ninterface values, replacing CTR\nencryption with other encryption modes is a localized change.  The constructor\ncalls must be edited, but because the surrounding code must treat the result only\nas a\nStream\n, it won't notice the difference.\nInterfaces and methods\nSince almost anything can have methods attached, almost anything can\nsatisfy an interface.  One illustrative example is in the\nhttp\npackage, which defines the\nHandler\ninterface.  Any object\nthat implements\nHandler\ncan serve HTTP requests.\ntype Handler interface {\n    ServeHTTP(ResponseWriter, *Request)\n}\nResponseWriter\nis itself an interface that provides access\nto the methods needed to return the response to the client.\nThose methods include the standard\nWrite\nmethod, so an\nhttp.ResponseWriter\ncan be used wherever an\nio.Writer\ncan be used.\nRequest\nis a struct containing a parsed representation\nof the request from the client.\nFor brevity, let's ignore POSTs and assume HTTP requests are always\nGETs; that simplification does not affect the way the handlers are set up.\nHere's a trivial implementation of a handler to count the number of times\nthe page is visited.\n// Simple counter server.\ntype Counter struct {\n    n int\n}\n\nfunc (ctr *Counter) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n    ctr.n++\n    fmt.Fprintf(w, \"counter = %d\\n\", ctr.n)\n}\n(Keeping with our theme, note how\nFprintf\ncan print to an\nhttp.ResponseWriter\n.)\nIn a real server, access to\nctr.n\nwould need protection from\nconcurrent access.\nSee the\nsync\nand\natomic\npackages for suggestions.\nFor reference, here's how to attach such a server to a node on the URL tree.\nimport \"net/http\"\n...\nctr := new(Counter)\nhttp.Handle(\"/counter\", ctr)\nBut why make\nCounter\na struct?  An integer is all that's needed.\n(The receiver needs to be a pointer so the increment is visible to the caller.)\n// Simpler counter server.\ntype Counter int\n\nfunc (ctr *Counter) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n    *ctr++\n    fmt.Fprintf(w, \"counter = %d\\n\", *ctr)\n}\nWhat if your program has some internal state that needs to be notified that a page\nhas been visited?  Tie a channel to the web page.\n// A channel that sends a notification on each visit.\n// (Probably want the channel to be buffered.)\ntype Chan chan *http.Request\n\nfunc (ch Chan) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n    ch <- req\n    fmt.Fprint(w, \"notification sent\")\n}\nFinally, let's say we wanted to present on\n/args\nthe arguments\nused when invoking the server binary.\nIt's easy to write a function to print the arguments.\nfunc ArgServer() {\n    fmt.Println(os.Args)\n}\nHow do we turn that into an HTTP server?  We could make\nArgServer\na method of some type whose value we ignore, but there's a cleaner way.\nSince we can define a method for any type except pointers and interfaces,\nwe can write a method for a function.\nThe\nhttp\npackage contains this code:\n// The HandlerFunc type is an adapter to allow the use of\n// ordinary functions as HTTP handlers.  If f is a function\n// with the appropriate signature, HandlerFunc(f) is a\n// Handler object that calls f.\ntype HandlerFunc func(ResponseWriter, *Request)\n\n// ServeHTTP calls f(w, req).\nfunc (f HandlerFunc) ServeHTTP(w ResponseWriter, req *Request) {\n    f(w, req)\n}\nHandlerFunc\nis a type with a method,\nServeHTTP\n,\nso values of that type can serve HTTP requests.  Look at the implementation\nof the method: the receiver is a function,\nf\n, and the method\ncalls\nf\n.  That may seem odd but it's not that different from, say,\nthe receiver being a channel and the method sending on the channel.\nTo make\nArgServer\ninto an HTTP server, we first modify it\nto have the right signature.\n// Argument server.\nfunc ArgServer(w http.ResponseWriter, req *http.Request) {\n    fmt.Fprintln(w, os.Args)\n}\nArgServer\nnow has the same signature as\nHandlerFunc\n,\nso it can be converted to that type to access its methods,\njust as we converted\nSequence\nto\nIntSlice\nto access\nIntSlice.Sort\n.\nThe code to set it up is concise:\nhttp.Handle(\"/args\", http.HandlerFunc(ArgServer))\nWhen someone visits the page\n/args\n,\nthe handler installed at that page has value\nArgServer\nand type\nHandlerFunc\n.\nThe HTTP server will invoke the method\nServeHTTP\nof that type, with\nArgServer\nas the receiver, which will in turn call\nArgServer\n(via the invocation\nf(w, req)\ninside\nHandlerFunc.ServeHTTP\n).\nThe arguments will then be displayed.\nIn this section we have made an HTTP server from a struct, an integer,\na channel, and a function, all because interfaces are just sets of\nmethods, which can be defined for (almost) any type.\nThe blank identifier\nWe've mentioned the blank identifier a couple of times now, in the context of\nfor\nrange\nloops\nand\nmaps\n.\nThe blank identifier can be assigned or declared with any value of any type, with the\nvalue discarded harmlessly.\nIt's a bit like writing to the Unix\n/dev/null\nfile:\nit represents a write-only value\nto be used as a place-holder\nwhere a variable is needed but the actual value is irrelevant.\nIt has uses beyond those we've seen already.\nThe blank identifier in multiple assignment\nThe use of a blank identifier in a\nfor\nrange\nloop is a\nspecial case of a general situation: multiple assignment.\nIf an assignment requires multiple values on the left side,\nbut one of the values will not be used by the program,\na blank identifier on the left-hand-side of\nthe assignment avoids the need\nto create a dummy variable and makes it clear that the\nvalue is to be discarded.\nFor instance, when calling a function that returns\na value and an error, but only the error is important,\nuse the blank identifier to discard the irrelevant value.\nif _, err := os.Stat(path); os.IsNotExist(err) {\n    fmt.Printf(\"%s does not exist\\n\", path)\n}\nOccasionally you'll see code that discards the error value in order\nto ignore the error; this is terrible practice. Always check error returns;\nthey're provided for a reason.\n// Bad! This code will crash if path does not exist.\nfi, _ := os.Stat(path)\nif fi.IsDir() {\n    fmt.Printf(\"%s is a directory\\n\", path)\n}\nUnused imports and variables\nIt is an error to import a package or to declare a variable without using it.\nUnused imports bloat the program and slow compilation,\nwhile a variable that is initialized but not used is at least\na wasted computation and perhaps indicative of a\nlarger bug.\nWhen a program is under active development, however,\nunused imports and variables often arise and it can\nbe annoying to delete them just to have the compilation proceed,\nonly to have them be needed again later.\nThe blank identifier provides a workaround.\nThis half-written program has two unused imports\n(\nfmt\nand\nio\n)\nand an unused variable (\nfd\n),\nso it will not compile, but it would be nice to see if the\ncode so far is correct.\npackage main\n\nimport (\n    \"fmt\"\n    \"io\"\n    \"log\"\n    \"os\"\n)\n\nfunc main() {\n    fd, err := os.Open(\"test.go\")\n    if err != nil {\n        log.Fatal(err)\n    }\n// TODO: use fd.\n}\nTo silence complaints about the unused imports, use a\nblank identifier to refer to a symbol from the imported package.\nSimilarly, assigning the unused variable\nfd\nto the blank identifier will silence the unused variable error.\nThis version of the program does compile.\npackage main\n\nimport (\n    \"fmt\"\n    \"io\"\n    \"log\"\n    \"os\"\n)\n\nvar _ = fmt.Printf\n// For debugging; delete when done.\nvar _ io.Reader\n// For debugging; delete when done.\nfunc main() {\n    fd, err := os.Open(\"test.go\")\n    if err != nil {\n        log.Fatal(err)\n    }\n// TODO: use fd.\n_ = fd\n}\nBy convention, the global declarations to silence import errors\nshould come right after the imports and be commented,\nboth to make them easy to find and as a reminder to clean things up later.\nImport for side effect\nAn unused import like\nfmt\nor\nio\nin the\nprevious example should eventually be used or removed:\nblank assignments identify code as a work in progress.\nBut sometimes it is useful to import a package only for its\nside effects, without any explicit use.\nFor example, during its\ninit\nfunction,\nthe\nnet/http/pprof\npackage registers HTTP handlers that provide\ndebugging information. It has an exported API, but\nmost clients need only the handler registration and\naccess the data through a web page.\nTo import the package only for its side effects, rename the package\nto the blank identifier:\nimport _ \"net/http/pprof\"\nThis form of import makes clear that the package is being\nimported for its side effects, because there is no other possible\nuse of the package: in this file, it doesn't have a name.\n(If it did, and we didn't use that name, the compiler would reject the program.)\nInterface checks\nAs we saw in the discussion of\ninterfaces\nabove,\na type need not declare explicitly that it implements an interface.\nInstead, a type implements the interface just by implementing the interface's methods.\nIn practice, most interface conversions are static and therefore checked at compile time.\nFor example, passing an\n*os.File\nto a function\nexpecting an\nio.Reader\nwill not compile unless\n*os.File\nimplements the\nio.Reader\ninterface.\nSome interface checks do happen at run-time, though.\nOne instance is in the\nencoding/json\npackage, which defines a\nMarshaler\ninterface. When the JSON encoder receives a value that implements that interface,\nthe encoder invokes the value's marshaling method to convert it to JSON\ninstead of doing the standard conversion.\nThe encoder checks this property at run time with a\ntype assertion\nlike:\nm, ok := val.(json.Marshaler)\nIf it's necessary only to ask whether a type implements an interface, without\nactually using the interface itself, perhaps as part of an error check, use the blank\nidentifier to ignore the type-asserted value:\nif _, ok := val.(json.Marshaler); ok {\n    fmt.Printf(\"value %v of type %T implements json.Marshaler\\n\", val, val)\n}\nOne place this situation arises is when it is necessary to guarantee within the package implementing the type that\nit actually satisfies the interface.\nIf a type—for example,\njson.RawMessage\n—needs\na custom JSON representation, it should implement\njson.Marshaler\n, but there are no static conversions that would\ncause the compiler to verify this automatically.\nIf the type inadvertently fails to satisfy the interface, the JSON encoder will still work,\nbut will not use the custom implementation.\nTo guarantee that the implementation is correct,\na global declaration using the blank identifier can be used in the package:\nvar _ json.Marshaler = (*RawMessage)(nil)\nIn this declaration, the assignment involving a conversion of a\n*RawMessage\nto a\nMarshaler\nrequires that\n*RawMessage\nimplements\nMarshaler\n,\nand that property will be checked at compile time.\nShould the\njson.Marshaler\ninterface change, this package\nwill no longer compile and we will be on notice that it needs to be updated.\nThe appearance of the blank identifier in this construct indicates that\nthe declaration exists only for the type checking,\nnot to create a variable.\nDon't do this for every type that satisfies an interface, though.\nBy convention, such declarations are only used\nwhen there are no static conversions already present in the code,\nwhich is a rare event.\nEmbedding\nGo does not provide the typical, type-driven notion of subclassing,\nbut it does have the ability to “borrow” pieces of an\nimplementation by\nembedding\ntypes within a struct or\ninterface.\nInterface embedding is very simple.\nWe've mentioned the\nio.Reader\nand\nio.Writer\ninterfaces before;\nhere are their definitions.\ntype Reader interface {\n    Read(p []byte) (n int, err error)\n}\n\ntype Writer interface {\n    Write(p []byte) (n int, err error)\n}\nThe\nio\npackage also exports several other interfaces\nthat specify objects that can implement several such methods.\nFor instance, there is\nio.ReadWriter\n, an interface\ncontaining both\nRead\nand\nWrite\n.\nWe could specify\nio.ReadWriter\nby listing the\ntwo methods explicitly, but it's easier and more evocative\nto embed the two interfaces to form the new one, like this:\n// ReadWriter is the interface that combines the Reader and Writer interfaces.\ntype ReadWriter interface {\n    Reader\n    Writer\n}\nThis says just what it looks like: A\nReadWriter\ncan do\nwhat a\nReader\ndoes\nand\nwhat a\nWriter\ndoes; it is a union of the embedded interfaces.\nOnly interfaces can be embedded within interfaces.\nThe same basic idea applies to structs, but with more far-reaching\nimplications.  The\nbufio\npackage has two struct types,\nbufio.Reader\nand\nbufio.Writer\n, each of\nwhich of course implements the analogous interfaces from package\nio\n.\nAnd\nbufio\nalso implements a buffered reader/writer,\nwhich it does by combining a reader and a writer into one struct\nusing embedding: it lists the types within the struct\nbut does not give them field names.\n// ReadWriter stores pointers to a Reader and a Writer.\n// It implements io.ReadWriter.\ntype ReadWriter struct {\n    *Reader  // *bufio.Reader\n    *Writer  // *bufio.Writer\n}\nThe embedded elements are pointers to structs and of course\nmust be initialized to point to valid structs before they\ncan be used.\nThe\nReadWriter\nstruct could be written as\ntype ReadWriter struct {\n    reader *Reader\n    writer *Writer\n}\nbut then to promote the methods of the fields and to\nsatisfy the\nio\ninterfaces, we would also need\nto provide forwarding methods, like this:\nfunc (rw *ReadWriter) Read(p []byte) (n int, err error) {\n    return rw.reader.Read(p)\n}\nBy embedding the structs directly, we avoid this bookkeeping.\nThe methods of embedded types come along for free, which means that\nbufio.ReadWriter\nnot only has the methods of\nbufio.Reader\nand\nbufio.Writer\n,\nit also satisfies all three interfaces:\nio.Reader\n,\nio.Writer\n, and\nio.ReadWriter\n.\nThere's an important way in which embedding differs from subclassing.  When we embed a type,\nthe methods of that type become methods of the outer type,\nbut when they are invoked the receiver of the method is the inner type, not the outer one.\nIn our example, when the\nRead\nmethod of a\nbufio.ReadWriter\nis\ninvoked, it has exactly the same effect as the forwarding method written out above;\nthe receiver is the\nreader\nfield of the\nReadWriter\n, not the\nReadWriter\nitself.\nEmbedding can also be a simple convenience.\nThis example shows an embedded field alongside a regular, named field.\ntype Job struct {\n    Command string\n    *log.Logger\n}\nThe\nJob\ntype now has the\nPrint\n,\nPrintf\n,\nPrintln\nand other\nmethods of\n*log.Logger\n.  We could have given the\nLogger\na field name, of course, but it's not necessary to do so.  And now, once\ninitialized, we can\nlog to the\nJob\n:\njob.Println(\"starting now...\")\nThe\nLogger\nis a regular field of the\nJob\nstruct,\nso we can initialize it in the usual way inside the constructor for\nJob\n, like this,\nfunc NewJob(command string, logger *log.Logger) *Job {\n    return &Job{command, logger}\n}\nor with a composite literal,\njob := &Job{command, log.New(os.Stderr, \"Job: \", log.Ldate)}\nIf we need to refer to an embedded field directly, the type name of the field,\nignoring the package qualifier, serves as a field name, as it did\nin the\nRead\nmethod of our\nReadWriter\nstruct.\nHere, if we needed to access the\n*log.Logger\nof a\nJob\nvariable\njob\n,\nwe would write\njob.Logger\n,\nwhich would be useful if we wanted to refine the methods of\nLogger\n.\nfunc (job *Job) Printf(format string, args ...interface{}) {\n    job.Logger.Printf(\"%q: %s\", job.Command, fmt.Sprintf(format, args...))\n}\nEmbedding types introduces the problem of name conflicts but the rules to resolve\nthem are simple.\nFirst, a field or method\nX\nhides any other item\nX\nin a more deeply\nnested part of the type.\nIf\nlog.Logger\ncontained a field or method called\nCommand\n, the\nCommand\nfield\nof\nJob\nwould dominate it.\nSecond, if the same name appears at the same nesting level, it is usually an error;\nit would be erroneous to embed\nlog.Logger\nif the\nJob\nstruct\ncontained another field or method called\nLogger\n.\nHowever, if the duplicate name is never mentioned in the program outside the type definition, it is OK.\nThis qualification provides some protection against changes made to types embedded from outside; there\nis no problem if a field is added that conflicts with another field in another subtype if neither field\nis ever used.\nConcurrency\nShare by communicating\nConcurrent programming is a large topic and there is space only for some\nGo-specific highlights here.\nConcurrent programming in many environments is made difficult by the\nsubtleties required to implement correct access to shared variables.  Go encourages\na different approach in which shared values are passed around on channels\nand, in fact, never actively shared by separate threads of execution.\nOnly one goroutine has access to the value at any given time.\nData races cannot occur, by design.\nTo encourage this way of thinking we have reduced it to a slogan:\nDo not communicate by sharing memory;\ninstead, share memory by communicating.\nThis approach can be taken too far.  Reference counts may be best done\nby putting a mutex around an integer variable, for instance.  But as a\nhigh-level approach, using channels to control access makes it easier\nto write clear, correct programs.\nOne way to think about this model is to consider a typical single-threaded\nprogram running on one CPU. It has no need for synchronization primitives.\nNow run another such instance; it too needs no synchronization.  Now let those\ntwo communicate; if the communication is the synchronizer, there's still no need\nfor other synchronization.  Unix pipelines, for example, fit this model\nperfectly.  Although Go's approach to concurrency originates in Hoare's\nCommunicating Sequential Processes (CSP),\nit can also be seen as a type-safe generalization of Unix pipes.\nGoroutines\nThey're called\ngoroutines\nbecause the existing\nterms—threads, coroutines, processes, and so on—convey\ninaccurate connotations.  A goroutine has a simple model: it is a\nfunction executing concurrently with other goroutines in the same\naddress space.  It is lightweight, costing little more than the\nallocation of stack space.\nAnd the stacks start small, so they are cheap, and grow\nby allocating (and freeing) heap storage as required.\nGoroutines are multiplexed onto multiple OS threads so if one should\nblock, such as while waiting for I/O, others continue to run.  Their\ndesign hides many of the complexities of thread creation and\nmanagement.\nPrefix a function or method call with the\ngo\nkeyword to run the call in a new goroutine.\nWhen the call completes, the goroutine\nexits, silently.  (The effect is similar to the Unix shell's\n&\nnotation for running a command in the\nbackground.)\ngo list.Sort()  // run list.Sort concurrently; don't wait for it.\nA function literal can be handy in a goroutine invocation.\nfunc Announce(message string, delay time.Duration) {\n    go func() {\n        time.Sleep(delay)\n        fmt.Println(message)\n    }()  // Note the parentheses - must call the function.\n}\nIn Go, function literals are closures: the implementation makes\nsure the variables referred to by the function survive as long as they are active.\nThese examples aren't too practical because the functions have no way of signaling\ncompletion.  For that, we need channels.\nChannels\nLike maps, channels are allocated with\nmake\n, and\nthe resulting value acts as a reference to an underlying data structure.\nIf an optional integer parameter is provided, it sets the buffer size for the channel.\nThe default is zero, for an unbuffered or synchronous channel.\nci := make(chan int)            // unbuffered channel of integers\ncj := make(chan int, 0)         // unbuffered channel of integers\ncs := make(chan *os.File, 100)  // buffered channel of pointers to Files\nUnbuffered channels combine communication—the exchange of a value—with\nsynchronization—guaranteeing that two calculations (goroutines) are in\na known state.\nThere are lots of nice idioms using channels.  Here's one to get us started.\nIn the previous section we launched a sort in the background. A channel\ncan allow the launching goroutine to wait for the sort to complete.\nc := make(chan int)  // Allocate a channel.\n// Start the sort in a goroutine; when it completes, signal on the channel.\ngo func() {\n    list.Sort()\n    c <- 1  // Send a signal; value does not matter.\n}()\ndoSomethingForAWhile()\n<-c   // Wait for sort to finish; discard sent value.\nReceivers always block until there is data to receive.\nIf the channel is unbuffered, the sender blocks until the receiver has\nreceived the value.\nIf the channel has a buffer, the sender blocks only until the\nvalue has been copied to the buffer; if the buffer is full, this\nmeans waiting until some receiver has retrieved a value.\nA buffered channel can be used like a semaphore, for instance to\nlimit throughput.  In this example, incoming requests are passed\nto\nhandle\n, which sends a value into the channel, processes\nthe request, and then receives a value from the channel\nto ready the “semaphore” for the next consumer.\nThe capacity of the channel buffer limits the number of\nsimultaneous calls to\nprocess\n.\nvar sem = make(chan int, MaxOutstanding)\n\nfunc handle(r *Request) {\n    sem <- 1    // Wait for active queue to drain.\n    process(r)  // May take a long time.\n    <-sem       // Done; enable next request to run.\n}\n\nfunc Serve(queue chan *Request) {\n    for {\n        req := <-queue\n        go handle(req)  // Don't wait for handle to finish.\n    }\n}\nOnce\nMaxOutstanding\nhandlers are executing\nprocess\n,\nany more will block trying to send into the filled channel buffer,\nuntil one of the existing handlers finishes and receives from the buffer.\nThis design has a problem, though:\nServe\ncreates a new goroutine for\nevery incoming request, even though only\nMaxOutstanding\nof them can run at any moment.\nAs a result, the program can consume unlimited resources if the requests come in too fast.\nWe can address that deficiency by changing\nServe\nto\ngate the creation of the goroutines:\nfunc Serve(queue chan *Request) {\n    for req := range queue {\n        sem <- 1\n        go func() {\n            process(req)\n            <-sem\n        }()\n    }\n}\n(Note that in Go versions before 1.22 this code has a bug: the loop\nvariable is shared across all goroutines.\nSee the\nGo wiki\nfor details.)\nAnother approach that manages resources well is to start a fixed\nnumber of\nhandle\ngoroutines all reading from the request\nchannel.\nThe number of goroutines limits the number of simultaneous\ncalls to\nprocess\n.\nThis\nServe\nfunction also accepts a channel on which\nit will be told to exit; after launching the goroutines it blocks\nreceiving from that channel.\nfunc handle(queue chan *Request) {\n    for r := range queue {\n        process(r)\n    }\n}\n\nfunc Serve(clientRequests chan *Request, quit chan bool) {\n    // Start handlers\n    for i := 0; i < MaxOutstanding; i++ {\n        go handle(clientRequests)\n    }\n    <-quit  // Wait to be told to exit.\n}\nChannels of channels\nOne of the most important properties of Go is that\na channel is a first-class value that can be allocated and passed\naround like any other.  A common use of this property is\nto implement safe, parallel demultiplexing.\nIn the example in the previous section,\nhandle\nwas\nan idealized handler for a request but we didn't define the\ntype it was handling.  If that type includes a channel on which\nto reply, each client can provide its own path for the answer.\nHere's a schematic definition of type\nRequest\n.\ntype Request struct {\n    args        []int\n    f           func([]int) int\n    resultChan  chan int\n}\nThe client provides a function and its arguments, as well as\na channel inside the request object on which to receive the answer.\nfunc sum(a []int) (s int) {\n    for _, v := range a {\n        s += v\n    }\n    return\n}\n\nrequest := &Request{[]int{3, 4, 5}, sum, make(chan int)}\n// Send request\nclientRequests <- request\n// Wait for response.\nfmt.Printf(\"answer: %d\\n\", <-request.resultChan)\nOn the server side, the handler function is the only thing that changes.\nfunc handle(queue chan *Request) {\n    for req := range queue {\n        req.resultChan <- req.f(req.args)\n    }\n}\nThere's clearly a lot more to do to make it realistic, but this\ncode is a framework for a rate-limited, parallel, non-blocking RPC\nsystem, and there's not a mutex in sight.\nParallelization\nAnother application of these ideas is to parallelize a calculation\nacross multiple CPU cores.  If the calculation can be broken into\nseparate pieces that can execute independently, it can be parallelized,\nwith a channel to signal when each piece completes.\nLet's say we have an expensive operation to perform on a vector of items,\nand that the value of the operation on each item is independent,\nas in this idealized example.\ntype Vector []float64\n\n// Apply the operation to v[i], v[i+1] ... up to v[n-1].\nfunc (v Vector) DoSome(i, n int, u Vector, c chan int) {\n    for ; i < n; i++ {\n        v[i] += u.Op(v[i])\n    }\n    c <- 1    // signal that this piece is done\n}\nWe launch the pieces independently in a loop, one per CPU.\nThey can complete in any order but it doesn't matter; we just\ncount the completion signals by draining the channel after\nlaunching all the goroutines.\nconst numCPU = 4 // number of CPU cores\n\nfunc (v Vector) DoAll(u Vector) {\n    c := make(chan int, numCPU)  // Buffering optional but sensible.\n    for i := 0; i < numCPU; i++ {\n        go v.DoSome(i*len(v)/numCPU, (i+1)*len(v)/numCPU, u, c)\n    }\n    // Drain the channel.\n    for i := 0; i < numCPU; i++ {\n        <-c    // wait for one task to complete\n    }\n    // All done.\n}\nRather than create a constant value for numCPU, we can ask the runtime what\nvalue is appropriate.\nThe function\nruntime.NumCPU\nreturns the number of hardware CPU cores in the machine, so we could write\nvar numCPU = runtime.NumCPU()\nThere is also a function\nruntime.GOMAXPROCS\n,\nwhich reports (or sets)\nthe user-specified number of cores that a Go program can have running\nsimultaneously.\nIt defaults to the value of\nruntime.NumCPU\nbut can be\noverridden by setting the similarly named shell environment variable\nor by calling the function with a positive number.  Calling it with\nzero just queries the value.\nTherefore if we want to honor the user's resource request, we should write\nvar numCPU = runtime.GOMAXPROCS(0)\nBe sure not to confuse the ideas of concurrency—structuring a program\nas independently executing components—and parallelism—executing\ncalculations in parallel for efficiency on multiple CPUs.\nAlthough the concurrency features of Go can make some problems easy\nto structure as parallel computations, Go is a concurrent language,\nnot a parallel one, and not all parallelization problems fit Go's model.\nFor a discussion of the distinction, see the talk cited in\nthis\nblog post\n.\nA leaky buffer\nThe tools of concurrent programming can even make non-concurrent\nideas easier to express.  Here's an example abstracted from an RPC\npackage.  The client goroutine loops receiving data from some source,\nperhaps a network.  To avoid allocating and freeing buffers, it keeps\na free list, and uses a buffered channel to represent it.  If the\nchannel is empty, a new buffer gets allocated.\nOnce the message buffer is ready, it's sent to the server on\nserverChan\n.\nvar freeList = make(chan *Buffer, 100)\nvar serverChan = make(chan *Buffer)\n\nfunc client() {\n    for {\n        var b *Buffer\n        // Grab a buffer if available; allocate if not.\n        select {\n        case b = <-freeList:\n            // Got one; nothing more to do.\n        default:\n            // None free, so allocate a new one.\n            b = new(Buffer)\n        }\n        load(b)              // Read next message from the net.\n        serverChan <- b      // Send to server.\n    }\n}\nThe server loop receives each message from the client, processes it,\nand returns the buffer to the free list.\nfunc server() {\n    for {\n        b := <-serverChan    // Wait for work.\n        process(b)\n        // Reuse buffer if there's room.\n        select {\n        case freeList <- b:\n            // Buffer on free list; nothing more to do.\n        default:\n            // Free list full, just carry on.\n        }\n    }\n}\nThe client attempts to retrieve a buffer from\nfreeList\n;\nif none is available, it allocates a fresh one.\nThe server's send to\nfreeList\nputs\nb\nback\non the free list unless the list is full, in which case the\nbuffer is dropped on the floor to be reclaimed by\nthe garbage collector.\n(The\ndefault\nclauses in the\nselect\nstatements execute when no other case is ready,\nmeaning that the\nselects\nnever block.)\nThis implementation builds a leaky bucket free list\nin just a few lines, relying on the buffered channel and\nthe garbage collector for bookkeeping.\nErrors\nLibrary routines must often return some sort of error indication to\nthe caller.\nAs mentioned earlier, Go's multivalue return makes it\neasy to return a detailed error description alongside the normal\nreturn value.\nIt is good style to use this feature to provide detailed error information.\nFor example, as we'll see,\nos.Open\ndoesn't\njust return a\nnil\npointer on failure, it also returns an\nerror value that describes what went wrong.\nBy convention, errors have type\nerror\n,\na simple built-in interface.\ntype error interface {\n    Error() string\n}\nA library writer is free to implement this interface with a\nricher model under the covers, making it possible not only\nto see the error but also to provide some context.\nAs mentioned, alongside the usual\n*os.File\nreturn value,\nos.Open\nalso returns an\nerror value.\nIf the file is opened successfully, the error will be\nnil\n,\nbut when there is a problem, it will hold an\nos.PathError\n:\n// PathError records an error and the operation and\n// file path that caused it.\ntype PathError struct {\n    Op string    // \"open\", \"unlink\", etc.\n    Path string  // The associated file.\n    Err error    // Returned by the system call.\n}\n\nfunc (e *PathError) Error() string {\n    return e.Op + \" \" + e.Path + \": \" + e.Err.Error()\n}\nPathError\n's\nError\ngenerates\na string like this:\nopen /etc/passwx: no such file or directory\nSuch an error, which includes the problematic file name, the\noperation, and the operating system error it triggered, is useful even\nif printed far from the call that caused it;\nit is much more informative than the plain\n\"no such file or directory\".\nWhen feasible, error strings should identify their origin, such as by having\na prefix naming the operation or package that generated the error.  For example, in package\nimage\n, the string representation for a decoding error due to an\nunknown format is \"image: unknown format\".\nCallers that care about the precise error details can\nuse a type switch or a type assertion to look for specific\nerrors and extract details.  For\nPathErrors\nthis might include examining the internal\nErr\nfield for recoverable failures.\nfor try := 0; try < 2; try++ {\n    file, err = os.Create(filename)\n    if err == nil {\n        return\n    }\n    if e, ok := err.(*os.PathError); ok && e.Err == syscall.ENOSPC {\n        deleteTempFiles()  // Recover some space.\n        continue\n    }\n    return\n}\nThe second\nif\nstatement here is another\ntype assertion\n.\nIf it fails,\nok\nwill be false, and\ne\nwill be\nnil\n.\nIf it succeeds,\nok\nwill be true, which means the\nerror was of type\n*os.PathError\n, and then so is\ne\n,\nwhich we can examine for more information about the error.\nPanic\nThe usual way to report an error to a caller is to return an\nerror\nas an extra return value.  The canonical\nRead\nmethod is a well-known instance; it returns a byte\ncount and an\nerror\n.  But what if the error is\nunrecoverable?  Sometimes the program simply cannot continue.\nFor this purpose, there is a built-in function\npanic\nthat in effect creates a run-time error that will stop the program\n(but see the next section).  The function takes a single argument\nof arbitrary type—often a string—to be printed as the\nprogram dies.  It's also a way to indicate that something impossible has\nhappened, such as exiting an infinite loop.\n// A toy implementation of cube root using Newton's method.\nfunc CubeRoot(x float64) float64 {\n    z := x/3   // Arbitrary initial value\n    for i := 0; i < 1e6; i++ {\n        prevz := z\n        z -= (z*z*z-x) / (3*z*z)\n        if veryClose(z, prevz) {\n            return z\n        }\n    }\n    // A million iterations has not converged; something is wrong.\n    panic(fmt.Sprintf(\"CubeRoot(%g) did not converge\", x))\n}\nThis is only an example but real library functions should\navoid\npanic\n.  If the problem can be masked or worked\naround, it's always better to let things continue to run rather\nthan taking down the whole program.  One possible counterexample\nis during initialization: if the library truly cannot set itself up,\nit might be reasonable to panic, so to speak.\nvar user = os.Getenv(\"USER\")\n\nfunc init() {\n    if user == \"\" {\n        panic(\"no value for $USER\")\n    }\n}\nRecover\nWhen\npanic\nis called, including implicitly for run-time\nerrors such as indexing a slice out of bounds or failing a type\nassertion, it immediately stops execution of the current function\nand begins unwinding the stack of the goroutine, running any deferred\nfunctions along the way.  If that unwinding reaches the top of the\ngoroutine's stack, the program dies.  However, it is possible to\nuse the built-in function\nrecover\nto regain control\nof the goroutine and resume normal execution.\nA call to\nrecover\nstops the unwinding and returns the\nargument passed to\npanic\n.  Because the only code that\nruns while unwinding is inside deferred functions,\nrecover\nis only useful inside deferred functions.\nOne application of\nrecover\nis to shut down a failing goroutine\ninside a server without killing the other executing goroutines.\nfunc server(workChan <-chan *Work) {\n    for work := range workChan {\n        go safelyDo(work)\n    }\n}\n\nfunc safelyDo(work *Work) {\n    defer func() {\n        if err := recover(); err != nil {\n            log.Println(\"work failed:\", err)\n        }\n    }()\n    do(work)\n}\nIn this example, if\ndo(work)\npanics, the result will be\nlogged and the goroutine will exit cleanly without disturbing the\nothers.  There's no need to do anything else in the deferred closure;\ncalling\nrecover\nhandles the condition completely.\nBecause\nrecover\nalways returns\nnil\nunless called directly\nfrom a deferred function, deferred code can call library routines that themselves\nuse\npanic\nand\nrecover\nwithout failing.  As an example,\nthe deferred function in\nsafelyDo\nmight call a logging function before\ncalling\nrecover\n, and that logging code would run unaffected\nby the panicking state.\nWith our recovery pattern in place, the\ndo\nfunction (and anything it calls) can get out of any bad situation\ncleanly by calling\npanic\n.  We can use that idea to\nsimplify error handling in complex software.  Let's look at an\nidealized version of a\nregexp\npackage, which reports\nparsing errors by calling\npanic\nwith a local\nerror type.  Here's the definition of\nError\n,\nan\nerror\nmethod, and the\nCompile\nfunction.\n// Error is the type of a parse error; it satisfies the error interface.\ntype Error string\nfunc (e Error) Error() string {\n    return string(e)\n}\n\n// error is a method of *Regexp that reports parsing errors by\n// panicking with an Error.\nfunc (regexp *Regexp) error(err string) {\n    panic(Error(err))\n}\n\n// Compile returns a parsed representation of the regular expression.\nfunc Compile(str string) (regexp *Regexp, err error) {\n    regexp = new(Regexp)\n    // doParse will panic if there is a parse error.\n    defer func() {\n        if e := recover(); e != nil {\n            regexp = nil    // Clear return value.\n            err = e.(Error) // Will re-panic if not a parse error.\n        }\n    }()\n    return regexp.doParse(str), nil\n}\nIf\ndoParse\npanics, the recovery block will set the\nreturn value to\nnil\n—deferred functions can modify\nnamed return values.  It will then check, in the assignment\nto\nerr\n, that the problem was a parse error by asserting\nthat it has the local type\nError\n.\nIf it does not, the type assertion will fail, causing a run-time error\nthat continues the stack unwinding as though nothing had interrupted\nit.\nThis check means that if something unexpected happens, such\nas an index out of bounds, the code will fail even though we\nare using\npanic\nand\nrecover\nto handle\nparse errors.\nWith error handling in place, the\nerror\nmethod (because it's a\nmethod bound to a type, it's fine, even natural, for it to have the same name\nas the builtin\nerror\ntype)\nmakes it easy to report parse errors without worrying about unwinding\nthe parse stack by hand:\nif pos == 0 {\n    re.error(\"'*' illegal at start of expression\")\n}\nUseful though this pattern is, it should be used only within a package.\nParse\nturns its internal\npanic\ncalls into\nerror\nvalues; it does not expose\npanics\nto its client.  That is a good rule to follow.\nBy the way, this re-panic idiom changes the panic value if an actual\nerror occurs.  However, both the original and new failures will be\npresented in the crash report, so the root cause of the problem will\nstill be visible.  Thus this simple re-panic approach is usually\nsufficient—it's a crash after all—but if you want to\ndisplay only the original value, you can write a little more code to\nfilter unexpected problems and re-panic with the original error.\nThat's left as an exercise for the reader.\nA web server\nLet's finish with a complete Go program, a web server.\nThis one is actually a kind of web re-server.\nGoogle provides a service at\nchart.apis.google.com\nthat does automatic formatting of data into charts and graphs.\nIt's hard to use interactively, though,\nbecause you need to put the data into the URL as a query.\nThe program here provides a nicer interface to one form of data: given a short piece of text,\nit calls on the chart server to produce a QR code, a matrix of boxes that encode the\ntext.\nThat image can be grabbed with your cell phone's camera and interpreted as,\nfor instance, a URL, saving you typing the URL into the phone's tiny keyboard.\nHere's the complete program.\nAn explanation follows.\npackage main\n\nimport (\n    \"flag\"\n    \"html/template\"\n    \"log\"\n    \"net/http\"\n)\n\nvar addr = flag.String(\"addr\", \":1718\", \"http service address\")\n// Q=17, R=18\nvar templ = template.Must(template.New(\"qr\").Parse(templateStr))\n\nfunc main() {\n    flag.Parse()\n    http.Handle(\"/\", http.HandlerFunc(QR))\n    err := http.ListenAndServe(*addr, nil)\n    if err != nil {\n        log.Fatal(\"ListenAndServe:\", err)\n    }\n}\n\nfunc QR(w http.ResponseWriter, req *http.Request) {\n    templ.Execute(w, req.FormValue(\"s\"))\n}\n\nconst templateStr = `\n<html>\n<head>\n<title>QR Link Generator</title>\n</head>\n<body>\n{{if .}}\n<img src=\"http://chart.apis.google.com/chart?chs=300x300&cht=qr&choe=UTF-8&chl={{.}}\" />\n<br>\n{{.}}\n<br>\n<br>\n{{end}}\n<form action=\"/\" name=f method=\"GET\">\n    <input maxLength=1024 size=70 name=s value=\"\" title=\"Text to QR Encode\">\n    <input type=submit value=\"Show QR\" name=qr>\n</form>\n</body>\n</html>\n`\nThe pieces up to\nmain\nshould be easy to follow.\nThe one flag sets a default HTTP port for our server.  The template\nvariable\ntempl\nis where the fun happens. It builds an HTML template\nthat will be executed by the server to display the page; more about\nthat in a moment.\nThe\nmain\nfunction parses the flags and, using the mechanism\nwe talked about above, binds the function\nQR\nto the root path\nfor the server.  Then\nhttp.ListenAndServe\nis called to start the\nserver; it blocks while the server runs.\nQR\njust receives the request, which contains form data, and\nexecutes the template on the data in the form value named\ns\n.\nThe template package\nhtml/template\nis powerful;\nthis program just touches on its capabilities.\nIn essence, it rewrites a piece of HTML text on the fly by substituting elements derived\nfrom data items passed to\ntempl.Execute\n, in this case the\nform value.\nWithin the template text (\ntemplateStr\n),\ndouble-brace-delimited pieces denote template actions.\nThe piece from\n{{if .}}\nto\n{{end}}\nexecutes only if the value of the current data item, called\n.\n(dot),\nis non-empty.\nThat is, when the string is empty, this piece of the template is suppressed.\nThe two snippets\n{{.}}\nsay to show the data presented to\nthe template—the query string—on the web page.\nThe HTML template package automatically provides appropriate escaping so the\ntext is safe to display.\nThe rest of the template string is just the HTML to show when the page loads.\nIf this is too quick an explanation, see the\ndocumentation\nfor the template package for a more thorough discussion.\nAnd there you have it: a useful web server in a few lines of code plus some\ndata-driven HTML text.\nGo is powerful enough to make a lot happen in a few lines.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nDownload and install\nDownload and install\nDownload and install Go quickly with the steps described here.\nFor other content on installing, you might be interested in:\nManaging Go installations\n-- How to\n    install multiple versions and uninstall.\nInstalling Go from source\n-- How to\n    check out the sources, build them on your own machine, and run them.\nDownload\nGo installation\nSelect the tab for your computer's operating system below, then follow its\n  installation instructions.\nLinux\nMac\nWindows\nRemove any previous Go installation\nby deleting the /usr/local/go folder\n\t\t(if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh\n\t\tGo tree in /usr/local/go:\n$ rm -rf /usr/local/go && tar -C /usr/local -xzf\ngo1.14.3.linux-amd64.tar.gz\n(You may need to run each command separately with the necessary permissions, as root or through\nsudo\n.)\nDo not\nuntar the archive into an existing /usr/local/go tree. This is known to\n\t\t  produce broken Go installations.\nAdd /usr/local/go/bin to the\nPATH\nenvironment variable.\nYou can do this by adding the following line to your $HOME/.profile or\n          /etc/profile (for a system-wide installation):\nexport PATH=$PATH:/usr/local/go/bin\nNote:\nChanges made to a profile file may not apply\n          until the next time you log into your computer. To apply the changes\n          immediately, just run the shell commands directly or execute them from\n          the profile using a command such as\nsource $HOME/.profile\n.\nVerify that you've installed Go by opening a command prompt and typing\n        the following command:\n$ go version\nConfirm that the command prints the installed version of Go.\nOpen the package file you downloaded and follow the prompts to install\n        Go.\nThe package installs the Go distribution to /usr/local/go. The package\n          should put the /usr/local/go/bin directory in your\nPATH\nenvironment variable. You may need to restart any\n          open Terminal sessions for the change to take effect.\nVerify that you've installed Go by opening a command prompt and typing\n        the following command:\n$ go version\nConfirm that the command prints the installed version of Go.\nOpen the MSI file you downloaded and follow the prompts to install Go.\nBy default, the installer will install Go to\nProgram Files\nor\nProgram Files (x86)\n. You can change the\n          location as needed. After installing, you will need to close and\n          reopen any open command prompts so that changes to the environment\n          made by the installer are reflected at the command prompt.\nVerify that you've installed Go.\nIn\nWindows\n, click the\nStart\nmenu.\nIn the menu's search box, type\ncmd\n, then press the\nEnter\nkey.\nIn the Command Prompt window that appears, type the following\n            command:\n$ go version\nConfirm that the command prints the installed version of Go.\nYou're all set!\nVisit the\nGetting Started tutorial\nto write\n  some simple Go code. It takes about 10 minutes to complete.\nReport Issues\nIf you spot bugs, mistakes, or inconsistencies in the Go project's code or documentation, please let us know by filing a ticket on our\nissue tracker.\nOf course, you should check it's not an existing issue before creating a new one.\nFiling a ticket\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nTutorials\nTutorial: Get started with Go\nTutorial: Get started with Go\nIn this tutorial, you'll get a brief introduction to Go programming. Along the\n  way, you will:\nInstall Go (if you haven't already).\nWrite some simple \"Hello, world\" code.\nUse the\ngo\ncommand to run your code.\nUse the Go package discovery tool to find packages you can use in your own\n    code.\nCall functions of an external module.\nNote:\nFor other tutorials, see\nTutorials\n.\nPrerequisites\nSome programming experience.\nThe code here is pretty\n    simple, but it helps to know something about functions.\nA tool to edit your code.\nAny text editor you have will\n    work fine. Most text editors have good support for Go. The most popular are\n    VSCode (free), GoLand (paid), and Vim (free).\nA command terminal.\nGo works well using any terminal on\n    Linux and Mac, and on PowerShell or cmd in Windows.\nInstall Go\nJust use the\nDownload and install\nsteps.\nWrite some code\nGet started with Hello, World.\nOpen a command prompt and cd to your home directory.\nOn Linux or Mac:\ncd\nOn Windows:\ncd %HOMEPATH%\nCreate a hello directory for your first Go source code.\nFor example, use the following commands:\nmkdir hello\ncd hello\nEnable dependency tracking for your code.\nWhen your code imports packages contained in other modules, you manage\n      those dependencies through your code's own module. That module is defined\n      by a go.mod file that tracks the modules that provide those packages. That\n      go.mod file stays with your code, including in your source code\n      repository.\nTo enable dependency tracking for your code by creating a go.mod file, run\n      the\ngo mod init\ncommand,\n        giving it the name of the module your code will be in. The name is the\n        module's module path.\nIn actual development, the module path will typically be the repository\n        location where your source code will be kept. For example, the module\n        path might be\ngithub.com/mymodule\n. If you plan to publish\n        your module for others to use, the module path\nmust\nbe a\n        location from which Go tools can download your module. For more about\n        naming a module with a module path, see\nManaging\n            dependencies\n.\nFor the purposes of this tutorial, just use\nexample/hello\n.\n$ go mod init example/hello\ngo: creating new go.mod: module example/hello\nIn your text editor, create a file hello.go in which to write your code.\nPaste the following code into your hello.go file and save the file.\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"Hello, World!\")\n}\nThis is your Go code. In this code, you:\nDeclare a\nmain\npackage (a package is a way to group\n        functions, and it's made up of all the files in the same directory).\nImport the popular\nfmt\npackage\n,\n        which contains functions for formatting text, including printing to the\n        console. This package is one of the\nstandard library\npackages you got\n        when you installed Go.\nImplement a\nmain\nfunction to print a message to the\n        console. A\nmain\nfunction executes by default when you run\n        the\nmain\npackage.\nRun your code to see the greeting.\n$ go run .\nHello, World!\nThe\ngo run\ncommand\n      is one of many\ngo\ncommands you'll use to get things done with\n      Go. Use the following command to get a list of the others:\n$ go help\nCall code in an external package\nWhen you need your code to do something that might have been implemented by\n  someone else, you can look for a package that has functions you can use in\n  your code.\nMake your printed message a little more interesting with a function from an\n    external module.\nVisit pkg.go.dev and\nsearch for a \"quote\" package\n.\nIn the search results, locate and click on the v1 of the\nrsc.io/quote\npackage\n        (it should be listed with the \"Other major versions\" of\nrsc.io/quote/v4\n).\nIn the\nDocumentation\nsection, under\nIndex\n, note the\n        list of functions you can call from your code. You'll use the\nGo\nfunction.\nAt the top of this page, note that package\nquote\nis\n        included in the\nrsc.io/quote\nmodule.\nYou can use the pkg.go.dev site to find published modules whose packages\n      have functions you can use in your own code. Packages are published in\n      modules -- like\nrsc.io/quote\n-- where others can use them.\n      Modules are improved with new versions over time, and you can upgrade your\n      code to use the improved versions.\nIn your Go code, import the\nrsc.io/quote\npackage and add a call\n    to its\nGo\nfunction.\nAfter adding the highlighted lines, your code should include the\n      following:\npackage main\n\nimport \"fmt\"\nimport \"rsc.io/quote\"\nfunc main() {\nfmt.Println(quote.Go())\n}\nAdd new module requirements and sums.\nGo will add the\nquote\nmodule as a requirement, as well as a\n      go.sum file for use in authenticating the module. For more, see\nAuthenticating modules\nin the Go\n      Modules Reference.\n$ go mod tidy\ngo: finding module for package rsc.io/quote\ngo: found rsc.io/quote in rsc.io/quote v1.5.2\nRun your code to see the message generated by the function you're calling.\n$ go run .\nDon't communicate by sharing memory, share memory by communicating.\nNotice that your code calls the\nGo\nfunction, printing a\n      clever message about communication.\nWhen you ran\ngo mod tidy\n, it located and downloaded the\nrsc.io/quote\nmodule that contains the package you imported.\n      By default, it downloaded the latest version -- v1.5.2.\nWrite more code\nWith this quick introduction, you got Go installed and learned some of the\n  basics. To write some more code with another tutorial, take a look at\nCreate a Go module\n.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nTutorials\nTutorial: Create a Go module\nTutorial: Create a Go module\nThis is the first part of a tutorial that introduces a few fundamental\n  features of the Go language. If you're just getting started with Go, be sure\n  to take a look at\nTutorial: Get started with Go\n, which introduces\n  the\ngo\ncommand, Go modules, and very simple Go code.\nIn this tutorial you'll create two modules. The first is a library which is\n  intended to be imported by other libraries or applications. The second is a\n  caller application which will use the first.\nThis tutorial's sequence includes seven brief topics that each illustrate a\n  different part of the language.\nCreate a module -- Write a small module with functions you can call from\n    another module.\nCall your code from another module\n--\n    Import and use your new module.\nReturn and handle an error\n-- Add simple\n    error handling.\nReturn a random greeting\n-- Handle data\n    in slices (Go's dynamically-sized arrays).\nReturn greetings for multiple people\n-- Store key/value pairs in a map.\nAdd a test\n-- Use Go's built-in unit testing\n    features to test your code.\nCompile and install the application\n--\n    Compile and install your code locally.\nNote:\nFor other tutorials, see\nTutorials\n.\nPrerequisites\nSome programming experience.\nThe code here is pretty\n    simple, but it helps to know something about functions, loops, and arrays.\nA tool to edit your code.\nAny text editor you have will\n    work fine. Most text editors have good support for Go. The most popular are\n    VSCode (free), GoLand (paid), and Vim (free).\nA command terminal.\nGo works well using any terminal on\n    Linux and Mac, and on PowerShell or cmd in Windows.\nStart a module that others can use\nStart by creating a Go module. In a\n  module, you collect one or more related packages for a discrete and useful set\n  of functions. For example, you might create a module with packages that have\n  functions for doing financial analysis so that others writing financial\n  applications can use your work. For more about developing modules, see\nDeveloping and publishing modules\n.\nGo code is grouped into packages, and packages are grouped into modules. Your\n  module specifies dependencies needed to run your code, including the Go\n  version and the set of other modules it requires.\nAs you add or improve functionality in your module, you publish new versions\n  of the module. Developers writing code that calls functions in your module can\n  import the module's updated packages and test with the new version before\n  putting it into production use.\nOpen a command prompt and\ncd\nto your home directory.\nOn Linux or Mac:\ncd\nOn Windows:\ncd %HOMEPATH%\nCreate a\ngreetings\ndirectory for your Go module source code.\nFor example, from your home directory use the following commands:\nmkdir greetings\ncd greetings\nStart your module using the\ngo mod init\ncommand\n.\nRun the\ngo mod init\ncommand, giving it your module path --\n      here, use\nexample.com/greetings\n. If you publish a module,\n      this\nmust\nbe a path from which your module can be downloaded by\n      Go tools. That would be your code's repository.\nFor more on naming your module with a module path, see\nManaging\n        dependencies\n.\n$ go mod init example.com/greetings\ngo: creating new go.mod: module example.com/greetings\nThe\ngo mod init\ncommand creates a go.mod file to track your\n      code's dependencies. So far, the file includes only the name of your\n      module and the Go version your code supports. But as you add dependencies,\n      the go.mod file will list the versions your code depends on. This keeps\n      builds reproducible and gives you direct control over which module\n      versions to use.\nIn your text editor, create a file in which to write your code and call it\n    greetings.go.\nPaste the following code into your greetings.go file and save the file.\npackage greetings\n\nimport \"fmt\"\n\n// Hello returns a greeting for the named person.\nfunc Hello(name string) string {\n    // Return a greeting that embeds the name in a message.\n    message := fmt.Sprintf(\"Hi, %v. Welcome!\", name)\n    return message\n}\nThis is the first code for your module. It returns a greeting to any\n      caller that asks for one. You'll write code that calls this function in\n      the next step.\nIn this code, you:\nDeclare a\ngreetings\npackage to collect related functions.\nImplement a\nHello\nfunction to return the greeting.\nThis function takes a\nname\nparameter whose type is\nstring\n. The function also returns a\nstring\n.\n          In Go, a function whose name starts with a capital letter can be\n          called by a function not in the same package. This is known in Go as\n          an exported name. For more about exported names, see\nExported names\nin the\n          Go tour.\nDeclare a\nmessage\nvariable to hold your greeting.\nIn Go, the\n:=\noperator is a shortcut for declaring and\n          initializing a variable in one line (Go uses the value on the right to\n          determine the variable's type). Taking the long way, you might have\n          written this as:\nvar message string\nmessage = fmt.Sprintf(\"Hi, %v. Welcome!\", name)\nUse the\nfmt\npackage's\nSprintf\nfunction\nto create a greeting message. The\n        first argument is a format string, and\nSprintf\nsubstitutes\n        the\nname\nparameter's value for the\n%v\nformat\n        verb. Inserting the value of the\nname\nparameter completes\n        the greeting text.\nReturn the formatted greeting text to the caller.\nIn the next step, you'll call this function from another module.\nCall your code from another module >\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nTutorials\nTutorial: Getting started with multi-module workspaces\nTutorial: Getting started with multi-module workspaces\nThis tutorial introduces the basics of multi-module workspaces in Go.\nWith multi-module workspaces, you can tell the Go command that you’re\nwriting code in multiple modules at the same time and easily build and\nrun code in those modules.\nIn this tutorial, you’ll create two modules in a shared multi-module\nworkspace, make changes across those modules, and see the results\nof those changes in a build.\nNote:\nFor other tutorials, see\nTutorials\n.\nPrerequisites\nAn installation of Go 1.18 or later.\nA tool to edit your code.\nAny text editor you have will work fine.\nA command terminal.\nGo works well using any terminal on Linux and Mac,\nand on PowerShell or cmd in Windows.\nThis tutorial requires go1.18 or later. Make sure you’ve installed Go at Go 1.18 or later using the\nlinks at\ngo.dev/dl\n.\nCreate a module for your code\nTo begin, create a module for the code you’ll write.\nOpen a command prompt and change to your home directory.\nOn Linux or Mac:\n$ cd\nOn Windows:\nC:\\> cd %HOMEPATH%\nThe rest of the tutorial will show a $ as the prompt. The commands you use\nwill work on Windows too.\nFrom the command prompt, create a directory for your code called workspace.\n$ mkdir workspace\n$ cd workspace\nInitialize the module\nOur example will create a new module\nhello\nthat will depend on the golang.org/x/example module.\nCreate the hello module:\n$ mkdir hello\n$ cd hello\n$ go mod init example.com/hello\ngo: creating new go.mod: module example.com/hello\nAdd a dependency on the golang.org/x/example/hello/reverse package by using\ngo get\n.\n$ go get golang.org/x/example/hello/reverse\nCreate hello.go in the hello directory with the following contents:\npackage main\n\nimport (\n    \"fmt\"\n\n    \"golang.org/x/example/hello/reverse\"\n)\n\nfunc main() {\n    fmt.Println(reverse.String(\"Hello\"))\n}\nNow, run the hello program:\n$ go run .\nolleH\nCreate the workspace\nIn this step, we’ll create a\ngo.work\nfile to specify a workspace with the module.\nInitialize the workspace\nIn the\nworkspace\ndirectory, run:\n$ go work init ./hello\nThe\ngo work init\ncommand tells\ngo\nto create a\ngo.work\nfile\nfor a workspace containing the modules in the\n./hello\ndirectory.\nThe\ngo\ncommand produces a\ngo.work\nfile that looks like this:\ngo 1.18\n\nuse ./hello\nThe\ngo.work\nfile has similar syntax to\ngo.mod\n.\nThe\ngo\ndirective tells Go which version of Go the file should be\ninterpreted with. It’s similar to the\ngo\ndirective in the\ngo.mod\nfile.\nThe\nuse\ndirective tells Go that the module in the\nhello\ndirectory should be main modules when doing a build.\nSo in any subdirectory of\nworkspace\nthe module will be active.\nRun the program in the workspace directory\nIn the\nworkspace\ndirectory, run:\n$ go run ./hello\nolleH\nThe Go command includes all the modules in the workspace as main modules. This allows us\nto refer to a package in the module, even outside the module. Running the\ngo run\ncommand\noutside the module or the workspace would result in an error because the\ngo\ncommand\nwouldn’t know which modules to use.\nNext, we’ll add a local copy of the\ngolang.org/x/example/hello\nmodule to the workspace.\nThat module is stored in a subdirectory of the\ngo.googlesource.com/example\nGit repository.\nWe’ll then add a new function to the\nreverse\npackage that we can use instead of\nString\n.\nDownload and modify the\ngolang.org/x/example/hello\nmodule\nIn this step, we’ll download a copy of the Git repo containing the\ngolang.org/x/example/hello\nmodule,\nadd it to the workspace, and then add a new function to it that we will use from the hello program.\nClone the repository\nFrom the workspace directory, run the\ngit\ncommand to clone the repository:\n$ git clone https://go.googlesource.com/example\nCloning into 'example'...\nremote: Total 165 (delta 27), reused 165 (delta 27)\nReceiving objects: 100% (165/165), 434.18 KiB | 1022.00 KiB/s, done.\nResolving deltas: 100% (27/27), done.\nAdd the module to the workspace\nThe Git repo was just checked out into\n./example\n.\nThe source code for the\ngolang.org/x/example/hello\nmodule is in\n./example/hello\n.\nAdd it to the workspace:\n$ go work use ./example/hello\nThe\ngo work use\ncommand adds a new module to the go.work file. It will now look like this:\ngo 1.18\n\nuse (\n    ./hello\n    ./example/hello\n)\nThe workspace now includes both the\nexample.com/hello\nmodule and the\ngolang.org/x/example/hello\nmodule,\nwhich provides the\ngolang.org/x/example/hello/reverse\npackage.\nThis will allow us to use the new code we will write in our copy of the\nreverse\npackage\ninstead of the version of the package in the module cache\nthat we downloaded with the\ngo get\ncommand.\nAdd the new function.\nWe’ll add a new function to reverse a number to the\ngolang.org/x/example/hello/reverse\npackage.\nCreate a new file named\nint.go\nin the\nworkspace/example/hello/reverse\ndirectory containing the following contents:\npackage reverse\n\nimport \"strconv\"\n\n// Int returns the decimal reversal of the integer i.\nfunc Int(i int) int {\n    i, _ = strconv.Atoi(String(strconv.Itoa(i)))\n    return i\n}\nModify the hello program to use the function.\nModify the contents of\nworkspace/hello/hello.go\nto contain the following contents:\npackage main\n\nimport (\n    \"fmt\"\n\n    \"golang.org/x/example/hello/reverse\"\n)\n\nfunc main() {\n    fmt.Println(reverse.String(\"Hello\"), reverse.Int(24601))\n}\nRun the code in the workspace\nFrom the workspace directory, run\n$ go run ./hello\nolleH 10642\nThe Go command finds the\nexample.com/hello\nmodule specified in the\ncommand line in the\nhello\ndirectory specified by the\ngo.work\nfile, and similarly resolves the\ngolang.org/x/example/hello/reverse\nimport using\nthe\ngo.work\nfile.\ngo.work\ncan be used instead of adding\nreplace\ndirectives to work across multiple modules.\nSince the two modules are in the same workspace it’s easy\nto make a change in one module and use it in another.\nFuture step\nNow, to properly release these modules we’d need to make a release of the\ngolang.org/x/example/hello\nmodule, for example at\nv0.1.0\n. This is usually done by tagging a commit on the module’s version\ncontrol repository. See the\nmodule release workflow documentation\nfor more details. Once the release is done, we can increase the requirement on the\ngolang.org/x/example/hello\nmodule in\nhello/go.mod\n:\ncd hello\ngo get golang.org/x/example/hello@v0.1.0\nThat way, the\ngo\ncommand can properly resolve the modules outside the workspace.\nLearn more about workspaces\nThe\ngo\ncommand has a couple of subcommands for working with workspaces in addition to\ngo work init\nwhich\nwe saw earlier in the tutorial:\ngo work use [-r] [dir]\nadds a\nuse\ndirective to the\ngo.work\nfile for\ndir\n,\nif it exists, and removes the\nuse\ndirectory if the argument directory doesn’t exist. The\n-r\nflag examines subdirectories of\ndir\nrecursively.\ngo work edit\nedits the\ngo.work\nfile similarly to\ngo mod edit\ngo work sync\nsyncs dependencies from the workspace’s build list into each of the workspace modules.\nSee\nWorkspaces\nin the Go Modules Reference for more detail on\nworkspaces and\ngo.work\nfiles.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nTutorials\nTutorial: Developing a RESTful API with Go and Gin\nTutorial: Developing a RESTful API with Go and Gin\nThis tutorial introduces the basics of writing a RESTful web service API with Go\nand the\nGin Web Framework\n(Gin).\nYou’ll get the most out of this tutorial if you have a basic familiarity with Go\nand its tooling. If this is your first exposure to Go, please see\nTutorial: Get started with Go\nfor a quick introduction.\nGin simplifies many coding tasks associated with building web applications,\nincluding web services. In this tutorial, you’ll use Gin to route requests,\nretrieve request details, and marshal JSON for responses.\nIn this tutorial, you will build a RESTful API server with two endpoints. Your\nexample project will be a repository of data about vintage jazz records.\nThe tutorial includes the following sections:\nDesign API endpoints.\nCreate a folder for your code.\nCreate the data.\nWrite a handler to return all items.\nWrite a handler to add a new item.\nWrite a handler to return a specific item.\nNote:\nFor other tutorials, see\nTutorials\n.\nTo try this as an interactive tutorial you complete in Google Cloud Shell,\nclick the button below.\nPrerequisites\nAn installation of Go 1.16 or later.\nFor installation instructions, see\nInstalling Go\n.\nA tool to edit your code.\nAny text editor you have will work fine.\nA command terminal.\nGo works well using any terminal on Linux and Mac,\nand on PowerShell or cmd in Windows.\nThe curl tool.\nOn Linux and Mac, this should already be installed. On\nWindows, it’s included on Windows 10 Insider build 17063 and later. For earlier\nWindows versions, you might need to install it. For more, see\nTar and Curl Come to Windows\n.\nDesign API endpoints\nYou’ll build an API that provides access to a store selling vintage recordings\non vinyl. So you’ll need to provide endpoints through which a client can get\nand add albums for users.\nWhen developing an API, you typically begin by designing the endpoints. Your\nAPI’s users will have more success if the endpoints are easy to understand.\nHere are the endpoints you’ll create in this tutorial.\n/albums\nGET\n– Get a list of all albums, returned as JSON.\nPOST\n– Add a new album from request data sent as JSON.\n/albums/:id\nGET\n– Get an album by its ID, returning the album data as JSON.\nNext, you’ll create a folder for your code.\nCreate a folder for your code\nTo begin, create a project for the code you’ll write.\nOpen a command prompt and change to your home directory.\nOn Linux or Mac:\n$ cd\nOn Windows:\nC:\\> cd %HOMEPATH%\nUsing the command prompt, create a directory for your code called\nweb-service-gin.\n$ mkdir web-service-gin\n$ cd web-service-gin\nCreate a module in which you can manage dependencies.\nRun the\ngo mod init\ncommand, giving it the path of the module your code\nwill be in.\n$ go mod init example/web-service-gin\ngo: creating new go.mod: module example/web-service-gin\nThis command creates a go.mod file in which dependencies you add will be\nlisted for tracking. For more about naming a module with a module path, see\nManaging dependencies\n.\nNext, you’ll design data structures for handling data.\nCreate the data\nTo keep things simple for the tutorial, you’ll store data in memory. A more\ntypical API would interact with a database.\nNote that storing data in memory means that the set of albums will be lost each\ntime you stop the server, then recreated when you start it.\nWrite the code\nUsing your text editor, create a file called main.go in the web-service\ndirectory. You’ll write your Go code in this file.\nInto main.go, at the top of the file, paste the following package declaration.\npackage main\nA standalone program (as opposed to a library) is always in package\nmain\n.\nBeneath the package declaration, paste the following declaration of an\nalbum\nstruct. You’ll use this to store album data in memory.\nStruct tags such as\njson:\"artist\"\nspecify what a field’s name should be\nwhen the struct’s contents are serialized into JSON. Without them, the JSON\nwould use the struct’s capitalized field names – a style not as common in\nJSON.\n// album represents data about a record album.\ntype album struct {\n    ID     string  `json:\"id\"`\n    Title  string  `json:\"title\"`\n    Artist string  `json:\"artist\"`\n    Price  float64 `json:\"price\"`\n}\nBeneath the struct declaration you just added, paste the following slice of\nalbum\nstructs containing data you’ll use to start.\n// albums slice to seed record album data.\nvar albums = []album{\n    {ID: \"1\", Title: \"Blue Train\", Artist: \"John Coltrane\", Price: 56.99},\n    {ID: \"2\", Title: \"Jeru\", Artist: \"Gerry Mulligan\", Price: 17.99},\n    {ID: \"3\", Title: \"Sarah Vaughan and Clifford Brown\", Artist: \"Sarah Vaughan\", Price: 39.99},\n}\nNext, you’ll write code to implement your first endpoint.\nWrite a handler to return all items\nWhen the client makes a request at\nGET /albums\n, you want to return all the\nalbums as JSON.\nTo do this, you’ll write the following:\nLogic to prepare a response\nCode to map the request path to your logic\nNote that this is the reverse of how they’ll be executed at runtime, but you’re\nadding dependencies first, then the code that depends on them.\nWrite the code\nBeneath the struct code you added in the preceding section, paste the\nfollowing code to get the album list.\nThis\ngetAlbums\nfunction creates JSON from the slice of\nalbum\nstructs,\nwriting the JSON into the response.\n// getAlbums responds with the list of all albums as JSON.\nfunc getAlbums(c *gin.Context) {\n    c.IndentedJSON(http.StatusOK, albums)\n}\nIn this code, you:\nWrite a\ngetAlbums\nfunction that takes a\ngin.Context\nparameter. Note that you could have given this function any name – neither\nGin nor Go require a particular function name format.\ngin.Context\nis the most important part of Gin. It carries request\ndetails, validates and serializes JSON, and more. (Despite the similar\nname, this is different from Go’s built-in\ncontext\npackage.)\nCall\nContext.IndentedJSON\nto serialize the struct into JSON and add it to the response.\nThe function’s first argument is the HTTP status code you want to send to\nthe client. Here, you’re passing the\nStatusOK\nconstant from the\nnet/http\npackage to indicate\n200 OK\n.\nNote that you can replace\nContext.IndentedJSON\nwith a call to\nContext.JSON\nto send more compact JSON. In practice, the indented form is much easier to\nwork with when debugging and the size difference is usually small.\nNear the top of main.go, just beneath the\nalbums\nslice declaration, paste\nthe code below to assign the handler function to an endpoint path.\nThis sets up an association in which\ngetAlbums\nhandles requests to the\n/albums\nendpoint path.\nfunc main() {\n    router := gin.Default()\n    router.GET(\"/albums\", getAlbums)\n\n    router.Run(\"localhost:8080\")\n}\nIn this code, you:\nInitialize a Gin router using\nDefault\n.\nUse the\nGET\nfunction to associate the\nGET\nHTTP method and\n/albums\npath with a handler\nfunction.\nNote that you’re passing the\nname\nof the\ngetAlbums\nfunction. This is\ndifferent from passing the\nresult\nof the function, which you would do by\npassing\ngetAlbums()\n(note the parenthesis).\nUse the\nRun\nfunction to attach the router to an\nhttp.Server\nand start the server.\nNear the top of main.go, just beneath the package declaration, import the\npackages you’ll need to support the code you’ve just written.\nThe first lines of code should look like this:\npackage main\n\nimport (\n    \"net/http\"\n\n    \"github.com/gin-gonic/gin\"\n)\nSave main.go.\nRun the code\nBegin tracking the Gin module as a dependency.\nAt the command line, use\ngo get\nto add the github.com/gin-gonic/gin module as a dependency for your module.\nUse a dot argument to mean “get dependencies for code in the current\ndirectory.”\n$ go get .\ngo get: added github.com/gin-gonic/gin v1.7.2\nGo resolved and downloaded this dependency to satisfy the\nimport\ndeclaration you added in the previous step.\nFrom the command line in the directory containing main.go, run the code.\nUse a dot argument to mean “run code in the current directory.”\n$ go run .\nOnce the code is running, you have a running HTTP server to which you can\nsend requests.\nFrom a new command line window, use\ncurl\nto make a request to your running\nweb service.\n$ curl http://localhost:8080/albums\nThe command should display the data you seeded the service with.\n[\n        {\n                \"id\": \"1\",\n                \"title\": \"Blue Train\",\n                \"artist\": \"John Coltrane\",\n                \"price\": 56.99\n        },\n        {\n                \"id\": \"2\",\n                \"title\": \"Jeru\",\n                \"artist\": \"Gerry Mulligan\",\n                \"price\": 17.99\n        },\n        {\n                \"id\": \"3\",\n                \"title\": \"Sarah Vaughan and Clifford Brown\",\n                \"artist\": \"Sarah Vaughan\",\n                \"price\": 39.99\n        }\n]\nYou’ve started an API! In the next section, you’ll create another endpoint with\ncode to handle a\nPOST\nrequest to add an item.\nWrite a handler to add a new item\nWhen the client makes a\nPOST\nrequest at\n/albums\n, you want to add the album\ndescribed in the request body to the existing albums’ data.\nTo do this, you’ll write the following:\nLogic to add the new album to the existing list.\nA bit of code to route the\nPOST\nrequest to your logic.\nWrite the code\nAdd code to add albums data to the list of albums.\nSomewhere after the\nimport\nstatements, paste the following code. (The end\nof the file is a good place for this code, but Go doesn’t enforce the order\nin which you declare functions.)\n// postAlbums adds an album from JSON received in the request body.\nfunc postAlbums(c *gin.Context) {\n    var newAlbum album\n\n    // Call BindJSON to bind the received JSON to\n    // newAlbum.\n    if err := c.BindJSON(&newAlbum); err != nil {\n        return\n    }\n\n    // Add the new album to the slice.\n    albums = append(albums, newAlbum)\n    c.IndentedJSON(http.StatusCreated, newAlbum)\n}\nIn this code, you:\nUse\nContext.BindJSON\nto bind the request body to\nnewAlbum\n.\nAppend the\nalbum\nstruct initialized from the JSON to the\nalbums\nslice.\nAdd a\n201\nstatus code to the response, along with JSON representing\nthe album you added.\nChange your\nmain\nfunction so that it includes the\nrouter.POST\nfunction,\nas in the following.\nfunc main() {\n    router := gin.Default()\n    router.GET(\"/albums\", getAlbums)\n    router.POST(\"/albums\", postAlbums)\n\n    router.Run(\"localhost:8080\")\n}\nIn this code, you:\nAssociate the\nPOST\nmethod at the\n/albums\npath with the\npostAlbums\nfunction.\nWith Gin, you can associate a handler with an HTTP method-and-path\ncombination. In this way, you can separately route requests sent to a\nsingle path based on the method the client is using.\nRun the code\nIf the server is still running from the last section, stop it.\nFrom the command line in the directory containing main.go, run the code.\n$ go run .\nFrom a different command line window, use\ncurl\nto make a request to your\nrunning web service.\n$ curl http://localhost:8080/albums \\\n    --include \\\n    --header \"Content-Type: application/json\" \\\n    --request \"POST\" \\\n    --data '{\"id\": \"4\",\"title\": \"The Modern Sound of Betty Carter\",\"artist\": \"Betty Carter\",\"price\": 49.99}'\nThe command should display headers and JSON for the added album.\nHTTP/1.1 201 Created\nContent-Type: application/json; charset=utf-8\nDate: Wed, 02 Jun 2021 00:34:12 GMT\nContent-Length: 116\n\n{\n    \"id\": \"4\",\n    \"title\": \"The Modern Sound of Betty Carter\",\n    \"artist\": \"Betty Carter\",\n    \"price\": 49.99\n}\nAs in the previous section, use\ncurl\nto retrieve the full list of albums,\nwhich you can use to confirm that the new album was added.\n$ curl http://localhost:8080/albums \\\n    --header \"Content-Type: application/json\" \\\n    --request \"GET\"\nThe command should display the album list.\n[\n        {\n                \"id\": \"1\",\n                \"title\": \"Blue Train\",\n                \"artist\": \"John Coltrane\",\n                \"price\": 56.99\n        },\n        {\n                \"id\": \"2\",\n                \"title\": \"Jeru\",\n                \"artist\": \"Gerry Mulligan\",\n                \"price\": 17.99\n        },\n        {\n                \"id\": \"3\",\n                \"title\": \"Sarah Vaughan and Clifford Brown\",\n                \"artist\": \"Sarah Vaughan\",\n                \"price\": 39.99\n        },\n        {\n                \"id\": \"4\",\n                \"title\": \"The Modern Sound of Betty Carter\",\n                \"artist\": \"Betty Carter\",\n                \"price\": 49.99\n        }\n]\nIn the next section, you’ll add code to handle a\nGET\nfor a specific item.\nWrite a handler to return a specific item\nWhen the client makes a request to\nGET /albums/[id]\n, you want to return the\nalbum whose ID matches the\nid\npath parameter.\nTo do this, you will:\nAdd logic to retrieve the requested album.\nMap the path to the logic.\nWrite the code\nBeneath the\npostAlbums\nfunction you added in the preceding section, paste\nthe following code to retrieve a specific album.\nThis\ngetAlbumByID\nfunction will extract the ID in the request path, then\nlocate an album that matches.\n// getAlbumByID locates the album whose ID value matches the id\n// parameter sent by the client, then returns that album as a response.\nfunc getAlbumByID(c *gin.Context) {\n    id := c.Param(\"id\")\n\n    // Loop over the list of albums, looking for\n    // an album whose ID value matches the parameter.\n    for _, a := range albums {\n        if a.ID == id {\n            c.IndentedJSON(http.StatusOK, a)\n            return\n        }\n    }\n    c.IndentedJSON(http.StatusNotFound, gin.H{\"message\": \"album not found\"})\n}\nIn this code, you:\nUse\nContext.Param\nto retrieve the\nid\npath parameter from the URL. When you map this\nhandler to a path, you’ll include a placeholder for the parameter in the\npath.\nLoop over the\nalbum\nstructs in the slice, looking for one whose\nID\nfield value matches the\nid\nparameter value. If it’s found, you serialize\nthat\nalbum\nstruct to JSON and return it as a response with a\n200 OK\nHTTP code.\nAs mentioned above, a real-world service would likely use a database\nquery to perform this lookup.\nReturn an HTTP\n404\nerror with\nhttp.StatusNotFound\nif the album isn’t found.\nFinally, change your\nmain\nso that it includes a new call to\nrouter.GET\n,\nwhere the path is now\n/albums/:id\n, as shown in the following example.\nfunc main() {\n    router := gin.Default()\n    router.GET(\"/albums\", getAlbums)\n    router.GET(\"/albums/:id\", getAlbumByID)\n    router.POST(\"/albums\", postAlbums)\n\n    router.Run(\"localhost:8080\")\n}\nIn this code, you:\nAssociate the\n/albums/:id\npath with the\ngetAlbumByID\nfunction. In\nGin, the colon preceding an item in the path signifies that the item is\na path parameter.\nRun the code\nIf the server is still running from the last section, stop it.\nFrom the command line in the directory containing main.go, run the code to\nstart the server.\n$ go run .\nFrom a different command line window, use\ncurl\nto make a request to your\nrunning web service.\n$ curl http://localhost:8080/albums/2\nThe command should display JSON for the album whose ID you used. If the\nalbum wasn’t found, you’ll get JSON with an error message.\n{\n        \"id\": \"2\",\n        \"title\": \"Jeru\",\n        \"artist\": \"Gerry Mulligan\",\n        \"price\": 17.99\n}\nConclusion\nCongratulations! You’ve just used Go and Gin to write a simple RESTful web\nservice.\nSuggested next topics:\nIf you’re new to Go, you’ll find useful best practices described in\nEffective Go\nand\nHow to write Go code\n.\nThe\nGo Tour\nis a great step-by-step\nintroduction to Go fundamentals.\nFor more about Gin, see the\nGin Web Framework package documentation\nor the\nGin Web Framework docs\n.\nCompleted code\nThis section contains the code for the application you build with this tutorial.\npackage main\n\nimport (\n    \"net/http\"\n\n    \"github.com/gin-gonic/gin\"\n)\n\n// album represents data about a record album.\ntype album struct {\n    ID     string  `json:\"id\"`\n    Title  string  `json:\"title\"`\n    Artist string  `json:\"artist\"`\n    Price  float64 `json:\"price\"`\n}\n\n// albums slice to seed record album data.\nvar albums = []album{\n    {ID: \"1\", Title: \"Blue Train\", Artist: \"John Coltrane\", Price: 56.99},\n    {ID: \"2\", Title: \"Jeru\", Artist: \"Gerry Mulligan\", Price: 17.99},\n    {ID: \"3\", Title: \"Sarah Vaughan and Clifford Brown\", Artist: \"Sarah Vaughan\", Price: 39.99},\n}\n\nfunc main() {\n    router := gin.Default()\n    router.GET(\"/albums\", getAlbums)\n    router.GET(\"/albums/:id\", getAlbumByID)\n    router.POST(\"/albums\", postAlbums)\n\n    router.Run(\"localhost:8080\")\n}\n\n// getAlbums responds with the list of all albums as JSON.\nfunc getAlbums(c *gin.Context) {\n    c.IndentedJSON(http.StatusOK, albums)\n}\n\n// postAlbums adds an album from JSON received in the request body.\nfunc postAlbums(c *gin.Context) {\n    var newAlbum album\n\n    // Call BindJSON to bind the received JSON to\n    // newAlbum.\n    if err := c.BindJSON(&newAlbum); err != nil {\n        return\n    }\n\n    // Add the new album to the slice.\n    albums = append(albums, newAlbum)\n    c.IndentedJSON(http.StatusCreated, newAlbum)\n}\n\n// getAlbumByID locates the album whose ID value matches the id\n// parameter sent by the client, then returns that album as a response.\nfunc getAlbumByID(c *gin.Context) {\n    id := c.Param(\"id\")\n\n    // Loop through the list of albums, looking for\n    // an album whose ID value matches the parameter.\n    for _, a := range albums {\n        if a.ID == id {\n            c.IndentedJSON(http.StatusOK, a)\n            return\n        }\n    }\n    c.IndentedJSON(http.StatusNotFound, gin.H{\"message\": \"album not found\"})\n}\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nTutorials\nTutorial: Getting started with generics\nTutorial: Getting started with generics\nThis tutorial introduces the basics of generics in Go. With generics, you can\ndeclare and use functions or types that are written to work with any of a set\nof types provided by calling code.\nIn this tutorial, you’ll declare two simple non-generic functions, then capture\nthe same logic in a single generic function.\nYou’ll progress through the following sections:\nCreate a folder for your code.\nAdd non-generic functions.\nAdd a generic function to handle multiple types.\nRemove type arguments when calling the generic function.\nDeclare a type constraint.\nNote:\nFor other tutorials, see\nTutorials\n.\nNote:\nIf you prefer, you can use\nthe Go playground in “Go dev branch” mode\nto edit and run your program instead.\nPrerequisites\nAn installation of Go 1.18 or later.\nFor installation instructions, see\nInstalling Go\n.\nA tool to edit your code.\nAny text editor you have will work fine.\nA command terminal.\nGo works well using any terminal on Linux and Mac,\nand on PowerShell or cmd in Windows.\nCreate a folder for your code\nTo begin, create a folder for the code you’ll write.\nOpen a command prompt and change to your home directory.\nOn Linux or Mac:\n$ cd\nOn Windows:\nC:\\> cd %HOMEPATH%\nThe rest of the tutorial will show a $ as the prompt. The commands you use\nwill work on Windows too.\nFrom the command prompt, create a directory for your code called generics.\n$ mkdir generics\n$ cd generics\nCreate a module to hold your code.\nRun the\ngo mod init\ncommand, giving it your new code’s module path.\n$ go mod init example/generics\ngo: creating new go.mod: module example/generics\nNote:\nFor production code, you’d specify a module path that’s more specific\nto your own needs. For more, be sure to see\nManaging dependencies\n.\nNext, you’ll add some simple code to work with maps.\nAdd non-generic functions\nIn this step, you’ll add two functions that each add together the values of a\nmap and return the total.\nYou’re declaring two functions instead of one because you’re working with two\ndifferent types of maps: one that stores\nint64\nvalues, and one that stores\nfloat64\nvalues.\nWrite the code\nUsing your text editor, create a file called main.go in the generics\ndirectory. You’ll write your Go code in this file.\nInto main.go, at the top of the file, paste the following package\ndeclaration.\npackage main\nA standalone program (as opposed to a library) is always in package\nmain\n.\nBeneath the package declaration, paste the following two function\ndeclarations.\n// SumInts adds together the values of m.\nfunc SumInts(m map[string]int64) int64 {\n    var s int64\n    for _, v := range m {\n        s += v\n    }\n    return s\n}\n\n// SumFloats adds together the values of m.\nfunc SumFloats(m map[string]float64) float64 {\n    var s float64\n    for _, v := range m {\n        s += v\n    }\n    return s\n}\nIn this code, you:\nDeclare two functions to add together the values of a map and return\nthe sum.\nSumFloats\ntakes a map of\nstring\nto\nfloat64\nvalues.\nSumInts\ntakes a map of\nstring\nto\nint64\nvalues.\nAt the top of main.go, beneath the package declaration, paste the following\nmain\nfunction to initialize the two maps and use them as arguments when\ncalling the functions you declared in the preceding step.\nfunc main() {\n    // Initialize a map for the integer values\n    ints := map[string]int64{\n        \"first\":  34,\n        \"second\": 12,\n    }\n\n    // Initialize a map for the float values\n    floats := map[string]float64{\n        \"first\":  35.98,\n        \"second\": 26.99,\n    }\n\n    fmt.Printf(\"Non-Generic Sums: %v and %v\\n\",\n        SumInts(ints),\n        SumFloats(floats))\n}\nIn this code, you:\nInitialize a map of\nfloat64\nvalues and a map of\nint64\nvalues, each\nwith two entries.\nCall the two functions you declared earlier to find the sum of each\nmap’s values.\nPrint the result.\nNear the top of main.go, just beneath the package declaration, import the\npackage you’ll need to support the code you’ve just written.\nThe first lines of code should look like this:\npackage main\n\nimport \"fmt\"\nSave main.go.\nRun the code\nFrom the command line in the directory containing main.go, run the code.\n$ go run .\nNon-Generic Sums: 46 and 62.97\nWith generics, you can write one function here instead of two. Next, you’ll\nadd a single generic function for maps containing either integer or float values.\nAdd a generic function to handle multiple types\nIn this section, you’ll add a single generic function that can receive a map\ncontaining either integer or float values, effectively replacing the two\nfunctions you just wrote with a single function.\nTo support values of either type, that single function will need a way to\ndeclare what types it supports. Calling code, on the other hand, will need a\nway to specify whether it is calling with an integer or float map.\nTo support this, you’ll write a function that declares\ntype parameters\nin\naddition to its ordinary function parameters. These type parameters make the\nfunction generic, enabling it to work with arguments of different types. You’ll\ncall the function with\ntype arguments\nand ordinary function arguments.\nEach type parameter has a\ntype constraint\nthat acts as a kind of meta-type\nfor the type parameter. Each type constraint specifies the permissible type\narguments that calling code can use for the respective type parameter.\nWhile a type parameter’s constraint typically represents a set of types, at\ncompile time the type parameter stands for a single type – the type provided\nas a type argument by the calling code. If the type argument’s type isn’t\nallowed by the type parameter’s constraint, the code won’t compile.\nKeep in mind that a type parameter must support all the operations the generic\ncode is performing on it. For example, if your function’s code were to try to\nperform\nstring\noperations (such as indexing) on a type parameter whose\nconstraint included numeric types, the code wouldn’t compile.\nIn the code you’re about to write, you’ll use a constraint that allows either\ninteger or float types.\nWrite the code\nBeneath the two functions you added previously, paste the following generic\nfunction.\n// SumIntsOrFloats sums the values of map m. It supports both int64 and float64\n// as types for map values.\nfunc SumIntsOrFloats[K comparable, V int64 | float64](m map[K]V) V {\n    var s V\n    for _, v := range m {\n        s += v\n    }\n    return s\n}\nIn this code, you:\nDeclare a\nSumIntsOrFloats\nfunction with two type parameters (inside\nthe square brackets),\nK\nand\nV\n, and one argument that uses the type\nparameters,\nm\nof type\nmap[K]V\n. The function returns a value of\ntype\nV\n.\nSpecify for the\nK\ntype parameter the type constraint\ncomparable\n.\nIntended specifically for cases like these, the\ncomparable\nconstraint\nis predeclared in Go. It allows any type whose values may be used as an\noperand of the comparison operators\n==\nand\n!=\n. Go requires that map\nkeys be comparable. So declaring\nK\nas\ncomparable\nis necessary so you\ncan use\nK\nas the key in the map variable. It also ensures that calling\ncode uses an allowable type for map keys.\nSpecify for the\nV\ntype parameter a constraint that is a union of two\ntypes:\nint64\nand\nfloat64\n. Using\n|\nspecifies a union of the two\ntypes, meaning that this constraint allows either type. Either type\nwill be permitted by the compiler as an argument in the calling code.\nSpecify that the\nm\nargument is of type\nmap[K]V\n, where\nK\nand\nV\nare the types already specified for the type parameters. Note that we\nknow\nmap[K]V\nis a valid map type because\nK\nis a comparable type. If\nwe hadn’t declared\nK\ncomparable, the compiler would reject the\nreference to\nmap[K]V\n.\nIn main.go, beneath the code you already have, paste the following code.\nfmt.Printf(\"Generic Sums: %v and %v\\n\",\n    SumIntsOrFloats[string, int64](ints),\n    SumIntsOrFloats[string, float64](floats))\nIn this code, you:\nCall the generic function you just declared, passing each of the maps\nyou created.\nSpecify type arguments – the type names in square brackets – to be\nclear about the types that should replace type parameters in the\nfunction you’re calling.\nAs you’ll see in the next section, you can often omit the type\narguments in the function call. Go can often infer them from your code.\nPrint the sums returned by the function.\nRun the code\nFrom the command line in the directory containing main.go, run the code.\n$ go run .\nNon-Generic Sums: 46 and 62.97\nGeneric Sums: 46 and 62.97\nTo run your code, in each call the compiler replaced the type parameters with\nthe concrete types specified in that call.\nIn calling the generic function you wrote, you specified type arguments that\ntold the compiler what types to use in place of the function’s type parameters.\nAs you’ll see in the next section, in many cases you can omit these type\narguments because the compiler can infer them.\nRemove type arguments when calling the generic function\nIn this section, you’ll add a modified version of the generic function call,\nmaking a small change to simplify the calling code. You’ll remove the type\narguments, which aren’t needed in this case.\nYou can omit type arguments in calling code when the Go compiler can infer the\ntypes you want to use. The compiler infers type arguments from the types of\nfunction arguments.\nNote that this isn’t always possible. For example, if you needed to call a\ngeneric function that had no arguments, you would need to include the type\narguments in the function call.\nWrite the code\nIn main.go, beneath the code you already have, paste the following code.\nfmt.Printf(\"Generic Sums, type parameters inferred: %v and %v\\n\",\n    SumIntsOrFloats(ints),\n    SumIntsOrFloats(floats))\nIn this code, you:\nCall the generic function, omitting the type arguments.\nRun the code\nFrom the command line in the directory containing main.go, run the code.\n$ go run .\nNon-Generic Sums: 46 and 62.97\nGeneric Sums: 46 and 62.97\nGeneric Sums, type parameters inferred: 46 and 62.97\nNext, you’ll further simplify the function by capturing the union of integers\nand floats into a type constraint you can reuse, such as from other code.\nDeclare a type constraint\nIn this last section, you’ll move the constraint you defined earlier into its\nown interface so that you can reuse it in multiple places. Declaring\nconstraints in this way helps streamline code, such as when a constraint is\nmore complex.\nYou declare a\ntype constraint\nas an interface. The constraint allows any\ntype implementing the interface. For example, if you declare a type constraint\ninterface with three methods, then use it with a type parameter in a generic\nfunction, type arguments used to call the function must have all of those\nmethods.\nConstraint interfaces can also refer to specific types, as you’ll see in this\nsection.\nWrite the code\nJust above\nmain\n, immediately after the import statements, paste the\nfollowing code to declare a type constraint.\ntype Number interface {\n    int64 | float64\n}\nIn this code, you:\nDeclare the\nNumber\ninterface type to use as a type constraint.\nDeclare a union of\nint64\nand\nfloat64\ninside the interface.\nEssentially, you’re moving the union from the function declaration\ninto a new type constraint. That way, when you want to constrain a type\nparameter to either\nint64\nor\nfloat64\n, you can use this\nNumber\ntype constraint instead of writing out\nint64 | float64\n.\nBeneath the functions you already have, paste the following generic\nSumNumbers\nfunction.\n// SumNumbers sums the values of map m. It supports both integers\n// and floats as map values.\nfunc SumNumbers[K comparable, V Number](m map[K]V) V {\n    var s V\n    for _, v := range m {\n        s += v\n    }\n    return s\n}\nIn this code, you:\nDeclare a generic function with the same logic as the generic function\nyou declared previously, but with the new interface type instead of the\nunion as the type constraint. As before, you use the type parameters\nfor the argument and return types.\nIn main.go, beneath the code you already have, paste the following code.\nfmt.Printf(\"Generic Sums with Constraint: %v and %v\\n\",\n    SumNumbers(ints),\n    SumNumbers(floats))\nIn this code, you:\nCall\nSumNumbers\nwith each map, printing the sum from the values of\neach.\nAs in the preceding section, you omit the type arguments (the type\nnames in square brackets) in calls to the generic function. The Go\ncompiler can infer the type argument from other arguments.\nRun the code\nFrom the command line in the directory containing main.go, run the code.\n$ go run .\nNon-Generic Sums: 46 and 62.97\nGeneric Sums: 46 and 62.97\nGeneric Sums, type parameters inferred: 46 and 62.97\nGeneric Sums with Constraint: 46 and 62.97\nConclusion\nNicely done! You’ve just introduced yourself to generics in Go.\nSuggested next topics:\nThe\nGo Tour\nis a great step-by-step\nintroduction to Go fundamentals.\nYou’ll find useful Go best practices described in\nEffective Go\nand\nHow to write Go code\n.\nCompleted code\nYou can run this program in the\nGo playground\n. On the\nplayground simply click the\nRun\nbutton.\npackage main\n\nimport \"fmt\"\n\ntype Number interface {\n    int64 | float64\n}\n\nfunc main() {\n    // Initialize a map for the integer values\n    ints := map[string]int64{\n        \"first\": 34,\n        \"second\": 12,\n    }\n\n    // Initialize a map for the float values\n    floats := map[string]float64{\n        \"first\": 35.98,\n        \"second\": 26.99,\n    }\n\n    fmt.Printf(\"Non-Generic Sums: %v and %v\\n\",\n        SumInts(ints),\n        SumFloats(floats))\n\n    fmt.Printf(\"Generic Sums: %v and %v\\n\",\n        SumIntsOrFloats[string, int64](ints),\n        SumIntsOrFloats[string, float64](floats))\n\n    fmt.Printf(\"Generic Sums, type parameters inferred: %v and %v\\n\",\n        SumIntsOrFloats(ints),\n        SumIntsOrFloats(floats))\n\n    fmt.Printf(\"Generic Sums with Constraint: %v and %v\\n\",\n        SumNumbers(ints),\n        SumNumbers(floats))\n}\n\n// SumInts adds together the values of m.\nfunc SumInts(m map[string]int64) int64 {\n    var s int64\n    for _, v := range m {\n        s += v\n    }\n    return s\n}\n\n// SumFloats adds together the values of m.\nfunc SumFloats(m map[string]float64) float64 {\n    var s float64\n    for _, v := range m {\n        s += v\n    }\n    return s\n}\n\n// SumIntsOrFloats sums the values of map m. It supports both floats and integers\n// as map values.\nfunc SumIntsOrFloats[K comparable, V int64 | float64](m map[K]V) V {\n    var s V\n    for _, v := range m {\n        s += v\n    }\n    return s\n}\n\n// SumNumbers sums the values of map m. Its supports both integers\n// and floats as map values.\nfunc SumNumbers[K comparable, V Number](m map[K]V) V {\n    var s V\n    for _, v := range m {\n        s += v\n    }\n    return s\n}\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nTutorials\nTutorial: Getting started with fuzzing\nTutorial: Getting started with fuzzing\nThis tutorial introduces the basics of fuzzing in Go. With fuzzing, random data\nis run against your test in an attempt to find vulnerabilities or crash-causing\ninputs. Some examples of vulnerabilities that can be found by fuzzing are SQL\ninjection, buffer overflow, denial of service and cross-site scripting attacks.\nIn this tutorial, you’ll write a fuzz test for a simple function, run the go\ncommand, and debug and fix issues in the code.\nFor help with terminology throughout this tutorial, see the\nGo Fuzzing\nglossary\n.\nYou’ll progress through the following sections:\nCreate a folder for your code.\nAdd code to test.\nAdd a unit test.\nAdd a fuzz test.\nFix two bugs.\nExplore additional resources.\nNote:\nFor other tutorials, see\nTutorials\n.\nNote:\nGo fuzzing currently supports a subset of built-in types, listed in\nthe\nGo Fuzzing docs\n, with support for more built-in\ntypes to be added in the future.\nPrerequisites\nAn installation of Go 1.18 or later.\nFor installation instructions, see\nInstalling Go\n.\nA tool to edit your code.\nAny text editor you have will work fine.\nA command terminal.\nGo works well using any terminal on Linux and Mac, and\non PowerShell or cmd in Windows.\nAn environment that supports fuzzing.\nGo fuzzing with coverage\ninstrumentation is only available on AMD64 and ARM64 architectures currently.\nCreate a folder for your code\nTo begin, create a folder for the code you’ll write.\nOpen a command prompt and change to your home directory.\nOn Linux or Mac:\n$ cd\nOn Windows:\nC:\\> cd %HOMEPATH%\nThe rest of the tutorial will show a $ as the prompt. The commands you use\nwill work on Windows too.\nFrom the command prompt, create a directory for your code called fuzz.\n$ mkdir fuzz\n$ cd fuzz\nCreate a module to hold your code.\nRun the\ngo mod init\ncommand, giving it your new code’s module path.\n$ go mod init example/fuzz\ngo: creating new go.mod: module example/fuzz\nNote:\nFor production code, you’d specify a module path that’s more\nspecific to your own needs. For more, be sure to see\nManaging\ndependencies\n.\nNext, you’ll add some simple code to reverse a string, which we’ll fuzz later.\nAdd code to test\nIn this step, you’ll add a function to reverse a string.\nWrite the code\nUsing your text editor, create a file called main.go in the fuzz directory.\nInto main.go, at the top of the file, paste the following package\ndeclaration.\npackage main\nA standalone program (as opposed to a library) is always in package\nmain\n.\nBeneath the package declaration, paste the following function declaration.\nfunc Reverse(s string) string {\n    b := []byte(s)\n    for i, j := 0, len(b)-1; i < len(b)/2; i, j = i+1, j-1 {\n        b[i], b[j] = b[j], b[i]\n    }\n    return string(b)\n}\nThis function will accept a\nstring\n, loop over it a\nbyte\nat a time, and\nreturn the reversed string at the end.\nNote:\nThis code is based on the\nstringutil.Reverse\nfunction within\ngolang.org/x/example.\nAt the top of main.go, beneath the package declaration, paste the following\nmain\nfunction to initialize a string, reverse it, print the output, and\nrepeat.\nfunc main() {\n    input := \"The quick brown fox jumped over the lazy dog\"\n    rev := Reverse(input)\n    doubleRev := Reverse(rev)\n    fmt.Printf(\"original: %q\\n\", input)\n    fmt.Printf(\"reversed: %q\\n\", rev)\n    fmt.Printf(\"reversed again: %q\\n\", doubleRev)\n}\nThis function will run a few\nReverse\noperations, then print the output to\nthe command line. This can be helpful for seeing the code in action, and\npotentially for debugging.\nThe\nmain\nfunction uses the fmt package, so you will need to import it.\nThe first lines of code should look like this:\npackage main\n\nimport \"fmt\"\nRun the code\nFrom the command line in the directory containing main.go, run the code.\n$ go run .\noriginal: \"The quick brown fox jumped over the lazy dog\"\nreversed: \"god yzal eht revo depmuj xof nworb kciuq ehT\"\nreversed again: \"The quick brown fox jumped over the lazy dog\"\nYou can see the original string, the result of reversing it, then the result of\nreversing it again, which is equivalent to the original.\nNow that the code is running, it’s time to test it.\nAdd a unit test\nIn this step, you will write a basic unit test for the\nReverse\nfunction.\nWrite the code\nUsing your text editor, create a file called reverse_test.go in the fuzz\ndirectory.\nPaste the following code into reverse_test.go.\npackage main\n\nimport (\n    \"testing\"\n)\n\nfunc TestReverse(t *testing.T) {\n    testcases := []struct {\n        in, want string\n    }{\n        {\"Hello, world\", \"dlrow ,olleH\"},\n        {\" \", \" \"},\n        {\"!12345\", \"54321!\"},\n    }\n    for _, tc := range testcases {\n        rev := Reverse(tc.in)\n        if rev != tc.want {\n                t.Errorf(\"Reverse: %q, want %q\", rev, tc.want)\n        }\n    }\n}\nThis simple test will assert that the listed input strings will be correctly\nreversed.\nRun the code\nRun the unit test using\ngo test\n$ go test\nPASS\nok      example/fuzz  0.013s\nNext, you will change the unit test into a fuzz test.\nAdd a fuzz test\nThe unit test has limitations, namely that each input must be added to the test\nby the developer. One benefit of fuzzing is that it comes up with inputs for\nyour code, and may identify edge cases that the test cases you came up with\ndidn’t reach.\nIn this section you will convert the unit test to a fuzz test so that you can\ngenerate more inputs with less work!\nNote that you can keep unit tests, benchmarks, and fuzz tests in the same\n*_test.go file, but for this example you will convert the unit test to a fuzz\ntest.\nWrite the code\nIn your text editor, replace the unit test in reverse_test.go with the following\nfuzz test.\nfunc FuzzReverse(f *testing.F) {\n    testcases := []string{\"Hello, world\", \" \", \"!12345\"}\n    for _, tc := range testcases {\n        f.Add(tc)  // Use f.Add to provide a seed corpus\n    }\n    f.Fuzz(func(t *testing.T, orig string) {\n        rev := Reverse(orig)\n        doubleRev := Reverse(rev)\n        if orig != doubleRev {\n            t.Errorf(\"Before: %q, after: %q\", orig, doubleRev)\n        }\n        if utf8.ValidString(orig) && !utf8.ValidString(rev) {\n            t.Errorf(\"Reverse produced invalid UTF-8 string %q\", rev)\n        }\n    })\n}\nFuzzing has a few limitations as well. In your unit test, you could predict the\nexpected output of the\nReverse\nfunction, and verify that the actual output met\nthose expectations.\nFor example, in the test case\nReverse(\"Hello, world\")\nthe unit test specifies\nthe return as\n\"dlrow ,olleH\"\n.\nWhen fuzzing, you can’t predict the expected output, since you don’t have\ncontrol over the inputs.\nHowever, there are a few properties of the\nReverse\nfunction that you can\nverify in a fuzz test. The two properties being checked in this fuzz test are:\nReversing a string twice preserves the original value\nThe reversed string preserves its state as valid UTF-8.\nNote the syntax differences between the unit test and the fuzz test:\nThe function begins with FuzzXxx instead of TestXxx, and takes\n*testing.F\ninstead of\n*testing.T\nWhere you would expect to see a\nt.Run\nexecution, you instead see\nf.Fuzz\nwhich takes a fuzz target function whose parameters are\n*testing.T\nand the\ntypes to be fuzzed. The inputs from your unit test are provided as seed corpus\ninputs using\nf.Add\n.\nEnsure the new package,\nunicode/utf8\nhas been imported.\npackage main\n\nimport (\n    \"testing\"\n    \"unicode/utf8\"\n)\nWith the unit test converted to a fuzz test, it’s time to run the test again.\nRun the code\nRun the fuzz test without fuzzing it to make sure the seed inputs pass.\n$ go test\nPASS\nok      example/fuzz  0.013s\nYou can also run\ngo test -run=FuzzReverse\nif you have other tests in that\nfile, and you only wish to run the fuzz test.\nRun\nFuzzReverse\nwith fuzzing, to see if any randomly generated string\ninputs will cause a failure. This is executed using\ngo test\nwith a new\nflag,\n-fuzz\n, set to the parameter\nFuzz\n. Copy the command below.\n$ go test -fuzz=Fuzz\nAnother useful flag is\n-fuzztime\n, which restricts the time fuzzing takes.\nFor example, specifying\n-fuzztime 10s\nin the test below would mean that,\nas long as no failures occurred earlier, the test would exit by default\nafter 10 seconds had elapsed. See\nthis\nsection\nof the cmd/go\ndocumentation to see other testing flags.\nNow, run the command you just copied.\n$ go test -fuzz=Fuzz\nfuzz: elapsed: 0s, gathering baseline coverage: 0/3 completed\nfuzz: elapsed: 0s, gathering baseline coverage: 3/3 completed, now fuzzing with 8 workers\nfuzz: minimizing 38-byte failing input file...\n--- FAIL: FuzzReverse (0.01s)\n    --- FAIL: FuzzReverse (0.00s)\n        reverse_test.go:20: Reverse produced invalid UTF-8 string \"\\x9c\\xdd\"\n\n    Failing input written to testdata/fuzz/FuzzReverse/af69258a12129d6cbba438df5d5f25ba0ec050461c116f777e77ea7c9a0d217a\n    To re-run:\n    go test -run=FuzzReverse/af69258a12129d6cbba438df5d5f25ba0ec050461c116f777e77ea7c9a0d217a\nFAIL\nexit status 1\nFAIL    example/fuzz  0.030s\nA failure occurred while fuzzing, and the input that caused the problem is\nwritten to a seed corpus file that will be run the next time\ngo test\nis\ncalled, even without the\n-fuzz\nflag. To view the input that caused the\nfailure, open the corpus file written to the testdata/fuzz/FuzzReverse\ndirectory in a text editor. Your seed corpus file may contain a different\nstring, but the format will be the same.\ngo test fuzz v1\nstring(\"泃\")\nThe first line of the corpus file indicates the encoding version. Each\nfollowing line represents the value of each type making up the corpus entry.\nSince the fuzz target only takes 1 input, there is only 1 value after the\nversion.\nRun\ngo test\nagain without the\n-fuzz\nflag; the new failing seed corpus\nentry will be used:\n$ go test\n--- FAIL: FuzzReverse (0.00s)\n    --- FAIL: FuzzReverse/af69258a12129d6cbba438df5d5f25ba0ec050461c116f777e77ea7c9a0d217a (0.00s)\n        reverse_test.go:20: Reverse produced invalid string\nFAIL\nexit status 1\nFAIL    example/fuzz  0.016s\nSince our test has failed, it’s time to debug.\nFix the invalid string error\nIn this section, you will debug the failure, and fix the bug.\nFeel free to spend some time thinking about this and trying to fix the issue\nyourself before moving on.\nDiagnose the error\nThere are a few different ways you could debug this error. If you are using VS\nCode as your text editor, you can\nset up your\ndebugger\nto\ninvestigate.\nIn this tutorial, we will log useful debugging info to your terminal.\nFirst, consider the docs for\nutf8.ValidString\n.\nValidString reports whether s consists entirely of valid UTF-8-encoded runes.\nThe current\nReverse\nfunction reverses the string byte-by-byte, and therein\nlies our problem. In order to preserve the UTF-8-encoded runes of the original\nstring, we must instead reverse the string rune-by-rune.\nTo examine why the input (in this case, the Chinese character\n泃\n) is causing\nReverse\nto produce an invalid string when reversed, you can inspect the number\nof runes in the reversed string.\nWrite the code\nIn your text editor, replace the fuzz target within\nFuzzReverse\nwith the\nfollowing.\nf.Fuzz(func(t *testing.T, orig string) {\n    rev := Reverse(orig)\n    doubleRev := Reverse(rev)\n    t.Logf(\"Number of runes: orig=%d, rev=%d, doubleRev=%d\", utf8.RuneCountInString(orig), utf8.RuneCountInString(rev), utf8.RuneCountInString(doubleRev))\n    if orig != doubleRev {\n        t.Errorf(\"Before: %q, after: %q\", orig, doubleRev)\n    }\n    if utf8.ValidString(orig) && !utf8.ValidString(rev) {\n        t.Errorf(\"Reverse produced invalid UTF-8 string %q\", rev)\n    }\n})\nThis\nt.Logf\nline will print to the command line if an error occurs, or if\nexecuting the test with\n-v\n, which can help you debug this particular issue.\nRun the code\nRun the test using go test\n$ go test\n--- FAIL: FuzzReverse (0.00s)\n    --- FAIL: FuzzReverse/28f36ef487f23e6c7a81ebdaa9feffe2f2b02b4cddaa6252e87f69863046a5e0 (0.00s)\n        reverse_test.go:16: Number of runes: orig=1, rev=3, doubleRev=1\n        reverse_test.go:21: Reverse produced invalid UTF-8 string \"\\x83\\xb3\\xe6\"\nFAIL\nexit status 1\nFAIL    example/fuzz    0.598s\nThe entire seed corpus used strings in which every character was a single byte.\nHowever, characters such as 泃 can require several bytes. Thus, reversing the\nstring byte-by-byte will invalidate multi-byte characters.\nNote:\nIf you’re curious about how Go deals with strings, read the blog post\nStrings, bytes, runes and characters in Go\nfor a\ndeeper understanding.\nWith a better understanding of the bug, correct the error in the\nReverse\nfunction.\nFix the error\nTo correct the\nReverse\nfunction, let’s traverse the string by runes, instead\nof by bytes.\nWrite the code\nIn your text editor, replace the existing Reverse() function with the following.\nfunc Reverse(s string) string {\n    r := []rune(s)\n    for i, j := 0, len(r)-1; i < len(r)/2; i, j = i+1, j-1 {\n        r[i], r[j] = r[j], r[i]\n    }\n    return string(r)\n}\nThe key difference is that\nReverse\nis now iterating over each\nrune\nin the\nstring, rather than each\nbyte\n. Note that this is just an example, and does not\nhandle\ncombining characters\ncorrectly.\nRun the code\nRun the test using\ngo test\n$ go test\nPASS\nok      example/fuzz  0.016s\nThe test now passes!\nFuzz it again with\ngo test -fuzz\n, to see if there are any new bugs.\n$ go test -fuzz=Fuzz\nfuzz: elapsed: 0s, gathering baseline coverage: 0/37 completed\nfuzz: minimizing 506-byte failing input file...\nfuzz: elapsed: 0s, gathering baseline coverage: 5/37 completed\n--- FAIL: FuzzReverse (0.02s)\n    --- FAIL: FuzzReverse (0.00s)\n        reverse_test.go:33: Before: \"\\x91\", after: \"�\"\n\n    Failing input written to testdata/fuzz/FuzzReverse/1ffc28f7538e29d79fce69fef20ce5ea72648529a9ca10bea392bcff28cd015c\n    To re-run:\n    go test -run=FuzzReverse/1ffc28f7538e29d79fce69fef20ce5ea72648529a9ca10bea392bcff28cd015c\nFAIL\nexit status 1\nFAIL    example/fuzz  0.032s\nWe can see that the string is different from the original after being\nreversed twice. This time the input itself is invalid unicode. How is this\npossible if we’re fuzzing with strings?\nLet’s debug again.\nFix the double reverse error\nIn this section, you will debug the double reverse failure and fix the bug.\nFeel free to spend some time thinking about this and trying to fix the issue\nyourself before moving on.\nDiagnose the error\nLike before, there are several ways you could debug this failure. In this case,\nusing a\ndebugger\nwould be a great approach.\nIn this tutorial, we will log useful debugging info in the\nReverse\nfunction.\nLook closely at the reversed string to spot the error. In Go,\na string is a\nread only slice of bytes\n, and can contain bytes\nthat aren’t valid UTF-8. The original string is a byte slice with one byte,\n'\\x91'\n. When the input string is set to\n[]rune\n, Go encodes the byte slice to\nUTF-8, and replaces the byte with the UTF-8 character �. When we compare the\nreplacement UTF-8 character to the input byte slice, they are clearly not equal.\nWrite the code\nIn your text editor, replace the\nReverse\nfunction with the following.\nfunc Reverse(s string) string {\n    fmt.Printf(\"input: %q\\n\", s)\n    r := []rune(s)\n    fmt.Printf(\"runes: %q\\n\", r)\n    for i, j := 0, len(r)-1; i < len(r)/2; i, j = i+1, j-1 {\n        r[i], r[j] = r[j], r[i]\n    }\n    return string(r)\n}\nThis will help us understand what is going wrong when converting the string\nto a slice of runes.\nRun the code\nThis time, we only want to run the failing test in order to inspect the logs. To\ndo this, we will use\ngo test -run\n.\nTo run a specific corpus entry within FuzzXxx/testdata, you can provide\n{FuzzTestName}/{filename} to\n-run\n. This can be helpful when debugging.\nIn this case, set the\n-run\nflag equal to the exact hash of the failing test.\nCopy and paste the unique hash from your terminal;\nit will be different than the one below.\n$ go test -run=FuzzReverse/28f36ef487f23e6c7a81ebdaa9feffe2f2b02b4cddaa6252e87f69863046a5e0\ninput: \"\\x91\"\nrunes: ['�']\ninput: \"�\"\nrunes: ['�']\n--- FAIL: FuzzReverse (0.00s)\n    --- FAIL: FuzzReverse/28f36ef487f23e6c7a81ebdaa9feffe2f2b02b4cddaa6252e87f69863046a5e0 (0.00s)\n        reverse_test.go:16: Number of runes: orig=1, rev=1, doubleRev=1\n        reverse_test.go:18: Before: \"\\x91\", after: \"�\"\nFAIL\nexit status 1\nFAIL    example/fuzz    0.145s\nKnowing that the input is invalid unicode, let’s fix the error in our\nReverse\nfunction.\nFix the error\nTo fix this issue, let’s return an error if the input to\nReverse\nisn’t valid\nUTF-8.\nWrite the code\nIn your text editor, replace the existing\nReverse\nfunction with the\nfollowing.\nfunc Reverse(s string) (string, error) {\n    if !utf8.ValidString(s) {\n        return s, errors.New(\"input is not valid UTF-8\")\n    }\n    r := []rune(s)\n    for i, j := 0, len(r)-1; i < len(r)/2; i, j = i+1, j-1 {\n        r[i], r[j] = r[j], r[i]\n    }\n    return string(r), nil\n}\nThis change will return an error if the input string contains characters\nwhich are not valid UTF-8.\nSince the Reverse function now returns an error, modify the\nmain\nfunction to\ndiscard the extra error value. Replace the existing\nmain\nfunction with the\nfollowing.\nfunc main() {\n    input := \"The quick brown fox jumped over the lazy dog\"\n    rev, revErr := Reverse(input)\n    doubleRev, doubleRevErr := Reverse(rev)\n    fmt.Printf(\"original: %q\\n\", input)\n    fmt.Printf(\"reversed: %q, err: %v\\n\", rev, revErr)\n    fmt.Printf(\"reversed again: %q, err: %v\\n\", doubleRev, doubleRevErr)\n}\nThese calls to\nReverse\nshould return a nil error, since the input\nstring is valid UTF-8.\nYou will need to import the errors and the unicode/utf8 packages.\nThe import statement in main.go should look like the following.\nimport (\n    \"errors\"\n    \"fmt\"\n    \"unicode/utf8\"\n)\nModify the reverse_test.go file to check for errors and skip the test if\nerrors are generated by returning.\nfunc FuzzReverse(f *testing.F) {\n    testcases := []string {\"Hello, world\", \" \", \"!12345\"}\n    for _, tc := range testcases {\n        f.Add(tc)  // Use f.Add to provide a seed corpus\n    }\n    f.Fuzz(func(t *testing.T, orig string) {\n        rev, err1 := Reverse(orig)\n        if err1 != nil {\n            return\n        }\n        doubleRev, err2 := Reverse(rev)\n        if err2 != nil {\n             return\n        }\n        if orig != doubleRev {\n            t.Errorf(\"Before: %q, after: %q\", orig, doubleRev)\n        }\n        if utf8.ValidString(orig) && !utf8.ValidString(rev) {\n            t.Errorf(\"Reverse produced invalid UTF-8 string %q\", rev)\n        }\n    })\n}\nRather than returning, you can also call\nt.Skip()\nto stop the execution of\nthat fuzz input.\nRun the code\nRun the test using go test\n$ go test\nPASS\nok      example/fuzz  0.019s\nFuzz it with\ngo test -fuzz=Fuzz\n, then after a few seconds has passed, stop\nfuzzing with\nctrl-C\n. The fuzz test will run until it encounters a failing\ninput unless you pass the\n-fuzztime\nflag. The default is to run forever if no\nfailures occur, and the process can be interrupted with\nctrl-C\n.\n$ go test -fuzz=Fuzz\nfuzz: elapsed: 0s, gathering baseline coverage: 0/38 completed\nfuzz: elapsed: 0s, gathering baseline coverage: 38/38 completed, now fuzzing with 4 workers\nfuzz: elapsed: 3s, execs: 86342 (28778/sec), new interesting: 2 (total: 35)\nfuzz: elapsed: 6s, execs: 193490 (35714/sec), new interesting: 4 (total: 37)\nfuzz: elapsed: 9s, execs: 304390 (36961/sec), new interesting: 4 (total: 37)\n...\nfuzz: elapsed: 3m45s, execs: 7246222 (32357/sec), new interesting: 8 (total: 41)\n^Cfuzz: elapsed: 3m48s, execs: 7335316 (31648/sec), new interesting: 8 (total: 41)\nPASS\nok      example/fuzz  228.000s\nFuzz it with\ngo test -fuzz=Fuzz -fuzztime 30s\nwhich will fuzz for 30\nseconds before exiting if no failure was found.\n$ go test -fuzz=Fuzz -fuzztime 30s\nfuzz: elapsed: 0s, gathering baseline coverage: 0/5 completed\nfuzz: elapsed: 0s, gathering baseline coverage: 5/5 completed, now fuzzing with 4 workers\nfuzz: elapsed: 3s, execs: 80290 (26763/sec), new interesting: 12 (total: 12)\nfuzz: elapsed: 6s, execs: 210803 (43501/sec), new interesting: 14 (total: 14)\nfuzz: elapsed: 9s, execs: 292882 (27360/sec), new interesting: 14 (total: 14)\nfuzz: elapsed: 12s, execs: 371872 (26329/sec), new interesting: 14 (total: 14)\nfuzz: elapsed: 15s, execs: 517169 (48433/sec), new interesting: 15 (total: 15)\nfuzz: elapsed: 18s, execs: 663276 (48699/sec), new interesting: 15 (total: 15)\nfuzz: elapsed: 21s, execs: 771698 (36143/sec), new interesting: 15 (total: 15)\nfuzz: elapsed: 24s, execs: 924768 (50990/sec), new interesting: 16 (total: 16)\nfuzz: elapsed: 27s, execs: 1082025 (52427/sec), new interesting: 17 (total: 17)\nfuzz: elapsed: 30s, execs: 1172817 (30281/sec), new interesting: 17 (total: 17)\nfuzz: elapsed: 31s, execs: 1172817 (0/sec), new interesting: 17 (total: 17)\nPASS\nok      example/fuzz  31.025s\nFuzzing passed!\nIn addition to the\n-fuzz\nflag, several new flags have been added to\ngo test\nand can be viewed in the\ndocumentation\n.\nSee\nGo Fuzzing\nfor more\ninformation on terms used in fuzzing output. For example, “new interesting”\nrefers to inputs that expand the code coverage of the existing fuzz test\ncorpus. The number of “new interesting” inputs can be expected to increase\nsharply as fuzzing begins, spike several times as new code paths are\ndiscovered, then taper off over time.\nConclusion\nNicely done! You’ve just introduced yourself to fuzzing in Go.\nThe next step is to choose a function in your code that you’d like to fuzz, and\ntry it out! If fuzzing finds a bug in your code, consider adding it to the\ntrophy case\n.\nIf you experience any problems or have an idea for a feature,\nfile an\nissue\n.\nFor discussion and general feedback about the feature, you can also participate\nin the\n#fuzzing channel\nin\nGophers Slack.\nCheck out the documentation at\ngo.dev/security/fuzz\nfor\nfurther reading.\nCompleted code\n— main.go —\npackage main\n\nimport (\n    \"errors\"\n    \"fmt\"\n    \"unicode/utf8\"\n)\n\nfunc main() {\n    input := \"The quick brown fox jumped over the lazy dog\"\n    rev, revErr := Reverse(input)\n    doubleRev, doubleRevErr := Reverse(rev)\n    fmt.Printf(\"original: %q\\n\", input)\n    fmt.Printf(\"reversed: %q, err: %v\\n\", rev, revErr)\n    fmt.Printf(\"reversed again: %q, err: %v\\n\", doubleRev, doubleRevErr)\n}\n\nfunc Reverse(s string) (string, error) {\n    if !utf8.ValidString(s) {\n        return s, errors.New(\"input is not valid UTF-8\")\n    }\n    r := []rune(s)\n    for i, j := 0, len(r)-1; i < len(r)/2; i, j = i+1, j-1 {\n        r[i], r[j] = r[j], r[i]\n    }\n    return string(r), nil\n}\n— reverse_test.go —\npackage main\n\nimport (\n    \"testing\"\n    \"unicode/utf8\"\n)\n\nfunc FuzzReverse(f *testing.F) {\n    testcases := []string{\"Hello, world\", \" \", \"!12345\"}\n    for _, tc := range testcases {\n        f.Add(tc) // Use f.Add to provide a seed corpus\n    }\n    f.Fuzz(func(t *testing.T, orig string) {\n        rev, err1 := Reverse(orig)\n        if err1 != nil {\n            return\n        }\n        doubleRev, err2 := Reverse(rev)\n        if err2 != nil {\n            return\n        }\n        if orig != doubleRev {\n            t.Errorf(\"Before: %q, after: %q\", orig, doubleRev)\n        }\n        if utf8.ValidString(orig) && !utf8.ValidString(rev) {\n            t.Errorf(\"Reverse produced invalid UTF-8 string %q\", rev)\n        }\n    })\n}\nBack to top\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\n/doc/articles/\nWriting Web Applications\nWriting Web Applications\nIntroduction\nCovered in this tutorial:\nCreating a data structure with load and save methods\nUsing the\nnet/http\npackage to build web applications\nUsing the\nhtml/template\npackage to process HTML templates\nUsing the\nregexp\npackage to validate user input\nUsing closures\nAssumed knowledge:\nProgramming experience\nUnderstanding of basic web technologies (HTTP, HTML)\nSome UNIX/DOS command-line knowledge\nGetting Started\nAt present, you need to have a FreeBSD, Linux, macOS, or Windows machine to run Go.\nWe will use\n$\nto represent the command prompt.\nInstall Go (see the\nInstallation Instructions\n).\nMake a new directory for this tutorial inside your\nGOPATH\nand cd to it:\n$ mkdir gowiki\n$ cd gowiki\nCreate a file named\nwiki.go\n, open it in your favorite editor, and\nadd the following lines:\npackage main\n\nimport (\n    \"fmt\"\n    \"os\"\n)\nWe import the\nfmt\nand\nos\npackages from the Go\nstandard library. Later, as we implement additional functionality, we will\nadd more packages to this\nimport\ndeclaration.\nData Structures\nLet's start by defining the data structures. A wiki consists of a series of\ninterconnected pages, each of which has a title and a body (the page content).\nHere, we define\nPage\nas a struct with two fields representing\nthe title and body.\ntype Page struct {\n    Title string\n    Body  []byte\n}\nThe type\n[]byte\nmeans \"a\nbyte\nslice\".\n(See\nSlices: usage and\ninternals\nfor more on slices.)\nThe\nBody\nelement is a\n[]byte\nrather than\nstring\nbecause that is the type expected by the\nio\nlibraries we will use, as you'll see below.\nThe\nPage\nstruct describes how page data will be stored in memory.\nBut what about persistent storage? We can address that by creating a\nsave\nmethod on\nPage\n:\nfunc (p *Page) save() error {\n    filename := p.Title + \".txt\"\n    return os.WriteFile(filename, p.Body, 0600)\n}\nThis method's signature reads: \"This is a method named\nsave\nthat\ntakes as its receiver\np\n, a pointer to\nPage\n. It takes\nno parameters, and returns a value of type\nerror\n.\"\nThis method will save the\nPage\n's\nBody\nto a text\nfile. For simplicity, we will use the\nTitle\nas the file name.\nThe\nsave\nmethod returns an\nerror\nvalue because\nthat is the return type of\nWriteFile\n(a standard library function\nthat writes a byte slice to a file).  The\nsave\nmethod returns the\nerror value, to let the application handle it should anything go wrong while\nwriting the file.  If all goes well,\nPage.save()\nwill return\nnil\n(the zero-value for pointers, interfaces, and some other\ntypes).\nThe octal integer literal\n0600\n, passed as the third parameter to\nWriteFile\n, indicates that the file should be created with\nread-write permissions for the current user only. (See the Unix man page\nopen(2)\nfor details.)\nIn addition to saving pages, we will want to load pages, too:\nfunc loadPage(title string) *Page {\n    filename := title + \".txt\"\n    body, _ := os.ReadFile(filename)\n    return &Page{Title: title, Body: body}\n}\nThe function\nloadPage\nconstructs the file name from the title\nparameter, reads the file's contents into a new variable\nbody\n, and\nreturns a pointer to a\nPage\nliteral constructed with the proper\ntitle and body values.\nFunctions can return multiple values. The standard library function\nos.ReadFile\nreturns\n[]byte\nand\nerror\n.\nIn\nloadPage\n, error isn't being handled yet; the \"blank identifier\"\nrepresented by the underscore (\n_\n) symbol is used to throw away the\nerror return value (in essence, assigning the value to nothing).\nBut what happens if\nReadFile\nencounters an error?  For example,\nthe file might not exist. We should not ignore such errors.  Let's modify the\nfunction to return\n*Page\nand\nerror\n.\nfunc loadPage(title string) (*Page, error) {\n    filename := title + \".txt\"\n    body, err := os.ReadFile(filename)\n    if err != nil {\n        return nil, err\n    }\n    return &Page{Title: title, Body: body}, nil\n}\nCallers of this function can now check the second parameter; if it is\nnil\nthen it has successfully loaded a Page. If not, it will be an\nerror\nthat can be handled by the caller (see the\nlanguage specification\nfor details).\nAt this point we have a simple data structure and the ability to save to and\nload from a file. Let's write a\nmain\nfunction to test what we've\nwritten:\nfunc main() {\n    p1 := &Page{Title: \"TestPage\", Body: []byte(\"This is a sample Page.\")}\n    p1.save()\n    p2, _ := loadPage(\"TestPage\")\n    fmt.Println(string(p2.Body))\n}\nAfter compiling and executing this code, a file named\nTestPage.txt\nwould be created, containing the contents of\np1\n. The file would\nthen be read into the struct\np2\n, and its\nBody\nelement\nprinted to the screen.\nYou can compile and run the program like this:\n$ go build wiki.go\n$ ./wiki\nThis is a sample Page.\n(If you're using Windows you must type \"\nwiki\n\" without the\n\"\n./\n\" to run the program.)\nClick here to view the code we've written so far.\nIntroducing the\nnet/http\npackage (an interlude)\nHere's a full working example of a simple web server:\n//go:build ignore\npackage main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n)\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    fmt.Fprintf(w, \"Hi there, I love %s!\", r.URL.Path[1:])\n}\n\nfunc main() {\n    http.HandleFunc(\"/\", handler)\n    log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\nThe\nmain\nfunction begins with a call to\nhttp.HandleFunc\n, which tells the\nhttp\npackage to\nhandle all requests to the web root (\n\"/\"\n) with\nhandler\n.\nIt then calls\nhttp.ListenAndServe\n, specifying that it should\nlisten on port 8080 on any interface (\n\":8080\"\n). (Don't\nworry about its second parameter,\nnil\n, for now.)\nThis function will block until the program is terminated.\nListenAndServe\nalways returns an error, since it only returns when an\nunexpected error occurs.\nIn order to log that error we wrap the function call with\nlog.Fatal\n.\nThe function\nhandler\nis of the type\nhttp.HandlerFunc\n.\nIt takes an\nhttp.ResponseWriter\nand an\nhttp.Request\nas\nits arguments.\nAn\nhttp.ResponseWriter\nvalue assembles the HTTP server's response; by writing\nto it, we send data to the HTTP client.\nAn\nhttp.Request\nis a data structure that represents the client\nHTTP request.\nr.URL.Path\nis the path component\nof the request URL. The trailing\n[1:]\nmeans\n\"create a sub-slice of\nPath\nfrom the 1st character to the end.\"\nThis drops the leading \"/\" from the path name.\nIf you run this program and access the URL:\nhttp://localhost:8080/monkeys\nthe program would present a page containing:\nHi there, I love monkeys!\nUsing\nnet/http\nto serve wiki pages\nTo use the\nnet/http\npackage, it must be imported:\nimport (\n    \"fmt\"\n    \"os\"\n    \"log\"\n\"net/http\"\n)\nLet's create a handler,\nviewHandler\nthat will allow users to\nview a wiki page. It will handle URLs prefixed with \"/view/\".\nfunc viewHandler(w http.ResponseWriter, r *http.Request) {\n    title := r.URL.Path[len(\"/view/\"):]\n    p, _ := loadPage(title)\n    fmt.Fprintf(w, \"<h1>%s</h1><div>%s</div>\", p.Title, p.Body)\n}\nAgain, note the use of\n_\nto ignore the\nerror\nreturn value from\nloadPage\n. This is done here for simplicity\nand generally considered bad practice. We will attend to this later.\nFirst, this function extracts the page title from\nr.URL.Path\n,\nthe path component of the request URL.\nThe\nPath\nis re-sliced with\n[len(\"/view/\"):]\nto drop\nthe leading\n\"/view/\"\ncomponent of the request path.\nThis is because the path will invariably begin with\n\"/view/\"\n,\nwhich is not part of the page's title.\nThe function then loads the page data, formats the page with a string of simple\nHTML, and writes it to\nw\n, the\nhttp.ResponseWriter\n.\nTo use this handler, we rewrite our\nmain\nfunction to\ninitialize\nhttp\nusing the\nviewHandler\nto handle\nany requests under the path\n/view/\n.\nfunc main() {\n    http.HandleFunc(\"/view/\", viewHandler)\n    log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\nClick here to view the code we've written so far.\nLet's create some page data (as\ntest.txt\n), compile our code, and\ntry serving a wiki page.\nOpen\ntest.txt\nfile in your editor, and save the string \"Hello world\" (without quotes)\nin it.\n$ go build wiki.go\n$ ./wiki\n(If you're using Windows you must type \"\nwiki\n\" without the\n\"\n./\n\" to run the program.)\nWith this web server running, a visit to\nhttp://localhost:8080/view/test\nshould show a page titled \"test\" containing the words \"Hello world\".\nEditing Pages\nA wiki is not a wiki without the ability to edit pages. Let's create two new\nhandlers: one named\neditHandler\nto display an 'edit page' form,\nand the other named\nsaveHandler\nto save the data entered via the\nform.\nFirst, we add them to\nmain()\n:\nfunc main() {\n    http.HandleFunc(\"/view/\", viewHandler)\n    http.HandleFunc(\"/edit/\", editHandler)\n    http.HandleFunc(\"/save/\", saveHandler)\n    log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\nThe function\neditHandler\nloads the page\n(or, if it doesn't exist, create an empty\nPage\nstruct),\nand displays an HTML form.\nfunc editHandler(w http.ResponseWriter, r *http.Request) {\n    title := r.URL.Path[len(\"/edit/\"):]\n    p, err := loadPage(title)\n    if err != nil {\n        p = &Page{Title: title}\n    }\n    fmt.Fprintf(w, \"<h1>Editing %s</h1>\"+\n        \"<form action=\\\"/save/%s\\\" method=\\\"POST\\\">\"+\n        \"<textarea name=\\\"body\\\">%s</textarea><br>\"+\n        \"<input type=\\\"submit\\\" value=\\\"Save\\\">\"+\n        \"</form>\",\n        p.Title, p.Title, p.Body)\n}\nThis function will work fine, but all that hard-coded HTML is ugly.\nOf course, there is a better way.\nThe\nhtml/template\npackage\nThe\nhtml/template\npackage is part of the Go standard library.\nWe can use\nhtml/template\nto keep the HTML in a separate file,\nallowing us to change the layout of our edit page without modifying the\nunderlying Go code.\nFirst, we must add\nhtml/template\nto the list of imports. We\nalso won't be using\nfmt\nanymore, so we have to remove that.\nimport (\n\"html/template\"\n\"os\"\n    \"net/http\"\n)\nLet's create a template file containing the HTML form.\nOpen a new file named\nedit.html\n, and add the following lines:\n<h1>Editing {{.Title}}</h1>\n\n<form action=\"/save/{{.Title}}\" method=\"POST\">\n<div><textarea name=\"body\" rows=\"20\" cols=\"80\">{{printf \"%s\" .Body}}</textarea></div>\n<div><input type=\"submit\" value=\"Save\"></div>\n</form>\nModify\neditHandler\nto use the template, instead of the hard-coded\nHTML:\nfunc editHandler(w http.ResponseWriter, r *http.Request) {\n    title := r.URL.Path[len(\"/edit/\"):]\n    p, err := loadPage(title)\n    if err != nil {\n        p = &Page{Title: title}\n    }\n    t, _ := template.ParseFiles(\"edit.html\")\n    t.Execute(w, p)\n}\nThe function\ntemplate.ParseFiles\nwill read the contents of\nedit.html\nand return a\n*template.Template\n.\nThe method\nt.Execute\nexecutes the template, writing the\ngenerated HTML to the\nhttp.ResponseWriter\n.\nThe\n.Title\nand\n.Body\ndotted identifiers refer to\np.Title\nand\np.Body\n.\nTemplate directives are enclosed in double curly braces.\nThe\nprintf \"%s\" .Body\ninstruction is a function call\nthat outputs\n.Body\nas a string instead of a stream of bytes,\nthe same as a call to\nfmt.Printf\n.\nThe\nhtml/template\npackage helps guarantee that only safe and\ncorrect-looking HTML is generated by template actions. For instance, it\nautomatically escapes any greater than sign (\n>\n), replacing it\nwith\n&gt;\n, to make sure user data does not corrupt the form\nHTML.\nSince we're working with templates now, let's create a template for our\nviewHandler\ncalled\nview.html\n:\n<h1>{{.Title}}</h1>\n\n<p>[<a href=\"/edit/{{.Title}}\">edit</a>]</p>\n\n<div>{{printf \"%s\" .Body}}</div>\nModify\nviewHandler\naccordingly:\nfunc viewHandler(w http.ResponseWriter, r *http.Request) {\n    title := r.URL.Path[len(\"/view/\"):]\n    p, _ := loadPage(title)\n    t, _ := template.ParseFiles(\"view.html\")\n    t.Execute(w, p)\n}\nNotice that we've used almost exactly the same templating code in both\nhandlers. Let's remove this duplication by moving the templating code\nto its own function:\nfunc renderTemplate(w http.ResponseWriter, tmpl string, p *Page) {\n    t, _ := template.ParseFiles(tmpl + \".html\")\n    t.Execute(w, p)\n}\nAnd modify the handlers to use that function:\nfunc viewHandler(w http.ResponseWriter, r *http.Request) {\n    title := r.URL.Path[len(\"/view/\"):]\n    p, _ := loadPage(title)\n    renderTemplate(w, \"view\", p)\n}\nfunc editHandler(w http.ResponseWriter, r *http.Request) {\n    title := r.URL.Path[len(\"/edit/\"):]\n    p, err := loadPage(title)\n    if err != nil {\n        p = &Page{Title: title}\n    }\n    renderTemplate(w, \"edit\", p)\n}\nIf we comment out the registration of our unimplemented save handler in\nmain\n, we can once again build and test our program.\nClick here to view the code we've written so far.\nHandling non-existent pages\nWhat if you visit\n/view/APageThatDoesntExist\n? You'll see a page containing\nHTML. This is because it ignores the error return value from\nloadPage\nand continues to try and fill out the template\nwith no data. Instead, if the requested Page doesn't exist, it should\nredirect the client to the edit Page so the content may be created:\nfunc viewHandler(w http.ResponseWriter, r *http.Request) {\n    title := r.URL.Path[len(\"/view/\"):]\n    p, err := loadPage(title)\n    if err != nil {\n        http.Redirect(w, r, \"/edit/\"+title, http.StatusFound)\n        return\n    }\n    renderTemplate(w, \"view\", p)\n}\nThe\nhttp.Redirect\nfunction adds an HTTP status code of\nhttp.StatusFound\n(302) and a\nLocation\nheader to the HTTP response.\nSaving Pages\nThe function\nsaveHandler\nwill handle the submission of forms\nlocated on the edit pages. After uncommenting the related line in\nmain\n, let's implement the handler:\nfunc saveHandler(w http.ResponseWriter, r *http.Request) {\n    title := r.URL.Path[len(\"/save/\"):]\n    body := r.FormValue(\"body\")\n    p := &Page{Title: title, Body: []byte(body)}\n    p.save()\n    http.Redirect(w, r, \"/view/\"+title, http.StatusFound)\n}\nThe page title (provided in the URL) and the form's only field,\nBody\n, are stored in a new\nPage\n.\nThe\nsave()\nmethod is then called to write the data to a file,\nand the client is redirected to the\n/view/\npage.\nThe value returned by\nFormValue\nis of type\nstring\n.\nWe must convert that value to\n[]byte\nbefore it will fit into\nthe\nPage\nstruct. We use\n[]byte(body)\nto perform\nthe conversion.\nError handling\nThere are several places in our program where errors are being ignored.  This\nis bad practice, not least because when an error does occur the program will\nhave unintended behavior. A better solution is to handle the errors and return\nan error message to the user. That way if something does go wrong, the server\nwill function exactly how we want and the user can be notified.\nFirst, let's handle the errors in\nrenderTemplate\n:\nfunc renderTemplate(w http.ResponseWriter, tmpl string, p *Page) {\n    t, err := template.ParseFiles(tmpl + \".html\")\n    if err != nil {\n        http.Error(w, err.Error(), http.StatusInternalServerError)\n        return\n    }\n    err = t.Execute(w, p)\n    if err != nil {\n        http.Error(w, err.Error(), http.StatusInternalServerError)\n    }\n}\nThe\nhttp.Error\nfunction sends a specified HTTP response code\n(in this case \"Internal Server Error\") and error message.\nAlready the decision to put this in a separate function is paying off.\nNow let's fix up\nsaveHandler\n:\nfunc saveHandler(w http.ResponseWriter, r *http.Request) {\n    title := r.URL.Path[len(\"/save/\"):]\n    body := r.FormValue(\"body\")\n    p := &Page{Title: title, Body: []byte(body)}\n    err := p.save()\n    if err != nil {\n        http.Error(w, err.Error(), http.StatusInternalServerError)\n        return\n    }\n    http.Redirect(w, r, \"/view/\"+title, http.StatusFound)\n}\nAny errors that occur during\np.save()\nwill be reported\nto the user.\nTemplate caching\nThere is an inefficiency in this code:\nrenderTemplate\ncalls\nParseFiles\nevery time a page is rendered.\nA better approach would be to call\nParseFiles\nonce at program\ninitialization, parsing all templates into a single\n*Template\n.\nThen we can use the\nExecuteTemplate\nmethod to render a specific template.\nFirst we create a global variable named\ntemplates\n, and initialize\nit with\nParseFiles\n.\nvar templates = template.Must(template.ParseFiles(\"edit.html\", \"view.html\"))\nThe function\ntemplate.Must\nis a convenience wrapper that panics\nwhen passed a non-nil\nerror\nvalue, and otherwise returns the\n*Template\nunaltered. A panic is appropriate here; if the templates\ncan't be loaded the only sensible thing to do is exit the program.\nThe\nParseFiles\nfunction takes any number of string arguments that\nidentify our template files, and parses those files into templates that are\nnamed after the base file name. If we were to add more templates to our\nprogram, we would add their names to the\nParseFiles\ncall's\narguments.\nWe then modify the\nrenderTemplate\nfunction to call the\ntemplates.ExecuteTemplate\nmethod with the name of the appropriate\ntemplate:\nfunc renderTemplate(w http.ResponseWriter, tmpl string, p *Page) {\n    err := templates.ExecuteTemplate(w, tmpl+\".html\", p)\n    if err != nil {\n        http.Error(w, err.Error(), http.StatusInternalServerError)\n    }\n}\nNote that the template name is the template file name, so we must\nappend\n\".html\"\nto the\ntmpl\nargument.\nValidation\nAs you may have observed, this program has a serious security flaw: a user\ncan supply an arbitrary path to be read/written on the server. To mitigate\nthis, we can write a function to validate the title with a regular expression.\nFirst, add\n\"regexp\"\nto the\nimport\nlist.\nThen we can create a global variable to store our validation\nexpression:\nvar validPath = regexp.MustCompile(\"^/(edit|save|view)/([a-zA-Z0-9]+)$\")\nThe function\nregexp.MustCompile\nwill parse and compile the\nregular expression, and return a\nregexp.Regexp\n.\nMustCompile\nis distinct from\nCompile\nin that it will\npanic if the expression compilation fails, while\nCompile\nreturns\nan\nerror\nas a second parameter.\nNow, let's write a function that uses the\nvalidPath\nexpression to validate path and extract the page title:\nfunc getTitle(w http.ResponseWriter, r *http.Request) (string, error) {\n    m := validPath.FindStringSubmatch(r.URL.Path)\n    if m == nil {\n        http.NotFound(w, r)\n        return \"\", errors.New(\"invalid Page Title\")\n    }\n    return m[2], nil\n// The title is the second subexpression.\n}\nIf the title is valid, it will be returned along with a\nnil\nerror value. If the title is invalid, the function will write a\n\"404 Not Found\" error to the HTTP connection, and return an error to the\nhandler. To create a new error, we have to import the\nerrors\npackage.\nLet's put a call to\ngetTitle\nin each of the handlers:\nfunc viewHandler(w http.ResponseWriter, r *http.Request) {\n    title, err := getTitle(w, r)\n    if err != nil {\n        return\n    }\n    p, err := loadPage(title)\n    if err != nil {\n        http.Redirect(w, r, \"/edit/\"+title, http.StatusFound)\n        return\n    }\n    renderTemplate(w, \"view\", p)\n}\nfunc editHandler(w http.ResponseWriter, r *http.Request) {\n    title, err := getTitle(w, r)\n    if err != nil {\n        return\n    }\n    p, err := loadPage(title)\n    if err != nil {\n        p = &Page{Title: title}\n    }\n    renderTemplate(w, \"edit\", p)\n}\nfunc saveHandler(w http.ResponseWriter, r *http.Request) {\n    title, err := getTitle(w, r)\n    if err != nil {\n        return\n    }\n    body := r.FormValue(\"body\")\n    p := &Page{Title: title, Body: []byte(body)}\n    err = p.save()\n    if err != nil {\n        http.Error(w, err.Error(), http.StatusInternalServerError)\n        return\n    }\n    http.Redirect(w, r, \"/view/\"+title, http.StatusFound)\n}\nIntroducing Function Literals and Closures\nCatching the error condition in each handler introduces a lot of repeated code.\nWhat if we could wrap each of the handlers in a function that does this\nvalidation and error checking? Go's\nfunction\nliterals\nprovide a powerful means of abstracting functionality\nthat can help us here.\nFirst, we re-write the function definition of each of the handlers to accept\na title string:\nfunc viewHandler(w http.ResponseWriter, r *http.Request, title string)\nfunc editHandler(w http.ResponseWriter, r *http.Request, title string)\nfunc saveHandler(w http.ResponseWriter, r *http.Request, title string)\nNow let's define a wrapper function that\ntakes a function of the above\ntype\n, and returns a function of type\nhttp.HandlerFunc\n(suitable to be passed to the function\nhttp.HandleFunc\n):\nfunc makeHandler(fn func (http.ResponseWriter, *http.Request, string)) http.HandlerFunc {\n    return func(w http.ResponseWriter, r *http.Request) {\n        // Here we will extract the page title from the Request,\n        // and call the provided handler 'fn'\n    }\n}\nThe returned function is called a closure because it encloses values defined\noutside of it. In this case, the variable\nfn\n(the single argument\nto\nmakeHandler\n) is enclosed by the closure. The variable\nfn\nwill be one of our save, edit, or view handlers.\nNow we can take the code from\ngetTitle\nand use it here\n(with some minor modifications):\nfunc makeHandler(fn func(http.ResponseWriter, *http.Request, string)) http.HandlerFunc {\n    return func(w http.ResponseWriter, r *http.Request) {\n        m := validPath.FindStringSubmatch(r.URL.Path)\n        if m == nil {\n            http.NotFound(w, r)\n            return\n        }\n        fn(w, r, m[2])\n    }\n}\nThe closure returned by\nmakeHandler\nis a function that takes\nan\nhttp.ResponseWriter\nand\nhttp.Request\n(in other\nwords, an\nhttp.HandlerFunc\n).\nThe closure extracts the\ntitle\nfrom the request path, and\nvalidates it with the\nvalidPath\nregexp. If the\ntitle\nis invalid, an error will be written to the\nResponseWriter\nusing the\nhttp.NotFound\nfunction.\nIf the\ntitle\nis valid, the enclosed handler function\nfn\nwill be called with the\nResponseWriter\n,\nRequest\n, and\ntitle\nas arguments.\nNow we can wrap the handler functions with\nmakeHandler\nin\nmain\n, before they are registered with the\nhttp\npackage:\nfunc main() {\n    http.HandleFunc(\"/view/\", makeHandler(viewHandler))\n    http.HandleFunc(\"/edit/\", makeHandler(editHandler))\n    http.HandleFunc(\"/save/\", makeHandler(saveHandler))\n\n    log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\nFinally we remove the calls to\ngetTitle\nfrom the handler functions,\nmaking them much simpler:\nfunc viewHandler(w http.ResponseWriter, r *http.Request, title string) {\n    p, err := loadPage(title)\n    if err != nil {\n        http.Redirect(w, r, \"/edit/\"+title, http.StatusFound)\n        return\n    }\n    renderTemplate(w, \"view\", p)\n}\nfunc editHandler(w http.ResponseWriter, r *http.Request, title string) {\n    p, err := loadPage(title)\n    if err != nil {\n        p = &Page{Title: title}\n    }\n    renderTemplate(w, \"edit\", p)\n}\nfunc saveHandler(w http.ResponseWriter, r *http.Request, title string) {\n    body := r.FormValue(\"body\")\n    p := &Page{Title: title, Body: []byte(body)}\n    err := p.save()\n    if err != nil {\n        http.Error(w, err.Error(), http.StatusInternalServerError)\n        return\n    }\n    http.Redirect(w, r, \"/view/\"+title, http.StatusFound)\n}\nTry it out!\nClick here to view the final code listing.\nRecompile the code, and run the app:\n$ go build wiki.go\n$ ./wiki\nVisiting\nhttp://localhost:8080/view/ANewPage\nshould present you with the page edit form. You should then be able to\nenter some text, click 'Save', and be redirected to the newly created page.\nOther tasks\nHere are some simple tasks you might want to tackle on your own:\nStore templates in\ntmpl/\nand page data in\ndata/\n.\nAdd a handler to make the web root redirect to\n/view/FrontPage\n.\nSpruce up the page templates by making them valid HTML and adding some\n\tCSS rules.\nImplement inter-page linking by converting instances of\n[PageName]\nto\n<a href=\"/view/PageName\">PageName</a>\n.\n\t(hint: you could use\nregexp.ReplaceAllFunc\nto do this)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nFrequently Asked Questions (FAQ)\nFrequently Asked Questions (FAQ)\nOrigins\nWhat is the purpose of the project?\nAt the time of Go’s inception in 2007 the programming world was different from today.\nProduction software was usually written in C++ or Java,\nGitHub did not exist, most computers were not yet multiprocessors,\nand other than Visual Studio and Eclipse there were few IDEs or other high-level tools available\nat all, let alone for free on the Internet.\nMeanwhile, we had become frustrated by the undue complexity required\nto build large software projects with\nthe languages we were using and their associated build systems.\nComputers had become enormously quicker since languages such as\nC, C++ and Java were first developed but the act of programming had not\nitself advanced nearly as much.\nAlso, it was clear that multiprocessors were becoming universal but\nmost languages offered little help to program them efficiently\nand safely.\nWe decided to take a step back and think about what major issues were\ngoing to dominate software engineering in the years ahead as technology\ndeveloped, and how a new language might help address them.\nFor instance, the rise of multicore CPUs argued that a language should\nprovide first-class support for some sort of concurrency or parallelism.\nAnd to make resource management tractable in a large concurrent program,\ngarbage collection, or at least some sort of safe automatic memory management was required.\nThese considerations led to\na series of discussions\nfrom which Go arose, first as a set of ideas and\ndesiderata, then as a language.\nAn overarching goal was that Go do more to help the working programmer\nby enabling tooling, automating mundane tasks such as code formatting,\nand removing obstacles to working on large code bases.\nA much more expansive description of the goals of Go and how\nthey are met, or at least approached, is available in the article,\nGo at Google: Language Design in the Service of Software Engineering\n.\nWhat is the history of the project?\nRobert Griesemer, Rob Pike and Ken Thompson started sketching the\ngoals for a new language on the white board on September 21, 2007.\nWithin a few days the goals had settled into a plan to do something\nand a fair idea of what it would be.  Design continued part-time in\nparallel with unrelated work.  By January 2008, Ken had started work\non a compiler with which to explore ideas; it generated C code as its\noutput.  By mid-year the language had become a full-time project and\nhad settled enough to attempt a production compiler.  In May 2008,\nIan Taylor independently started on a GCC front end for Go using the\ndraft specification.  Russ Cox joined in late 2008 and helped move the language\nand libraries from prototype to reality.\nGo became a public open source project on November 10, 2009.\nCountless people from the community have contributed ideas, discussions, and code.\nThere are now millions of Go programmers—gophers—around the world,\nand there are more every day.\nGo’s success has far exceeded our expectations.\nWhat’s the origin of the gopher mascot?\nThe mascot and logo were designed by\nRenée French\n, who also designed\nGlenda\n,\nthe Plan 9 bunny.\nA\nblog post\nabout the gopher explains how it was\nderived from one she used for a\nWFMU\nT-shirt design some years ago.\nThe logo and mascot are covered by the\nCreative Commons Attribution 4.0\nlicense.\nThe gopher has a\nmodel sheet\nillustrating his characteristics and how to represent them correctly.\nThe model sheet was first shown in a\ntalk\nby Renée at Gophercon in 2016.\nHe has unique features; he’s the\nGo gopher\n, not just any old gopher.\nIs the language called Go or Golang?\nThe language is called Go.\nThe “golang” moniker arose because the web site was\noriginally\ngolang.org\n.\n(There was no\n.dev\ndomain then.)\nMany use the golang name, though, and it is handy as\na label.\nFor instance, the social media tag for the language is “#golang”.\nThe language’s name is just plain Go, regardless.\nA side note: Although the\nofficial logo\nhas two capital letters, the language name is written Go, not GO.\nWhy did you create a new language?\nGo was born out of frustration with existing languages and\nenvironments for the work we were doing at Google.\nProgramming had become too\ndifficult and the choice of languages was partly to blame.  One had to\nchoose either efficient compilation, efficient execution, or ease of\nprogramming; all three were not available in the same mainstream\nlanguage.  Programmers who could were choosing ease over\nsafety and efficiency by moving to dynamically typed languages such as\nPython and JavaScript rather than C++ or, to a lesser extent, Java.\nWe were not alone in our concerns.\nAfter many years with a pretty quiet landscape for programming languages,\nGo was among the first of several new languages—Rust,\nElixir, Swift, and more—that have made programming language development\nan active, almost mainstream field again.\nGo addressed these issues by attempting to combine the ease of programming of an interpreted,\ndynamically typed\nlanguage with the efficiency and safety of a statically typed, compiled language.\nIt also aimed to be better adapted to current hardware, with support for networked and multicore\ncomputing.\nFinally, working with Go is intended to be\nfast\n: it should take\nat most a few seconds to build a large executable on a single computer.\nMeeting these goals led us to rethink some of the programming approaches\nfrom our current languages, leading to:\na compositional rather than hierarchical type system;\nsupport for concurrency and garbage collection; rigid specification of dependencies;\nand so on.\nThese cannot be handled well by libraries or tools; a new\nlanguage was called for.\nThe article\nGo at Google\ndiscusses the background and motivation behind the design of the Go language,\nas well as providing more detail about many of the answers presented in this FAQ.\nWhat are Go’s ancestors?\nGo is mostly in the C family (basic syntax),\nwith significant input from the Pascal/Modula/Oberon\nfamily (declarations, packages),\nplus some ideas from languages\ninspired by Tony Hoare’s CSP,\nsuch as Newsqueak and Limbo (concurrency).\nHowever, it is a new language across the board.\nIn every respect the language was designed by thinking\nabout what programmers do and how to make programming, at least the\nkind of programming we do, more effective, which means more fun.\nWhat are the guiding principles in the design?\nWhen Go was designed, Java and C++ were the most commonly\nused languages for writing servers, at least at Google.\nWe felt that these languages required\ntoo much bookkeeping and repetition.\nSome programmers reacted by moving towards more dynamic,\nfluid languages like Python, at the cost of efficiency and\ntype safety.\nWe felt it should be possible to have the efficiency,\nthe safety, and the fluidity in a single language.\nGo attempts to reduce the amount of typing in both senses of the word.\nThroughout its design, we have tried to reduce clutter and\ncomplexity.  There are no forward declarations and no header files;\neverything is declared exactly once.  Initialization is expressive,\nautomatic, and easy to use.  Syntax is clean and light on keywords.\nRepetition (\nfoo.Foo* myFoo = new(foo.Foo)\n) is reduced by\nsimple type derivation using the\n:=\ndeclare-and-initialize construct.  And perhaps most radically, there\nis no type hierarchy: types just\nare\n, they don’t have to\nannounce their relationships.  These simplifications allow Go to be\nexpressive yet comprehensible without sacrificing productivity.\nAnother important principle is to keep the concepts orthogonal.\nMethods can be implemented for any type; structures represent data while\ninterfaces represent abstraction; and so on.  Orthogonality makes it\neasier to understand what happens when things combine.\nUsage\nIs Google using Go internally?\nYes. Go is used widely in production inside Google.\nOne example is Google’s download server,\ndl.google.com\n,\nwhich delivers Chrome binaries and other large installables such as\napt-get\npackages.\nGo is not the only language used at Google, far from it, but it is a key language\nfor a number of areas including\nsite reliability engineering (SRE)\nand large-scale data processing.\nIt is also a key part of the software that runs Google Cloud.\nWhat other companies use Go?\nGo usage is growing worldwide, especially but by no means exclusively\nin the cloud computing space.\nA couple of major cloud infrastructure projects written in Go are\nDocker and Kubernetes,\nbut there are many more.\nIt’s not just cloud, though, as you can see from the list of\ncompanies on the\ngo.dev web site\nalong with some\nsuccess stories\n.\nAlso, the Go Wiki includes a\npage\n,\nupdated regularly, that lists some of the many companies using Go.\nThe Wiki also has a page with links to more\nsuccess stories\nabout companies and projects that are using the language.\nDo Go programs link with C/C++ programs?\nIt is possible to use C and Go together in the same address space,\nbut it is not a natural fit and can require special interface software.\nAlso, linking C with Go code gives up the memory\nsafety and stack management properties that Go provides.\nSometimes it’s absolutely necessary to use C libraries to solve a problem,\nbut doing so always introduces an element of risk not present with\npure Go code, so do so with care.\nIf you do need to use C with Go, how to proceed depends on the Go\ncompiler implementation.\nThe “standard” compiler, part of the Go toolchain supported by the\nGo team at Google, is called\ngc\n.\nIn addition, there are also a GCC-based compiler (\ngccgo\n) and\nan LLVM-based compiler (\ngollvm\n),\nas well as a growing list of unusual ones serving different purposes,\nsometimes implementing language subsets,\nsuch as\nTinyGo\n.\nGc\nuses a different calling convention and linker from C and\ntherefore cannot be called directly from C programs, or vice versa.\nThe\ncgo\nprogram provides the mechanism for a\n“foreign function interface” to allow safe calling of\nC libraries from Go code.\nSWIG extends this capability to C++ libraries.\nYou can also use\ncgo\nand SWIG with\ngccgo\nand\ngollvm\n.\nSince they use a traditional ABI, it’s also possible, with great care,\nto link code from these compilers directly with GCC/LLVM-compiled C or C++ programs.\nHowever, doing so safely requires an understanding of the calling conventions for\nall languages concerned, as well as concern for stack limits when calling C or C++\nfrom Go.\nWhat IDEs does Go support?\nThe Go project does not include a custom IDE, but the language and\nlibraries have been designed to make it easy to analyze source code.\nAs a consequence, most well-known editors and IDEs support Go well,\neither directly or through a plugin.\nThe Go team also supports a Go language server for the LSP protocol, called\ngopls\n.\nTools that support LSP can use\ngopls\nto integrate language-specific support.\nThe list of well-known IDEs and editors that offer good Go support\nincludes Emacs, Vim, VSCode, Atom, Eclipse, Sublime, IntelliJ\n(through a custom variant called GoLand), and many more.\nChances are your favorite environment is a productive one for\nprogramming in Go.\nDoes Go support Google’s protocol buffers?\nA separate open source project provides the necessary compiler plugin and library.\nIt is available at\ngithub.com/golang/protobuf/\n.\nDesign\nDoes Go have a runtime?\nGo has an extensive runtime library, often just called the\nruntime\n,\nthat is part of every Go program.\nThis library implements garbage collection, concurrency,\nstack management, and other critical features of the Go language.\nAlthough it is more central to the language, Go’s runtime is analogous\nto\nlibc\n, the C library.\nIt is important to understand, however, that Go’s runtime does not\ninclude a virtual machine, such as is provided by the Java runtime.\nGo programs are compiled ahead of time to native machine code\n(or JavaScript or WebAssembly, for some variant implementations).\nThus, although the term is often used to describe the virtual\nenvironment in which a program runs, in Go the word “runtime”\nis just the name given to the library providing critical language services.\nWhat’s up with Unicode identifiers?\nWhen designing Go, we wanted to make sure that it was not\noverly ASCII-centric,\nwhich meant extending the space of identifiers from the\nconfines of 7-bit ASCII.\nGo’s rule—identifier characters must be\nletters or digits as defined by Unicode—is simple to understand\nand to implement but has restrictions.\nCombining characters are\nexcluded by design, for instance,\nand that excludes some languages such as Devanagari.\nThis rule has one other unfortunate consequence.\nSince an exported identifier must begin with an\nupper-case letter, identifiers created from characters\nin some languages can, by definition, not be exported.\nFor now the\nonly solution is to use something like\nX日本語\n, which\nis clearly unsatisfactory.\nSince the earliest version of the language, there has been considerable\nthought into how best to expand the identifier space to accommodate\nprogrammers using other native languages.\nExactly what to do remains an active topic of discussion, and a future\nversion of the language may be more liberal in its definition\nof an identifier.\nFor instance, it might adopt some of the ideas from the Unicode\norganization’s\nrecommendations\nfor identifiers.\nWhatever happens, it must be done compatibly while preserving\n(or perhaps expanding) the way letter case determines visibility of\nidentifiers, which remains one of our favorite features of Go.\nFor the time being, we have a simple rule that can be expanded later\nwithout breaking programs, one that avoids bugs that would surely arise\nfrom a rule that admits ambiguous identifiers.\nWhy does Go not have feature X?\nEvery language contains novel features and omits someone’s favorite\nfeature. Go was designed with an eye on felicity of programming, speed of\ncompilation, orthogonality of concepts, and the need to support features\nsuch as concurrency and garbage collection. Your favorite feature may be\nmissing because it doesn’t fit, because it affects compilation speed or\nclarity of design, or because it would make the fundamental system model\ntoo difficult.\nIf it bothers you that Go is missing feature\nX\n,\nplease forgive us and investigate the features that Go does have. You might find that\nthey compensate in interesting ways for the lack of\nX\n.\nWhen did Go get generic types?\nThe Go 1.18 release added type parameters to the language.\nThis permits a form of polymorphic or generic programming.\nSee the\nlanguage spec\nand the\nproposal\nfor details.\nWhy was Go initially released without generic types?\nGo was intended as a language for writing server programs that would be\neasy to maintain over time.\n(See\nthis article\nfor more background.)\nThe design concentrated on things like scalability, readability, and\nconcurrency.\nPolymorphic programming did not seem essential to the language’s\ngoals at the time, and so was initially left out for simplicity.\nGenerics are convenient but they come at a cost in complexity in the\ntype system and run-time.\nIt took a while to develop a design that we believe gives value\nproportionate to the complexity.\nWhy does Go not have exceptions?\nWe believe that coupling exceptions to a control\nstructure, as in the\ntry-catch-finally\nidiom, results in\nconvoluted code.  It also tends to encourage programmers to label\ntoo many ordinary errors, such as failing to open a file, as\nexceptional.\nGo takes a different approach.  For plain error handling, Go’s multi-value\nreturns make it easy to report an error without overloading the return value.\nA canonical error type, coupled with Go’s other features\n,\nmakes error handling pleasant but quite different\nfrom that in other languages.\nGo also has a couple\nof built-in functions to signal and recover from truly exceptional\nconditions.  The recovery mechanism is executed only as part of a\nfunction’s state being torn down after an error, which is sufficient\nto handle catastrophe but requires no extra control structures and,\nwhen used well, can result in clean error-handling code.\nSee the\nDefer, Panic, and Recover\narticle for details.\nAlso, the\nErrors are values\nblog post\ndescribes one approach to handling errors cleanly in Go by demonstrating that,\nsince errors are just values, the full power of Go can be deployed in error handling.\nWhy does Go not have assertions?\nGo doesn’t provide assertions. They are undeniably convenient, but our\nexperience has been that programmers use them as a crutch to avoid thinking\nabout proper error handling and reporting. Proper error handling means that\nservers continue to operate instead of crashing after a non-fatal error.\nProper error reporting means that errors are direct and to the point,\nsaving the programmer from interpreting a large crash trace. Precise\nerrors are particularly important when the programmer seeing the errors is\nnot familiar with the code.\nWe understand that this is a point of contention. There are many things in\nthe Go language and libraries that differ from modern practices, simply\nbecause we feel it’s sometimes worth trying a different approach.\nWhy build concurrency on the ideas of CSP?\nConcurrency and multi-threaded programming have over time\ndeveloped a reputation for difficulty.  We believe this is due partly to complex\ndesigns such as\npthreads\nand partly to overemphasis on low-level details\nsuch as mutexes, condition variables, and memory barriers.\nHigher-level interfaces enable much simpler code, even if there are still\nmutexes and such under the covers.\nOne of the most successful models for providing high-level linguistic support\nfor concurrency comes from Hoare’s Communicating Sequential Processes, or CSP.\nOccam and Erlang are two well known languages that stem from CSP.\nGo’s concurrency primitives derive from a different part of the family tree\nwhose main contribution is the powerful notion of channels as first class objects.\nExperience with several earlier languages has shown that the CSP model\nfits well into a procedural language framework.\nWhy goroutines instead of threads?\nGoroutines are part of making concurrency easy to use.  The idea, which has\nbeen around for a while, is to multiplex independently executing\nfunctions—coroutines—onto a set of threads.\nWhen a coroutine blocks, such as by calling a blocking system call,\nthe run-time automatically moves other coroutines on the same operating\nsystem thread to a different, runnable thread so they won’t be blocked.\nThe programmer sees none of this, which is the point.\nThe result, which we call goroutines, can be very cheap: they have little\noverhead beyond the memory for the stack, which is just a few kilobytes.\nTo make the stacks small, Go’s run-time uses resizable, bounded stacks.  A newly\nminted goroutine is given a few kilobytes, which is almost always enough.\nWhen it isn’t, the run-time grows (and shrinks) the memory for storing\nthe stack automatically, allowing many goroutines to live in a modest\namount of memory.\nThe CPU overhead averages about three cheap instructions per function call.\nIt is practical to create hundreds of thousands of goroutines in the same\naddress space.\nIf goroutines were just threads, system resources would\nrun out at a much smaller number.\nWhy are map operations not defined to be atomic?\nAfter long discussion it was decided that the typical use of maps did not require\nsafe access from multiple goroutines, and in those cases where it did, the map was\nprobably part of some larger data structure or computation that was already\nsynchronized.  Therefore requiring that all map operations grab a mutex would slow\ndown most programs and add safety to few.  This was not an easy decision,\nhowever, since it means uncontrolled map access can crash the program.\nThe language does not preclude atomic map updates.  When required, such\nas when hosting an untrusted program, the implementation could interlock\nmap access.\nMap access is unsafe only when updates are occurring.\nAs long as all goroutines are only reading—looking up elements in the map,\nincluding iterating through it using a\nfor\nrange\nloop—and not changing the map\nby assigning to elements or doing deletions,\nit is safe for them to access the map concurrently without synchronization.\nAs an aid to correct map use, some implementations of the language\ncontain a special check that automatically reports at run time when a map is modified\nunsafely by concurrent execution.\nAlso there is a type in the sync library called\nsync.Map\nthat works\nwell for certain usage patterns such as static caches, although it is not\nsuitable as a general replacement for the builtin map type.\nWill you accept my language change?\nPeople often suggest improvements to the language—the\nmailing list\ncontains a rich history of such discussions—but very few of these changes have\nbeen accepted.\nAlthough Go is an open source project, the language and libraries are protected\nby a\ncompatibility promise\nthat prevents\nchanges that break existing programs, at least at the source code level\n(programs may need to be recompiled occasionally to stay current).\nIf your proposal violates the Go 1 specification we cannot even entertain the\nidea, regardless of its merit.\nA future major release of Go may be incompatible with Go 1, but discussions\non that topic have only just begun and one thing is certain:\nthere will be very few such incompatibilities introduced in the process.\nMoreover, the compatibility promise encourages us to provide an automatic path\nforward for old programs to adapt should that situation arise.\nEven if your proposal is compatible with the Go 1 spec, it might\nnot be in the spirit of Go’s design goals.\nThe article\nGo\nat Google: Language Design in the Service of Software Engineering\nexplains Go’s origins and the motivation behind its design.\nTypes\nIs Go an object-oriented language?\nYes and no. Although Go has types and methods and allows an\nobject-oriented style of programming, there is no type hierarchy.\nThe concept of “interface” in Go provides a different approach that\nwe believe is easy to use and in some ways more general. There are\nalso ways to embed types in other types to provide something\nanalogous—but not identical—to subclassing.\nMoreover, methods in Go are more general than in C++ or Java:\nthey can be defined for any sort of data, even built-in types such\nas plain, “unboxed” integers.\nThey are not restricted to structs (classes).\nAlso, the lack of a type hierarchy makes “objects” in Go feel much more\nlightweight than in languages such as C++ or Java.\nHow do I get dynamic dispatch of methods?\nThe only way to have dynamically dispatched methods is through an\ninterface. Methods on a struct or any other concrete type are always resolved statically.\nWhy is there no type inheritance?\nObject-oriented programming, at least in the best-known languages,\ninvolves too much discussion of the relationships between types,\nrelationships that often could be derived automatically.  Go takes a\ndifferent approach.\nRather than requiring the programmer to declare ahead of time that two\ntypes are related, in Go a type automatically satisfies any interface\nthat specifies a subset of its methods.  Besides reducing the\nbookkeeping, this approach has real advantages.  Types can satisfy\nmany interfaces at once, without the complexities of traditional\nmultiple inheritance.\nInterfaces can be very lightweight—an interface with\none or even zero methods can express a useful concept.\nInterfaces can be added after the fact if a new idea comes along\nor for testing—without annotating the original types.\nBecause there are no explicit relationships between types\nand interfaces, there is no type hierarchy to manage or discuss.\nIt’s possible to use these ideas to construct something analogous to\ntype-safe Unix pipes.  For instance, see how\nfmt.Fprintf\nenables formatted printing to any output, not just a file, or how the\nbufio\npackage can be completely separate from file I/O,\nor how the\nimage\npackages generate compressed\nimage files.  All these ideas stem from a single interface\n(\nio.Writer\n) representing a single method\n(\nWrite\n).  And that’s only scratching the surface.\nGo’s interfaces have a profound influence on how programs are structured.\nIt takes some getting used to but this implicit style of type\ndependency is one of the most productive things about Go.\nWhy is\nlen\na function and not a method?\nWe debated this issue but decided\nimplementing\nlen\nand friends as functions was fine in practice and\ndidn’t complicate questions about the interface (in the Go type sense)\nof basic types.\nWhy does Go not support overloading of methods and operators?\nMethod dispatch is simplified if it doesn’t need to do type matching as well.\nExperience with other languages told us that having a variety of\nmethods with the same name but different signatures was occasionally useful\nbut that it could also be confusing and fragile in practice.  Matching only by name\nand requiring consistency in the types was a major simplifying decision\nin Go’s type system.\nRegarding operator overloading, it seems more a convenience than an absolute\nrequirement.  Again, things are simpler without it.\nWhy doesn’t Go have “implements” declarations?\nA Go type implements an interface by implementing the methods of that interface,\nnothing more.  This property allows interfaces to be defined and used without\nneeding to modify existing code.  It enables a kind of\nstructural typing\nthat\npromotes separation of concerns and improves code re-use, and makes it easier\nto build on patterns that emerge as the code develops.\nThe semantics of interfaces is one of the main reasons for Go’s nimble,\nlightweight feel.\nSee the\nquestion on type inheritance\nfor more detail.\nHow can I guarantee my type satisfies an interface?\nYou can ask the compiler to check that the type\nT\nimplements the\ninterface\nI\nby attempting an assignment using the zero value for\nT\nor pointer to\nT\n, as appropriate:\ntype T struct{}\nvar _ I = T{}       // Verify that T implements I.\nvar _ I = (*T)(nil) // Verify that *T implements I.\nIf\nT\n(or\n*T\n, accordingly) doesn’t implement\nI\n, the mistake will be caught at compile time.\nIf you wish the users of an interface to explicitly declare that they implement\nit, you can add a method with a descriptive name to the interface’s method set.\nFor example:\ntype Fooer interface {\n    Foo()\n    ImplementsFooer()\n}\nA type must then implement the\nImplementsFooer\nmethod to be a\nFooer\n, clearly documenting the fact and announcing it in\ngo doc\n’s output.\ntype Bar struct{}\nfunc (b Bar) ImplementsFooer() {}\nfunc (b Bar) Foo() {}\nMost code doesn’t make use of such constraints, since they limit the utility of\nthe interface idea. Sometimes, though, they’re necessary to resolve ambiguities\namong similar interfaces.\nWhy doesn’t type T satisfy the Equal interface?\nConsider this simple interface to represent an object that can compare\nitself with another value:\ntype Equaler interface {\n    Equal(Equaler) bool\n}\nand this type,\nT\n:\ntype T int\nfunc (t T) Equal(u T) bool { return t == u } // does not satisfy Equaler\nUnlike the analogous situation in some polymorphic type systems,\nT\ndoes not implement\nEqualer\n.\nThe argument type of\nT.Equal\nis\nT\n,\nnot literally the required type\nEqualer\n.\nIn Go, the type system does not promote the argument of\nEqual\n; that is the programmer’s responsibility, as\nillustrated by the type\nT2\n, which does implement\nEqualer\n:\ntype T2 int\nfunc (t T2) Equal(u Equaler) bool { return t == u.(T2) }  // satisfies Equaler\nEven this isn’t like other type systems, though, because in Go\nany\ntype that satisfies\nEqualer\ncould be passed as the\nargument to\nT2.Equal\n, and at run time we must\ncheck that the argument is of type\nT2\n.\nSome languages arrange to make that guarantee at compile time.\nA related example goes the other way:\ntype Opener interface {\n   Open() Reader\n}\n\nfunc (t T3) Open() *os.File\nIn Go,\nT3\ndoes not satisfy\nOpener\n,\nalthough it might in another language.\nWhile it is true that Go’s type system does less for the programmer\nin such cases, the lack of subtyping makes the rules about\ninterface satisfaction very easy to state: are the function’s names\nand signatures exactly those of the interface?\nGo’s rule is also easy to implement efficiently.\nWe feel these benefits offset the lack of\nautomatic type promotion.\nCan I convert a []T to an []interface{}?\nNot directly.\nIt is disallowed by the language specification because the two types\ndo not have the same representation in memory.\nIt is necessary to copy the elements individually to the destination\nslice. This example converts a slice of\nint\nto a slice of\ninterface{}\n:\nt := []int{1, 2, 3, 4}\ns := make([]interface{}, len(t))\nfor i, v := range t {\n    s[i] = v\n}\nCan I convert []T1 to []T2 if T1 and T2 have the same underlying type?\nThis last line of this code sample does not compile.\ntype T1 int\ntype T2 int\nvar t1 T1\nvar x = T2(t1) // OK\nvar st1 []T1\nvar sx = ([]T2)(st1) // NOT OK\nIn Go, types are closely tied to methods, in that every named type has\na (possibly empty) method set.\nThe general rule is that you can change the name of the type being\nconverted (and thus possibly change its method set) but you can’t\nchange the name (and method set) of elements of a composite type.\nGo requires you to be explicit about type conversions.\nWhy is my nil error value not equal to nil?\nUnder the covers, interfaces are implemented as two elements, a type\nT\nand a value\nV\n.\nV\nis a concrete value such as an\nint\n,\nstruct\nor pointer, never an interface itself, and has\ntype\nT\n.\nFor instance, if we store the\nint\nvalue 3 in an interface,\nthe resulting interface value has, schematically,\n(\nT=int\n,\nV=3\n).\nThe value\nV\nis also known as the interface’s\ndynamic\nvalue,\nsince a given interface variable might hold different values\nV\n(and corresponding types\nT\n)\nduring the execution of the program.\nAn interface value is\nnil\nonly if the\nV\nand\nT\nare both unset, (\nT=nil\n,\nV\nis not set),\nIn particular, a\nnil\ninterface will always hold a\nnil\ntype.\nIf we store a\nnil\npointer of type\n*int\ninside\nan interface value, the inner type will be\n*int\nregardless of the value of the pointer:\n(\nT=*int\n,\nV=nil\n).\nSuch an interface value will therefore be non-\nnil\neven when the pointer value\nV\ninside is\nnil\n.\nThis situation can be confusing, and arises when a\nnil\nvalue is\nstored inside an interface value such as an\nerror\nreturn:\nfunc returnsError() error {\n    var p *MyError = nil\n    if bad() {\n        p = ErrBad\n    }\n    return p // Will always return a non-nil error.\n}\nIf all goes well, the function returns a\nnil\np\n,\nso the return value is an\nerror\ninterface\nvalue holding (\nT=*MyError\n,\nV=nil\n).\nThis means that if the caller compares the returned error to\nnil\n,\nit will always look as if there was an error even if nothing bad happened.\nTo return a proper\nnil\nerror\nto the caller,\nthe function must return an explicit\nnil\n:\nfunc returnsError() error {\n    if bad() {\n        return ErrBad\n    }\n    return nil\n}\nIt’s a good idea for functions\nthat return errors always to use the\nerror\ntype in\ntheir signature (as we did above) rather than a concrete type such\nas\n*MyError\n, to help guarantee the error is\ncreated correctly. As an example,\nos.Open\nreturns an\nerror\neven though, if not\nnil\n,\nit’s always of concrete type\n*os.PathError\n.\nSimilar situations to those described here can arise whenever interfaces are used.\nJust keep in mind that if any concrete value\nhas been stored in the interface, the interface will not be\nnil\n.\nFor more information, see\nThe Laws of Reflection\n.\nWhy do zero-size types behave oddly?\nGo supports zero-size types, such as a struct with no fields\n(\nstruct{}\n) or an array with no elements (\n[0]byte\n).\nThere is nothing you can store in a zero-size type, but these types\nare sometimes useful when no value is needed, as in\nmap[int]struct{}\nor a type that has methods but no value.\nDifferent variables with a zero-size type may be placed at the same\nlocation in memory.\nThis is safe as no value can be stored in those variables.\nMoreover, the language does not make any guarantees as to whether\npointers to two different zero-size variables will compare equal or\nnot.\nSuch comparisons may even return\ntrue\nat one point in the program\nand then return\nfalse\nat a different point, depending on exactly how\nthe program is compiled and executed.\nA separate issue with zero-size types is that a pointer to a zero-size\nstruct field must not overlap with a pointer to a different object in\nmemory.\nThat could cause confusion in the garbage collector.\nThis means that if the last field in a struct is zero-size, the struct\nwill be padded to ensure that a pointer to the last field does not\noverlap with memory that immediately follows the struct.\nThus, this program:\nfunc main() {\n    type S struct {\n        f1 byte\n        f2 struct{}\n    }\n    fmt.Println(unsafe.Sizeof(S{}))\n}\nwill print\n2\n, not\n1\n, in most Go implementations.\nWhy are there no untagged unions, as in C?\nUntagged unions would violate Go’s memory safety\nguarantees.\nWhy does Go not have variant types?\nVariant types, also known as algebraic types, provide a way to specify\nthat a value might take one of a set of other types, but only those\ntypes. A common example in systems programming would specify that an\nerror is, say, a network error, a security error or an application\nerror and allow the caller to discriminate the source of the problem\nby examining the type of the error. Another example is a syntax tree\nin which each node can be a different type: declaration, statement,\nassignment and so on.\nWe considered adding variant types to Go, but after discussion\ndecided to leave them out because they overlap in confusing ways\nwith interfaces. What would happen if the elements of a variant type\nwere themselves interfaces?\nAlso, some of what variant types address is already covered by the\nlanguage. The error example is easy to express using an interface\nvalue to hold the error and a type switch to discriminate cases.  The\nsyntax tree example is also doable, although not as elegantly.\nWhy does Go not have covariant result types?\nCovariant result types would mean that an interface like\ntype Copyable interface {\n    Copy() interface{}\n}\nwould be satisfied by the method\nfunc (v Value) Copy() Value\nbecause\nValue\nimplements the empty interface.\nIn Go method types must match exactly, so\nValue\ndoes not\nimplement\nCopyable\n.\nGo separates the notion of what a\ntype does—its methods—from the type’s implementation.\nIf two methods return different types, they are not doing the same thing.\nProgrammers who want covariant result types are often trying to\nexpress a type hierarchy through interfaces.\nIn Go it’s more natural to have a clean separation between interface\nand implementation.\nValues\nWhy does Go not provide implicit numeric conversions?\nThe convenience of automatic conversion between numeric types in C is\noutweighed by the confusion it causes.  When is an expression unsigned?\nHow big is the value?  Does it overflow?  Is the result portable, independent\nof the machine on which it executes?\nIt also complicates the compiler; C’s “usual arithmetic conversions”\nare not easy to implement and inconsistent across architectures.\nFor reasons of portability, we decided to make things clear and straightforward\nat the cost of some explicit conversions in the code.\nThe definition of constants in Go—arbitrary precision values free\nof signedness and size annotations—ameliorates matters considerably,\nthough.\nA related detail is that, unlike in C,\nint\nand\nint64\nare distinct types even if\nint\nis a 64-bit type.  The\nint\ntype is generic; if you care about how many bits an integer holds, Go\nencourages you to be explicit.\nHow do constants work in Go?\nAlthough Go is strict about conversion between variables of different\nnumeric types, constants in the language are much more flexible.\nLiteral constants such as\n23\n,\n3.14159\nand\nmath.Pi\noccupy a sort of ideal number space, with arbitrary precision and\nno overflow or underflow.\nFor instance, the value of\nmath.Pi\nis specified to 63 decimal digits\nin the source code, and constant expressions involving the value keep\nprecision beyond what a\nfloat64\ncould hold.\nOnly when the constant or constant expression is assigned to a\nvariable—a memory location in the program—does\nit become a “computer” number with\nthe usual floating-point properties and precision.\nAlso,\nbecause they are just numbers, not typed values, constants in Go can be\nused more freely than variables, thereby softening some of the awkwardness\naround the strict conversion rules.\nOne can write expressions such as\nsqrt2 := math.Sqrt(2)\nwithout complaint from the compiler because the ideal number\n2\ncan be converted safely and accurately\nto a\nfloat64\nfor the call to\nmath.Sqrt\n.\nA blog post titled\nConstants\nexplores this topic in more detail.\nWhy are maps built in?\nThe same reason strings are: they are such a powerful and important data\nstructure that providing one excellent implementation with syntactic support\nmakes programming more pleasant.  We believe that Go’s implementation of maps\nis strong enough that it will serve for the vast majority of uses.\nIf a specific application can benefit from a custom implementation, it’s possible\nto write one but it will not be as convenient syntactically; this seems a reasonable tradeoff.\nWhy don’t maps allow slices as keys?\nMap lookup requires an equality operator, which slices do not implement.\nThey don’t implement equality because equality is not well defined on such types;\nthere are multiple considerations involving shallow vs. deep comparison, pointer vs.\nvalue comparison, how to deal with recursive types, and so on.\nWe may revisit this issue—and implementing equality for slices\nwill not invalidate any existing programs—but without a clear idea of what\nequality of slices should mean, it was simpler to leave it out for now.\nEquality is defined for structs and arrays, so they can be used as map keys.\nWhy are maps, slices, and channels references while arrays are values?\nThere’s a lot of history on that topic.  Early on, maps and channels\nwere syntactically pointers and it was impossible to declare or use a\nnon-pointer instance.  Also, we struggled with how arrays should work.\nEventually we decided that the strict separation of pointers and\nvalues made the language harder to use.  Changing these\ntypes to act as references to the associated, shared data structures resolved\nthese issues. This change added some regrettable complexity to the\nlanguage but had a large effect on usability: Go became a more\nproductive, comfortable language when it was introduced.\nWriting Code\nHow are libraries documented?\nFor access to documentation from the command line, the\ngo\ntool has a\ndoc\nsubcommand that provides a textual interface to the documentation\nfor declarations, files, packages and so on.\nThe global package discovery page\npkg.go.dev/pkg/\n.\nruns a server that extracts package documentation from Go source code\nanywhere on the web\nand serves it as HTML with links to the declarations and related elements.\nIt is the easiest way to learn about existing Go libraries.\nIn the early days of the project, there was a similar program,\ngodoc\n,\nthat could also be run to extract documentation for files on the local machine;\npkg.go.dev/pkg/\nis essentially a descendant.\nAnother descendant is the\npkgsite\ncommand that, like\ngodoc\n, can be run locally, although\nit is not yet integrated into\nthe results shown by\ngo\ndoc\n.\nIs there a Go programming style guide?\nThere is no explicit style guide, although there is certainly\na recognizable “Go style”.\nGo has established conventions to guide decisions around\nnaming, layout, and file organization.\nThe document\nEffective Go\ncontains some advice on these topics.\nMore directly, the program\ngofmt\nis a pretty-printer\nwhose purpose is to enforce layout rules; it replaces the usual\ncompendium of dos and don’ts that allows interpretation.\nAll the Go code in the repository, and the vast majority in the\nopen source world, has been run through\ngofmt\n.\nThe document titled\nGo Code Review Comments\nis a collection of very short essays about details of Go idiom that are often\nmissed by programmers.\nIt is a handy reference for people doing code reviews for Go projects.\nHow do I submit patches to the Go libraries?\nThe library sources are in the\nsrc\ndirectory of the repository.\nIf you want to make a significant change, please discuss on the mailing list before embarking.\nSee the document\nContributing to the Go project\nfor more information about how to proceed.\nWhy does “go get” use HTTPS when cloning a repository?\nCompanies often permit outgoing traffic only on the standard TCP ports 80 (HTTP)\nand 443 (HTTPS), blocking outgoing traffic on other ports, including TCP port 9418\n(git) and TCP port 22 (SSH).\nWhen using HTTPS instead of HTTP,\ngit\nenforces certificate validation by\ndefault, providing protection against man-in-the-middle, eavesdropping and tampering attacks.\nThe\ngo get\ncommand therefore uses HTTPS for safety.\nGit\ncan be configured to authenticate over HTTPS or to use SSH in place of HTTPS.\nTo authenticate over HTTPS, you can add a line\nto the\n$HOME/.netrc\nfile that git consults:\nmachine github.com login *USERNAME* password *APIKEY*\nFor GitHub accounts, the password can be a\npersonal access token\n.\nGit\ncan also be configured to use SSH in place of HTTPS for URLs matching a given prefix.\nFor example, to use SSH for all GitHub access,\nadd these lines to your\n~/.gitconfig\n:\n[url \"ssh://git@github.com/\"]\n    insteadOf = https://github.com/\nWhen working with private modules, but using a public module proxy for dependencies, you may need to set\nGOPRIVATE\n.\nSee\nprivate modules\nfor details and additional settings.\nHow should I manage package versions using “go get”?\nThe Go toolchain has a built-in system for managing versioned sets of related packages, known as\nmodules\n.\nModules were introduced in\nGo 1.11\nand have been ready for production use since\n1.14\n.\nTo create a project using modules, run\ngo mod init\n.\nThis command creates a\ngo.mod\nfile that tracks dependency versions.\ngo mod init example/project\nTo add, upgrade, or downgrade a dependency, run\ngo get\n:\ngo get golang.org/x/text@v0.3.5\nSee\nTutorial: Create a module\nfor more information on getting started.\nSee\nDeveloping modules\nfor guides on managing dependencies with modules.\nPackages within modules should maintain backward compatibility as they evolve, following the\nimport compatibility rule\n:\nIf an old package and a new package have the same import path,\nthe new package must be backwards compatible with the old package.\nThe\nGo 1 compatibility guidelines\nare a good reference here:\ndon’t remove exported names, encourage tagged composite literals, and so on.\nIf different functionality is required, add a new name instead of changing an old one.\nModules codify this with\nsemantic versioning\nand semantic import versioning.\nIf a break in compatibility is required, release a module at a new major version.\nModules at major version 2 and higher require a\nmajor version suffix\nas part of their path (like\n/v2\n).\nThis preserves the import compatibility rule: packages in different major versions of a module have distinct paths.\nPointers and Allocation\nWhen are function parameters passed by value?\nAs in all languages in the C family, everything in Go is passed by value.\nThat is, a function always gets a copy of the\nthing being passed, as if there were an assignment statement assigning the\nvalue to the parameter.  For instance, passing an\nint\nvalue\nto a function makes a copy of the\nint\n, and passing a pointer\nvalue makes a copy of the pointer, but not the data it points to.\n(See a\nlater section\nfor a discussion of how this affects method receivers.)\nMap and slice values behave like pointers: they are descriptors that\ncontain pointers to the underlying map or slice data.  Copying a map or\nslice value doesn’t copy the data it points to.  Copying an interface value\nmakes a copy of the thing stored in the interface value.  If the interface\nvalue holds a struct, copying the interface value makes a copy of the\nstruct.  If the interface value holds a pointer, copying the interface value\nmakes a copy of the pointer, but again not the data it points to.\nNote that this discussion is about the semantics of the operations.\nActual implementations may apply optimizations to avoid copying\nas long as the optimizations do not change the semantics.\nWhen should I use a pointer to an interface?\nAlmost never. Pointers to interface values arise only in rare, tricky situations involving\ndisguising an interface value’s type for delayed evaluation.\nIt is a common mistake to pass a pointer to an interface value\nto a function expecting an interface. The compiler will complain about this\nerror but the situation can still be confusing, because sometimes a\npointer\nis necessary to satisfy an interface\n.\nThe insight is that although a pointer to a concrete type can satisfy\nan interface, with one exception\na pointer to an interface can never satisfy an interface\n.\nConsider the variable declaration,\nvar w io.Writer\nThe printing function\nfmt.Fprintf\ntakes as its first argument\na value that satisfies\nio.Writer\n—something that implements\nthe canonical\nWrite\nmethod. Thus we can write\nfmt.Fprintf(w, \"hello, world\\n\")\nIf however we pass the address of\nw\n, the program will not compile.\nfmt.Fprintf(&w, \"hello, world\\n\") // Compile-time error.\nThe one exception is that any value, even a pointer to an interface, can be assigned to\na variable of empty interface type (\ninterface{}\n).\nEven so, it’s almost certainly a mistake if the value is a pointer to an interface;\nthe result can be confusing.\nShould I define methods on values or pointers?\nfunc (s *MyStruct) pointerMethod() { } // method on pointer\nfunc (s MyStruct)  valueMethod()   { } // method on value\nFor programmers unaccustomed to pointers, the distinction between these\ntwo examples can be confusing, but the situation is actually very simple.\nWhen defining a method on a type, the receiver (\ns\nin the above\nexamples) behaves exactly as if it were an argument to the method.\nWhether to define the receiver as a value or as a pointer is the same\nquestion, then, as whether a function argument should be a value or\na pointer.\nThere are several considerations.\nFirst, and most important, does the method need to modify the\nreceiver?\nIf it does, the receiver\nmust\nbe a pointer.\n(Slices and maps act as references, so their story is a little\nmore subtle, but for instance to change the length of a slice\nin a method the receiver must still be a pointer.)\nIn the examples above, if\npointerMethod\nmodifies\nthe fields of\ns\n,\nthe caller will see those changes, but\nvalueMethod\nis called with a copy of the caller’s argument (that’s the definition\nof passing a value), so changes it makes will be invisible to the caller.\nBy the way, in Java method receivers have always been pointers,\nalthough their pointer nature is somewhat disguised\n(and recent developments are bringing value receivers to Java).\nIt is the value receivers in Go that are unusual.\nSecond is the consideration of efficiency. If the receiver is large,\na big\nstruct\nfor instance, it may be cheaper to\nuse a pointer receiver.\nNext is consistency. If some of the methods of the type must have\npointer receivers, the rest should too, so the method set is\nconsistent regardless of how the type is used.\nSee the section on\nmethod sets\nfor details.\nFor types such as basic types, slices, and small\nstructs\n,\na value receiver is very cheap so unless the semantics of the method\nrequires a pointer, a value receiver is efficient and clear.\nWhat’s the difference between new and make?\nIn short:\nnew\nallocates memory, while\nmake\ninitializes\nthe slice, map, and channel types.\nSee the\nrelevant section\nof Effective Go\nfor more details.\nWhat is the size of an\nint\non a 64 bit machine?\nThe sizes of\nint\nand\nuint\nare implementation-specific\nbut the same as each other on a given platform.\nFor portability, code that relies on a particular\nsize of value should use an explicitly sized type, like\nint64\n.\nOn 32-bit machines the compilers use 32-bit integers by default,\nwhile on 64-bit machines integers have 64 bits.\n(Historically, this was not always true.)\nOn the other hand, floating-point scalars and complex\ntypes are always sized (there are no\nfloat\nor\ncomplex\nbasic types),\nbecause programmers should be aware of precision when using floating-point numbers.\nThe default type used for an (untyped) floating-point constant is\nfloat64\n.\nThus\nfoo\n:=\n3.0\ndeclares a variable\nfoo\nof type\nfloat64\n.\nFor a\nfloat32\nvariable initialized by an (untyped) constant, the variable type\nmust be specified explicitly in the variable declaration:\nvar foo float32 = 3.0\nAlternatively, the constant must be given a type with a conversion as in\nfoo := float32(3.0)\n.\nHow do I know whether a variable is allocated on the heap or the stack?\nFrom a correctness standpoint, you don’t need to know.\nEach variable in Go exists as long as there are references to it.\nThe storage location chosen by the implementation is irrelevant to the\nsemantics of the language.\nThe storage location does have an effect on writing efficient programs.\nWhen possible, the Go compilers will allocate variables that are\nlocal to a function in that function’s stack frame.  However, if the\ncompiler cannot prove that the variable is not referenced after the\nfunction returns, then the compiler must allocate the variable on the\ngarbage-collected heap to avoid dangling pointer errors.\nAlso, if a local variable is very large, it might make more sense\nto store it on the heap rather than the stack.\nIn the current compilers, if a variable has its address taken, that variable\nis a candidate for allocation on the heap. However, a basic\nescape\nanalysis\nrecognizes some cases when such variables will not\nlive past the return from the function and can reside on the stack.\nWhy does my Go process use so much virtual memory?\nThe Go memory allocator reserves a large region of virtual memory as an arena\nfor allocations. This virtual memory is local to the specific Go process; the\nreservation does not deprive other processes of memory.\nTo find the amount of actual memory allocated to a Go process, use the Unix\ntop\ncommand and consult the\nRES\n(Linux) or\nRSIZE\n(macOS) columns.\nConcurrency\nWhat operations are atomic? What about mutexes?\nA description of the atomicity of operations in Go can be found in\nthe\nGo Memory Model\ndocument.\nLow-level synchronization and atomic primitives are available in the\nsync\nand\nsync/atomic\npackages.\nThese packages are good for simple tasks such as incrementing\nreference counts or guaranteeing small-scale mutual exclusion.\nFor higher-level operations, such as coordination among\nconcurrent servers, higher-level techniques can lead\nto nicer programs, and Go supports this approach through\nits goroutines and channels.\nFor instance, you can structure your program so that only one\ngoroutine at a time is ever responsible for a particular piece of data.\nThat approach is summarized by the original\nGo proverb\n,\nDo not communicate by sharing memory. Instead, share memory by communicating.\nSee the\nShare Memory By Communicating\ncode walk\nand its\nassociated article\nfor a detailed discussion of this concept.\nLarge concurrent programs are likely to borrow from both these toolkits.\nWhy doesn’t my program run faster with more CPUs?\nWhether a program runs faster with more CPUs depends on the problem\nit is solving.\nThe Go language provides concurrency primitives, such as goroutines\nand channels, but concurrency only enables parallelism\nwhen the underlying problem is intrinsically parallel.\nProblems that are intrinsically sequential cannot be sped up by adding\nmore CPUs, while those that can be broken into pieces that can\nexecute in parallel can be sped up, sometimes dramatically.\nSometimes adding more CPUs can slow a program down.\nIn practical terms, programs that spend more time\nsynchronizing or communicating than doing useful computation\nmay experience performance degradation when using\nmultiple OS threads.\nThis is because passing data between threads involves switching\ncontexts, which has significant cost, and that cost can increase\nwith more CPUs.\nFor instance, the\nprime sieve example\nfrom the Go specification has no significant parallelism although it launches many\ngoroutines; increasing the number of threads (CPUs) is more likely to slow it down than\nto speed it up.\nFor more detail on this topic see the talk entitled\nConcurrency is not Parallelism\n.\nHow can I control the number of CPUs?\nThe number of CPUs available simultaneously to executing goroutines is\ncontrolled by the\nGOMAXPROCS\nshell environment variable,\nwhose default value is the number of CPU cores available.\nPrograms with the potential for parallel execution should therefore\nachieve it by default on a multiple-CPU machine.\nTo change the number of parallel CPUs to use,\nset the environment variable or use the similarly-named\nfunction\nof the runtime package to configure the\nrun-time support to utilize a different number of threads.\nSetting it to 1 eliminates the possibility of true parallelism,\nforcing independent goroutines to take turns executing.\nThe runtime can allocate more threads than the value\nof\nGOMAXPROCS\nto service multiple outstanding\nI/O requests.\nGOMAXPROCS\nonly affects how many goroutines\ncan actually execute at once; arbitrarily more may be blocked\nin system calls.\nGo’s goroutine scheduler does well at balancing goroutines\nand threads, and can even preempt execution of a goroutine\nto make sure others on the same thread are not starved.\nHowever, it is not perfect.\nIf you see performance issues,\nsetting\nGOMAXPROCS\non a per-application basis may help.\nWhy is there no goroutine ID?\nGoroutines do not have names; they are just anonymous workers.\nThey expose no unique identifier, name, or data structure to the programmer.\nSome people are surprised by this, expecting the\ngo\nstatement to return some item that can be used to access and control\nthe goroutine later.\nThe fundamental reason goroutines are anonymous is so that\nthe full Go language is available when programming concurrent code.\nBy contrast, the usage patterns that develop when threads and goroutines are\nnamed can restrict what a library using them can do.\nHere is an illustration of the difficulties.\nOnce one names a goroutine and constructs a model around\nit, it becomes special, and one is tempted to associate all computation\nwith that goroutine, ignoring the possibility\nof using multiple, possibly shared goroutines for the processing.\nIf the\nnet/http\npackage associated per-request\nstate with a goroutine,\nclients would be unable to use more goroutines\nwhen serving a request.\nMoreover, experience with libraries such as those for graphics systems\nthat require all processing to occur on the “main thread”\nhas shown how awkward and limiting the approach can be when\ndeployed in a concurrent language.\nThe very existence of a special thread or goroutine forces\nthe programmer to distort the program to avoid crashes\nand other problems caused by inadvertently operating\non the wrong thread.\nFor those cases where a particular goroutine is truly special,\nthe language provides features such as channels that can be\nused in flexible ways to interact with it.\nFunctions and Methods\nWhy do T and *T have different method sets?\nAs the\nGo specification\nsays,\nthe method set of a type\nT\nconsists of all methods\nwith receiver type\nT\n,\nwhile that of the corresponding pointer\ntype\n*T\nconsists of all methods with receiver\n*T\nor\nT\n.\nThat means the method set of\n*T\nincludes that of\nT\n,\nbut not the reverse.\nThis distinction arises because\nif an interface value contains a pointer\n*T\n,\na method call can obtain a value by dereferencing the pointer,\nbut if an interface value contains a value\nT\n,\nthere is no safe way for a method call to obtain a pointer.\n(Doing so would allow a method to modify the contents of\nthe value inside the interface, which is not permitted by\nthe language specification.)\nEven in cases where the compiler could take the address of a value\nto pass to the method, if the method modifies the value the changes\nwill be lost in the caller.\nAs an example, if the code below were valid:\nvar buf bytes.Buffer\nio.Copy(buf, os.Stdin)\nit would copy standard input into a\ncopy\nof\nbuf\n,\nnot into\nbuf\nitself.\nThis is almost never the desired behavior and is therefore disallowed by the language.\nWhat happens with closures running as goroutines?\nDue to the way loop variables work, before Go version 1.22 (see\nthe end of this section for an update),\nsome confusion could arise when using closures with concurrency.\nConsider the following program:\nfunc main() {\n    done := make(chan bool)\n\n    values := []string{\"a\", \"b\", \"c\"}\n    for _, v := range values {\n        go func() {\n            fmt.Println(v)\n            done <- true\n        }()\n    }\n\n    // wait for all goroutines to complete before exiting\n    for _ = range values {\n        <-done\n    }\n}\nOne might mistakenly expect to see\na, b, c\nas the output.\nWhat you’ll probably see instead is\nc, c, c\n.  This is because\neach iteration of the loop uses the same instance of the variable\nv\n, so\neach closure shares that single variable. When the closure runs, it prints the\nvalue of\nv\nat the time\nfmt.Println\nis executed,\nbut\nv\nmay have been modified since the goroutine was launched.\nTo help detect this and other problems before they happen, run\ngo vet\n.\nTo bind the current value of\nv\nto each closure as it is launched, one\nmust modify the inner loop to create a new variable each iteration.\nOne way is to pass the variable as an argument to the closure:\nfor _, v := range values {\n        go func(\nu\nstring) {\n            fmt.Println(\nu\n)\n            done <- true\n        }(\nv\n)\n    }\nIn this example, the value of\nv\nis passed as an argument to the\nanonymous function. That value is then accessible inside the function as\nthe variable\nu\n.\nEven easier is just to create a new variable, using a declaration style that may\nseem odd but works fine in Go:\nfor _, v := range values {\nv := v\n// create a new 'v'.\n        go func() {\n            fmt.Println(\nv\n)\n            done <- true\n        }()\n    }\nThis behavior of the language, not defining a new variable for\neach iteration, was considered a mistake in retrospect,\nand has been addressed in\nGo 1.22\n, which\ndoes indeed create a new variable for each iteration, eliminating this issue.\nControl Flow\nWhy does Go not have the\n?:\noperator?\nThere is no ternary testing operation in Go.\nYou may use the following to achieve the same\nresult:\nif expr {\n    n = trueVal\n} else {\n    n = falseVal\n}\nThe reason\n?:\nis absent from Go is that the language’s designers\nhad seen the operation used too often to create impenetrably complex expressions.\nThe\nif-else\nform, although longer,\nis unquestionably clearer.\nA language needs only one conditional control flow construct.\nType Parameters\nWhy does Go have type parameters?\nType parameters permit what is known as generic programming, in which\nfunctions and data structures are defined in terms of types that are\nspecified later, when those functions and data structures are used.\nFor example, they make it possible to write a function that returns\nthe minimum of two values of any ordered type, without having to write\na separate version for each possible type.\nFor a more in-depth explanation with examples see the blog post\nWhy Generics?\n.\nHow are generics implemented in Go?\nThe compiler can choose whether to compile each instantiation\nseparately or whether to compile similar instantiations as\na single implementation.\nThe single implementation approach is similar to a function with an\ninterface parameter.\nDifferent compilers will make different choices for different cases.\nThe standard Go compiler ordinarily emits a single instantiation\nfor every type argument with the same shape, where the shape is\ndetermined by properties of the type such as the size and the location\nof pointers that it contains.\nFuture releases may experiment with the tradeoff between compile\ntime, run-time efficiency, and code size.\nHow do generics in Go compare to generics in other languages?\nThe basic functionality in all languages is similar: it is possible to\nwrite types and functions using types that are specified later.\nThat said, there are some differences.\nJava\nIn Java, the compiler checks generic types at compile time but removes\nthe types at run time.\nThis is known as\ntype erasure\n.\nFor example, a Java type known as\nList<Integer>\nat\ncompile time will become the non-generic type\nList\nat run\ntime.\nThis means, for example, that when using the Java form of type\nreflection it is impossible to distinguish a value of\ntype\nList<Integer>\nfrom a value of\ntype\nList<Float>\n.\nIn Go the reflection information for a generic type includes the full\ncompile-time type information.\nJava uses type wildcards such as\nList<? extends Number>\nor\nList<? super Number>\nto implement generic\ncovariance and contravariance.\nGo does not have these concepts, which makes generic types in Go much\nsimpler.\nC++\nTraditionally C++ templates do not enforce any constraints on type\narguments, although C++20 supports optional constraints via\nconcepts\n.\nIn Go constraints are mandatory for all type parameters.\nC++20 concepts are expressed as small code fragments that must compile\nwith the type arguments.\nGo constraints are interface types that define the set of all\npermitted type arguments.\nC++ supports template metaprogramming; Go does not.\nIn practice, all C++ compilers compile each template at the point\nwhere it is instantiated; as noted above, Go can and does use\ndifferent approaches for different instantiations.\nRust\nThe Rust version of constraints is known as trait bounds.\nIn Rust the association between a trait bound and a type must be\ndefined explicitly, either in the crate that defines the trait bound\nor the crate that defines the type.\nIn Go type arguments implicitly satisfy constraints, just as Go types\nimplicitly implement interface types.\nThe Rust standard library defines standard traits for operations such as\ncomparison or addition; the Go standard library does not, as these can\nbe expressed in user code via interface types. The one exception\nis Go’s\ncomparable\npredefined interface, which\ncaptures a property not expressible in the type system.\nPython\nPython is not a statically typed language, so one can reasonably say\nthat all Python functions are always generic by default: they can\nalways be called with values of any type, and any type errors are\ndetected at run time.\nWhy does Go use square brackets for type parameter lists?\nJava and C++ use angle brackets for type parameter lists, as in\nJava\nList<Integer>\nand C++\nstd::vector<int>\n.\nHowever, that option was not available for Go, because it leads to\na syntactic problem: when parsing code within a function, such\nas\nv := F<T>\n, at the point of seeing\nthe\n<\nit’s ambiguous whether we are seeing an\ninstantiation or an expression using the\n<\noperator.\nThis is very difficult to resolve without type information.\nFor example, consider a statement like\na, b = w < x, y > (z)\nWithout type information, it is impossible to decide whether the right\nhand side of the assignment is a pair of expressions (\nw < x\nand\ny > z\n), or whether it is a generic function\ninstantiation and call that returns two result values\n(\n(w<x, y>)(z)\n).\nIt is a key design decision of Go that parsing be possible without\ntype information, which seems impossible when using angle brackets for\ngenerics.\nGo is not unique or original in using square brackets; there are other\nlanguages such as Scala that also use square brackets for generic\ncode.\nWhy does Go not support methods with type parameters?\nGo permits a generic type to have methods, but, other than the\nreceiver, the arguments to those methods cannot use parameterized\ntypes.\nWe do not anticipate that Go will ever add generic methods.\nThe problem is how to implement them.\nSpecifically, consider checking whether a value in an\ninterface implements another interface with additional methods.\nFor example, consider this type, an empty struct with a\ngeneric\nNop\nmethod that returns its argument, for any possible type:\ntype Empty struct{}\n\nfunc (Empty) Nop[T any](x T) T {\n    return x\n}\nNow suppose an\nEmpty\nvalue is stored in an\nany\nand passed\nto other code that checks what it can do:\nfunc TryNops(x any) {\n    if x, ok := x.(interface{ Nop(string) string }); ok {\n        fmt.Printf(\"string %s\\n\", x.Nop(\"hello\"))\n    }\n    if x, ok := x.(interface{ Nop(int) int }); ok {\n        fmt.Printf(\"int %d\\n\", x.Nop(42))\n    }\n    if x, ok := x.(interface{ Nop(io.Reader) io.Reader }); ok {\n        data, err := io.ReadAll(x.Nop(strings.NewReader(\"hello world\")))\n        fmt.Printf(\"reader %q %v\\n\", data, err)\n    }\n}\nHow does that code work if\nx\nis an\nEmpty\n?\nIt seems that\nx\nmust satisfy all three tests,\nalong with any other form with any other type.\nWhat code runs when those methods are called?\nFor non-generic methods, the compiler generates the code\nfor all method implementations and links them into the final program.\nBut for generic methods, there can be an infinite number of method\nimplementations, so a different strategy is needed.\nThere are four choices:\nAt link time, make a list of all the possible dynamic interface checks,\nand then look for types that would satisfy them but are missing\ncompiled methods, and then reinvoke the compiler to add those methods.\nThis would make builds significantly slower, by needing to stop after\nlinking and repeat some compilations. It would especially slow down\nincremental builds. Worse, it is possible that the newly compiled method\ncode would itself have new dynamic interface checks, and the process\nwould have to be repeated. Examples can be constructed where\nthe process never even finishes.\nImplement some kind of JIT, compiling the needed method code at runtime.\nGo benefits greatly from the simplicity and predictable performance\nof being purely ahead-of-time compiled.\nWe are reluctant to take on the complexity of a JIT just to implement\none language feature.\nArrange to emit a slow fallback for each generic method that uses\na table of functions for every possible language operation on the type parameter,\nand then use that fallback implementation for the dynamic tests.\nThis approach would make a generic method parameterized by an\nunexpected type much slower than the same method\nparameterized by a type observed at compile time.\nThis would make performance much less predictable.\nDefine that generic methods cannot be used to satisfy interfaces at all.\nInterfaces are an essential part of programming in Go.\nDisallowing generic methods from satisfying interfaces is unacceptable\nfrom a design point of view.\nNone of these choices are good ones, so we chose “none of the above.”\nInstead of methods with type parameters, use top-level functions with\ntype parameters, or add the type parameters to the receiver type.\nFor more details, including more examples, see the\nproposal\n.\nWhy can’t I use a more specific type for the receiver of a parameterized type?\nThe method declarations of a generic type are written with a receiver\nthat includes the type parameter names.\nPerhaps because of the similarity of the syntax for specifying types\nat a call site,\nsome have thought this provides a mechanism for producing\na method customized for certain type arguments by naming\na specific type in the receiver, such as\nstring\n:\ntype S[T any] struct { f T }\n\nfunc (s S[string]) Add(t string) string {\n    return s.f + t\n}\nThis fails because the word\nstring\nis taken by\nthe compiler to be the name of the type argument in the method.\nThe compiler error message will be something like “\noperator + not defined on s.f (variable of type string)\n”.\nThis can be confusing because the\n+\noperator\nworks fine on the predeclared type\nstring\n,\nbut the declaration has overwritten, for this method, the definition of\nstring\n,\nand the operator does not work on that unrelated version of\nstring\n.\nIt’s valid to overwrite a predeclared name like this, but is an odd thing to do and\noften a mistake.\nWhy can’t the compiler infer the type argument in my program?\nThere are many cases where a programmer can easily see what the type\nargument for a generic type or function must be, but the language does\nnot permit the compiler to infer it.\nType inference is intentionally limited to ensure that there is never\nany confusion as to which type is inferred.\nExperience with other languages suggests that unexpected type\ninference can lead to considerable confusion when reading and\ndebugging a program.\nIt is always possible to specify the explicit type argument to be used\nin the call.\nIn the future new forms of inference may be supported, as long as the\nrules remain simple and clear.\nPackages and Testing\nHow do I create a multifile package?\nPut all the source files for the package in a directory by themselves.\nSource files can refer to items from different files at will; there is\nno need for forward declarations or a header file.\nOther than being split into multiple files, the package will compile and test\njust like a single-file package.\nHow do I write a unit test?\nCreate a new file ending in\n_test.go\nin the same directory\nas your package sources. Inside that file,\nimport \"testing\"\nand write functions of the form\nfunc TestFoo(t *testing.T) {\n    ...\n}\nRun\ngo test\nin that directory.\nThat script finds the\nTest\nfunctions,\nbuilds a test binary, and runs it.\nSee the\nHow to Write Go Code\ndocument,\nthe\ntesting\npackage\nand the\ngo test\nsubcommand for more details.\nWhere is my favorite helper function for testing?\nGo’s standard\ntesting\npackage makes it easy to write unit tests, but it lacks\nfeatures provided in other language’s testing frameworks such as assertion functions.\nAn\nearlier section\nof this document explained why Go\ndoesn’t have assertions, and\nthe same arguments apply to the use of\nassert\nin tests.\nProper error handling means letting other tests run after one has failed, so\nthat the person debugging the failure gets a complete picture of what is\nwrong. It is more useful for a test to report that\nisPrime\ngives the wrong answer for 2, 3, 5, and 7 (or for\n2, 4, 8, and 16) than to report that\nisPrime\ngives the wrong\nanswer for 2 and therefore no more tests were run. The programmer who\ntriggers the test failure may not be familiar with the code that fails.\nTime invested writing a good error message now pays off later when the\ntest breaks.\nA related point is that testing frameworks tend to develop into mini-languages\nof their own, with conditionals and controls and printing mechanisms,\nbut Go already has all those capabilities; why recreate them?\nWe’d rather write tests in Go; it’s one fewer language to learn and the\napproach keeps the tests straightforward and easy to understand.\nIf the amount of extra code required to write\ngood errors seems repetitive and overwhelming, the test might work better if\ntable-driven, iterating over a list of inputs and outputs defined\nin a data structure (Go has excellent support for data structure literals).\nThe work to write a good test and good error messages will then be amortized over many\ntest cases. The standard Go library is full of illustrative examples, such as in\nthe formatting tests for the\nfmt\npackage\n.\nWhy isn’t\nX\nin the standard library?\nThe standard library’s purpose is to support the runtime library, connect to\nthe operating system, and provide key functionality that many Go\nprograms require, such as formatted I/O and networking.\nIt also contains elements important for web programming, including\ncryptography and support for standards like HTTP, JSON, and XML.\nThere is no clear criterion that defines what is included because for\na long time, this was the\nonly\nGo library.\nThere are criteria that define what gets added today, however.\nNew additions to the standard library are rare and the bar for\ninclusion is high.\nCode included in the standard library bears a large ongoing maintenance cost\n(often borne by those other than the original author),\nis subject to the\nGo 1 compatibility promise\n(blocking fixes to any flaws in the API),\nand is subject to the Go\nrelease schedule\n,\npreventing bug fixes from being available to users quickly.\nMost new code should live outside of the standard library and be accessible\nvia the\ngo\ntool\n’s\ngo get\ncommand.\nSuch code can have its own maintainers, release cycle,\nand compatibility guarantees.\nUsers can find packages and read their documentation at\npkg.go.dev\n.\nAlthough there are pieces in the standard library that don’t really belong,\nsuch as\nlog/syslog\n, we continue to maintain everything in the\nlibrary because of the Go 1 compatibility promise.\nBut we encourage most new code to live elsewhere.\nImplementation\nWhat compiler technology is used to build the compilers?\nThere are several production compilers for Go, and a number of others\nin development for various platforms.\nThe default compiler,\ngc\n, is included with the\nGo distribution as part of the support for the\ngo\ncommand.\nGc\nwas originally written in C\nbecause of the difficulties of bootstrapping—you’d need a Go compiler to\nset up a Go environment.\nBut things have advanced and since the Go 1.5 release the compiler has been\na Go program.\nThe compiler was converted from C to Go using automatic translation tools, as\ndescribed in this\ndesign document\nand\ntalk\n.\nThus the compiler is now “self-hosting”, which means we needed to face\nthe bootstrapping problem.\nThe solution is to have a working Go installation already in place,\njust as one normally has with a working C installation.\nThe story of how to bring up a new Go environment from source\nis described\nhere\nand\nhere\n.\nGc\nis written in Go with a recursive descent parser\nand uses a custom loader, also written in Go but\nbased on the Plan 9 loader, to generate ELF/Mach-O/PE binaries.\nThe\nGccgo\ncompiler is a front end written in C++\nwith a recursive descent parser coupled to the\nstandard GCC back end. An experimental\nLLVM back end\nis\nusing the same front end.\nAt the beginning of the project we considered using LLVM for\ngc\nbut decided it was too large and slow to meet\nour performance goals.\nMore important in retrospect, starting with LLVM would have made it\nharder to introduce some of the ABI and related changes, such as\nstack management, that Go requires but are not part of the standard\nC setup.\nGo turned out to be a fine language in which to implement a Go compiler,\nalthough that was not its original goal.\nNot being self-hosting from the beginning allowed Go’s design to\nconcentrate on its original use case, which was networked servers.\nHad we decided Go should compile itself early on, we might have\nended up with a language targeted more for compiler construction,\nwhich is a worthy goal but not the one we had initially.\nAlthough\ngc\nhas its own implementation, a native lexer and\nparser are available in the\ngo/parser\npackage\nand there is also a native\ntype checker\n.\nThe\ngc\ncompiler uses variants of these libraries.\nHow is the run-time support implemented?\nAgain due to bootstrapping issues, the run-time code was originally written mostly in C (with a\ntiny bit of assembler) but it has since been translated to Go\n(except for some assembler bits).\nGccgo\n’s run-time support uses\nglibc\n.\nThe\ngccgo\ncompiler implements goroutines using\na technique called segmented stacks,\nsupported by recent modifications to the gold linker.\nGollvm\nsimilarly is built on the corresponding\nLLVM infrastructure.\nWhy is my trivial program such a large binary?\nThe linker in the\ngc\ntoolchain\ncreates statically-linked binaries by default.\nAll Go binaries therefore include the Go\nruntime, along with the run-time type information necessary to support dynamic\ntype checks, reflection, and even panic-time stack traces.\nA simple C “hello, world” program compiled and linked statically using\ngcc on Linux is around 750 kB, including an implementation of\nprintf\n.\nAn equivalent Go program using\nfmt.Printf\nweighs a couple of megabytes, but that includes\nmore powerful run-time support and type and debugging information.\nA Go program compiled with\ngc\ncan be linked with\nthe\n-ldflags=-w\nflag to disable DWARF generation,\nremoving debugging information from the binary but with no\nother loss of functionality.\nThis can reduce the binary size substantially.\nCan I stop these complaints about my unused variable/import?\nThe presence of an unused variable may indicate a bug, while\nunused imports just slow down compilation,\nan effect that can become substantial as a program accumulates\ncode and programmers over time.\nFor these reasons, Go refuses to compile programs with unused\nvariables or imports,\ntrading short-term convenience for long-term build speed and\nprogram clarity.\nStill, when developing code, it’s common to create these situations\ntemporarily and it can be annoying to have to edit them out before the\nprogram will compile.\nSome have asked for a compiler option to turn those checks off\nor at least reduce them to warnings.\nSuch an option has not been added, though,\nbecause compiler options should not affect the semantics of the\nlanguage and because the Go compiler does not report warnings, only\nerrors that prevent compilation.\nThere are two reasons for having no warnings.  First, if it’s worth\ncomplaining about, it’s worth fixing in the code.  (Conversely, if it’s not\nworth fixing, it’s not worth mentioning.) Second, having the compiler\ngenerate warnings encourages the implementation to warn about weak\ncases that can make compilation noisy, masking real errors that\nshould\nbe fixed.\nIt’s easy to address the situation, though.  Use the blank identifier\nto let unused things persist while you’re developing.\nimport \"unused\"\n\n// This declaration marks the import as used by referencing an\n// item from the package.\nvar _ = unused.Item  // TODO: Delete before committing!\n\nfunc main() {\n    debugData := debug.Profile()\n    _ = debugData // Used only during debugging.\n    ....\n}\nNowadays, most Go programmers use a tool,\ngoimports\n,\nwhich automatically rewrites a Go source file to have the correct imports,\neliminating the unused imports issue in practice.\nThis program is easily connected to most editors and IDEs to run automatically when a Go source file is written.\nThis functionality is also built into\ngopls\n, as\ndiscussed above\n.\nWhy does my virus-scanning software think my Go distribution or compiled binary is infected?\nThis is a common occurrence, especially on Windows machines, and is almost always a false positive.\nCommercial virus scanning programs are often confused by the structure of Go binaries, which\nthey don’t see as often as those compiled from other languages.\nIf you’ve just installed the Go distribution and the system reports it is infected, that’s certainly a mistake.\nTo be really thorough, you can verify the download by comparing the checksum with those on the\ndownloads page\n.\nIn any case, if you believe the report is in error, please report a bug to the supplier of your virus scanner.\nMaybe in time virus scanners can learn to understand Go programs.\nPerformance\nWhy does Go perform badly on benchmark X?\nOne of Go’s design goals is to approach the performance of C for comparable\nprograms, yet on some benchmarks it does quite poorly, including several\nin\ngolang.org/x/exp/shootout\n.\nThe slowest depend on libraries for which versions of comparable performance\nare not available in Go.\nFor instance,\npidigits.go\ndepends on a multi-precision math package, and the C\nversions, unlike Go’s, use\nGMP\n(which is\nwritten in optimized assembler).\nBenchmarks that depend on regular expressions\n(\nregex-dna.go\n,\nfor instance) are essentially comparing Go’s native\nregexp package\nto\nmature, highly optimized regular expression libraries like PCRE.\nBenchmark games are won by extensive tuning and the Go versions of most\nof the benchmarks need attention.  If you measure truly comparable C\nand Go programs\n(\nreverse-complement.go\nis one example), you’ll see the two languages are much closer in raw performance\nthan this suite would indicate.\nStill, there is room for improvement. The compilers are good but could be\nbetter, many libraries need major performance work, and the garbage collector\nisn’t fast enough yet. (Even if it were, taking care not to generate unnecessary\ngarbage can have a huge effect.)\nIn any case, Go can often be very competitive.\nThere has been significant improvement in the performance of many programs\nas the language and tools have developed.\nSee the blog post about\nprofiling\nGo programs\nfor an informative example.\nIt’s quite old but still contains helpful information.\nChanges from C\nWhy is the syntax so different from C?\nOther than declaration syntax, the differences are not major and stem\nfrom two desires.  First, the syntax should feel light, without too\nmany mandatory keywords, repetition, or arcana.  Second, the language\nhas been designed to be easy to analyze\nand can be parsed without a symbol table.  This makes it much easier\nto build tools such as debuggers, dependency analyzers, automated\ndocumentation extractors, IDE plug-ins, and so on.  C and its\ndescendants are notoriously difficult in this regard.\nWhy are declarations backwards?\nThey’re only backwards if you’re used to C. In C, the notion is that a\nvariable is declared like an expression denoting its type, which is a\nnice idea, but the type and expression grammars don’t mix very well and\nthe results can be confusing; consider function pointers.  Go mostly\nseparates expression and type syntax and that simplifies things (using\nprefix\n*\nfor pointers is an exception that proves the rule).  In C,\nthe declaration\nint* a, b;\ndeclares\na\nto be a pointer but not\nb\n; in Go\nvar a, b *int\ndeclares both to be pointers.  This is clearer and more regular.\nAlso, the\n:=\nshort declaration form argues that a full variable\ndeclaration should present the same order as\n:=\nso\nvar a uint64 = 1\nhas the same effect as\na := uint64(1)\nParsing is also simplified by having a distinct grammar for types that\nis not just the expression grammar; keywords such as\nfunc\nand\nchan\nkeep things clear.\nSee the article about\nGo’s Declaration Syntax\nfor more details.\nWhy is there no pointer arithmetic?\nSafety.  Without pointer arithmetic it’s possible to create a\nlanguage that can never derive an illegal address that succeeds\nincorrectly.  Compiler and hardware technology have advanced to the\npoint where a loop using array indices can be as efficient as a loop\nusing pointer arithmetic.  Also, the lack of pointer arithmetic can\nsimplify the implementation of the garbage collector.\nWhy are\n++\nand\n--\nstatements and not expressions?  And why postfix, not prefix?\nWithout pointer arithmetic, the convenience value of pre- and postfix\nincrement operators drops.  By removing them from the expression\nhierarchy altogether, expression syntax is simplified and the messy\nissues around order of evaluation of\n++\nand\n--\n(consider\nf(i++)\nand\np[i] = q[++i]\n)\nare eliminated as well.  The simplification is\nsignificant.  As for postfix vs. prefix, either would work fine but\nthe postfix version is more traditional; insistence on prefix arose\nwith the STL, a library for a language whose name contains, ironically, a\npostfix increment.\nWhy are there braces but no semicolons? And why can’t I put the opening brace on the next line?\nGo uses brace brackets for statement grouping, a syntax familiar to\nprogrammers who have worked with any language in the C family.\nSemicolons, however, are for parsers, not for people, and we wanted to\neliminate them as much as possible.  To achieve this goal, Go borrows\na trick from BCPL: the semicolons that separate statements are in the\nformal grammar but are injected automatically, without lookahead, by\nthe lexer at the end of any line that could be the end of a statement.\nThis works very well in practice but has the effect that it forces a\nbrace style.  For instance, the opening brace of a function cannot\nappear on a line by itself.\nSome have argued that the lexer should do lookahead to permit the\nbrace to live on the next line.  We disagree.  Since Go code is meant\nto be formatted automatically by\ngofmt\n,\nsome\nstyle must be chosen.  That style may differ from what\nyou’ve used in C or Java, but Go is a different language and\ngofmt\n’s style is as good as any other.  More\nimportant—much more important—the advantages of a single,\nprogrammatically mandated format for all Go programs greatly outweigh\nany perceived disadvantages of the particular style.\nNote too that Go’s style means that an interactive implementation of\nGo can use the standard syntax one line at a time without special rules.\nWhy do garbage collection?  Won’t it be too expensive?\nOne of the biggest sources of bookkeeping in systems programs is\nmanaging the lifetimes of allocated objects.\nIn languages such as C in which it is done manually,\nit can consume a significant amount of programmer time and is\noften the cause of pernicious bugs.\nEven in languages like C++ or Rust that provide mechanisms\nto assist, those mechanisms can have a significant effect on the\ndesign of the software, often adding programming overhead\nof its own.\nWe felt it was critical to eliminate such\nprogrammer overheads, and advances in garbage collection\ntechnology in the last few years gave us confidence that it\ncould be implemented cheaply enough, and with low enough\nlatency, that it could be a viable approach for networked\nsystems.\nMuch of the difficulty of concurrent programming\nhas its roots in the object lifetime problem:\nas objects get passed among threads it becomes cumbersome\nto guarantee they become freed safely.\nAutomatic garbage collection makes concurrent code far easier to write.\nOf course, implementing garbage collection in a concurrent environment is\nitself a challenge, but meeting it once rather than in every\nprogram helps everyone.\nFinally, concurrency aside, garbage collection makes interfaces\nsimpler because they don’t need to specify how memory is managed across them.\nThis is not to say that the recent work in languages\nlike Rust that bring new ideas to the problem of managing\nresources is misguided; we encourage this work and are excited to see\nhow it evolves.\nBut Go takes a more traditional approach by addressing\nobject lifetimes through\ngarbage collection, and garbage collection alone.\nThe current implementation is a mark-and-sweep collector.\nIf the machine is a multiprocessor, the collector runs on a separate CPU\ncore in parallel with the main program.\nMajor work on the collector in recent years has reduced pause times\noften to the sub-millisecond range, even for large heaps,\nall but eliminating one of the major objections to garbage collection\nin networked servers.\nWork continues to refine the algorithm, reduce overhead and\nlatency further, and to explore new approaches.\nThe 2018\nISMM keynote\nby Rick Hudson of the Go team\ndescribes the progress so far and suggests some future approaches.\nOn the topic of performance, keep in mind that Go gives the programmer\nconsiderable control over memory layout and allocation, much more than\nis typical in garbage-collected languages. A careful programmer can reduce\nthe garbage collection overhead dramatically by using the language well;\nsee the article about\nprofiling Go programs\nfor a worked example,\nincluding a demonstration of Go’s profiling tools.\nRelease Notes\nLearn about what's new in each Go release.\nView release notes\nCode of Conduct\nGuidelines for participating in Go community spaces and reporting process for handing issues.\nView more\nBrand Guidelines\nGuidance about reusing the Go logo, gopher mascot, etc.\nView guidelines\nContribute Guide\nLearn how to file bugs, pull requests, or otherwise contribute to the Go ecosystem.\nView guide\nGet connected\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nDiagnostics\nDiagnostics\nIntroduction\nThe Go ecosystem provides a large suite of APIs and tools to\ndiagnose logic and performance problems in Go programs. This page\nsummarizes the available tools and helps Go users pick the right one\nfor their specific problem.\nDiagnostics solutions can be categorized into the following groups:\nProfiling\n: Profiling tools analyze the complexity and costs of a\nGo program such as its memory usage and frequently called\nfunctions to identify the expensive sections of a Go program.\nTracing\n: Tracing is a way to instrument code to analyze latency\nthroughout the lifecycle of a call or user request. Traces provide an\noverview of how much latency each component contributes to the overall\nlatency in a system. Traces can span multiple Go processes.\nDebugging\n: Debugging allows us to pause a Go program and examine\nits execution. Program state and flow can be verified with debugging.\nRuntime statistics and events\n: Collection and analysis of runtime stats and events\nprovides a high-level overview of the health of Go programs. Spikes/dips of metrics\nhelps us to identify changes in throughput, utilization, and performance.\nNote: Some diagnostics tools may interfere with each other. For example, precise\nmemory profiling skews CPU profiles and goroutine blocking profiling affects scheduler\ntrace. Use tools in isolation to get more precise info.\nProfiling\nProfiling is useful for identifying expensive or frequently called sections\nof code. The Go runtime provides\nprofiling data\nin the format expected by the\npprof visualization tool\n.\nThe profiling data can be collected during testing\nvia\ngo\ntest\nor endpoints made available from the\nnet/http/pprof\npackage. Users need to collect the profiling data and use pprof tools to filter\nand visualize the top code paths.\nPredefined profiles provided by the\nruntime/pprof\npackage:\ncpu\n: CPU profile determines where a program spends\nits time while actively consuming CPU cycles (as opposed to while sleeping or waiting for I/O).\nheap\n: Heap profile reports memory allocation samples;\nused to monitor current and historical memory usage, and to check for memory leaks.\nthreadcreate\n: Thread creation profile reports the sections\nof the program that lead the creation of new OS threads.\ngoroutine\n: Goroutine profile reports the stack traces of all current goroutines.\nblock\n: Block profile shows where goroutines block waiting on synchronization\nprimitives (including timer channels). Block profile is not enabled by default;\nuse\nruntime.SetBlockProfileRate\nto enable it.\nmutex\n: Mutex profile reports the lock contentions. When you think your\nCPU is not fully utilized due to a mutex contention, use this profile. Mutex profile\nis not enabled by default, see\nruntime.SetMutexProfileFraction\nto enable it.\nWhat other profilers can I use to profile Go programs?\nOn Linux,\nperf tools\ncan be used for profiling Go programs. Perf can profile\nand unwind cgo/SWIG code and kernel, so it can be useful to get insights into\nnative/kernel performance bottlenecks. On macOS,\nInstruments\nsuite can be used profile Go programs.\nCan I profile my production services?\nYes. It is safe to profile programs in production, but enabling\nsome profiles (e.g. the CPU profile) adds cost. You should expect to\nsee performance downgrade. The performance penalty can be estimated\nby measuring the overhead of the profiler before turning it on in\nproduction.\nYou may want to periodically profile your production services.\nEspecially in a system with many replicas of a single process, selecting\na random replica periodically is a safe option.\nSelect a production process, profile it for\nX seconds for every Y seconds and save the results for visualization and\nanalysis; then repeat periodically. Results may be manually and/or automatically\nreviewed to find problems.\nCollection of profiles can interfere with each other,\nso it is recommended to collect only a single profile at a time.\nWhat are the best ways to visualize the profiling data?\nThe Go tools provide text, graph, and\ncallgrind\nvisualization of the profile data using\ngo tool pprof\n.\nRead\nProfiling Go programs\nto see them in action.\nListing of the most expensive calls as text.\nVisualization of the most expensive calls as a graph.\nWeblist view displays the expensive parts of the source line by line in\nan HTML page. In the following example, 530ms is spent in the\nruntime.concatstrings\nand cost of each line is presented\nin the listing.\nVisualization of the most expensive calls as weblist.\nAnother way to visualize profile data is a\nflame graph\n.\nFlame graphs allow you to move in a specific ancestry path, so you can zoom\nin/out of specific sections of code.\nThe\nupstream pprof\nhas support for flame graphs.\nFlame graphs offers visualization to spot the most expensive code-paths.\nAm I restricted to the built-in profiles?\nAdditionally to what is provided by the runtime, Go users can create\ntheir custom profiles via\npprof.Profile\nand use the existing tools to examine them.\nCan I serve the profiler handlers (/debug/pprof/...) on a different path and port?\nYes. The\nnet/http/pprof\npackage registers its handlers to the default\nmux by default, but you can also register them yourself by using the handlers\nexported from the package.\nFor example, the following example will serve the pprof.Profile\nhandler on :7777 at /custom_debug_path/profile:\npackage main\n\nimport (\n\t\"log\"\n\t\"net/http\"\n\t\"net/http/pprof\"\n)\n\nfunc main() {\n\tmux := http.NewServeMux()\n\tmux.HandleFunc(\"/custom_debug_path/profile\", pprof.Profile)\n\tlog.Fatal(http.ListenAndServe(\":7777\", mux))\n}\nTracing\nTracing is a way to instrument code to analyze latency throughout the\nlifecycle of a chain of calls. Go provides\ngolang.org/x/net/trace\npackage as a minimal tracing backend per Go node and provides a minimal\ninstrumentation library with a simple dashboard. Go also provides\nan execution tracer to trace the runtime events within an interval.\nTracing enables us to:\nInstrument and analyze application latency in a Go process.\nMeasure the cost of specific calls in a long chain of calls.\nFigure out the utilization and performance improvements.\nBottlenecks are not always obvious without tracing data.\nIn monolithic systems, it's relatively easy to collect diagnostic data\nfrom the building blocks of a program. All modules live within one\nprocess and share common resources to report logs, errors, and other\ndiagnostic information. Once your system grows beyond a single process and\nstarts to become distributed, it becomes harder to follow a call starting\nfrom the front-end web server to all of its back-ends until a response is\nreturned back to the user. This is where distributed tracing plays a big\nrole to instrument and analyze your production systems.\nDistributed tracing is a way to instrument code to analyze latency throughout\nthe lifecycle of a user request. When a system is distributed and when\nconventional profiling and debugging tools don’t scale, you might want\nto use distributed tracing tools to analyze the performance of your user\nrequests and RPCs.\nDistributed tracing enables us to:\nInstrument and profile application latency in a large system.\nTrack all RPCs within the lifecycle of a user request and see integration issues\nthat are only visible in production.\nFigure out performance improvements that can be applied to our systems.\nMany bottlenecks are not obvious before the collection of tracing data.\nThe Go ecosystem provides various distributed tracing libraries per tracing system\nand backend-agnostic ones.\nIs there a way to automatically intercept each function call and create traces?\nGo doesn’t provide a way to automatically intercept every function call and create\ntrace spans. You need to manually instrument your code to create, end, and annotate spans.\nHow should I propagate trace headers in Go libraries?\nYou can propagate trace identifiers and tags in the\ncontext.Context\n.\nThere is no canonical trace key or common representation of trace headers\nin the industry yet. Each tracing provider is responsible for providing propagation\nutilities in their Go libraries.\nWhat other low-level events from the standard library or\nruntime can be included in a trace?\nThe standard library and runtime are trying to expose several additional APIs\nto notify on low level internal events. For example,\nhttptrace.ClientTrace\nprovides APIs to follow low-level events in the life cycle of an outgoing request.\nThere is an ongoing effort to retrieve low-level runtime events from\nthe runtime execution tracer and allow users to define and record their user events.\nDebugging\nDebugging is the process of identifying why a program misbehaves.\nDebuggers allow us to understand a program’s execution flow and current state.\nThere are several styles of debugging; this section will only focus on attaching\na debugger to a program and core dump debugging.\nGo users mostly use the following debuggers:\nDelve\n:\nDelve is a debugger for the Go programming language. It has\nsupport for Go’s runtime concepts and built-in types. Delve is\ntrying to be a fully featured reliable debugger for Go programs.\nGDB\n:\nGo provides GDB support via the standard Go compiler and Gccgo.\nThe stack management, threading, and runtime contain aspects that differ\nenough from the execution model GDB expects that they can confuse the\ndebugger, even when the program is compiled with gccgo. Even though\nGDB can be used to debug Go programs, it is not ideal and may\ncreate confusion.\nHow well do debuggers work with Go programs?\nThe\ngc\ncompiler performs optimizations such as\nfunction inlining and variable registerization. These optimizations\nsometimes make debugging with debuggers harder. There is an ongoing\neffort to improve the quality of the DWARF information generated for\noptimized binaries. Until those improvements are available, we recommend\ndisabling optimizations when building the code being debugged. The following\ncommand builds a package with no compiler optimizations:\n$ go build -gcflags=all=\"-N -l\"\nAs part of the improvement effort, Go 1.10 introduced a new compiler\nflag\n-dwarflocationlists\n. The flag causes the compiler to\nadd location lists that helps debuggers work with optimized binaries.\nThe following command builds a package with optimizations but with\nthe DWARF location lists:\n$ go build -gcflags=\"-dwarflocationlists=true\"\nWhat’s the recommended debugger user interface?\nEven though both delve and gdb provides CLIs, most editor integrations\nand IDEs provides debugging-specific user interfaces.\nIs it possible to do postmortem debugging with Go programs?\nA core dump file is a file that contains the memory dump of a running\nprocess and its process status. It is primarily used for post-mortem\ndebugging of a program and to understand its state\nwhile it is still running. These two cases make debugging of core\ndumps a good diagnostic aid to postmortem and analyze production\nservices. It is possible to obtain core files from Go programs and\nuse delve or gdb to debug, see the\ncore dump debugging\npage for a step-by-step guide.\nRuntime statistics and events\nThe runtime provides stats and reporting of internal events for\nusers to diagnose performance and utilization problems at the\nruntime level.\nUsers can monitor these stats to better understand the overall\nhealth and performance of Go programs.\nSome frequently monitored stats and states:\nruntime.ReadMemStats\nreports the metrics related to heap\nallocation and garbage collection. Memory stats are useful for\nmonitoring how much memory resources a process is consuming,\nwhether the process can utilize memory well, and to catch\nmemory leaks.\ndebug.ReadGCStats\nreads statistics about garbage collection.\nIt is useful to see how much of the resources are spent on GC pauses.\nIt also reports a timeline of garbage collector pauses and pause time percentiles.\ndebug.Stack\nreturns the current stack trace. Stack trace\nis useful to see how many goroutines are currently running,\nwhat they are doing, and whether they are blocked or not.\ndebug.WriteHeapDump\nsuspends the execution of all goroutines\nand allows you to dump the heap to a file. A heap dump is a\nsnapshot of a Go process' memory at a given time. It contains all\nallocated objects as well as goroutines, finalizers, and more.\nruntime.NumGoroutine\nreturns the number of current goroutines.\nThe value can be monitored to see whether enough goroutines are\nutilized, or to detect goroutine leaks.\nExecution tracer\nGo comes with a runtime execution tracer to capture a wide range\nof runtime events. Scheduling, syscall, garbage collections,\nheap size, and other events are collected by runtime and available\nfor visualization by the go tool trace. Execution tracer is a tool\nto detect latency and utilization problems. You can examine how well\nthe CPU is utilized, and when networking or syscalls are a cause of\npreemption for the goroutines.\nTracer is useful to:\nUnderstand how your goroutines execute.\nUnderstand some of the core runtime events such as GC runs.\nIdentify poorly parallelized execution.\nHowever, it is not great for identifying hot spots such as\nanalyzing the cause of excessive memory or CPU usage.\nUse profiling tools instead first to address them.\nAbove, the go tool trace visualization shows the execution started\nfine, and then it became serialized. It suggests that there might\nbe lock contention for a shared resource that creates a bottleneck.\nSee\ngo\ntool\ntrace\nto collect and analyze runtime traces.\nGODEBUG\nRuntime also emits events and information if\nGODEBUG\nenvironmental variable is set accordingly.\nGODEBUG=gctrace=1 prints garbage collector events at\neach collection, summarizing the amount of memory collected\nand the length of the pause.\nGODEBUG=inittrace=1 prints a summary of execution time and memory allocation\ninformation for completed package initialization work.\nGODEBUG=schedtrace=X prints scheduling events every X milliseconds.\nThe GODEBUG environmental variable can be used to disable use of\ninstruction set extensions in the standard library and runtime.\nGODEBUG=cpu.all=off disables the use of all optional\ninstruction set extensions.\nGODEBUG=cpu.\nextension\n=off disables use of instructions from the\nspecified instruction set extension.\nextension\nis the lower case name for the instruction set extension\nsuch as\nsse41\nor\navx\n.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDocumentation\nA Guide to the Go Garbage Collector\nA Guide to the Go Garbage Collector\nIntroduction\nThis guide is intended to aid advanced Go users in better understanding their\napplication costs by providing insights into the Go garbage collector.\nIt also provides guidance on how Go users may use these insights to improve\ntheir applications' resource utilization.\nIt does not assume any knowledge of garbage collection, but does assume\nfamiliarity with the Go programming language.\nThe Go language takes responsibility for arranging the storage of Go values;\nin most cases, a Go developer need not care about where these values are stored,\nor why, if at all.\nIn practice, however, these values often need to be stored in computer\nphysical memory\nand physical memory is a finite resource.\nBecause it is finite, memory must be managed carefully and recycled in order to\navoid running out of it while executing a Go program.\nIt's the job of a Go implementation to allocate and recycle memory as needed.\nAnother term for automatically recycling memory is\ngarbage collection\n.\nAt a high level, a garbage\ncollector\n(or GC, for short) is a system that\nrecycles memory on behalf of the application by identifying which parts of memory\nare no longer needed.\nThe Go standard toolchain provides a runtime library that ships with every\napplication, and this runtime library includes a garbage collector.\nNote that the existence of a garbage collector as described by this guide\nis not guaranteed by the\nGo specification\n, only that\nthe underlying storage for Go values is managed by the language itself.\nThis omission is intentional and enables the use of radically different\nmemory management techniques.\nTherefore, this guide is about a specific implementation of the Go programming\nlanguage and\nmay not apply to other implementations\n.\nSpecifically, this following guide applies to the standard toolchain (the\ngc\nGo compiler and tools).\nGccgo and Gollvm both use a very similar GC implementation so many of the\nsame concepts apply, but details may vary.\nFurthermore, this is a living document and will change over time to best\nreflect the latest release of Go.\nThis document currently describes the garbage collector as of Go 1.19.\nWhere Go Values Live\nBefore we dive into the GC, let's first discuss the memory that doesn't need to\nbe managed by the GC.\nFor instance, non-pointer Go values stored in local variables will likely not be\nmanaged by the Go GC at all, and Go will instead arrange for memory to be\nallocated that's tied to the\nlexical scope\nin\nwhich it's created.\nIn general, this is more efficient than relying on the GC, because the Go\ncompiler is able to predetermine when that memory may be freed and emit\nmachine instructions that clean up.\nTypically, we refer to allocating memory for Go values this way as\n\"stack allocation,\" because the space is stored on the goroutine stack.\nGo values whose memory cannot be allocated this way, because the Go compiler\ncannot determine its lifetime, are said to\nescape to the heap\n.\n\"The heap\" can be thought of as a catch-all for memory allocation, for when Go\nvalues need to be placed\nsomewhere\n.\nThe act of allocating memory on the heap is typically referred to as \"dynamic\nmemory allocation\" because both the compiler and the runtime can make very few\nassumptions as to how this memory is used and when it can be cleaned up.\nThat's where a GC comes in: it's a system that specifically identifies and\ncleans up dynamic memory allocations.\nThere are many reasons why a Go value might need to escape to the heap.\nOne reason could be that its size is dynamically determined.\nConsider for instance the backing array of a slice whose initial size is\ndetermined by a variable, rather than a constant.\nNote that escaping to the heap must also be transitive: if a reference to a\nGo value is written into another Go value that has already been determined to\nescape, that value must also escape.\nWhether a Go value escapes or not is a function of the context in which it is\nused and the Go compiler's escape analysis algorithm.\nIt would be fragile and difficult to try to enumerate precisely when values\nescape: the algorithm itself is fairly sophisticated and changes between Go\nreleases.\nFor more details on how to identify which values escape and which do not, see\nthe section on\neliminating heap allocations\n.\nTracing Garbage Collection\nGarbage collection may refer to many different methods of automatically\nrecycling memory; for example, reference counting.\nIn the context of this document, garbage collection refers to\ntracing\ngarbage collection, which identifies in-use, so-called\nlive\n, objects by\nfollowing pointers transitively.\nLet's define these terms more rigorously.\nObject\n—An object is a dynamically allocated piece of memory\n\t\tthat contains one or more Go values.\nPointer\n—A memory address that references any value within an\n\t\tobject.\n\t\tThis naturally includes Go values of the form\n*T\n, but also includes\n\t\tparts of built-in Go values.\n\t\tStrings, slices, channels, maps, and interface values all contain memory\n\t\taddresses that the GC must trace.\nTogether, objects and pointers to other objects form the\nobject graph\n.\nTo identify live memory, the GC walks the object graph starting at the\nprogram's\nroots\n, pointers that identify objects that are definitely\nin-use by the program.\nTwo examples of roots are local variables and global variables.\nThe process of walking the object graph is referred to as\nscanning\n.\nAnother phrase you might see in the Go documentation is whether an object is\nreachable\n, which just means that the object can be discovered by the\nscanning process.\nNote also that,\nwith one\nexception\n, once memory becomes unreachable, it stays unreachable.\nThis basic algorithm is common to all tracing GCs.\nWhere tracing GCs differ is what they do once they discover memory is live.\nGo's GC uses the mark-sweep technique, which means that in order to keep track\nof its progress, the GC also\nmarks\nthe values it encounters as live.\nOnce tracing is complete, the GC then walks over all memory in the heap and\nmakes all memory that is\nnot\nmarked available for allocation.\nThis process is called\nsweeping\n.\nOne alternative technique you may be familiar with is to actually\nmove\nthe objects to a new part of memory and leave behind a forwarding pointer that\nis later used to update all the application's pointers.\nWe call a GC that moves objects in this way a\nmoving\nGC; Go has a\nnon-moving\nGC.\nThe GC cycle\nBecause the Go GC is a mark-sweep GC, it broadly operates in two phases: the\nmark phase, and the sweep phase.\nWhile this statement might seem tautological, it contains an important insight:\nit's not possible to release memory back to be allocated until\nall\nmemory\nhas been traced, because there may still be an un-scanned pointer keeping\nan object alive.\nAs a result, the act of sweeping must be entirely separated from the act of\nmarking.\nFurthermore, the GC may also not be active at all, when there's no GC-related\nwork to do.\nThe GC continuously rotates through these three phases of sweeping, off, and\nmarking in what's known as the\nGC cycle\n.\nFor the purposes of this document, consider the GC cycle starting with sweeping,\nturning off, then marking.\nThe next few sections will focus on building intuition for the costs of the\nGC to aid users in tweaking GC parameters for their own benefit.\nUnderstanding costs\nThe GC is inherently a complex piece of software built on even more complex\nsystems.\nIt's easy to become mired in detail when trying to understand the GC and\ntweak its behavior.\nThis section is intended to provide a framework for reasoning about the cost\nof the Go GC and its tuning parameters.\nTo begin with, consider this model of GC cost based on three simple axioms.\nThe GC involves only two resources: physical memory, and CPU time.\nThe GC's memory costs consist of live heap memory, new heap memory\n\t\tallocated before the mark phase, and space for metadata that, even\n\t\tif proportional to the previous costs, are small in comparison.\nGC memory cost for cycle N = live heap from cycle N-1 + new heap\nLive heap memory is memory that was determined to be live by the\n\t\tprevious GC cycle, while new heap memory is any memory allocated in the\n\t\tcurrent cycle, which may or may not be live by the end.\n\t\tHow much memory is live at any given point in time is a property of the\n\t\tprogram, and not something the GC can directly control.\nThe GC's CPU costs are modeled as a fixed cost per cycle, and a\n\t\tmarginal cost that scales proportionally with the size of the live\n\t\theap.\nGC CPU time for cycle N = Fixed CPU time cost per cycle + average CPU time cost per byte * live heap memory found in cycle N\nThe fixed CPU time cost per cycle includes things that happen a constant number\n\t\tof times each cycle, like initializing data structures for the next GC cycle.\n\t\tThis cost is typically small, and is included just for completeness.\nMost of the CPU cost of the GC is marking and scanning, which is captured by\n\t\tthe marginal cost.\n\t\tThe average cost of marking and scanning depends on the GC implementation, but\n\t\talso on the behavior of the program.\n\t\tFor example, more pointers means more GC work, because at minimum the GC needs\n\t\tto visit all the pointers in the program.\n\t\tStructures like linked lists and trees are also more difficult for the GC to\n\t\twalk in parallel, increasing the average cost per byte.\nThis model ignores sweeping costs, which are proportional to total heap memory,\n\t\tincluding memory that is dead (it must be made available for allocation).\n\t\tFor Go's current GC implementation, sweeping is so much faster than marking and\n\t\tscanning that the cost is negligible in comparison.\nThis model is simple but effective: it accurately categorizes the dominant\ncosts of the GC.\nIt also tells us that the\ntotal CPU cost\nof the garbage collector depends on\nthe total number of GC cycles in a given time frame.\nFinally, embedded in this model is a fundamental time/space trade-off for the GC.\nTo see why, let's explore a constrained but useful scenario: the\nsteady state\n.\nThe steady state of an application, from the GC's perspective, is defined by the\nfollowing properties:\nThe rate at which the application allocates new memory (in bytes per\n\t\tsecond) is constant.\nThis means that, from the GC's perspective, the application's workload\n\t\tlooks approximately the same over time.\n\t\tFor example, for a web service, this would be a constant request rate\n\t\twith, on average, the same kinds of requests being made, with the average\n\t\tlifetime of each request staying roughly constant.\nThe marginal costs of the GC are constant.\nThis means that statistics of the object graph, such as the distribution\n\t\tof object sizes, the number of pointers, and the average depth of data\n\t\tstructures, remain the same from cycle to cycle.\nLet's work through an example.\nAssume some application allocates 10 MiB/s, the GC can scan at a rate\nof 100 MiB/cpu-second (this is made up), and fixed GC costs are zero.\nThe steady state makes no assumptions about the size of the live heap,\nbut for simplicity, let's say this application's live heap is always 10 MiB.\n(Note: a constant live heap does not mean that all newly allocated memory is\ndead.\nIt means that, after the GC runs,\nsome mix\nof old and new heap memory\nis live.)\nIf each GC cycle happens exactly every 1 cpu-second, then our example\napplication, in the steady state, will have a 20 MiB total heap size on each GC\ncycle.\nAnd with every GC cycle, the GC will need 0.1 cpu-seconds to do its work,\nresulting in a 10% overhead.\nNow let's say each GC cycle happens less often, once every 2 cpu-seconds.\nThen, our example application, in the steady state, will have a 30 MiB total\nheap size on each GC cycle.\nBut with every GC cycle, the GC will\nstill only need 0.1 cpu-seconds\nto do its work.\nSo this means that our GC overhead just went down, from 10% to 5%, at the\ncost of 50% more memory being used.\nThis change in overheads is the fundamental time/space trade-off mentioned\nearlier.\nAnd\nGC frequency\nis at the center of this trade-off:\nif we execute the GC more frequently, then we use less memory, and vice versa.\nBut how often does the GC actually execute?\nIn Go, deciding when the GC should start is the main parameter which the user\nhas control over.\nGOGC\nAt a high level, GOGC determines the trade-off between GC CPU and memory.\nIt works by determining the target heap size after each GC cycle, a target value\nfor the total heap size in the next cycle.\nThe GC's goal is to finish a collection cycle before the total heap size\nexceeds the target heap size.\nTotal heap size is defined as the live heap size at the end of the previous\ncycle, plus any new heap memory allocated by the application since the previous\ncycle.\nMeanwhile, target heap memory is defined as:\nTarget heap memory = Live heap + (Live heap + GC roots) * GOGC / 100\nAs an example, consider a Go program with a live heap size of 8 MiB, 1 MiB\nof goroutine stacks, and 1 MiB of pointers in global variables.\nThen, with a GOGC value of 100, the amount of new memory that will be allocated\nbefore the next GC runs will be 10 MiB, or 100% of the 10 MiB of work, for a\ntotal heap footprint of 18 MiB.\nWith a GOGC value of 50, then it'll be 50%, or 5 MiB.\nWith a GOGC value of 200, it'll be 200%, or 20 MiB.\nNote: GOGC includes the root set only as of Go 1.18.\nPreviously, it would only count the live heap.\nOften, the amount of memory in goroutine stacks is quite small and the live\nheap size dominates all other sources of GC work, but in cases where programs\nhad hundreds of thousands of goroutines, the GC was making poor judgements.\nThe heap target controls GC frequency: the bigger the target, the longer the GC\ncan wait to start another mark phase and vice versa.\nWhile the precise formula is useful for making estimates, it's best to think of\nGOGC in terms of its fundamental purpose: a parameter that picks a point in the\nGC CPU and memory trade-off.\nThe key takeaway is that\ndoubling GOGC will double heap memory overheads and\nroughly halve GC CPU cost\n, and vice versa.\n(To see a full explanation as to why, see the\nappendix\n.)\nNote: the target heap size is just a target, and there are several reasons why\nthe GC cycle might not finish right at that target.\nFor one, a large enough heap allocation can simply exceed the target.\nHowever, other reasons appear in GC implementations that go beyond the\nGC model\nthis guide has been using thus far.\nFor some more detail, see the\nlatency section\n, but the\ncomplete details may be found in the\nadditional\nresources\n.\nGOGC may be configured through either the\nGOGC\nenvironment\nvariable (which all Go programs recognize), or through the\nSetGCPercent\nAPI in the\nruntime/debug\npackage.\nNote that GOGC may also be used to turn off the GC entirely (provided the\nmemory limit\ndoes not apply) by setting\nGOGC=off\nor calling\nSetGCPercent(-1)\n.\nConceptually, this setting is equivalent to setting GOGC to a value of\ninfinity, as the amount of new memory before a GC is triggered is unbounded.\nTo better understand everything we've discussed so far, try out the interactive\nvisualization below that is built on the\nGC cost model\ndiscussed earlier.\nThis visualization depicts the execution of some program whose non-GC work takes\n10 seconds of CPU time to complete.\nIn the first second it performs some initialization step (growing its live heap)\nbefore settling into a steady state.\nThe application allocates 200 MiB in total, with 20 MiB live at a time.\nIt assumes that the only relevant GC work to complete comes from the live heap,\nand that (unrealistically) the application uses no additional memory.\nUse the slider to adjust the value of GOGC to see how the application responds\nin terms of total duration and GC overhead.\nEach GC cycle ends while the new heap drops to zero.\nThe time taken while the new heap drops to zero is the combined time for the\nmark phase for cycle N, and the sweep phase for the cycle N+1.\nNote that this visualization (and all the visualizations in this guide) assume\nthe application is paused while the GC executes, so GC CPU costs are fully\nrepresented by the time it takes for new heap memory to drop to zero.\nThis is only to make visualization simpler; the same intuition still applies.\nThe X axis shifts to always show the full CPU-time duration of the program.\nNotice that additional CPU time used by the GC increases the overall duration.\nGOGC\nNotice that the GC always incurs some CPU and peak memory overhead.\nAs GOGC increases, CPU overhead decreases, but peak memory increases\nproportionally to the live heap size.\nAs GOGC decreases, the peak memory requirement decreases at the expense of\nadditional CPU overhead.\nNote: the graph displays CPU time, not wall-clock time to complete the program.\nIf the program runs on 1 CPU and fully utilizes its resources, then these are\nequivalent.\nA real-world program likely runs on a multi-core system and does not 100%\nutilize the CPUs at all times.\nIn these cases the wall-time impact of the GC will be lower.\nNote: the Go GC has a minimum total heap size of 4 MiB, so if the GOGC-set\ntarget is ever below that, it gets rounded up.\nThe visualization reflects this detail.\nHere's another example that's a little bit more dynamic and realistic.\nOnce again, the application takes 10 CPU-seconds to complete without the GC, but\nthe steady state allocation rate increases dramatically half-way through, and\nthe live heap size shifts around a bit in the first phase.\nThis example demonstrates how the steady state might look when the live heap\nsize is actually changing, and how a higher allocation rate leads to more\nfrequent GC cycles.\nGOGC\nMemory limit\nUntil Go 1.19, GOGC was the sole parameter that could be used to modify the GC's\nbehavior.\nWhile it works great as a way to set a trade-off, it doesn't take into account\nthat available memory is finite.\nConsider what happens when there's a transient spike in the live heap size:\nbecause the GC will pick a total heap size proportional to that live heap size,\nGOGC must be configured such for the\npeak\nlive heap size, even if in the\nusual case a higher GOGC value provides a better trade-off.\nThe visualization below demonstrates this transient heap spike situation.\nGOGC\nIf the example workload is running in a container with a bit over 60 MiB of\nmemory available, then GOGC can't be increased beyond 100, even though the rest\nof the GC cycles have the available memory to make use of that extra memory.\nFurthermore, in some applications, these transient peaks can be rare and hard to\npredict, leading to occasional, unavoidable, and potentially costly\nout-of-memory conditions.\nThat's why in the 1.19 release, Go added support for setting a runtime memory\nlimit.\nThe memory limit may be configured either via the\nGOMEMLIMIT\nenvironment variable which all Go programs recognize, or through the\nSetMemoryLimit\nfunction available in the\nruntime/debug\npackage.\nThis memory limit sets a maximum on the\ntotal amount of memory that the Go\nruntime can use\n.\nThe specific set of memory included is defined in terms of\nruntime.MemStats\nas the expression\nSys\n-\nHeapReleased\nor equivalently in terms of the\nruntime/metrics\npackage,\n/memory/classes/total:bytes\n-\n/memory/classes/heap/released:bytes\nBecause the Go GC has explicit control over how much heap memory it uses, it\nsets the total heap size based on this memory limit and how much other memory\nthe Go runtime uses.\nThe visualization below depicts the same single-phase steady state workload from\nthe GOGC section, but this time with an extra 10 MiB of overhead from the Go\nruntime and with an adjustable memory limit.\nTry shifting around both GOGC and the memory limit and see what happens.\nGOGC\nMemory Limit\nNotice that when the memory limit is lowered below the peak memory that's\ndetermined by GOGC (42 MiB for a GOGC of 100), the GC runs more frequently to\nkeep the peak memory within the limit.\nReturning to our previous example of the transient heap spike, by setting a\nmemory limit and turning up GOGC, we can get the best of both worlds: no memory\nlimit breach, and better resource economy.\nTry out the interactive visualization below.\nGOGC\nMemory Limit\nNotice that with some values of GOGC and the memory limit, peak memory use\nstops at whatever the memory limit is, but that the rest of the program's\nexecution still obeys the total heap size rule set by GOGC.\nThis observation leads to another interesting detail: even when GOGC is set to\noff, the memory limit is still respected!\nIn fact, this particular configuration represents a\nmaximization of resource\neconomy\nbecause it sets the minimum GC frequency required to maintain some\nmemory limit.\nIn this case,\nall\nof the program's execution has the heap size rise to\nmeet the memory limit.\nNow, while the memory limit is clearly a powerful tool,\nthe use of\na memory limit does not come without a cost\n, and certainly doesn't\ninvalidate the utility of GOGC.\nConsider what happens when the live heap grows large enough to bring total\nmemory use close to the memory limit.\nIn the steady state visualization above, try turning GOGC off and then slowly\nlowering the memory limit further and further to see what happens.\nNotice that the total time the application takes will start to grow in an\nunbounded manner as the GC is constantly executing to maintain an impossible\nmemory limit.\nThis situation, where the program fails to make reasonable progress due to\nconstant GC cycles, is called\nthrashing\n.\nIt's particularly dangerous because it effectively stalls the program.\nEven worse, it can happen for exactly the same situation we were trying to\navoid with GOGC: a large enough transient heap spike can cause a program to\nstall indefinitely!\nTry reducing the memory limit (around 30 MiB or lower) in the transient heap\nspike visualization and notice how the worst behavior specifically starts with\nthe heap spike.\nIn many cases, an indefinite stall is worse than an out-of-memory condition,\nwhich tends to result in a much faster failure.\nFor this reason, the memory limit is defined to be\nsoft\n.\nThe Go runtime makes no guarantees that it will maintain this memory limit\nunder all circumstances; it only promises some reasonable amount of effort.\nThis relaxation of the memory limit is critical to avoiding thrashing behavior,\nbecause it gives the GC a way out: let memory use surpass the limit to avoid\nspending too much time in the GC.\nHow this works internally is the GC sets an upper limit on the amount\nof CPU time it can use over some time window (with some hysteresis for very\nshort transient spikes in CPU use).\nThis limit is currently set at roughly 50%, with a\n2 * GOMAXPROCS\nCPU-second window.\nThe consequence of limiting GC CPU time is that the GC's work is delayed,\nmeanwhile the Go program may continue allocating new heap memory, even beyond\nthe memory limit.\nThe intuition behind the 50% GC CPU limit is based on the worst-case impact\non a program with ample available memory.\nIn the case of a misconfiguration of the memory limit, where it is set too\nlow mistakenly, the program will slow down at most by 2x, because the GC\ncan't take more than 50% of its CPU time away.\nNote: the visualizations on this page do not simulate the GC CPU limit.\nSuggested uses\nWhile the memory limit is a powerful tool, and the Go runtime takes steps to\nmitigate the worst behaviors from misuse, it's still important to use it\nthoughtfully.\nBelow is a collection of tidbits of advice about where the memory limit is most\nuseful and applicable, and where it might cause more harm than good.\nDo\ntake advantage of the memory limit when the execution\n\t\tenvironment of your Go program is entirely within your control, and\n\t\tthe Go program is the only program with access to some set of resources\n\t\t(i.e. some kind of memory reservation, like a container memory limit).\nA good example is the deployment of a web service into containers with\n\t\ta fixed amount of available memory.\nIn this case, a good rule of thumb is to leave an additional 5-10%\n\t\tof headroom to account for memory sources the Go runtime is unaware of.\nDo\nfeel free to adjust the memory limit in real time to adapt to\n\t\tchanging conditions.\nA good example is a cgo program where C libraries temporarily need to\n\t\tuse substantially more memory.\nDon't\nset GOGC to off with a memory limit if the Go program\n\t\tmight share some of its limited memory with other programs, and those\n\t\tprograms are generally decoupled from the Go program.\n\t\tInstead, keep the memory limit since it may help to curb undesirable\n\t\ttransient behavior, but set GOGC to some smaller, reasonable value for\n\t\tthe average case.\nWhile it may be tempting to try and \"reserve\" memory for co-tenant\n\t\tprograms, unless the programs are fully synchronized (e.g. the Go\n\t\tprogram calls some subprocess and blocks while its callee executes),\n\t\tthe result will be less reliable as inevitably both programs will\n\t\tneed more memory.\n\t\tLetting the Go program use less memory when it doesn't need it will\n\t\tgenerate a more reliable result overall.\n\t\tThis advice also applies to overcommit situations, where the sum of\n\t\tmemory limits of containers running on one machine may exceed the\n\t\tactual physical memory available to the machine.\nDon't\nuse the memory limit when deploying to an execution\n\t\tenvironment you don't control, especially when your program's memory\n\t\tuse is proportional to its inputs.\nA good example is a CLI tool or a desktop application.\n\t\tBaking a memory limit into the program when it's unclear what kind of\n\t\tinputs it might be fed, or how much memory might be available on the\n\t\tsystem can lead to confusing crashes and poor performance.\n\t\tPlus, an advanced end-user can always set a memory limit if they wish.\nDon't\nset a memory limit to avoid out-of-memory conditions when\n\t\ta program is already close to its environment's memory limits.\nThis effectively replaces an out-of-memory risk with a risk of\n\t\tsevere application slowdown, which is often not a favorable trade,\n\t\teven with the efforts Go makes to mitigate thrashing.\n\t\tIn such a case, it would be much more effective to either increase the\n\t\tenvironment's memory limits (and\nthen\npotentially set a memory\n\t\tlimit) or decrease GOGC (which provides a much cleaner trade-off than\n\t\tthrashing-mitigation does).\nLatency\nThe visualizations in this document have modeled the application as paused while\nthe GC is executing.\nGC implementations do exist that behave this way, and they're referred to as\n\"stop-the-world\" GCs.\nThe Go GC, however, is not fully stop-the-world and does most of its work\nconcurrently with the application.\nThis is primarily to reduce application\nlatencies\n.\nSpecifically, the end-to-end duration of a single unit of computation (e.g. a\nweb request).\nThus far, this document mainly considered application\nthroughput\n(e.g.\nweb requests handled per second).\nNote that each example in the\nGC cycle\nsection\nfocused on the total CPU duration of an executing program.\nHowever, such a duration is far less meaningful for say, a web service.\nWhile throughput is still important for a web service (i.e. queries per second),\noften the latency of each individual request matters even more.\nIn terms of latency, a stop-the-world GC may require a considerable length of\ntime to execute both its mark and sweep phases, during which the application,\nand in the context of a web service, any in-flight request, is unable to make\nfurther progress.\nInstead, the Go GC avoids making the length of any global application pauses\nproportional to the size of the heap, and that the core tracing algorithm is\nperformed while the application is actively executing.\n(The pauses are more strongly proportional to GOMAXPROCS algorithmically, but\nmost commonly are dominated by the time it takes to stop running goroutines.)\nCollecting concurrently is not without cost: in practice it often leads to a\ndesign with lower throughput than an equivalent stop-the-world garbage\ncollector.\nHowever,  it's important to note that\nlower latency does not inherently mean\nlower throughput\n, and the performance of the Go garbage collector has\nsteadily improved over time, in both latency and throughput.\nThe concurrent nature of Go's current GC does not invalidate anything discussed\nin this document so far: none of the statements relied on this design choice.\nGC frequency is still the primary way the GC trades off between CPU\ntime and memory for throughput, and in fact, it also takes on this role for\nlatency.\nThis is because most of the costs for the GC are incurred while the mark phase\nis active.\nThe key takeaway then, is that\nreducing GC frequency may also lead to latency\nimprovements\n.\nThis applies not only to reductions in GC frequency from modifying tuning\nparameters, like increasing GOGC and/or the memory limit, but also applies to\nthe optimizations described in the\noptimization guide\n.\nHowever, latency is often more complex to understand than throughput, because it\nis a product of the moment-to-moment execution of the program and not just an\naggregation of costs.\nAs a result, the connection between latency and GC frequency is less direct.\nBelow is a list of possible sources of latency for those inclined to dig\ndeeper.\nBrief stop-the-world pauses when the GC transitions between the mark\n\t\tand sweep phases,\nScheduling delays because the GC takes 25% of CPU resources when in the\n\t\tmark phase,\nUser goroutines assisting the GC in response to a high allocation rate,\nPointer writes requiring additional work while the GC is in the mark\n\t\tphase, and\nRunning goroutines must be suspended for their roots to be scanned.\nThese latency sources are visible in\nexecution traces\n, except for\npointer writes requiring additional work.\nFinalizers, cleanups, and weak pointers\nGarbage collection provides the illusion of infinite memory using only finite\nmemory.\nMemory is allocated but never explicitly freed, which enables simpler APIs and\nconcurrent algorithms compared to bare-bones manual memory management.\n(Some languages with manually managed memory use alternative approaches such\nas \"smart pointers\" and compile-time ownership tracking to ensure that objects\nare freed, but these features are deeply embedded into the API design\nconventions in these languages.)\nOnly the live objects—those reachable from a global variable or a\ncomputation in some goroutine—can affect the behavior of the program.\nAny time after an object becomes unreachable (\"dead\"), it may be safely\nrecycled by the GC.\nThis allows for a wide variety of GC designs, such as the tracing design used\nby Go today.\nThe death of an object is not an observable event at the language level.\nHowever, Go's runtime library provides three features that break that illusion:\ncleanups\n,\nweak pointers\n, and\nfinalizers\n.\nEach of these features provides some way to observe and react to object death,\nand in the case of finalizers, even reverse it.\nThis of course complicates Go programs and adds an additional burden to the GC\nimplementation.\nNonetheless, these features exist because they are useful in a variety of\ncircumstances, and Go programs use them and benefit from them all the time.\nFor the details of each feature, refer to its package documentation\n(\nruntime.AddCleanup\n,\nweak.Pointer\n,\nruntime.SetFinalizer\n).\nBelow is some general advice for using these features, outlines of common\nissues you can run into with each feature, and advice for testing uses of these\nfeatures.\nGeneral advice\nWrite unit tests.\nThe exact timing of cleanups, weak pointers, and finalizers can be difficult\n\t\tto predict, and it's easy to convince yourself that everything works, even\n\t\tafter many consecutive executions.\n\t\tBut it's also easy to make subtle mistakes.\nWriting tests\nfor them can be tricky, but\n\t\tgiven that they're so subtle to use, testing is even more important usual.\nAvoid using these features directly in typical Go code.\nThese are low-level features with subtle restrictions and behaviors.\n\t\tFor instance, there's no guarantee cleanups or finalizers will be run\n\t\tat program exit, or at all for that matter.\n\t\tThe long comments in their API documentation should be seen as a warning.\n\t\tThe vast majority of Go code does not benefit from using these features\n\t\tdirectly, only indirectly.\nEncapsulate the use of these mechanisms within a package.\nWhere possible, do not allow the use of these mechanisms to leak into\n\t\tthe public API of your package; provide interfaces that make it hard or\n\t\timpossible for users to misuse them.\n\t\tFor example, instead of asking the user to set up a cleanup on some\n\t\tC-allocated memory to free it, write a wrapper package and hide that\n\t\tdetail inside.\nRestrict access to objects that have finalizers, cleanups, and weak\n\t\tpointers to the package that created and applied them.\nThis is related to the previous point, but is worth calling out\n\t\texplicitly, since it's a very powerful pattern for using these\n\t\tfeatures in a less error-prone way.\n\t\tFor example, the\nunique package\nuses\n\t\tweak pointers under the hood, but completely encasulates the objects\n\t\tthat are weakly pointed-to.\n\t\tThose values can never be mutated by the rest of the application,\n\t\tit can only be copied through the\nValue method\n, preserving\n\t\tthe illusion of infinite memory for package users.\nPrefer cleaning up non-memory resources deterministically when possible,\n\t\twith finalizers and cleanups as a fallback.\nCleanups and finalizers are a good fit for memory resources such as\n\t\tmemory allocated externally, like from C, or references to an\nmmap\nmapping.\n\t\tMemory allocated by C's malloc must eventually be freed by C's free.\n\t\tA finalizer that calls\nfree\n, attached to a wrapper object\n\t\tfor the C memory, is a reasonable way to ensure that C memory is\n\t\teventually reclaimed as a consequence of garbage collection.\nHowever, non-memory resources, like file descriptors, tend to be subject to\n\t\tsystem limits that the Go runtime is generally unaware of.\n\t\tIn addition, the timing of the garbage collector in a given Go program\n\t\tis usually something a package author has little control over (for instance,\n\t\thow often the GC runs is controlled by\nGOGC\n, which can\n\t\tbe set by operators to a variety of different values in practice).\n\t\tThese two facts conspire to make cleanups and finalizers a bad fit to use\n\t\tas the only mechanism for releasing non-memory resources.\nIf you're a package author exposing an API that wraps some non-memory\n\t\tresource, consider providing an explicit API for releasing the resource\n\t\tdeterministically (through a\nClose\nmethod, or something similar),\n\t\trather than relying on the garbage collector through cleanups or finalizers.\n\t\tInstead, prefer to use cleanups and finalizers as a best-effort handler for\n\t\tprogrammer mistakes, either by cleaning up the resource anyway like\nos.File\ndoes, or by reporting the failure to\n\t\tdeterministically clean up back to the user.\nPrefer cleanups to finalizers.\nHistorically, finalizers were added to simplify the interface between Go code\n\t\tand C code and to clean up non-memory resources.\n\t\tThe intended use was to apply them to wrapper objects that owned C memory or\n\t\tsome other non-memory resource, so that the resource could be released once\n\t\tGo code was done using it.\n\t\tThese reasons at least partially explain why finalizers are narrowly scoped,\n\t\twhy any given object can only have one finalizer, and why that finalizer must\n\t\tbe attached to the first byte of the object only.\n\t\tThis limitation already stifles some use-cases.\n\t\tFor example, any package that wishes to internally cache some information about\n\t\tan object passed to it cannot clean up that information once the object is gone.\nBut worse than that, finalizers are inefficient and error-prone due to the fact\n\t\tthat they\nresurrect\n\t\tthe object\nthey're attached to, so that it can be passed to the finalizer\n\t\tfunction (and even continue to live beyond that, too).\n\t\tThis simple fact means that if the object is part of a reference cycle it can\n\t\tnever be freed, and the memory backing the object cannot be reused until at\n\t\tleast until the following garbage collection cycle.\nBecause finalizers resurrect objects, though, they do have a better-defined\n\t\texection order than cleanups.\n\t\tFor this reason, finalizers are still potentially (but rarely) useful for\n\t\tcleaning up structures that have complex destruction ordering requirements.\nBut for all other uses in Go 1.24 and beyond, we recommend you use cleanups\n\t\tbecause they are more flexible, less error-prone, and more efficient than\n\t\tfinalizers.\nCommon cleanup issues\nObjects with attached cleanups must not be reachable from the cleanup\n\t\tfunction (for example, through a captured local variable).\n\t\tThis will prevent the object from being reclaimed and the cleanup from\n\t\tever running.\nf := new(myFile)\nf.fd = syscall.Open(...)\nruntime.AddCleanup(f, func(fd int) {\n\tsyscall.Close(f.fd) // Mistake: We reference f, so this cleanup won't run!\n}, f.fd)\nObjects with attached cleanups must not be reachable from the argument\n\t\tto the cleanup function.\n\t\tThis will prevent the object from being reclaimed and the cleanup from\n\t\tever running.\nf := new(myFile)\nf.fd = syscall.Open(...)\nruntime.AddCleanup(f, func(f *myFile) {\n\tsyscall.Close(f.fd)\n}, f) // Mistake: We reference f, so this cleanup wouldn't ever run. This specific case also panics.\nFinalizers have a well-defined execution order, but cleanups do not.\n\t\tCleanups can also run concurrently with one another.\nruntime.GC\nwill not wait until cleanups for unreachable\n\t\tobjects are executed, only until they are all queued.\nCommon weak pointer issues\nWeak pointers can begin returning\nnil\nfrom their\nValue\nmethod at unexpected times.\n\t\tAlways guard the call to\nValue\nwith a\nnil\ncheck and have a backup plan.\nWhen weak pointers are used as map keys, they do not affect the\n\t\treachability of map values.\n\t\tTherefore, if a weak pointer map key points to an object that is also\n\t\treachable from the map value, that object will still be considered\n\t\treachable.\nCommon finalizer issues\nObjects with attached finalizers must not be reachable from themselves\n\t\tby any path (in other words, they cannot be in a reference cycle).\n\t\tThis will prevent the object from being reclaimed and the finalizer from\n\t\tever running.\nf := new(myCycle)\nf.self = f // Mistake: f is reachable from f, so this finalizer would never run.\nruntime.SetFinalizer(f, func(f *myCycle) {\n\t...\n})\nObjects with attached finalizers must not be reachable from the finalizer\n\t\tfunction (for example, through a captured local variable).\n\t\tThis will prevent the object from being reclaimed and the finalizer from\n\t\tever running.\nf := new(myFile)\nf.fd = syscall.Open(...)\nruntime.SetFinalizer(f, func(_ *myFile) {\n\tsyscall.Close(f.fd) // Mistake: We reference the outer f, so this cleanup won't run!\n})\nReference chains of objects with attached finalizers (say, in a linked list)\n\t\ttake, at minimum, as many GC cycles as there are objects in the chain\n\t\tto clean them all up.\n\t\tKeep finalizers shallow!\n// Mistake: reclaiming this linked list will take at least 10 GC cycles.\nnode := new(linkedListNode)\nfor range 10 {\n\ttmp := new(linkedListNode)\n\ttmp.next = node\n\tnode = tmp\n\truntime.SetFinalizer(node, func(node *linkedListNode) {\n\t\t...\n\t})\n}\nAvoid placing finalizers on objects returned at package boundaries.\n\t\tThis makes it possible for users of your package to call\nruntime.SetFinalizer\nto mutate the finalizer on the object\n\t\tyou return, which can be an unexpected behavior that users of your package\n\t\tmay end up relying on.\nruntime.GC\nwill not wait until finalizers for unreachable\n\t\tobjects are executed, only until they are all queued.\nTesting object death\nWhen using these features, it can sometimes be tricky to write tests for code that\nuses them.\nHere are some tips for writing robust tests for code that uses these features.\nAvoid running such tests in parallel with other tests.\n\t\tIt helps a lot to increase determinism as much as possible and to have\n\t\ta good handle on the state of the world at any given time.\nUse\nruntime.GC\nto establish a baseline upon entering the\n\t\ttest.\n\t\tUse\nruntime.GC\nto force weak pointers to\nnil\n,\n\t\tand to queue up cleanups and finalizers to run.\nruntime.GC\ndoes not wait for cleanups and finalizers to run,\n\t\tit only queues them.\nTo write the most robust tests possible, inject a way to block on a cleanup\n\t\tor finalizer from your test (for example, pass an optional channel to the\n\t\tcleanup and/or finalizer from the test, and write to the channel once it\n\t\thas finished executing).\n\t\tIf this is too hard or impossible, an alternative is to spin on a particular\n\t\tpost-cleanup state to be true.\n\t\tFor example, the\nos\ntests call\nruntime.Gosched\nin\n\t\ta loop that checks whether a file has been closed, once it becomes\n\t\tunreachable.\nIf writing tests for using finalizers, and you have a chain of objects\n\t\tthat use finalizers, you will need at minimum the length of the deepest\n\t\tchain the test can create of\nruntime.GC\ncalls to ensure\n\t\tall the finalizers run.\nTest in race mode to discover races between concurrent cleanups, and\n\t\tbetween cleanup and finalizer code and the rest of the codebase.\nAdditional resources\nWhile the information presented above is accurate, it lacks the detail to\nfully understand costs and trade-offs in the Go GC's design.\nFor more information, see the following additional resources.\nThe GC Handbook\n—An\n\t\t\texcellent general resource and reference on garbage collector design.\nTCMalloc\n—Design\n\t\t\tdocument for the C/C++ memory allocator TCMalloc, which the Go memory allocator is based on.\nGo 1.5 GC announcement\n—The\n\t\t\tblog post announcing the Go 1.5 concurrent GC, which describes the algorithm in more detail.\nGetting to Go\n—An\n\t\t\tin-depth presentation about the evolution of Go's GC design up to 2018.\nGo 1.5 concurrent GC pacing\n—Design\n\t\t\tdocument for determining when to start a concurrent mark phase.\nSmarter scavenging\n—Design\n\t\t\tdocument for revising the way the Go runtime returns memory to the operating system.\nScalable page allocator\n—Design\n\t\t\tdocument for revising the way the Go runtime manages memory it gets from the operating system.\nGC pacer redesign (Go 1.18)\n—Design\n\t\t\tdocument for revising the algorithm to determine when to start a concurrent mark phase.\nSoft memory limit (Go 1.19)\n—Design\n\t\t\tdocument for the soft memory limit.\nA note about virtual memory\nThis guide has largely focused on the physical memory use of the GC, but a\nquestion that comes up regularly is what exactly that means and how it compares\nto virtual memory (typically presented in programs like\ntop\nas\n\"VSS\").\nPhysical memory is memory housed in the actual physical RAM chip in most\ncomputers.\nVirtual memory\nis an\nabstraction over physical memory provided by the operating system to isolate\nprograms from one another.\nIt's also typically acceptable for programs to reserve virtual address space\nthat doesn't map to any physical addresses at all.\nBecause virtual memory is just a mapping maintained by the operating system,\nit is typically very cheap to make large virtual memory reservations that don't\nmap to physical memory.\nThe Go runtime generally relies upon this view of the cost of virtual memory in\na few ways:\nThe Go runtime never deletes virtual memory that it maps.\n\t\tInstead, it uses special operations that most operating systems\n\t\tprovide to explicitly release any physical memory resources\n\t\tassociated with some virtual memory range.\nThis technique is used explicitly to manage the\nmemory limit\nand return memory to the\n\t\toperating system that the Go runtime no longer needs.\n\t\tThe Go runtime also releases memory it no longer needs continuously\n\t\tin the background.\n\t\tSee\nthe additional resources\nfor\n\t\tmore information.\nOn 32-bit platforms, the Go runtime reserves between 128 MiB and 512 MiB\n\t\tof address space up-front for the heap to limit fragmentation issues.\nThe Go runtime uses large virtual memory address space reservations\n\t\tin the implementation of several internal data structures.\n\t\tOn 64-bit platforms, these typically have a minimum virtual memory\n\t\tfootprint of about 700 MiB.\n\t\tOn 32-bit platforms, their footprint is negligible.\nAs a result, virtual memory metrics such as \"VSS\" in\ntop\nare\ntypically not very useful in understanding a Go program's memory footprint.\nInstead, focus on \"RSS\" and similar measurements, which more directly reflect\nphysical memory usage.\nOptimization guide\nIdentifying costs\nBefore trying to optimize how your Go application interacts with the GC, it's\nimportant to first identify that the GC is a major cost in the first place.\nThe Go ecosystem provides a number of tools for identifying costs and optimizing\nGo applications.\nFor a brief overview of these tools, see the\nguide on diagnostics\n.\nHere, we'll focus on a subset of these tools and a reasonable order to apply\nthem in in order to understand GC impact and behavior.\nCPU profiles\nA good place to start is with\nCPU profiling\n.\n\t\tCPU profiling provides an overview of where CPU time is spent, though to the\n\t\tuntrained eye it may be difficult to identify the magnitude of the role the\n\t\tGC plays in a particular application.\n\t\tLuckily, understanding how the GC fits in mostly boils down to knowing what\n\t\tdifferent functions in the `runtime` package mean.\n\t\tBelow is a useful subset of these functions for interpreting CPU profiles.\nNote that the functions listed below are not leaf functions, so they may not show\n\t\tup in the default the\npprof\ntool provides with the\ntop\ncommand.\n\t\tInstead, use the\ntop -cum\ncommand or use the\nlist\ncommand on these functions directly and focus on the cumulative percent column.\nruntime.gcBgMarkWorker\n: Entrypoint to the\n\t\t\tbackground mark worker goroutines.\n\t\t\tTime spent here scales with GC frequency and the complexity and\n\t\t\tsize of the object graph.\n\t\t\tIt represents a baseline for how much time the application spends\n\t\t\tmarking and scanning.\nNote that within these goroutines, you will find calls to\nruntime.gcDrainMarkWorkerDedicated\n,\nruntime.gcDrainMarkWorkerFractional\n, and\nruntime.gcDrainMarkWorkerIdle\n,\n\t\t\twhich indicate worker type.\n\t\t\tIn a largely idle Go application, the Go GC is going to use up\n\t\t\tadditional (idle) CPU resources to get its job done faster, which\n\t\t\tis indicated with the\nruntime.gcDrainMarkWorkerIdle\nsymbol.\n\t\t\tAs a result, time here may represent a large fraction of CPU\n\t\t\tsamples, which the Go GC believes are free.\n\t\t\tIf the application becomes more active, CPU time in idle workers\n\t\t\twill drop.\n\t\t\tOne common reason this can happen is if an application runs entirely\n\t\t\tin one goroutine but\nGOMAXPROCS\nis >1.\nruntime.mallocgc\n: Entrypoint to the memory\n\t\t\tallocator for heap memory.\n\t\t\tA large amount of cumulative time spent here (>15%)\n\t\t\ttypically indicates a lot of memory being allocated.\nruntime.gcAssistAlloc\n: Function goroutines\n\t\t\tenter to yield some of their time to assist the GC with scanning\n\t\t\tand marking.\n\t\t\tA large amount of cumulative time spent here (>5%) indicates\n\t\t\tthat the application is likely out-pacing the GC with respect to how\n\t\t\tfast it's allocating.\n\t\t\tIt indicates a particularly high degree of impact from the GC,\n\t\t\tand also represents time the application spend marking and scanning.\n\t\t\tNote that this is included in the\nruntime.mallocgc\ncall tree, so it will inflate that as well.\nExecution traces\nWhile CPU profiles are great for identifying where time is spent in\n\t\taggregate, they're less useful for indicating performance costs that\n\t\tare more subtle, rare, or related to latency specifically.\n\t\tExecution traces\non\n\t\tthe other hand provide a rich and deep view into a short window of\n\t\ta Go program's execution.\n\t\tThey contain a variety of events related to the Go GC and specific\n\t\texecution paths can be directly observed, along with how the application\n\t\tmight interact with the Go GC.\n\t\tAll the GC events tracked are conveniently labeled as such in the trace\n\t\tviewer.\nSee the\ndocumentation for the\nruntime/trace\npackage for how to get started with\n\t\texecution traces.\nGC traces\nWhen all else fails, the Go GC provides a few different specific traces\n\t\tthat provide much deeper insights into GC behavior.\n\t\tThese traces are always printed directly to STDERR, one line per GC cycle,\n\t\tand are configured through the\nGODEBUG\nenvironment variable\n\t\tthat all Go programs recognize.\n\t\tThey're mostly useful for debugging the Go GC itself since they require\n\t\tsome familiarity with the specifics of the GC's implementation, but\n\t\tnonetheless can occasionally be useful to gain a better understanding of\n\t\tGC behavior.\nThe core GC trace is enabled by setting\nGODEBUG=gctrace=1\n.\n\t\tThe output produced by this trace is documented in the\nenvironment\n\t\tvariables section in the documentation for the\nruntime\npackage.\nA supplementary GC trace called the \"pacer trace\" provides even deeper\n\t\tinsights and is enabled by setting\nGODEBUG=gcpacertrace=1\n.\n\t\tInterpreting this output requires an understanding of the GC's \"pacer\"\n\t\t(see\nadditional resources\n), which is\n\t\toutside the scope of this guide.\nEliminating heap allocations\nOne way to reduce costs from the GC is to have the GC manage fewer values to begin\nwith.\nThe techniques described below can produce some of the largest improvements in\nperformance, because as the\nGOGC section\ndemonstrated, the\nallocation rate of a Go program is a major factor in GC frequency, the key\ncost metric used by this guide.\nHeap profiling\nAfter\nidentifying that the GC is a source of\nsignificant costs\n, the next step in eliminating heap allocations is to\nfind out where most of them are coming from.\nFor this purpose, memory profiles (really, heap memory profiles) are very\nuseful.\nCheck out the\ndocumentation\nfor how to get started with them.\nMemory profiles describe where in the program heap allocations come from,\nidentifying them by the stack trace at the point they were allocated.\nEach memory profile can break down memory in four ways.\ninuse_objects\n—Breaks down the number of objects that\n\tare live.\ninuse_space\n—Breaks down live objects by how much\n\tmemory they use in bytes.\nalloc_objects\n—Breaks down the number of objects\n\tthat have been allocated since the Go program began executing.\nalloc_space\n—Breaks down the total amount of memory\n\tallocated since the Go program began executing.\nSwitching between these different views of heap memory may be done with either\nthe\n-sample_index\nflag to the\npprof\ntool, or via the\nsample_index\noption when the tool is used interactively.\nNote: memory profiles by default only sample a subset of heap objects so they\nwill not contain information about every single heap allocation.\nHowever, this is sufficient to find hot-spots.\nTo change the sampling rate, see\nruntime.MemProfileRate\n.\nFor the purposes of reducing GC costs,\nalloc_space\nis typically the\nmost useful view as it directly corresponds to the allocation rate.\nThis view will indicate allocation hot spots that would provide the most benefit.\nEscape analysis\nOnce candidate heap allocation sites have been identified with the help of\nheap profiles\n, how can they be eliminated?\nThe key is to leverage the Go compiler's escape analysis to have the Go compiler\nfind alternative, and more efficient storage for this memory, for example\nin the goroutine stack.\nLuckily, the Go compiler has the ability to describe why it decides to escape\na Go value to the heap.\nWith that knowledge, it becomes a matter of reorganizing your source code to\nchange the outcome of the analysis (which is often the hardest part, but outside\nthe scope of this guide).\nAs for how to access the information from the Go compiler's escape analysis, the\nsimplest way is through a debug flag supported by the Go compiler that describes\nall optimizations it applied or did not apply to some package in a text format.\nThis includes whether or not values escape.\nTry the following command, where\n[package]\nis some Go package path.\n$ go build -gcflags=-m=3 [package]\nThis information can also be visualized as an overlay in\nan LSP-capable editor; it is exposed as a code action.\n\nFor example, in VS Code, invoke the \"Source Action... > Show compiler\noptimization details\" command to enable diagnostics for the current package.\n(You can also run the \"Go: Toggle compiler optimization details\"\ncommand.)\n\nUse this configuration setting to control which annotations are displayed:\nEnable the overlay for escape analysis by\nsetting\nui.diagnostic.annotations\nto include\nescape\n.\nFinally, the Go compiler provides this information in a machine-readable (JSON)\nformat that may be used to build additional custom tooling.\nFor more information on that, see the\ndocumentation in the source Go code\n.\nImplementation-specific optimizations\nThe Go GC is sensitive to the demographics of live memory, because a complex\ngraph of objects and pointers both limits parallelism and generates more work\nfor the GC.\nAs a result, the GC contains a few optimizations for specific common structures.\nThe most directly useful ones for performance optimization are listed below.\nNote: Applying the optimizations below may reduce the readability of your code\nby obscuring intent, and may fail to hold up across Go releases.\nPrefer to apply these optimizations only in the places they matter most.\nSuch places may be identified by using the tools listed in the\nsection on identifying costs\n.\nPointer-free values are segregated from other values.\nAs a result, it may be advantageous to eliminate pointers from data\n\t\tstructures that do not strictly need them, as this reduces the cache\n\t\tpressure the GC exerts on the program.\n\t\tAs a result, data structures that rely on indices over pointer values,\n\t\twhile less well-typed, may perform better.\n\t\tThis is only worth doing if it's clear that the object graph is complex\n\t\tand the GC is spending a lot of time marking and scanning.\nThe GC will stop scanning values at the last pointer in the value.\nAs a result, it may be advantageous to group pointer fields in\n\t\tstruct-typed values at the beginning of the value.\n\t\tThis is only worth doing if it's clear the application spends a lot of its\n\t\ttime marking and scanning.\n\t\t(In theory the compiler can do this automatically, but it is not yet\n\t\timplemented, and struct fields are arranged as written in the source\n\t\tcode.)\nFurthermore, the GC must interact with nearly every pointer it sees, so using\nindices into an slice, for example, instead of pointers, can aid in reducing GC\ncosts.\nLinux transparent huge pages (THP)\nWhen a program accesses memory, the CPU needs to translate the\nvirtual memory\naddresses it uses\ninto physical memory addresses that refer to the data it was trying to access.\nTo do this, the CPU consults the \"page table,\" a data structure that represents\nthe mapping from virtual to physical memory, managed by the operating system.\nEach entry in the page table represents an indivisible block of physical memory\ncalled a page, hence the name.\nTransparent huge pages (THP) is a Linux feature that transparently replaces pages of\nphysical memory backing contiguous virtual memory regions with bigger blocks of memory\ncalled huge pages.\nBy using bigger blocks, fewer page table entries are needed to represent the same memory\nregion, improving page table lookup times.\nHowever, bigger blocks mean more waste if only a small part of the huge page is used\nby the system.\nWhen running Go programs in production, enabling transparent huge pages on Linux\ncan improve throughput and latency at the cost of additional memory use.\nApplications with small heaps tend not to benefit from THP and may end up using a\nsubstantial amount of additional memory (as high as 50%).\nHowever, applications with big heaps (1 GiB or more) tend to benefit quite a bit\n(up to 10% throughput) without very much additional memory overhead (1-2% or less).\nBeing aware of your THP settings in either case can be helpful, and experimentation\nis always recommended.\nOne can enable or disable transparent huge pages in a Linux environment by modifying\n/sys/kernel/mm/transparent_hugepage/enabled\n.\nSee the\nofficial\nLinux admin guide\nfor more details.\nIf you choose to have your Linux production environment enable transparent huge pages,\nwe recommend the following additional settings for Go programs.\nSet\n/sys/kernel/mm/transparent_hugepage/defrag\nto\ndefer\nor\ndefer+madvise\n.\nThis setting controls how aggressively a Linux kernel coalesces regular\n\t\tpages into huge pages.\ndefer\ntells the kernel to coalesce huge pages lazily\n\t\tand in the background.\n\t\tA more aggressive setting can induce stalls in memory constrained systems\n\t\tand can often hurt application latencies.\ndefer+madvise\nis like\ndefer\n, but is friendlier\n\t\tto other applications on the system that request huge pages explicitly and\n\t\trequire them for performance.\nSet\n/sys/kernel/mm/transparent_hugepage/khugepaged/max_ptes_none\nto\n0\n.\nThis setting controls how many additional pages the Linux kernel daemon\n\t\tcan allocate when trying to allocate a huge page.\n\t\tThe default setting is maximally aggressive, and can often\nundo work the Go\n\t\truntime does to return memory to the OS\n.\n\t\tBefore Go 1.21, the Go runtime tried to mitigate the negative effects of the\n\t\tdefault setting, but it came with a CPU cost.\n\t\tWith Go 1.21+ and Linux 6.2+, the Go runtime no longer mutates huge page\n\t\tstate.\nIf you experience an increase in memory usage when upgrading to Go 1.21.1 or\n\t\tlater, try applying this setting; it will likely resolve your issue.\n\t\tAs an additional workaround, you can call\nthe\nPrctl\nfunction\nwith\nPR_SET_THP_DISABLE\nto disable huge pages at\n\t\tthe process level, or you can set\nGODEBUG=disablethp=1\n(to be\n\t\tadded in Go 1.21.6 and Go 1.22) to disable huge pages for heap memory.\n\t\tNote that the\nGODEBUG\nsetting may be removed in a future release.\nAppendix\nAdditional notes on GOGC\nThe\nGOGC section\nclaimed that doubling GOGC doubles heap\nmemory overheads and halves GC CPU costs.\nTo see why, let's break it down mathematically.\nFirstly, the heap target sets a target for the total heap size.\nThis target, however, mainly influences the new heap memory, because the live\nheap is fundamental to the application.\nTarget heap memory = Live heap + (Live heap + GC roots) * GOGC / 100\nTotal heap memory = Live heap + New heap memory\n⇒\nNew heap memory = (Live heap + GC roots) * GOGC / 100\nFrom this we can see that doubling GOGC would also double the amount of new heap\nmemory that application will allocate each cycle, which captures heap memory\noverheads.\nNote that\nLive heap + GC roots\nis an approximation of the amount of\nmemory the GC needs to scan.\nNext, let's look at GC CPU cost.\nTotal cost can be broken down as the cost per cycle, times GC frequency over\nsome time period T.\nTotal GC CPU cost = (GC CPU cost per cycle) * (GC frequency) * T\nGC CPU cost per cycle can be derived from the\nGC model\n:\nGC CPU cost per cycle = (Live heap + GC roots) * (Cost per byte) + Fixed cost\nNote that sweep phase costs are ignored here as mark and scan costs dominate.\nThe steady state is defined by a constant allocation rate and a constant cost\nper byte, so in the steady state we can derive a GC frequency from this new heap\nmemory:\nGC frequency = (Allocation rate) / (New heap memory) = (Allocation rate) / ((Live heap + GC roots) * GOGC / 100)\nPutting this together, we get the full equation for the total cost:\nTotal GC CPU cost = (Allocation rate) / ((Live heap + GC roots) * GOGC / 100) * ((Live heap + GC roots) * (Cost per byte) + Fixed cost) * T\nFor a sufficiently large heap (which represents most cases), the marginal costs\nof a GC cycle dominate the fixed costs.\nThis allows for a significant simplification of the total GC CPU cost formula.\nTotal GC CPU cost = (Allocation rate) / (GOGC / 100) * (Cost per byte) * T\nFrom this simplified formula, we can see that if we double GOGC, we halve total\nGC CPU cost.\n(Note that the visualizations in this guide do simulate fixed costs, so the GC\nCPU overheads reported by them will not exactly halve when GOGC doubles.)\nFurthermore, GC CPU costs are largely determined by allocation rate and the\ncost per byte to scan memory.\nFor more information on how to reduce these costs specifically, see the\noptimization guide\n.\nNote: there exists a discrepancy between the size of the live heap, and the\namount of that memory the GC actually needs to scan: the same size live heap but\nwith a different structure will result in a different CPU cost, but the same\nmemory cost, resulting a different trade-off.\nThis is why the structure of the heap is part of the definition of the\nsteady state.\nThe heap target should arguably only include the scannable live heap as a closer\napproximation of memory the GC needs to scan, but this leads to degenerate\nbehavior when there's a very small amount of scannable live heap but the live\nheap is otherwise large.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nManaging dependencies\nWhen your code uses external packages, those packages (distributed as modules)\nbecome dependencies. Over time, you may need to upgrade them or replace them. Go\nprovides dependency management tools that help you keep your Go applications\nsecure as you incorporate external dependencies.\nThis topic describes how to perform tasks to manage dependencies you take on in\nyour code. You can perform most of these with Go tools. This topic also\ndescribes how to perform a few other dependency-related tasks you might find\nuseful.\nSee also\nIf you’re new to working with dependencies as modules, take a look at the\nGetting started tutorial\nfor a brief introduction.\nUsing the\ngo\ncommand to manage dependencies helps ensure that your\nrequirements remain consistent and the content of your go.mod file is valid.\nFor reference on the commands, see\nCommand go\n.\nYou can also get help from the command line by typing\ngo help\ncommand-name\n, as with\ngo help mod tidy\n.\nGo commands you use to make dependency changes edit your go.mod file. For\nmore about the contents of the file, see\ngo.mod file reference\n.\nMaking your editor or IDE aware of Go modules can make the work of managing\nthem easier. For more on editors that support Go, see\nEditor plugins and\nIDEs\n.\nThis topic doesn’t describe how to develop, publish, and version modules for\nothers to use. For more on that, see\nDeveloping and publishing\nmodules\n.\nWorkflow for using and managing dependencies\nYou can get and use useful packages with Go tools. On\npkg.go.dev\n, you can search for packages you might find\nuseful, then use the\ngo\ncommand to import those packages into your own code to\ncall their functions.\nThe following lists the most common dependency management steps. For more about\neach, see the sections in this topic.\nLocate useful packages\non\npkg.go.dev\n.\nImport the packages\nyou want in your code.\nAdd your code to a module for dependency tracking (if it isn’t in a module\nalready). See\nEnabling dependency tracking\nAdd external packages as dependencies\nso you can manage\nthem.\nUpgrade or downgrade dependency versions\nas needed over time.\nManaging dependencies as modules\nIn Go, you manage dependencies as modules that contain the packages you import.\nThis process is supported by:\nA\ndecentralized system for publishing\nmodules and retrieving their code.\nDevelopers make their modules available for other developers to use from\ntheir own repository and publish with a version number.\nA\npackage search engine\nand documentation browser (pkg.go.dev) at which\nyou can find modules. See\nLocating and importing useful packages\n.\nA module\nversion numbering convention\nto help you understand a module’s\nstability and backward compatibility guarantees. See\nModule version\nnumbering\n.\nGo tools\nthat make it easier for you to manage dependencies, including\ngetting a module’s source, upgrading, and so on. See sections of this topic\nfor more.\nLocating and importing useful packages\nYou can search\npkg.go.dev\nto find packages with functions\nyou might find useful.\nWhen you’ve found a package you want to use in your code, locate the package\npath at the top of the page and click the Copy path button to copy the path to\nyour clipboard. In your own code, paste the path into an import statement, as in\nthe following example:\nimport \"rsc.io/quote\"\nAfter your code imports the package, enable dependency tracking and get the\npackage’s code to compile with. For more, see\nEnabling dependency tracking in\nyour code\nand\nAdding a dependency\n.\nEnabling dependency tracking in your code\nTo track and manage the dependencies you add, you begin by putting your code in\nits own module. This creates a go.mod file at the root of your source tree.\nDependencies you add will be listed in that file.\nTo add your code to its own module, use the\ngo mod init\ncommand\n. For example, from the command\nline, change to your code’s root directory, then run the command as in the\nfollowing example:\n$ go mod init example/mymodule\nThe\ngo mod init\ncommand’s argument is your module’s module path. If possible,\nthe module path should be the repository location of your source code.\nIf at first you don’t know the module’s eventual repository location, use a\nsafe substitute. This might be the name of a domain you own or another name you\ncontrol (such as your company name), along with a path following from the\nmodule’s name or source directory. For more, see\nNaming a module\n.\nAs you use Go tools to manage dependencies, the tools update the go.mod file so\nthat it maintains a current list of your dependencies.\nWhen you add dependencies, Go tools also create a go.sum file that contains\nchecksums of modules you depend on. Go uses this to verify the integrity of\ndownloaded module files, especially for other developers working on your\nproject.\nInclude the go.mod and go.sum files in your repository with your code.\nSee the\ngo.mod reference\nfor more.\nNaming a module\nWhen you run\ngo mod init\nto create a module for tracking dependencies, you\nspecify a module path that serves as the module’s name. The module path\nbecomes the import path prefix for packages in the module. Be sure to specify\na module path that won’t conflict with the module path of other modules.\nAt a minimum, a module path need only indicate something about its origin, such\nas a company or author or owner name. But the path might also be more\ndescriptive about what the module is or does.\nThe module path is typically of the following form:\n<prefix>/<descriptive-text>\nThe\nprefix\nis typically a string that partially describes the module, such\nas a string that describes its origin. This might be:\nThe location of the repository where Go tools can find the module’s source\ncode (required if you’re publishing the module).\nFor example, it might be\ngithub.com/<project-name>/\n.\nUse this best practice if you think you might publish the module for\nothers to use. For more about publishing, see\nDeveloping and publishing modules\n.\nA name you control.\nIf you’re not using a repository name, be sure to choose a prefix that\nyou’re confident won’t be used by others. A good choice is your\ncompany’s name. Avoid common terms such as\nwidgets\n,\nutilities\n, or\napp\n.\nFor the\ndescriptive text\n, a good choice would be a project name. Remember\nthat package names carry most of the weight of describing functionality.\nThe module path creates a namespace for those package names.\nReserved module path prefixes\nGo guarantees that the following strings won’t be used in package names.\ntest\n– You can use\ntest\nas a module path prefix for a module whose code\nis designed to locally test functions in another module.\nUse the\ntest\npath prefix for modules that are created as part of a test.\nFor example, your test itself might run\ngo mod init test\nand then set up\nthat module in some particular way in order to test with a Go source code\nanalysis tool.\nexample\n– Used as a module path prefix in some Go documentation, such as\nin tutorials where you’re creating a module just to track dependencies.\nNote that Go documentation also uses\nexample.com\nto illustrate when the\nexample might be a published module.\nAdding a dependency\nOnce you’re importing packages from a published module, you can add that module\nto manage as a dependency by using the\ngo get\ncommand\n.\nThe command does the following:\nIf needed, it adds\nrequire\ndirectives to your go.mod file for modules\nneeded to build packages named on the command line. A\nrequire\ndirective\ntracks the minimum version of a module that your module depends on. See the\ngo.mod reference\nfor more.\nIf needed, it downloads module source code so you can compile packages that\ndepend on them. It can download modules from a module proxy like\nproxy.golang.org or directly from version control repositories. The source\nis cached locally.\nYou can set the location from which Go tools download modules. For more, see\nSpecifying a module proxy server\n.\nThe following describes a few examples.\nTo add all dependencies for a package in your module, run a command like the\none below (\".\" refers to the package in the current directory):\n$ go get .\nTo add a specific dependency, specify its module path as an argument to the\ncommand.\n$ go get example.com/theirmodule\nThe command also authenticates each module it downloads. This ensures that it’s\nunchanged from when the module was published. If the module has changed since it\nwas published – for example, the developer changed the contents of the commit\n– Go tools will present a security error. This authentication check protects\nyou from modules that might have been tampered with.\nGetting a specific dependency version\nYou can get a specific version of a dependency module by specifying its version\nin the\ngo get\ncommand. The command updates the\nrequire\ndirective in your\ngo.mod file (though you can also update that manually).\nYou might want to do this if:\nYou want to get a specific pre-release version of a module to try out.\nYou’ve discovered that the version you’re currently requiring isn’t working\nfor you, so you want to get a version you know you can rely on.\nYou want to upgrade or downgrade a module you’re already requiring.\nHere are examples for using the\ngo get\ncommand\n:\nTo get a specific numbered version, append the module path with an @ sign\nfollowed by the version you want:\n$ go get example.com/theirmodule@v1.3.4\nTo get the latest version, append the module path with\n@latest\n:\n$ go get example.com/theirmodule@latest\nThe following go.mod file\nrequire\ndirective example (see the\ngo.mod\nreference\nfor more) illustrates how to require a specific version\nnumber:\nrequire example.com/theirmodule v1.3.4\nDiscovering available updates\nYou can check to see if there are newer versions of dependencies you’re already\nusing in your current module. Use the\ngo list\ncommand to display a list of\nyour module’s dependencies, along with the latest version available for that\nmodule. Once you’ve discovered available upgrades, you can try them out with your\ncode to decide whether or not to upgrade to new versions.\nFor more about the\ngo list\ncommand, see\ngo list -m\n.\nHere are a couple of examples.\nList all of the modules that are dependencies of your current module,\nalong with the latest version available for each:\n$ go list -m -u all\nDisplay the latest version available for a specific module:\n$ go list -m -u example.com/theirmodule\nUpgrading or downgrading a dependency\nYou can upgrade or downgrade a dependency module by using Go tools to discover\navailable versions, then add a different version as a dependency.\nTo discover new versions use the\ngo list\ncommand as described in\nDiscovering available updates\n.\nTo add a particular version as a dependency, use the\ngo get\ncommand as\ndescribed in\nGetting a specific dependency version\n.\nSynchronizing your code’s dependencies\nYou can ensure that you’re managing dependencies for all of your code’s imported\npackages while also removing dependencies for packages you’re no longer\nimporting.\nThis can be useful when you’ve been making changes to your code and\ndependencies, possibly creating a collection of managed dependencies and\ndownloaded modules that no longer match the collection specifically required by\nthe packages imported in your code.\nTo keep your managed dependency set tidy, use the\ngo mod tidy\ncommand. Using\nthe set of packages imported in your code, this command edits your go.mod file\nto add modules that are necessary but missing. It also removes unused modules\nthat don’t provide any relevant packages.\nThe command has no arguments except for one flag, -v, that prints information\nabout removed modules.\n$ go mod tidy\nDeveloping and testing against unpublished module code\nYou can specify that your code should use dependency modules that may not be\npublished. The code for these modules might be in their respective repositories,\nin a fork of those repositories, or on a drive with the current module that\nconsumes them.\nYou might want to do this when:\nYou want to make your own changes to an external module’s code, such as\nafter forking and/or cloning it. For example, you might want to prepare a\nfix to the module, then send it as a pull request to the module’s developer.\nYou’re building a new module and haven’t yet published it, so it’s\nunavailable on a repository where the\ngo get\ncommand can reach it.\nRequiring module code in a local directory\nYou can specify that the code for a required module is on the same local drive\nas the code that requires it. You might find this useful when you are:\nDeveloping your own separate module and want to test from the current module.\nFixing issues in or adding features to an external module and want to test\nfrom the current module. (Note that you can also require the external module\nfrom your own fork of its repository. For more, see\nRequiring external\nmodule code from your own repository fork\n.)\nTo tell Go commands to use the local copy of the module’s code, use the\nreplace\ndirective in your go.mod file to replace the module path given in a\nrequire\ndirective. See the\ngo.mod reference\nfor\nmore about directives.\nIn the following go.mod file example, the current module requires the external\nmodule\nexample.com/theirmodule\n, with a nonexistent version number\n(\nv0.0.0-unpublished\n) used to ensure the replacement works correctly. The\nreplace\ndirective then replaces the original module path with\n../theirmodule\n, a directory that is at the same level as the current module’s\ndirectory.\nmodule example.com/mymodule\n\ngo 1.23.0\n\nrequire example.com/theirmodule v0.0.0-unpublished\n\nreplace example.com/theirmodule v0.0.0-unpublished => ../theirmodule\nWhen setting up a\nrequire\n/\nreplace\npair, use the\ngo mod edit\nand\ngo get\ncommands\nto ensure that requirements described by the file remain consistent:\n$ go mod edit -replace=example.com/theirmodule@v0.0.0-unpublished=../theirmodule\n$ go get example.com/theirmodule@v0.0.0-unpublished\nNote:\nWhen you use the replace directive, Go tools don’t authenticate\nexternal modules as described in\nAdding a dependency\n.\nFor more about version numbers, see\nModule version numbering\n.\nRequiring external module code from your own repository fork\nWhen you have forked an external module’s repository (such as to fix an issue in\nthe module’s code or to add a feature), you can have Go tools use your fork for\nthe module’s source. This can be useful for testing changes from your own code.\n(Note that you can also require the module code in a directory that’s on the\nlocal drive with the module that requires it. For more, see\nRequiring module\ncode in a local directory\n.)\nYou do this by using a\nreplace\ndirective in your go.mod file to replace the\nexternal module’s original module path with a path to the fork in your\nrepository. This directs Go tools to use the replacement path (the fork’s\nlocation) when compiling, for example, while allowing you to leave\nimport\nstatements unchanged from the original module path.\nFor more about the\nreplace\ndirective, see the\ngo.mod file\nreference\n.\nIn the following go.mod file example, the current module requires the external\nmodule\nexample.com/theirmodule\n. The\nreplace\ndirective then replaces the\noriginal module path with\nexample.com/myfork/theirmodule\n, a fork of the\nmodule’s own repository.\nmodule example.com/mymodule\n\ngo 1.23.0\n\nrequire example.com/theirmodule v1.2.3\n\nreplace example.com/theirmodule v1.2.3 => example.com/myfork/theirmodule v1.2.3-fixed\nWhen setting up a\nrequire\n/\nreplace\npair, use Go tool commands to ensure that\nrequirements described by the file remain consistent. Use the\ngo list\ncommand to get the version in use by the current\nmodule. Then use the\ngo mod edit\ncommand to replace\nthe required module with the fork:\n$ go list -m example.com/theirmodule\nexample.com/theirmodule v1.2.3\n$ go mod edit -replace=example.com/theirmodule@v1.2.3=example.com/myfork/theirmodule@v1.2.3-fixed\nNote:\nWhen you use the\nreplace\ndirective, Go tools don’t authenticate\nexternal modules as described in\nAdding a dependency\n.\nFor more about version numbers, see\nModule version numbering\n.\nGetting a specific commit using a repository identifier\nYou can use the\ngo get\ncommand to add unpublished code for a module from a\nspecific commit in its repository.\nTo do this, you use the\ngo get\ncommand, specifying the code you want with an\n@\nsign. When you use\ngo get\n, the command will add to your go.mod file a\nrequire\ndirective that requires the external module, using a pseudo-version\nnumber based on details about the commit.\nThe following examples provide a few illustrations. These are based on a module\nwhose source is in a git repository.\nTo get the module at a specific commit, append the form @\ncommithash\n:\n$ go get example.com/theirmodule@4cf76c2\nTo get the module at a specific branch, append the form @\nbranchname\n:\n$ go get example.com/theirmodule@bugfixes\nRemoving a dependency\nWhen your code no longer uses any packages in a module, you can stop tracking\nthe module as a dependency.\nTo stop tracking all unused modules, run the\ngo mod tidy\ncommand\n. This command may also add missing dependencies\nneeded to build packages in your module.\n$ go mod tidy\nTo remove a specific dependency, use the\ngo get\ncommand\n, specifying the module’s module path and appending\n@none\n, as in the following example:\n$ go get example.com/theirmodule@none\nThe\ngo get\ncommand will also downgrade or remove other dependencies that\ndepend on the removed module.\nTool dependencies\nTool dependencies let you manage developer tools that are written in Go and used\nwhen working on your module. For example, you might use\nstringer\nwith\ngo generate\n, or a specific linter or formatter as part of\npreparing your change for submission.\nIn Go 1.24 and above, you can add a tool dependency with:\n$ go get -tool golang.org/x/tools/cmd/stringer\nThis will add a\ntool\ndirective\nto your\ngo.mod\nfile, and ensure the\nnecessary require directives are present. Once this directive is added you can\nrun the tool by passing the last\nnon-major-version\ncomponent of the tool’s import path to\ngo tool\n:\n$ go tool stringer\nIn the case that multiple tools share the last path fragment, or the path fragment\nmatches one of the tools shipped with the Go distribution, you must pass the full\npackage path instead:\n$ go tool golang.org/x/tools/cmd/stringer\nTo see a list of all tools currently available, run\ngo tool\nwith no arguments:\n$ go tool\nYou can manually add a\ntool\ndirective to your\ngo.mod\n, but you must ensure\nthat there is a\nrequire\ndirective for the module that defines the tool. The\neasiest way to add any missing\nrequire\ndirectives is to run:\n$ go mod tidy\nRequirements needed to satisfy tool dependencies behave like any other\nrequirements in your\nmodule graph\n. They\nparticipate in\nminimal version selection\nand respect\nrequire\n,\nreplace\nand\nexclude\ndirectives. Due to module\npruning, when you depend on a module that itself has a tool dependency,\nrequirements that exist just to satisfy that tool dependency do not usually\nbecome requirements of your module.\nThe\ntool\nmeta-pattern\nprovides a way to perform operations on all tools simultaneously. For example you can upgrade all tools with\ngo get -u tool\n, or install them all to $GOBIN with\ngo install tool\n.\nIn Go versions before 1.24, you can acheive something similar to a\ntool\ndirective by adding a blank import to a go file within the module that is\nexcluded from the build using\nbuild\nconstraints\n. If you do this, you can then\nuse\ngo run\nwith the full package path to run the tool.\nSpecifying a module proxy server\nWhen you use Go tools to work with modules, the tools by default download\nmodules from proxy.golang.org (a public Google-run module mirror) or directly\nfrom the module’s repository. You can specify that Go tools should instead use\nanother proxy server for downloading and authenticating modules.\nYou might want to do this if you (or your team) have set up or chosen a\ndifferent module proxy server that you want to use. For example, some set up a\nmodule proxy server in order to have greater control over how dependencies are\nused.\nTo specify another module proxy server for Go tools use, set the\nGOPROXY\nenvironment variable to the URL of one or more servers. Go tools will try each\nURL in the order you specify. By default,\nGOPROXY\nspecifies a public\nGoogle-run module proxy first, then direct download from the module’s repository\n(as specified in its module path):\nGOPROXY=\"https://proxy.golang.org,direct\"\nFor more about the\nGOPROXY\nenvironment variable, including values to support\nother behavior, see the\ngo\ncommand\nreference\n.\nYou can set the variable to URLs for other module proxy servers, separating URLs\nwith either a comma or a pipe.\nWhen you use a comma, Go tools will try the next URL in the list only if the\ncurrent URL returns an HTTP 404 or 410.\nGOPROXY=\"https://proxy.example.com,https://proxy2.example.com\"\nWhen you use a pipe, Go tools will try the next URL in the list regardless\nof the HTTP error code.\nGOPROXY=\"https://proxy.example.com|https://proxy2.example.com\"\nGo modules are frequently developed and distributed on version control servers\nand module proxies that aren’t available on the public internet. You can set the\nGOPRIVATE\nenvironment variable to configure the\ngo\ncommand\nto download and build modules from private sources.\nThen the go command can download and build modules from private sources.\nThe\nGOPRIVATE\nor\nGONOPROXY\nenvironment variables may be set to lists of glob\npatterns matching module prefixes that are private and should not be requested\nfrom any proxy. For example:\nGOPRIVATE=*.corp.example.com,*.research.example.com\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of GO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nCoverage profiling support for integration tests\nTable of Contents:\nOverview\nBuilding a binary for coverage profiling\nRunning a coverage-instrumented binary\nWorking with coverage data files\nFrequently Asked Questions\nResources\nGlossary\nBeginning in Go 1.20, Go supports collection of coverage profiles from applications and from integration tests, larger and more complex tests for Go programs.\nOverview\nGo provides easy-to-use support for collecting coverage profiles at the level of package unit tests via the “\ngo test -coverprofile=... <pkg_target>\n” command.\nStarting with Go 1.20, users can now collect coverage profiles for larger\nintegration tests\n: more heavy-weight, complex tests that perform multiple runs of a given application binary.\nFor unit tests, collecting a coverage profile and generating a report requires two steps: a\ngo test -coverprofile=...\nrun, followed by an invocation of\ngo tool cover {-func,-html}\nto generate a report.\nFor integration tests, three steps are needed: a\nbuild\nstep, a\nrun\nstep (which may involve multiple invocations of the binary from the build step), and finally a\nreporting\nstep, as described below.\nBuilding a binary for coverage profiling\nTo build an application for collecting coverage profiles, pass the\n-cover\nflag when invoking\ngo build\non your application binary target. See the section\nbelow\nfor a sample\ngo build -cover\ninvocation.\nThe resulting binary can then be run using an environment variable setting to capture coverage profiles (see the next section on\nrunning\n).\nHow packages are selected for instrumentation\nDuring a given “\ngo build -cover\n” invocation, the Go command will select packages in the main module for coverage profiling; other packages that feed into the build (dependencies listed in go.mod, or packages that are part of the Go standard library) will not be included by default.\nFor example, here is a toy program containing a main package, a local main-module package\ngreetings\nand a set of packages imported from outside the module, including (among others)\nrsc.io/quote\nand\nfmt\n(\nlink to full program\n).\n$ cat go.mod\nmodule mydomain.com\n\ngo 1.20\n\nrequire rsc.io/quote v1.5.2\n\nrequire (\n    golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c // indirect\n    rsc.io/sampler v1.3.0 // indirect\n)\n\n$ cat myprogram.go\npackage main\n\nimport (\n    \"fmt\"\n    \"mydomain.com/greetings\"\n    \"rsc.io/quote\"\n)\n\nfunc main() {\n    fmt.Printf(\"I say %q and %q\\n\", quote.Hello(), greetings.Goodbye())\n}\n$ cat greetings/greetings.go\npackage greetings\n\nfunc Goodbye() string {\n    return \"see ya\"\n}\n$ go build -cover -o myprogram.exe .\n$\nIf you build this program with the “\n-cover\n” command line flag and run it, exactly two packages will be included in the profile:\nmain\nand\nmydomain.com/greetings\n; the other dependent packages will be excluded.\nUsers who want to have more control over which packages are included for coverage can build with the “\n-coverpkg\n” flag. Example:\n$ go build -cover -o myprogramMorePkgs.exe -coverpkg=io,mydomain.com,rsc.io/quote .\n$\nIn the build above, the main package from\nmydomain.com\nas well as the\nrsc.io/quote\nand\nio\npackages are selected for profiling; since\nmydomain.com/greetings\nisn’t specifically listed, it will be excluded from the profile, even though it resides in the main module.\nRunning a coverage-instrumented binary\nBinaries built with “\n-cover\n” write out profile data files at the end of their execution to a directory specified via the environment variable\nGOCOVERDIR\n. Example:\n$ go build -cover -o myprogram.exe myprogram.go\n$ mkdir somedata\n$ GOCOVERDIR=somedata ./myprogram.exe\nI say \"Hello, world.\" and \"see ya\"\n$ ls somedata\ncovcounters.c6de772f99010ef5925877a7b05db4cc.2424989.1670252383678349347\ncovmeta.c6de772f99010ef5925877a7b05db4cc\n$\nNote the two files that were written to the directory\nsomedata\n: these (binary) files contain the coverage results.  See the following section on\nreporting\nfor more on how to produce human-readable results from these data files.\nIf the\nGOCOVERDIR\nenvironment variable is not set, a coverage-instrumented binary will still execute correctly, but will issue a warning.\nExample:\n$ ./myprogram.exe\nwarning: GOCOVERDIR not set, no coverage data emitted\nI say \"Hello, world.\" and \"see ya\"\n$\nTests involving multiple runs\nIntegration tests can in many cases involve multiple program runs; when the program is built with “\n-cover\n”, each run will produce a new data file. Example\n$ mkdir somedata2\n$ GOCOVERDIR=somedata2 ./myprogram.exe          // first run\nI say \"Hello, world.\" and \"see ya\"\n$ GOCOVERDIR=somedata2 ./myprogram.exe -flag    // second run\nI say \"Hello, world.\" and \"see ya\"\n$ ls somedata2\ncovcounters.890814fca98ac3a4d41b9bd2a7ec9f7f.2456041.1670259309405583534\ncovcounters.890814fca98ac3a4d41b9bd2a7ec9f7f.2456047.1670259309410891043\ncovmeta.890814fca98ac3a4d41b9bd2a7ec9f7f\n$\nCoverage data output files come in two flavors: meta-data files (containing the items that are invariant from run to run, such as source file names and function names), and counter data files (which record the parts of the program that executed).\nIn the example above, the first run produced two files (counter and meta), whereas the second run generated only a counter data file: since meta-data doesn’t change from run to run, it only needs to be written once.\nWorking with coverage data files\nGo 1.20 introduces a new tool, ‘\ncovdata\n’, that can be used to read and manipulate coverage data files from a\nGOCOVERDIR\ndirectory.\nGo’s\ncovdata\ntool runs in a variety of modes. The general form of a\ncovdata\ntool invocation takes the form\n$ go tool covdata <mode> -i=<dir1,dir2,...> ...flags...\nwhere the “\n-i\n” flag provides a list of directories to read, where each directories is derived from an execution of a coverage-instrumented binary (via\nGOCOVERDIR\n).\nCreating coverage profile reports\nThis section discusses how to use “\ngo tool covdata\n” to produce human-readable reports from coverage data files.\nReporting percent statements covered\nTo report a “percent statements covered” metric for each instrumented package, use the command “\ngo tool covdata percent -i=<directory>\n”.\nUsing the example from the\nrunning\nsection above:\n$ ls somedata\ncovcounters.c6de772f99010ef5925877a7b05db4cc.2424989.1670252383678349347\ncovmeta.c6de772f99010ef5925877a7b05db4cc\n$ go tool covdata percent -i=somedata\n    main    coverage: 100.0% of statements\n    mydomain.com/greetings  coverage: 100.0% of statements\n$\nThe “statements covered” percentages here correspond directly to those reported by\ngo test -cover\n.\nConverting to legacy text format\nYou can convert binary coverage data files into the legacy textual format generated by “\ngo test -coverprofile=<outfile>\n” using the covdata\ntextfmt\nselector. The resulting text file can then be used with “\ngo tool cover -func\n” or “\ngo tool cover -html\n” to create additional reports. Example:\n$ ls somedata\ncovcounters.c6de772f99010ef5925877a7b05db4cc.2424989.1670252383678349347\ncovmeta.c6de772f99010ef5925877a7b05db4cc\n$ go tool covdata textfmt -i=somedata -o profile.txt\n$ cat profile.txt\nmode: set\nmydomain.com/myprogram.go:10.13,12.2 1 1\nmydomain.com/greetings/greetings.go:3.23,5.2 1 1\n$ go tool cover -func=profile.txt\nmydomain.com/greetings/greetings.go:3:  Goodbye     100.0%\nmydomain.com/myprogram.go:10:       main        100.0%\ntotal:                  (statements)    100.0%\n$\nMerging\nThe\nmerge\nsubcommand of “\ngo tool covdata\n” can be used to merge together profiles from multiple data directories.\nFor example, consider a program that runs on both macOS and on Windows.\nThe author of this program might want to combine coverage profiles from separate\nruns on each operating system into a single profile corpus, so as to produce a\ncross-platform coverage summary.\nFor example:\n$ ls windows_datadir\ncovcounters.f3833f80c91d8229544b25a855285890.1025623.1667481441036838252\ncovcounters.f3833f80c91d8229544b25a855285890.1025628.1667481441042785007\ncovmeta.f3833f80c91d8229544b25a855285890\n$ ls macos_datadir\ncovcounters.b245ad845b5068d116a4e25033b429fb.1025358.1667481440551734165\ncovcounters.b245ad845b5068d116a4e25033b429fb.1025364.1667481440557770197\ncovmeta.b245ad845b5068d116a4e25033b429fb\n$ ls macos_datadir\n$ mkdir merged\n$ go tool covdata merge -i=windows_datadir,macos_datadir -o merged\n$\nThe merge operation above will combine the data from the specified input directories and write a new set of merged data files to the directory “merged”.\nPackage selection\nMost “\ngo tool covdata\n” commands support a “\n-pkg\n” flag to perform package selection as part of the operation; the argument to “\n-pkg\n” takes the same form as that used by the Go command’s “\n-coverpkg\n” flag.\nExample:\n$ ls somedata\ncovcounters.c6de772f99010ef5925877a7b05db4cc.2424989.1670252383678349347\ncovmeta.c6de772f99010ef5925877a7b05db4cc\n$ go tool covdata percent -i=somedata -pkg=mydomain.com/greetings\n    mydomain.com/greetings  coverage: 100.0% of statements\n$ go tool covdata percent -i=somedata -pkg=nonexistentpackage\n$\nThe “\n-pkg\n” flag can be used to select the specific subset of packages of interest for a given report.\nFrequently Asked Questions\nHow can I request coverage instrumentation for all imported packages mentioned in my\ngo.mod\nfile\nCan I use\ngo build -cover\nin GOPATH/GO111MODULE=off mode?\nIf my program panics, will coverage data be written?\nWill\n-coverpkg=main\nselect my main package for profiling?\nHow can I request coverage instrumentation for all imported packages mentioned in my\ngo.mod\nfile\nBy default,\ngo build -cover\nwill instrument all main module packages\nfor coverage, but will not instrument imports outside the main module\n(e.g. standard library packages or imports listed in\ngo.mod\n).\nOne way to request instrumentation for all non-stdlib dependencies\nis to feed the output of\ngo list\ninto\n-coverpkg\n.\nHere is an example, again using\nthe\nexample program\ncited above:\n$ go list -f '{{if not .Standard}}{{.ImportPath}}{{end}}' -deps . | paste -sd \",\" > pkgs.txt\n$ go build -o myprogram.exe -coverpkg=`cat pkgs.txt` .\n$ mkdir somedata\n$ GOCOVERDIR=somedata ./myprogram.exe\n$ go tool covdata percent -i=somedata\n    golang.org/x/text/internal/tag  coverage: 78.4% of statements\n    golang.org/x/text/language  coverage: 35.5% of statements\n    mydomain.com    coverage: 100.0% of statements\n    mydomain.com/greetings  coverage: 100.0% of statements\n    rsc.io/quote    coverage: 25.0% of statements\n    rsc.io/sampler  coverage: 86.7% of statements\n$\nCan I use\ngo build -cover\nin GO111MODULE=off mode?\nYes,\ngo build -cover\ndoes work with\nGO111MODULE=off\n.\nWhen building a program in GO111MODULE=off mode, only the package specifically named as the target on the command line will be instrumented for profiling. Use the\n-coverpkg\nflag to include additional packages in the profile.\nIf my program panics, will coverage data be written?\nPrograms built with\ngo build -cover\nwill only write out complete profile\ndata at the end of execution if the program invokes\nos.Exit()\nor returns\nnormally from\nmain.main\n.\nIf a program terminates in an unrecovered panic, or if the program hits a\nfatal exception (such as a segmentation violation, divide by zero, etc),\nprofile data from statements executed during the run will be lost.\nWill\n-coverpkg=main\nselect my main package for profiling?\nThe\n-coverpkg\nflag accepts a list of import paths, not a list of package names. If you want to select your\nmain\npackage for coverage instrumention, please identify it by import path, not by name. Example (using\nthis example program\n):\n$ go list -m\nmydomain.com\n$ go build -coverpkg=main -o oops.exe .\nwarning: no packages being built depend on matches for pattern main\n$ go build -coverpkg=mydomain.com -o myprogram.exe .\n$ mkdir somedata\n$ GOCOVERDIR=somedata ./myprogram.exe\nI say \"Hello, world.\" and \"see ya\"\n$ go tool covdata percent -i=somedata\n    mydomain.com    coverage: 100.0% of statements\n$\nResources\nBlog post introducing unit test coverage in Go 1.2\n:\nCoverage profiling for unit tests was introduced as part of the\nGo 1.2 release; see\nthis blog post\nfor details.\nDocumentation\n:\nThe\ncmd/go\npackage docs describe the\nbuild and test flags associated with coverage.\nTechnical details\n:\nDesign draft\nProposal\nGlossary\nunit test:\nTests within a\n*_test.go\nfile associated with a specific Go package, utilizing Go’s\ntesting\npackage.\nintegration test:\nA more comprehensive, heavier weight test for a given application or binary. Integration tests typically involve building a program or set of programs, then performing a series of runs of the programs using multiple inputs and scenarios, under control of a test harness that may or may not be based on Go’s\ntesting\npackage.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the GO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "go", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nWelcome\nWelcome to the\nHelm\ndocumentation. Helm is the package\nmanager for Kubernetes, and you can read detailed background information in the\nCNCF Helm Project Journey\nreport\n.\nHow the documentation is organized\nHelm has a lot of documentation. A high-level overview of how it’s organized\nwill help you know where to look for certain things:\nTutorials\ntake you by the hand through a series of steps to create\nyour first Helm chart. Start here if you’re new to Helm.\nTopic guides\ndiscuss key topics and concepts at a fairly high level\nand provide useful background information and explanation.\nCommunity Guides\ndiscuss topics centered around Helm’s community.\nStart here if you want to learn more about the development process of Helm\nitself and how you can contribute.\nHow-to guides\nare recipes. They guide you through the steps involved\nin addressing key problems and use-cases. They are more advanced than\ntutorials and assume some knowledge of how Helm works.\nQuicklinks\nQuickstart Guide\nHow to install and get started with Helm including instructions for distros, FAQs, and plugins.\nChart Guide\nPractical steps for building Helm charts—how to structure, sign, and sync your charts.\nVideo: Intro to Helm\nWatch Matt Farina and Josh Dolitsky present an introduction to Helm at KubeCon 2019.\nMigrating from v2 to v3\nRead our blog post on how to migrate from Helm v2 to Helm v3.\nHelm Security Audit\nHelm has been audited and deemed as recommended for public deployment during a third-party security audit funded by the CNCF.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nQuickstart Guide\nThis guide covers how you can quickly get started using Helm.\nPrerequisites\nThe following prerequisites are required for a successful and properly secured\nuse of Helm.\nA Kubernetes cluster\nDeciding what security configurations to apply to your installation, if any\nInstalling and configuring Helm.\nInstall Kubernetes or have access to a cluster\nYou must have Kubernetes installed. For the latest release of Helm, we\nrecommend the latest stable release of Kubernetes, which in most cases is the\nsecond-latest minor release.\nYou should also have a local configured copy of\nkubectl\n.\nSee the\nHelm Version Support Policy\nfor the maximum version skew supported between Helm and Kubernetes.\nInstall Helm\nDownload a binary release of the Helm client. You can use tools like\nhomebrew\n,\nor look at\nthe official releases page\n.\nFor more details, or for other options, see\nthe installation guide\n.\nInitialize a Helm Chart Repository\nOnce you have Helm ready, you can add a chart repository. Check\nArtifact\nHub\nfor available Helm chart\nrepositories.\n$\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nOnce this is installed, you will be able to list the charts you can install:\n$\nhelm search repo bitnami\nNAME                             \tCHART VERSION\tAPP VERSION  \tDESCRIPTION\nbitnami/bitnami-common           \t0.0.9        \t0.0.9        \tDEPRECATED Chart with custom templates used in ...\nbitnami/airflow                  \t8.0.2        \t2.0.0        \tApache Airflow is a platform to programmaticall...\nbitnami/apache                   \t8.2.3        \t2.4.46       \tChart for Apache HTTP Server\nbitnami/aspnet-core              \t1.2.3        \t3.1.9        \tASP.NET Core is an open-source framework create...\n#\n... and many more\nInstall an Example Chart\nTo install a chart, you can run the\nhelm install\ncommand. Helm has several\nways to find and install a chart, but the easiest is to use the\nbitnami\ncharts.\n$\nhelm repo update\n# Make sure we get the latest list of charts\n$\nhelm install bitnami/mysql --generate-name\nNAME: mysql-1612624192\nLAST DEPLOYED: Sat Feb  6 16:09:56 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES: ...\nIn the example above, the\nbitnami/mysql\nchart was released, and the name of\nour new release is\nmysql-1612624192\n.\nYou get a simple idea of the features of this MySQL chart by running\nhelm show chart bitnami/mysql\n. Or you could run\nhelm show all bitnami/mysql\nto get all\ninformation about the chart.\nWhenever you install a chart, a new release is created. So one chart can be\ninstalled multiple times into the same cluster. And each can be independently\nmanaged and upgraded.\nThe\nhelm install\ncommand is a very powerful command with many capabilities. To\nlearn more about it, check out the\nUsing Helm Guide\nLearn About Releases\nIt's easy to see what has been released using Helm:\n$\nhelm list\nNAME            \tNAMESPACE\tREVISION\tUPDATED                             \tSTATUS  \tCHART      \tAPP VERSION\nmysql-1612624192\tdefault  \t1       \t2021-02-06 16:09:56.283059 +0100 CET\tdeployed\tmysql-8.3.0\t8.0.23\nThe\nhelm list\n(or\nhelm ls\n) function will show you a list of all deployed releases.\nUninstall a Release\nTo uninstall a release, use the\nhelm uninstall\ncommand:\n$\nhelm uninstall mysql-1612624192\nrelease \"mysql-1612624192\" uninstalled\nThis will uninstall\nmysql-1612624192\nfrom Kubernetes, which will remove all\nresources associated with the release as well as the release history.\nIf the flag\n--keep-history\nis provided, release history will be kept. You will\nbe able to request information about that release:\n$\nhelm status mysql-1612624192\nStatus: UNINSTALLED\n...\nBecause Helm tracks your releases even after you've uninstalled them, you can\naudit a cluster's history, and even undelete a release (with\nhelm rollback\n).\nReading the Help Text\nTo learn more about the available Helm commands, use\nhelm help\nor type a\ncommand followed by the\n-h\nflag:\n$\nhelm get -h\nNext\nInstalling Helm →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nIntroduction to Helm\nAre you new to Helm? This is the place to start!\nQuicklinks\nQuickstart Guide\nHow to install and get started with Helm including instructions for distros, FAQs, and plugins.\nInstalling Helm\nLearn how to install and get running with Helm.\nUsing Helm\nExplains the basics of Helm.\nCheat Sheet\nHelm cheatsheet\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nQuickstart Guide\nThis guide covers how you can quickly get started using Helm.\nPrerequisites\nThe following prerequisites are required for a successful and properly secured\nuse of Helm.\nA Kubernetes cluster\nDeciding what security configurations to apply to your installation, if any\nInstalling and configuring Helm.\nInstall Kubernetes or have access to a cluster\nYou must have Kubernetes installed. For the latest release of Helm, we\nrecommend the latest stable release of Kubernetes, which in most cases is the\nsecond-latest minor release.\nYou should also have a local configured copy of\nkubectl\n.\nSee the\nHelm Version Support Policy\nfor the maximum version skew supported between Helm and Kubernetes.\nInstall Helm\nDownload a binary release of the Helm client. You can use tools like\nhomebrew\n,\nor look at\nthe official releases page\n.\nFor more details, or for other options, see\nthe installation guide\n.\nInitialize a Helm Chart Repository\nOnce you have Helm ready, you can add a chart repository. Check\nArtifact\nHub\nfor available Helm chart\nrepositories.\n$\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nOnce this is installed, you will be able to list the charts you can install:\n$\nhelm search repo bitnami\nNAME                             \tCHART VERSION\tAPP VERSION  \tDESCRIPTION\nbitnami/bitnami-common           \t0.0.9        \t0.0.9        \tDEPRECATED Chart with custom templates used in ...\nbitnami/airflow                  \t8.0.2        \t2.0.0        \tApache Airflow is a platform to programmaticall...\nbitnami/apache                   \t8.2.3        \t2.4.46       \tChart for Apache HTTP Server\nbitnami/aspnet-core              \t1.2.3        \t3.1.9        \tASP.NET Core is an open-source framework create...\n#\n... and many more\nInstall an Example Chart\nTo install a chart, you can run the\nhelm install\ncommand. Helm has several\nways to find and install a chart, but the easiest is to use the\nbitnami\ncharts.\n$\nhelm repo update\n# Make sure we get the latest list of charts\n$\nhelm install bitnami/mysql --generate-name\nNAME: mysql-1612624192\nLAST DEPLOYED: Sat Feb  6 16:09:56 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES: ...\nIn the example above, the\nbitnami/mysql\nchart was released, and the name of\nour new release is\nmysql-1612624192\n.\nYou get a simple idea of the features of this MySQL chart by running\nhelm show chart bitnami/mysql\n. Or you could run\nhelm show all bitnami/mysql\nto get all\ninformation about the chart.\nWhenever you install a chart, a new release is created. So one chart can be\ninstalled multiple times into the same cluster. And each can be independently\nmanaged and upgraded.\nThe\nhelm install\ncommand is a very powerful command with many capabilities. To\nlearn more about it, check out the\nUsing Helm Guide\nLearn About Releases\nIt's easy to see what has been released using Helm:\n$\nhelm list\nNAME            \tNAMESPACE\tREVISION\tUPDATED                             \tSTATUS  \tCHART      \tAPP VERSION\nmysql-1612624192\tdefault  \t1       \t2021-02-06 16:09:56.283059 +0100 CET\tdeployed\tmysql-8.3.0\t8.0.23\nThe\nhelm list\n(or\nhelm ls\n) function will show you a list of all deployed releases.\nUninstall a Release\nTo uninstall a release, use the\nhelm uninstall\ncommand:\n$\nhelm uninstall mysql-1612624192\nrelease \"mysql-1612624192\" uninstalled\nThis will uninstall\nmysql-1612624192\nfrom Kubernetes, which will remove all\nresources associated with the release as well as the release history.\nIf the flag\n--keep-history\nis provided, release history will be kept. You will\nbe able to request information about that release:\n$\nhelm status mysql-1612624192\nStatus: UNINSTALLED\n...\nBecause Helm tracks your releases even after you've uninstalled them, you can\naudit a cluster's history, and even undelete a release (with\nhelm rollback\n).\nReading the Help Text\nTo learn more about the available Helm commands, use\nhelm help\nor type a\ncommand followed by the\n-h\nflag:\n$\nhelm get -h\nNext\nInstalling Helm →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nInstalling Helm\nThis guide shows how to install the Helm CLI. Helm can be installed either from\nsource, or from pre-built binary releases.\nFrom The Helm Project\nThe Helm project provides two ways to fetch and install Helm. These are the\nofficial methods to get Helm releases. In addition to that, the Helm community\nprovides methods to install Helm through different package managers.\nInstallation through those methods can be found below the official methods.\nFrom the Binary Releases\nEvery\nrelease\nof Helm provides binary\nreleases for a variety of OSes. These binary versions can be manually downloaded\nand installed.\nDownload your\ndesired version\nUnpack it (\ntar -zxvf helm-v3.0.0-linux-amd64.tar.gz\n)\nFind the\nhelm\nbinary in the unpacked directory, and move it to its desired\ndestination (\nmv linux-amd64/helm /usr/local/bin/helm\n)\nFrom there, you should be able to run the client and\nadd the stable\nchart repository\n:\nhelm help\n.\nNote:\nHelm automated tests are performed for Linux AMD64 only during\nGitHub Actions builds and releases. Testing of other OSes are the responsibility of\nthe community requesting Helm for the OS in question.\nFrom Script\nHelm now has an installer script that will automatically grab the latest version\nof Helm and\ninstall it\nlocally\n.\nYou can fetch that script, and then execute it locally. It's well documented so\nthat you can read through it and understand what it is doing before you run it.\n$\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\n$\nchmod\n700\nget_helm.sh\n$\n./get_helm.sh\nYes, you can\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\nif\nyou want to live on the edge.\nThrough Package Managers\nThe Helm community provides the ability to install Helm through operating system\npackage managers. These are not supported by the Helm project and are not\nconsidered trusted 3rd parties.\nFrom Homebrew (macOS)\nMembers of the Helm community have contributed a Helm formula build to Homebrew.\nThis formula is generally up to date.\nbrew install helm\n(Note: There is also a formula for emacs-helm, which is a different project.)\nFrom Chocolatey (Windows)\nMembers of the Helm community have contributed a\nHelm\npackage\nbuild to\nChocolatey\n. This package is generally up to date.\nchoco install kubernetes-helm\nFrom Scoop (Windows)\nMembers of the Helm community have contributed a\nHelm\npackage\nbuild to\nScoop\n. This package is generally up to date.\nscoop install helm\nFrom Winget (Windows)\nMembers of the Helm community have contributed a\nHelm\npackage\nbuild to\nWinget\n. This package is generally up to date.\nwinget install Helm.Helm\nFrom Apt (Debian/Ubuntu)\nMembers of the Helm community have contributed a\nHelm\npackage\nfor Apt. This package is\ngenerally up to date.\ncurl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\nFrom dnf/yum (fedora)\nSince Fedora 35, helm is available on the official repository.\nYou can install helm with invoking:\nsudo dnf install helm\nFrom Snap\nThe\nSnapcrafters\ncommunity maintains the Snap\nversion of the\nHelm package\n:\nsudo snap install helm --classic\nFrom pkg (FreeBSD)\nMembers of the FreeBSD community have contributed a\nHelm\npackage\nbuild to the\nFreeBSD Ports Collection\n.\nThis package is generally up to date.\npkg install helm\nDevelopment Builds\nIn addition to releases you can download or install development snapshots of\nHelm.\nFrom Canary Builds\n\"Canary\" builds are versions of the Helm software that are built from the latest\nmain\nbranch. They are not official releases, and may not be stable. However,\nthey offer the opportunity to test the cutting edge features.\nCanary Helm binaries are stored at\nget.helm.sh\n. Here are\nlinks to the common builds:\nLinux AMD64\nmacOS AMD64\nExperimental Windows\nAMD64\nFrom Source (Linux, macOS)\nBuilding Helm from source is slightly more work, but is the best way to go if\nyou want to test the latest (pre-release) Helm version.\nYou must have a working Go environment.\n$\ngit clone https://github.com/helm/helm.git\n$\ncd\nhelm\n$\nmake\nIf required, it will fetch the dependencies and cache them, and validate\nconfiguration. It will then compile\nhelm\nand place it in\nbin/helm\n.\nConclusion\nIn most cases, installation is as simple as getting a pre-built\nhelm\nbinary.\nThis document covers additional cases for those who want to do more\nsophisticated things with Helm.\nOnce you have the Helm Client successfully installed, you can move on to using\nHelm to manage charts and\nadd the stable\nchart repository\n.\nPrev\n← Quickstart Guide\nNext\nUsing Helm →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nUsing Helm\nThis guide explains the basics of using Helm to manage packages on your\nKubernetes cluster. It assumes that you have already\ninstalled\nthe Helm client.\nIf you are simply interested in running a few quick commands, you may wish to\nbegin with the\nQuickstart Guide\n. This chapter\ncovers the particulars of Helm commands, and explains how to use Helm.\nThree Big Concepts\nA\nChart\nis a Helm package. It contains all of the resource definitions\nnecessary to run an application, tool, or service inside of a Kubernetes\ncluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an\nApt dpkg, or a Yum RPM file.\nA\nRepository\nis the place where charts can be collected and shared. It's like\nPerl's\nCPAN archive\nor the\nFedora Package\nDatabase\n, but for Kubernetes packages.\nA\nRelease\nis an instance of a chart running in a Kubernetes cluster. One chart\ncan often be installed many times into the same cluster. And each time it is\ninstalled, a new\nrelease\nis created. Consider a MySQL chart. If you want two\ndatabases running in your cluster, you can install that chart twice. Each one\nwill have its own\nrelease\n, which will in turn have its own\nrelease name\n.\nWith these concepts in mind, we can now explain Helm like this:\nHelm installs\ncharts\ninto Kubernetes, creating a new\nrelease\nfor each\ninstallation. And to find new charts, you can search Helm chart\nrepositories\n.\n'helm search': Finding Charts\nHelm comes with a powerful search command. It can be used to search two\ndifferent types of source:\nhelm search hub\nsearches\nthe Artifact Hub\n, which\nlists helm charts from dozens of different repositories.\nhelm search repo\nsearches the repositories that you have added to your local\nhelm client (with\nhelm repo add\n). This search is done over local data, and\nno public network connection is needed.\nYou can find publicly available charts by running\nhelm search hub\n:\n$\nhelm search hub wordpress\nURL                                                 CHART VERSION APP VERSION DESCRIPTION\nhttps://hub.helm.sh/charts/bitnami/wordpress        7.6.7         5.2.4       Web publishing platform for building blogs and ...\nhttps://hub.helm.sh/charts/presslabs/wordpress-...  v0.6.3        v0.6.3      Presslabs WordPress Operator Helm Chart\nhttps://hub.helm.sh/charts/presslabs/wordpress-...  v0.7.1        v0.7.1      A Helm chart for deploying a WordPress site on ...\nThe above searches for all\nwordpress\ncharts on Artifact Hub.\nWith no filter,\nhelm search hub\nshows you all of the available charts.\nhelm search hub\nexposes the URL to the location on\nartifacthub.io\nbut not the actual Helm repo.\nhelm search hub --list-repo-url\nexposes the actual Helm repo URL which comes in handy when you are looking to add a new repo:\nhelm repo add [NAME] [URL]\n.\nUsing\nhelm search repo\n, you can find the names of the charts in repositories\nyou have already added:\n$\nhelm repo add brigade https://brigadecore.github.io/charts\n\"brigade\" has been added to your repositories\n$\nhelm search repo brigade\nNAME                          CHART VERSION APP VERSION DESCRIPTION\nbrigade/brigade               1.3.2         v1.2.1      Brigade provides event-driven scripting of Kube...\nbrigade/brigade-github-app    0.4.1         v0.2.1      The Brigade GitHub App, an advanced gateway for...\nbrigade/brigade-github-oauth  0.2.0         v0.20.0     The legacy OAuth GitHub Gateway for Brigade\nbrigade/brigade-k8s-gateway   0.1.0                     A Helm chart for Kubernetes\nbrigade/brigade-project       1.0.0         v1.0.0      Create a Brigade project\nbrigade/kashti                0.4.0         v0.4.0      A Helm chart for Kubernetes\nHelm search uses a fuzzy string matching algorithm, so you can type parts of\nwords or phrases:\n$\nhelm search repo kash\nNAME            CHART VERSION APP VERSION DESCRIPTION\nbrigade/kashti  0.4.0         v0.4.0      A Helm chart for Kubernetes\nSearch is a good way to find available packages. Once you have found a package\nyou want to install, you can use\nhelm install\nto install it.\n'helm install': Installing a Package\nTo install a new package, use the\nhelm install\ncommand. At its simplest, it\ntakes two arguments: A release name that you pick, and the name of the chart you\nwant to install.\n$\nhelm install happy-panda bitnami/wordpress\nNAME: happy-panda\nLAST DEPLOYED: Tue Jan 26 10:27:17 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n** Please be patient while the chart is being deployed **\nYour WordPress site can be accessed through the following DNS name from within your cluster:\nhappy-panda-wordpress.default.svc.cluster.local (port 80)\nTo access your WordPress site from outside the cluster follow the steps below:\n1. Get the WordPress URL by running these commands:\nNOTE: It may take a few minutes for the LoadBalancer IP to be available.\nWatch the status with: 'kubectl get svc --namespace default -w happy-panda-wordpress'\nexport SERVICE_IP=$(kubectl get svc --namespace default happy-panda-wordpress --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\necho \"WordPress URL: http://$SERVICE_IP/\"\necho \"WordPress Admin URL: http://$SERVICE_IP/admin\"\n2. Open a browser and access WordPress using the obtained URL.\n3. Login with the following credentials below to see your blog:\necho Username: user\necho Password: $(kubectl get secret --namespace default happy-panda-wordpress -o jsonpath=\"{.data.wordpress-password}\" | base64 --decode)\nNow the\nwordpress\nchart is installed. Note that installing a chart creates a\nnew\nrelease\nobject. The release above is named\nhappy-panda\n. (If you want\nHelm to generate a name for you, leave off the release name and use\n--generate-name\n.)\nDuring installation, the\nhelm\nclient will print useful information about which\nresources were created, what the state of the release is, and also whether there\nare additional configuration steps you can or should take.\nHelm installs resources in the following order:\nNamespace\nNetworkPolicy\nResourceQuota\nLimitRange\nPodSecurityPolicy\nPodDisruptionBudget\nServiceAccount\nSecret\nSecretList\nConfigMap\nStorageClass\nPersistentVolume\nPersistentVolumeClaim\nCustomResourceDefinition\nClusterRole\nClusterRoleList\nClusterRoleBinding\nClusterRoleBindingList\nRole\nRoleList\nRoleBinding\nRoleBindingList\nService\nDaemonSet\nPod\nReplicationController\nReplicaSet\nDeployment\nHorizontalPodAutoscaler\nStatefulSet\nJob\nCronJob\nIngress\nAPIService\nHelm does not wait until all of the resources are running before it exits. Many\ncharts require Docker images that are over 600MB in size, and may take a long\ntime to install into the cluster.\nTo keep track of a release's state, or to re-read configuration information, you\ncan use\nhelm status\n:\n$\nhelm status happy-panda\nNAME: happy-panda\nLAST DEPLOYED: Tue Jan 26 10:27:17 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n** Please be patient while the chart is being deployed **\nYour WordPress site can be accessed through the following DNS name from within your cluster:\nhappy-panda-wordpress.default.svc.cluster.local (port 80)\nTo access your WordPress site from outside the cluster follow the steps below:\n1. Get the WordPress URL by running these commands:\nNOTE: It may take a few minutes for the LoadBalancer IP to be available.\nWatch the status with: 'kubectl get svc --namespace default -w happy-panda-wordpress'\nexport SERVICE_IP=$(kubectl get svc --namespace default happy-panda-wordpress --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\necho \"WordPress URL: http://$SERVICE_IP/\"\necho \"WordPress Admin URL: http://$SERVICE_IP/admin\"\n2. Open a browser and access WordPress using the obtained URL.\n3. Login with the following credentials below to see your blog:\necho Username: user\necho Password: $(kubectl get secret --namespace default happy-panda-wordpress -o jsonpath=\"{.data.wordpress-password}\" | base64 --decode)\nThe above shows the current state of your release.\nCustomizing the Chart Before Installing\nInstalling the way we have here will only use the default configuration options\nfor this chart. Many times, you will want to customize the chart to use your\npreferred configuration.\nTo see what options are configurable on a chart, use\nhelm show values\n:\n$\nhelm show values bitnami/wordpress\n#\n# Global Docker image parameters\n#\n# Please, note that this will override the image parameters, including dependencies, configured to use the global value\n#\n# Current available global Docker image parameters: imageRegistry and imagePullSecrets\n#\n#\n#\nglobal:\n#\nimageRegistry: myRegistryName\n#\nimagePullSecrets:\n#\n- myRegistryKeySecretName\n#\nstorageClass: myStorageClass\n#\n# Bitnami WordPress image version\n#\n# ref: https://hub.docker.com/r/bitnami/wordpress/tags/\n#\n#\nimage:\nregistry: docker.io\nrepository: bitnami/wordpress\ntag: 5.6.0-debian-10-r35\n[..]\nYou can then override any of these settings in a YAML formatted file, and then\npass that file during installation.\n$\necho\n'{mariadb.auth.database: user0db, mariadb.auth.username: user0}'\n> values.yaml\n$\nhelm install -f values.yaml bitnami/wordpress --generate-name\nThe above will create a default MariaDB user with the name\nuser0\n, and grant\nthis user access to a newly created\nuser0db\ndatabase, but will accept all the\nrest of the defaults for that chart.\nThere are two ways to pass configuration data during install:\n--values\n(or\n-f\n): Specify a YAML file with overrides. This can be\nspecified multiple times and the rightmost file will take precedence\n--set\n: Specify overrides on the command line.\nIf both are used,\n--set\nvalues are merged into\n--values\nwith higher\nprecedence. Overrides specified with\n--set\nare persisted in a Secret.\nValues that have been\n--set\ncan be viewed for a given release with\nhelm get values <release-name>\n. Values that have been\n--set\ncan be cleared by running\nhelm upgrade\nwith\n--reset-values\nspecified.\nThe Format and Limitations of\n--set\nThe\n--set\noption takes zero or more name/value pairs. At its simplest, it is\nused like this:\n--set name=value\n. The YAML equivalent of that is:\nname\n:\nvalue\nMultiple values are separated by\n,\ncharacters. So\n--set a=b,c=d\nbecomes:\na\n:\nb\nc\n:\nd\nMore complex expressions are supported. For example,\n--set outer.inner=value\nis translated into this:\nouter\n:\ninner\n:\nvalue\nLists can be expressed by enclosing values in\n{\nand\n}\n. For example,\n--set name={a, b, c}\ntranslates to:\nname\n:\n- a\n- b\n- c\nCertain name/key can be set to be\nnull\nor to be an empty array\n[]\n. For example,\n--set name=[],a=null\ntranslates\nname\n:\n- a\n- b\n- c\na\n:\nb\nto\nname\n:\n[]\na\n:\nnull\nAs of Helm 2.5.0, it is possible to access list items using an array index\nsyntax. For example,\n--set servers[0].port=80\nbecomes:\nservers\n:\n-\nport\n:\n80\nMultiple values can be set this way. The line\n--set servers[0].port=80,servers[0].host=example\nbecomes:\nservers\n:\n-\nport\n:\n80\nhost\n:\nexample\nSometimes you need to use special characters in your\n--set\nlines. You can use\na backslash to escape the characters;\n--set name=value1\\,value2\nwill become:\nname\n:\n\"value1,value2\"\nSimilarly, you can escape dot sequences as well, which may come in handy when\ncharts use the\ntoYaml\nfunction to parse annotations, labels and node\nselectors. The syntax for\n--set nodeSelector.\"kubernetes\\.io/role\"=master\nbecomes:\nnodeSelector\n:\nkubernetes.io/role\n:\nmaster\nDeeply nested data structures can be difficult to express using\n--set\n. Chart\ndesigners are encouraged to consider the\n--set\nusage when designing the format\nof a\nvalues.yaml\nfile (read more about\nValues Files\n).\nMore Installation Methods\nThe\nhelm install\ncommand can install from several sources:\nA chart repository (as we've seen above)\nA local chart archive (\nhelm install foo foo-0.1.1.tgz\n)\nAn unpacked chart directory (\nhelm install foo path/to/foo\n)\nA full URL (\nhelm install foo https://example.com/charts/foo-1.2.3.tgz\n)\n'helm upgrade' and 'helm rollback': Upgrading a Release, and Recovering on Failure\nWhen a new version of a chart is released, or when you want to change the\nconfiguration of your release, you can use the\nhelm upgrade\ncommand.\nAn upgrade takes an existing release and upgrades it according to the\ninformation you provide. Because Kubernetes charts can be large and complex,\nHelm tries to perform the least invasive upgrade. It will only update things\nthat have changed since the last release.\n$\nhelm upgrade -f panda.yaml happy-panda bitnami/wordpress\nIn the above case, the\nhappy-panda\nrelease is upgraded with the same chart,\nbut with a new YAML file:\nmariadb.auth.username\n:\nuser1\nWe can use\nhelm get values\nto see whether that new setting took effect.\n$\nhelm get values happy-panda\nmariadb:\nauth:\nusername: user1\nThe\nhelm get\ncommand is a useful tool for looking at a release in the cluster.\nAnd as we can see above, it shows that our new values from\npanda.yaml\nwere\ndeployed to the cluster.\nNow, if something does not go as planned during a release, it is easy to roll\nback to a previous release using\nhelm rollback [RELEASE] [REVISION]\n.\n$\nhelm rollback happy-panda\n1\nThe above rolls back our happy-panda to its very first release version. A\nrelease version is an incremental revision. Every time an install, upgrade, or\nrollback happens, the revision number is incremented by 1. The first revision\nnumber is always 1. And we can use\nhelm history [RELEASE]\nto see revision\nnumbers for a certain release.\nHelpful Options for Install/Upgrade/Rollback\nThere are several other helpful options you can specify for customizing the\nbehavior of Helm during an install/upgrade/rollback. Please note that this is\nnot a full list of cli flags. To see a description of all flags, just run\nhelm <command> --help\n.\n--timeout\n: A\nGo duration\nvalue\nto wait for Kubernetes commands to complete. This defaults to\n5m0s\n.\n--wait\n: Waits until all Pods are in a ready state, PVCs are bound,\nDeployments have minimum (\nDesired\nminus\nmaxUnavailable\n) Pods in ready\nstate and Services have an IP address (and Ingress if a\nLoadBalancer\n) before\nmarking the release as successful. It will wait for as long as the\n--timeout\nvalue. If timeout is reached, the release will be marked as\nFAILED\n. Note: In\nscenarios where Deployment has\nreplicas\nset to 1 and\nmaxUnavailable\nis not\nset to 0 as part of rolling update strategy,\n--wait\nwill return as ready as\nit has satisfied the minimum Pod in ready condition.\n--no-hooks\n: This skips running hooks for the command\n--recreate-pods\n(only available for\nupgrade\nand\nrollback\n): This flag\nwill cause all pods to be recreated (with the exception of pods belonging to\ndeployments). (DEPRECATED in Helm 3)\n'helm uninstall': Uninstalling a Release\nWhen it is time to uninstall a release from the cluster, use the\nhelm uninstall\ncommand:\n$\nhelm uninstall happy-panda\nThis will remove the release from the cluster. You can see all of your currently\ndeployed releases with the\nhelm list\ncommand:\n$\nhelm list\nNAME            VERSION UPDATED                         STATUS          CHART\ninky-cat        1       Wed Sep 28 12:59:46 2016        DEPLOYED        alpine-0.1.0\nFrom the output above, we can see that the\nhappy-panda\nrelease was\nuninstalled.\nIn previous versions of Helm, when a release was deleted, a record of its\ndeletion would remain. In Helm 3, deletion removes the release record as well.\nIf you wish to keep a deletion release record, use\nhelm uninstall --keep-history\n. Using\nhelm list --uninstalled\nwill only show releases that\nwere uninstalled with the\n--keep-history\nflag.\nThe\nhelm list --all\nflag will show you all release records that Helm has\nretained, including records for failed or deleted items (if\n--keep-history\nwas\nspecified):\n$\nhelm list --all\nNAME            VERSION UPDATED                         STATUS          CHART\nhappy-panda     2       Wed Sep 28 12:47:54 2016        UNINSTALLED     wordpress-10.4.5.6.0\ninky-cat        1       Wed Sep 28 12:59:46 2016        DEPLOYED        alpine-0.1.0\nkindred-angelf  2       Tue Sep 27 16:16:10 2016        UNINSTALLED     alpine-0.1.0\nNote that because releases are now deleted by default, it is no longer possible\nto rollback an uninstalled resource.\n'helm repo': Working with Repositories\nHelm 3 no longer ships with a default chart repository. The\nhelm repo\ncommand\ngroup provides commands to add, list, and remove repositories.\nYou can see which repositories are configured using\nhelm repo list\n:\n$\nhelm repo list\nNAME            URL\nstable          https://charts.helm.sh/stable\nmumoshu         https://mumoshu.github.io/charts\nAnd new repositories can be added with\nhelm repo add [NAME] [URL]\n:\n$\nhelm repo add dev https://example.com/dev-charts\nBecause chart repositories change frequently, at any point you can make sure\nyour Helm client is up to date by running\nhelm repo update\n.\nRepositories can be removed with\nhelm repo remove\n.\nCreating Your Own Charts\nThe\nChart Development Guide\nexplains how\nto develop your own charts. But you can get started quickly by using the\nhelm create\ncommand:\n$\nhelm create deis-workflow\nCreating deis-workflow\nNow there is a chart in\n./deis-workflow\n. You can edit it and create your own\ntemplates.\nAs you edit your chart, you can validate that it is well-formed by running\nhelm lint\n.\nWhen it's time to package the chart up for distribution, you can run the\nhelm package\ncommand:\n$\nhelm package deis-workflow\ndeis-workflow-0.1.0.tgz\nAnd that chart can now easily be installed by\nhelm install\n:\n$\nhelm install deis-workflow ./deis-workflow-0.1.0.tgz\n...\nCharts that are packaged can be loaded into chart repositories. See the\ndocumentation for\nHelm chart\nrepositories\nfor more details.\nConclusion\nThis chapter has covered the basic usage patterns of the\nhelm\nclient,\nincluding searching, installation, upgrading, and uninstalling. It has also\ncovered useful utility commands like\nhelm status\n,\nhelm get\n, and\nhelm repo\n.\nFor more information on these commands, take a look at Helm's built-in help:\nhelm help\n.\nIn the\nnext chapter\n, we look at the process of developing charts.\nPrev\n← Installing Helm\nNext\nCheat Sheet →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nCheat Sheet\nHelm cheatsheet featuring all the necessary commands required to manage an application through Helm.\nBasic interpretations/context\nChart:\nIt is the name of your chart in case it has been pulled and untarred.\nIt is <repo_name>/<chart_name> in case the repository has been added but chart not pulled.\nIt is the URL/Absolute path to the chart.\nName:\nIt is the name you want to give to your current helm chart installation.\nRelease:\nIs the name you assigned to an installation instance.\nRevision:\nIs the value from the Helm history command\nRepo-name:\nThe name of a repository.\nDIR:\nDirectory name/path\nChart Management\nhelm create <name>\n# Creates a chart directory along with the common files and directories used in a chart.\nhelm package <chart-path>\n# Packages a chart into a versioned chart archive file.\nhelm lint <chart>\n# Run tests to examine a chart and identify possible issues:\nhelm show all <chart>\n# Inspect a chart and list its contents:\nhelm show values <chart>\n# Displays the contents of the values.yaml file\nhelm pull <chart>\n# Download/pull chart\nhelm pull <chart> --untar\n=\ntrue\n# If set to true, will untar the chart after downloading it\nhelm pull <chart> --verify\n# Verify the package before using it\nhelm pull <chart> --version <number>\n# Default-latest is used, specify a version constraint for the chart version to use\nhelm dependency list <chart>\n# Display a list of a chart’s dependencies:\nInstall and Uninstall Apps\nhelm install <name> <chart>\n# Install the chart with a name\nhelm install <name> <chart> --namespace <namespace>\n# Install the chart in a specific namespace\nhelm install <name> <chart> --set\nkey1\n=\nval1,key2\n=\nval2\n# Set values on the command line (can specify multiple or separate values with commas)\nhelm install <name> <chart> --values <yaml-file/url>\n# Install the chart with your specified values\nhelm install <name> <chart> --dry-run --debug\n# Run a test installation to validate chart (p)\nhelm install <name> <chart> --verify\n# Verify the package before using it\nhelm install <name> <chart> --dependency-update\n# update dependencies if they are missing before installing the chart\nhelm uninstall <name>\n# Uninstall a release\nPerform App Upgrade and Rollback\nhelm upgrade <release> <chart>\n# Upgrade a release\nhelm upgrade <release> <chart> --atomic\n# If set, upgrade process rolls back changes made in case of failed upgrade.\nhelm upgrade <release> <chart> --dependency-update\n# update dependencies if they are missing before installing the chart\nhelm upgrade <release> <chart> --version <version_number>\n# specify a version constraint for the chart version to use\nhelm upgrade <release> <chart> --values\n# specify values in a YAML file or a URL (can specify multiple)\nhelm upgrade <release> <chart> --set\nkey1\n=\nval1,key2\n=\nval2\n# Set values on the command line (can specify multiple or separate valuese)\nhelm upgrade <release> <chart> --force\n# Force resource updates through a replacement strategy\nhelm rollback <release> <revision>\n# Roll back a release to a specific revision\nhelm rollback <release> <revision>  --cleanup-on-fail\n# Allow deletion of new resources created in this rollback when rollback fails\nList, Add, Remove, and Update Repositories\nhelm repo add <repo-name> <url>\n# Add a repository from the internet:\nhelm repo list\n# List added chart repositories\nhelm repo update\n# Update information of available charts locally from chart repositories\nhelm repo remove <repo_name>\n# Remove one or more chart repositories\nhelm repo index <DIR>\n# Read the current directory and generate an index file based on the charts found.\nhelm repo index <DIR> --merge\n# Merge the generated index with an existing index file\nhelm search repo <keyword>\n# Search repositories for a keyword in charts\nhelm search hub <keyword>\n# Search for charts in the Artifact Hub or your own hub instance\nHelm Release monitoring\nhelm list\n# Lists all of the releases for a specified namespace, uses current namespace context if namespace not specified\nhelm list --all\n# Show all releases without any filter applied, can use -a\nhelm list --all-namespaces\n# List releases across all namespaces, we can use -A\nhelm list -l\nkey1\n=\nvalue1,key2\n=\nvalue2\n# Selector (label query) to filter on, supports '=', '==', and '!='\nhelm list --date\n# Sort by release date\nhelm list --deployed\n# Show deployed releases. If no other is specified, this will be automatically enabled\nhelm list --pending\n# Show pending releases\nhelm list --failed\n# Show failed releases\nhelm list --uninstalled\n# Show uninstalled releases (if 'helm uninstall --keep-history' was used)\nhelm list --superseded\n# Show superseded releases\nhelm list -o yaml\n# Prints the output in the specified format. Allowed values: table, json, yaml (default table)\nhelm status <release>\n# This command shows the status of a named release.\nhelm status <release> --revision <number>\n# if set, display the status of the named release with revision\nhelm\nhistory\n<release>\n# Historical revisions for a given release.\nhelm env\n# Env prints out all the environment information in use by Helm.\nDownload Release Information\nhelm get all <release>\n# A human readable collection of information about the notes, hooks, supplied values, and generated manifest file of the given release.\nhelm get hooks <release>\n# This command downloads hooks for a given release. Hooks are formatted in YAML and separated by the YAML '---\\n' separator.\nhelm get manifest <release>\n# A manifest is a YAML-encoded representation of the Kubernetes resources that were generated from this release's chart(s). If a chart is dependent on other charts, those resources will also be included in the manifest.\nhelm get notes <release>\n# Shows notes provided by the chart of a named release.\nhelm get values <release>\n# Downloads a values file for a given release. use -o to format output\nPlugin Management\nhelm plugin install <path/url>\n# Install plugins\nhelm plugin list\n# View a list of all installed plugins\nhelm plugin update <plugin>\n# Update plugins\nhelm plugin uninstall <plugin>\n# Uninstall a plugin\nPrev\n← Using Helm\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nHow-to Guides\nHere you’ll find short answers to “How do I….?” types of questions. These how-to\nguides don’t cover topics in depth – you’ll find that material in the\nTopic\nGuides\n. However, these guides will help you quickly accomplish\ncommon tasks.\nQuicklinks\nChart Development Tips and Tricks\nCovers some of the tips and tricks Helm chart developers have learned while building production-quality charts.\nSyncing Your Chart Repository\nDescribes how to synchronize your local and remote chart repositories.\nChart Releaser Action to Automate GitHub Page Charts\nDescribe how to use Chart Releaser Action to automate releasing charts through GitHub pages.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nChart Development Tips and Tricks\nThis guide covers some of the tips and tricks Helm chart developers have learned\nwhile building production-quality charts.\nKnow Your Template Functions\nHelm uses\nGo templates\nfor templating your\nresource files. While Go ships several built-in functions, we have added many\nothers.\nFirst, we added all of the functions in the\nSprig\nlibrary\n, except\nenv\nand\nexpandenv\n, for security reasons.\nWe also added two special template functions:\ninclude\nand\nrequired\n. The\ninclude\nfunction allows you to bring in another template, and then pass the\nresults to other template functions.\nFor example, this template snippet includes a template called\nmytpl\n, then\nlowercases the result, then wraps that in double quotes.\nvalue\n:\n{{\ninclude \"mytpl\" . | lower | quote }}\nThe\nrequired\nfunction allows you to declare a particular values entry as\nrequired for template rendering. If the value is empty, the template rendering\nwill fail with a user submitted error message.\nThe following example of the\nrequired\nfunction declares an entry for\n.Values.who\nis required, and will print an error message when that entry is\nmissing:\nvalue\n:\n{{\nrequired \"A valid .Values.who entry required!\" .Values.who }}\nQuote Strings, Don't Quote Integers\nWhen you are working with string data, you are always safer quoting the strings\nthan leaving them as bare words:\nname\n:\n{{\n.Values.MyName | quote }}\nBut when working with integers\ndo not quote the values.\nThat can, in many\ncases, cause parsing errors inside of Kubernetes.\nport\n:\n{{\n.Values.Port }}\nThis remark does not apply to env variables values which are expected to be\nstring, even if they represent integers:\nenv\n:\n-\nname\n:\nHOST\nvalue\n:\n\"http://host\"\n-\nname\n:\nPORT\nvalue\n:\n\"1234\"\nUsing the 'include' Function\nGo provides a way of including one template in another using a built-in\ntemplate\ndirective. However, the built-in function cannot be used in Go\ntemplate pipelines.\nTo make it possible to include a template, and then perform an operation on that\ntemplate's output, Helm has a special\ninclude\nfunction:\n{{ include \"toYaml\" $value | indent 2 }}\nThe above includes a template called\ntoYaml\n, passes it\n$value\n, and then\npasses the output of that template to the\nindent\nfunction.\nBecause YAML ascribes significance to indentation levels and whitespace, this is\none great way to include snippets of code, but handle indentation in a relevant\ncontext.\nUsing the 'required' function\nGo provides a way for setting template options to control behavior when a map is\nindexed with a key that's not present in the map. This is typically set with\ntemplate.Options(\"missingkey=option\")\n, where\noption\ncan be\ndefault\n,\nzero\n, or\nerror\n. While setting this option to error will stop execution with\nan error, this would apply to every missing key in the map. There may be\nsituations where a chart developer wants to enforce this behavior for select\nvalues in the\nvalues.yaml\nfile.\nThe\nrequired\nfunction gives developers the ability to declare a value entry as\nrequired for template rendering. If the entry is empty in\nvalues.yaml\n, the\ntemplate will not render and will return an error message supplied by the\ndeveloper.\nFor example:\n{{ required \"A valid foo is required!\" .Values.foo }}\nThe above will render the template when\n.Values.foo\nis defined, but will fail\nto render and exit when\n.Values.foo\nis undefined.\nUsing the 'tpl' Function\nThe\ntpl\nfunction allows developers to evaluate strings as templates inside a\ntemplate. This is useful to pass a template string as a value to a chart or\nrender external configuration files. Syntax:\n{{ tpl TEMPLATE_STRING VALUES }}\nExamples:\n# values\ntemplate\n:\n\"{{ .Values.name }}\"\nname\n:\n\"Tom\"\n# template\n{{\ntpl .Values.template . }}\n# output\nTom\nRendering an external configuration file:\n# external configuration file conf/app.conf\nfirstName={{ .Values.firstName }}\nlastName={{ .Values.lastName }}\n# values\nfirstName\n:\nPeter\nlastName\n:\nParker\n# template\n{{\ntpl (.Files.Get \"conf/app.conf\") . }}\n# output\nfirstName=Peter\nlastName=Parker\nCreating Image Pull Secrets\nImage pull secrets are essentially a combination of\nregistry\n,\nusername\n, and\npassword\n. You may need them in an application you are deploying, but to\ncreate them requires running\nbase64\na couple of times. We can write a helper\ntemplate to compose the Docker configuration file for use as the Secret's\npayload. Here is an example:\nFirst, assume that the credentials are defined in the\nvalues.yaml\nfile like\nso:\nimageCredentials\n:\nregistry\n:\nquay.io\nusername\n:\nsomeone\npassword\n:\nsillyness\nemail\n:\nsomeone@host.com\nWe then define our helper template as follows:\n{{- define \"imagePullSecret\" }}\n{{- with .Values.imageCredentials }}\n{{- printf \"{\\\"auths\\\":{\\\"%s\\\":{\\\"username\\\":\\\"%s\\\",\\\"password\\\":\\\"%s\\\",\\\"email\\\":\\\"%s\\\",\\\"auth\\\":\\\"%s\\\"}}}\" .registry .username .password .email (printf \"%s:%s\" .username .password | b64enc) | b64enc }}\n{{- end }}\n{{- end }}\nFinally, we use the helper template in a larger template to create the Secret\nmanifest:\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\nname\n:\nmyregistrykey\ntype\n:\nkubernetes.io/dockerconfigjson\ndata\n:\n.dockerconfigjson\n:\n{{\ntemplate \"imagePullSecret\" . }}\nAutomatically Roll Deployments\nOften times ConfigMaps or Secrets are injected as configuration files in\ncontainers or there are other external dependency changes that require rolling\npods. Depending on the application a restart may be required should those be\nupdated with a subsequent\nhelm upgrade\n, but if the deployment spec itself\ndidn't change the application keeps running with the old configuration resulting\nin an inconsistent deployment.\nThe\nsha256sum\nfunction can be used to ensure a deployment's annotation section\nis updated if another file changes:\nkind\n:\nDeployment\nspec\n:\ntemplate\n:\nmetadata\n:\nannotations\n:\nchecksum/config\n:\n{{\ninclude (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n[...]\nNOTE: If you're adding this to a library chart you won't be able to access your\nfile in\n$.Template.BasePath\n. Instead you can reference your definition with\n{{ include (\"mylibchart.configmap\") . | sha256sum }}\n.\nIn the event you always want to roll your deployment, you can use a similar\nannotation step as above, instead replacing with a random string so it always\nchanges and causes the deployment to roll:\nkind\n:\nDeployment\nspec\n:\ntemplate\n:\nmetadata\n:\nannotations\n:\nrollme\n:\n{{\nrandAlphaNum 5 | quote }}\n[...]\nEach invocation of the template function will generate a unique random string.\nThis means that if it's necessary to sync the random strings used by multiple\nresources, all relevant resources will need to be in the same template file.\nBoth of these methods allow your Deployment to leverage the built in update\nstrategy logic to avoid taking downtime.\nNOTE: In the past we recommended using the\n--recreate-pods\nflag as another\noption. This flag has been marked as deprecated in Helm 3 in favor of the more\ndeclarative method above.\nTell Helm Not To Uninstall a Resource\nSometimes there are resources that should not be uninstalled when Helm runs a\nhelm uninstall\n. Chart developers can add an annotation to a resource to\nprevent it from being uninstalled.\nkind\n:\nSecret\nmetadata\n:\nannotations\n:\nhelm.sh/resource-policy\n:\nkeep\n[...]\nThe annotation\nhelm.sh/resource-policy: keep\ninstructs Helm to skip deleting\nthis resource when a helm operation (such as\nhelm uninstall\n,\nhelm upgrade\nor\nhelm rollback\n) would result in its deletion.\nHowever\n, this resource becomes\norphaned. Helm will no longer manage it in any way. This can lead to problems if\nusing\nhelm install --replace\non a release that has already been uninstalled,\nbut has kept resources.\nUsing \"Partials\" and Template Includes\nSometimes you want to create some reusable parts in your chart, whether they're\nblocks or template partials. And often, it's cleaner to keep these in their own\nfiles.\nIn the\ntemplates/\ndirectory, any file that begins with an underscore(\n_\n) is\nnot expected to output a Kubernetes manifest file. So by convention, helper\ntemplates and partials are placed in a\n_helpers.tpl\nfile.\nComplex Charts with Many Dependencies\nMany of the charts in the CNCF\nArtifact\nHub\nare \"building blocks\" for\ncreating more advanced applications. But charts may be used to create instances\nof large-scale applications. In such cases, a single umbrella chart may have\nmultiple subcharts, each of which functions as a piece of the whole.\nThe current best practice for composing a complex application from discrete\nparts is to create a top-level umbrella chart that exposes the global\nconfigurations, and then use the\ncharts/\nsubdirectory to embed each of the\ncomponents.\nYAML is a Superset of JSON\nAccording to the YAML specification, YAML is a superset of JSON. That means that\nany valid JSON structure ought to be valid in YAML.\nThis has an advantage: Sometimes template developers may find it easier to\nexpress a datastructure with a JSON-like syntax rather than deal with YAML's\nwhitespace sensitivity.\nAs a best practice, templates should follow a YAML-like syntax\nunless\nthe JSON\nsyntax substantially reduces the risk of a formatting issue.\nBe Careful with Generating Random Values\nThere are functions in Helm that allow you to generate random data,\ncryptographic keys, and so on. These are fine to use. But be aware that during\nupgrades, templates are re-executed. When a template run generates data that\ndiffers from the last run, that will trigger an update of that resource.\nInstall or Upgrade a Release with One Command\nHelm provides a way to perform an install-or-upgrade as a single command. Use\nhelm upgrade\nwith the\n--install\ncommand. This will cause Helm to see if the\nrelease is already installed. If not, it will run an install. If it is, then the\nexisting release will be upgraded.\n$\nhelm upgrade --install <release name> --values <values file> <chart directory>\nNext\nSyncing Your Chart Repository →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nSyncing Your Chart Repository\nNote: This example is specifically for a Google Cloud Storage (GCS) bucket\nwhich serves a chart repository.\nPrerequisites\nInstall the\ngsutil\ntool.\nWe\nrely heavily on the gsutil rsync functionality\nBe sure to have access to the Helm binary\nOptional: We recommend you set\nobject\nversioning\non your GCS bucket in case you accidentally delete something.\nSet up a local chart repository directory\nCreate a local directory like we did in\nthe chart repository guide\n, and place your packaged charts in that\ndirectory.\nFor example:\n$\nmkdir fantastic-charts\n$\nmv alpine-0.1.0.tgz fantastic-charts/\nGenerate an updated index.yaml\nUse Helm to generate an updated index.yaml file by passing in the directory path\nand the url of the remote repository to the\nhelm repo index\ncommand like this:\n$\nhelm repo index fantastic-charts/ --url https://fantastic-charts.storage.googleapis.com\nThis will generate an updated index.yaml file and place it in the\nfantastic-charts/\ndirectory.\nSync your local and remote chart repositories\nUpload the contents of the directory to your GCS bucket by running\nscripts/sync-repo.sh\nand pass in the local directory name and the GCS bucket\nname.\nFor example:\n$\npwd\n/Users/me/code/go/src/helm.sh/helm\n$\nscripts/sync-repo.sh fantastic-charts/ fantastic-charts\nGetting ready to sync your local directory (fantastic-charts/) to a remote repository at gs://fantastic-charts\nVerifying Prerequisites....\nThumbs up! Looks like you have gsutil. Let's continue.\nBuilding synchronization state...\nStarting synchronization\nWould copy file://fantastic-charts/alpine-0.1.0.tgz to gs://fantastic-charts/alpine-0.1.0.tgz\nWould copy file://fantastic-charts/index.yaml to gs://fantastic-charts/index.yaml\nAre you sure you would like to continue with these changes?? [y/N]} y\nBuilding synchronization state...\nStarting synchronization\nCopying file://fantastic-charts/alpine-0.1.0.tgz [Content-Type=application/x-tar]...\nUploading   gs://fantastic-charts/alpine-0.1.0.tgz:              740 B/740 B\nCopying file://fantastic-charts/index.yaml [Content-Type=application/octet-stream]...\nUploading   gs://fantastic-charts/index.yaml:                    347 B/347 B\nCongratulations your remote chart repository now matches the contents of fantastic-charts/\nUpdating your chart repository\nYou'll want to keep a local copy of the contents of your chart repository or use\ngsutil rsync\nto copy the contents of your remote chart repository to a local\ndirectory.\nFor example:\n$\ngsutil rsync -d -n gs://bucket-name local-dir/\n# the -n flag does a dry run\nBuilding synchronization state...\nStarting synchronization\nWould copy gs://bucket-name/alpine-0.1.0.tgz to file://local-dir/alpine-0.1.0.tgz\nWould copy gs://bucket-name/index.yaml to file://local-dir/index.yaml\n$\ngsutil rsync -d gs://bucket-name local-dir/\n# performs the copy actions\nBuilding synchronization state...\nStarting synchronization\nCopying gs://bucket-name/alpine-0.1.0.tgz...\nDownloading file://local-dir/alpine-0.1.0.tgz:                        740 B/740 B\nCopying gs://bucket-name/index.yaml...\nDownloading file://local-dir/index.yaml:                              346 B/346 B\nHelpful Links:\nDocumentation on\ngsutil\nrsync\nThe Chart Repository Guide\nDocumentation on\nobject versioning and concurrency\ncontrol\nin Google Cloud Storage\nPrev\n← Chart Development Tips and Tricks\nNext\nChart Releaser Action to Automate GitHub Page Charts →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nChart Releaser Action to Automate GitHub Page Charts\nThis guide describes how to use\nChart Releaser\nAction\nto automate\nreleasing charts through GitHub pages. Chart Releaser Action is a GitHub Action\nworkflow to turn a GitHub project into a self-hosted Helm chart repo, using\nhelm/chart-releaser\nCLI tool.\nRepository Changes\nCreate a Git repository under your GitHub organization. You could give the name\nof the repository as\nhelm-charts\n, though other names are also acceptable. The\nsources of all the charts can be placed under the\nmain\nbranch. The charts\nshould be placed under\n/charts\ndirectory at the top-level of the directory\ntree.\nThere should be another branch named\ngh-pages\nto publish the charts. The\nchanges to that branch will be automatically created by the Chart Releaser\nAction described here. However, you can create that\ngh-branch\nand add\nREADME.md\nfile, which is going to be visible to the users visiting the page.\nYou can add instruction in the\nREADME.md\nfor charts installation like this\n(replace\n<alias>\n,\n<orgname>\n, and\n<chart-name>\n):\n## Usage\n[Helm](https://helm.sh) must be installed to use the charts.  Please refer to\nHelm's [documentation](https://helm.sh/docs) to get started.\nOnce Helm has been set up correctly, add the repo as follows:\nhelm repo add <alias> https://<orgname>.github.io/helm-charts\nIf you had already added this repo earlier, run `helm repo update` to retrieve\nthe latest versions of the packages.  You can then run `helm search repo\n<alias>` to see the charts.\nTo install the <chart-name> chart:\nhelm install my-<chart-name> <alias>/<chart-name>\nTo uninstall the chart:\nhelm uninstall my-<chart-name>\nThe charts will be published to a website with URL like this:\nhttps://<orgname>.github.io/helm-charts\nGitHub Actions Workflow\nCreate GitHub Actions workflow file in the\nmain\nbranch at\n.github/workflows/release.yml\nname: Release Charts\non:\npush:\nbranches:\n- main\njobs:\nrelease:\npermissions:\ncontents: write\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v4\nwith:\nfetch-depth: 0\n- name: Configure Git\nrun: |\ngit config user.name \"$GITHUB_ACTOR\"\ngit config user.email \"$GITHUB_ACTOR@users.noreply.github.com\"\n- name: Run chart-releaser\nuses: helm/chart-releaser-action@v1.6.0\nenv:\nCR_TOKEN: \"${{ secrets.GITHUB_TOKEN }}\"\nThe above configuration uses\n@helm/chart-releaser-action\nto\nturn your GitHub project into a self-hosted Helm chart repo. It does this -\nduring every push to main - by checking each chart in your project, and whenever\nthere's a new chart version, creates a corresponding GitHub release named for\nthe chart version, adds Helm chart artifacts to the release, and creates or\nupdates an\nindex.yaml\nfile with metadata about those releases, which is then\nhosted on GitHub pages.\nThe Chart Releaser Action version number used in the above example is\nv1.6.0\n.\nYou can change it to the\nlatest available\nversion\n.\nNote: The Chart Releaser Action is almost always used in tandem with the\nHelm Testing\nAction\nand\nKind\nAction\n.\nPrev\n← Syncing Your Chart Repository\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nTopic Guides\nHere you’ll find introductions to all the key parts of Helm you’ll need or want\nto know.\nQuicklinks\nCharts\nExplains the chart format, and provides basic guidance for building charts with Helm.\nChart Hooks\nDescribes how to work with chart hooks.\nChart Tests\nDescribes how to run and test your charts.\nLibrary Charts\nExplains library charts and examples of usage\nHelm Provenance and Integrity\nDescribes how to verify the integrity and origin of a Chart.\nThe Chart Repository Guide\nHow to create and work with Helm chart repositories.\nUse OCI-based registries\nDescribes how to use OCI for Chart distribution.\nHelm Architecture\nDescribes the Helm architecture at a high level.\nAdvanced Helm Techniques\nExplains various advanced features for Helm power users\nKubernetes Distribution Guide\nCaptures information about using Helm in specific Kubernetes environments.\nRole-based Access Control\nExplains how Helm interacts with Kubernetes' Role-Based Access Control.\nThe Helm Plugins Guide\nIntroduces how to use and create plugins to extend Helm's functionality.\nMigrating Helm v2 to v3\nLearn how to migrate Helm v2 to v3.\nDeprecated Kubernetes APIs\nExplains deprecated Kubernetes APIs in Helm\nHelm Version Support Policy\nDescribes Helm's patch release policy as well as the maximum version skew supported between Helm and Kubernetes.\nPermissions management for SQL storage backend\nGet to know how to setup permissions when using SQL storage backend.\nRelease schedule policy\nDescribes Helm's release schedule policy.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nCharts\nHelm uses a packaging format called\ncharts\n. A chart is a collection of files\nthat describe a related set of Kubernetes resources. A single chart might be\nused to deploy something simple, like a memcached pod, or something complex,\nlike a full web app stack with HTTP servers, databases, caches, and so on.\nCharts are created as files laid out in a particular directory tree. They can be\npackaged into versioned archives to be deployed.\nIf you want to download and look at the files for a published chart, without\ninstalling it, you can do so with\nhelm pull chartrepo/chartname\n.\nThis document explains the chart format, and provides basic guidance for\nbuilding charts with Helm.\nThe Chart File Structure\nA chart is organized as a collection of files inside of a directory. The\ndirectory name is the name of the chart (without versioning information). Thus,\na chart describing WordPress would be stored in a\nwordpress/\ndirectory.\nInside of this directory, Helm will expect a structure that matches this:\nwordpress/\nChart.yaml          # A YAML file containing information about the chart\nLICENSE             # OPTIONAL: A plain text file containing the license for the chart\nREADME.md           # OPTIONAL: A human-readable README file\nvalues.yaml         # The default configuration values for this chart\nvalues.schema.json  # OPTIONAL: A JSON Schema for imposing a structure on the values.yaml file\ncharts/             # A directory containing any charts upon which this chart depends.\ncrds/               # Custom Resource Definitions\ntemplates/          # A directory of templates that, when combined with values,\n# will generate valid Kubernetes manifest files.\ntemplates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes\nHelm reserves use of the\ncharts/\n,\ncrds/\n, and\ntemplates/\ndirectories, and\nof the listed file names. Other files will be left as they are.\nThe Chart.yaml File\nThe\nChart.yaml\nfile is required for a chart. It contains the following fields:\napiVersion\n:\nThe chart API version (required)\nname\n:\nThe name of the chart (required)\nversion\n:\nA SemVer 2 version (required)\nkubeVersion\n:\nA SemVer range of compatible Kubernetes versions (optional)\ndescription\n:\nA single-sentence description of this project (optional)\ntype\n:\nThe type of the chart (optional)\nkeywords\n:\n- A list of keywords about this project (optional)\nhome\n:\nThe URL of this projects home page (optional)\nsources\n:\n- A list of URLs to source code for this project (optional)\ndependencies\n:\n# A list of the chart requirements (optional)\n-\nname\n:\nThe name of the chart (nginx)\nversion\n:\nThe version of the chart (\"1.2.3\")\nrepository\n:\n(optional) The repository URL (\"https://example.com/charts\") or alias (\"@repo-name\")\ncondition\n:\n(optional) A yaml path that resolves to a boolean, used for enabling/disabling charts (e.g. subchart1.enabled )\ntags\n:\n# (optional)\n- Tags can be used to group charts for enabling/disabling together\nimport-values\n:\n# (optional)\n- ImportValues holds the mapping of source values to parent key to be imported. Each item can be a string or pair of child/parent sublist items.\nalias\n:\n(optional) Alias to be used for the chart. Useful when you have to add the same chart multiple times\nmaintainers\n:\n# (optional)\n-\nname\n:\nThe maintainers name (required for each maintainer)\nemail\n:\nThe maintainers email (optional for each maintainer)\nurl\n:\nA URL for the maintainer (optional for each maintainer)\nicon\n:\nA URL to an SVG or PNG image to be used as an icon (optional).\nappVersion\n:\nThe version of the app that this contains (optional). Needn't be SemVer. Quotes recommended.\ndeprecated\n:\nWhether this chart is deprecated (optional, boolean)\nannotations\n:\nexample\n:\nA list of annotations keyed by name (optional).\nAs of\nv3.3.2\n, additional\nfields are not allowed.\nThe recommended approach is to add custom metadata in\nannotations\n.\nCharts and Versioning\nEvery chart must have a version number. A version must follow the\nSemVer\n2\nstandard. Unlike Helm Classic, Helm v2\nand later uses version numbers as release markers. Packages in repositories are\nidentified by name plus version.\nFor example, an\nnginx\nchart whose version field is set to\nversion: 1.2.3\nwill be named:\nnginx-1.2.3.tgz\nMore complex SemVer 2 names are also supported, such as\nversion: 1.2.3-alpha.1+ef365\n. But non-SemVer names are explicitly disallowed by the\nsystem.\nNOTE:\nWhereas Helm Classic and Deployment Manager were both very GitHub\noriented when it came to charts, Helm v2 and later does not rely upon or require\nGitHub or even Git. Consequently, it does not use Git SHAs for versioning at\nall.\nThe\nversion\nfield inside of the\nChart.yaml\nis used by many of the Helm\ntools, including the CLI. When generating a package, the\nhelm package\ncommand\nwill use the version that it finds in the\nChart.yaml\nas a token in the package\nname. The system assumes that the version number in the chart package name\nmatches the version number in the\nChart.yaml\n. Failure to meet this assumption\nwill cause an error.\nThe\napiVersion\nField\nThe\napiVersion\nfield should be\nv2\nfor Helm charts that require at least Helm\n3. Charts supporting previous Helm versions have an\napiVersion\nset to\nv1\nand\nare still installable by Helm 3.\nChanges from\nv1\nto\nv2\n:\nA\ndependencies\nfield defining chart dependencies, which were located in a\nseparate\nrequirements.yaml\nfile for\nv1\ncharts (see\nChart\nDependencies\n).\nThe\ntype\nfield, discriminating application and library charts (see\nChart\nTypes\n).\nThe\nappVersion\nField\nNote that the\nappVersion\nfield is not related to the\nversion\nfield. It is a\nway of specifying the version of the application. For example, the\ndrupal\nchart may have an\nappVersion: \"8.2.1\"\n, indicating that the version of Drupal\nincluded in the chart (by default) is\n8.2.1\n. This field is informational, and\nhas no impact on chart version calculations. Wrapping the version in quotes is highly recommended. It forces the YAML parser to treat the version number as a string. Leaving it unquoted can lead to parsing issues in some cases. For example, YAML interprets\n1.0\nas a floating point value, and a git commit SHA like\n1234e10\nas scientific notation.\nAs of Helm v3.5.0,\nhelm create\nwraps the default\nappVersion\nfield in quotes.\nThe\nkubeVersion\nField\nThe optional\nkubeVersion\nfield can define semver constraints on supported\nKubernetes versions. Helm will validate the version constraints when installing\nthe chart and fail if the cluster runs an unsupported Kubernetes version.\nVersion constraints may comprise space separated AND comparisons such as\n>= 1.13.0 < 1.15.0\nwhich themselves can be combined with the OR\n||\noperator like in the following\nexample\n>= 1.13.0 < 1.14.0 || >= 1.14.1 < 1.15.0\nIn this example the version\n1.14.0\nis excluded, which can make sense if a bug\nin certain versions is known to prevent the chart from running properly.\nApart from version constrains employing operators\n=\n!=\n>\n<\n>=\n<=\nthe\nfollowing shorthand notations are supported\nhyphen ranges for closed intervals, where\n1.1 - 2.3.4\nis equivalent to\n>= 1.1 <= 2.3.4\n.\nwildcards\nx\n,\nX\nand\n*\n, where\n1.2.x\nis equivalent to\n>= 1.2.0 < 1.3.0\n.\ntilde ranges (patch version changes allowed), where\n~1.2.3\nis equivalent to\n>= 1.2.3 < 1.3.0\n.\ncaret ranges (minor version changes allowed), where\n^1.2.3\nis equivalent to\n>= 1.2.3 < 2.0.0\n.\nFor a detailed explanation of supported semver constraints see\nMasterminds/semver\n.\nDeprecating Charts\nWhen managing charts in a Chart Repository, it is sometimes necessary to\ndeprecate a chart. The optional\ndeprecated\nfield in\nChart.yaml\ncan be used\nto mark a chart as deprecated. If the\nlatest\nversion of a chart in the\nrepository is marked as deprecated, then the chart as a whole is considered to\nbe deprecated. The chart name can be later reused by publishing a newer version\nthat is not marked as deprecated. The workflow for deprecating charts is:\nUpdate chart's\nChart.yaml\nto mark the chart as deprecated, bumping the\nversion\nRelease the new chart version in the Chart Repository\nRemove the chart from the source repository (e.g. git)\nChart Types\nThe\ntype\nfield defines the type of chart. There are two types:\napplication\nand\nlibrary\n. Application is the default type and it is the standard chart\nwhich can be operated on fully. The\nlibrary chart\nprovides utilities or functions for the\nchart builder. A library chart differs from an application chart because it is\nnot installable and usually doesn't contain any resource objects.\nNote:\nAn application chart can be used as a library chart. This is enabled\nby setting the type to\nlibrary\n. The chart will then be rendered as a library\nchart where all utilities and functions can be leveraged. All resource objects\nof the chart will not be rendered.\nChart LICENSE, README and NOTES\nCharts can also contain files that describe the installation, configuration,\nusage and license of a chart.\nA LICENSE is a plain text file containing the\nlicense\nfor the chart. The\nchart can contain a license as it may have programming logic in the templates\nand would therefore not be configuration only. There can also be separate\nlicense(s) for the application installed by the chart, if required.\nA README for a chart should be formatted in Markdown (README.md), and should\ngenerally contain:\nA description of the application or service the chart provides\nAny prerequisites or requirements to run the chart\nDescriptions of options in\nvalues.yaml\nand default values\nAny other information that may be relevant to the installation or\nconfiguration of the chart\nWhen hubs and other user interfaces display details about a chart that detail is\npulled from the content in the\nREADME.md\nfile.\nThe chart can also contain a short plain text\ntemplates/NOTES.txt\nfile that\nwill be printed out after installation, and when viewing the status of a\nrelease. This file is evaluated as a\ntemplate\n, and can\nbe used to display usage notes, next steps, or any other information relevant to\na release of the chart. For example, instructions could be provided for\nconnecting to a database, or accessing a web UI. Since this file is printed to\nSTDOUT when running\nhelm install\nor\nhelm status\n, it is recommended to keep\nthe content brief and point to the README for greater detail.\nChart Dependencies\nIn Helm, one chart may depend on any number of other charts. These dependencies\ncan be dynamically linked using the\ndependencies\nfield in\nChart.yaml\nor\nbrought in to the\ncharts/\ndirectory and managed manually.\nManaging Dependencies with the\ndependencies\nfield\nThe charts required by the current chart are defined as a list in the\ndependencies\nfield.\ndependencies\n:\n-\nname\n:\napache\nversion\n:\n1.2.3\nrepository\n:\nhttps://example.com/charts\n-\nname\n:\nmysql\nversion\n:\n3.2.1\nrepository\n:\nhttps://another.example.com/charts\nThe\nname\nfield is the name of the chart you want.\nThe\nversion\nfield is the version of the chart you want.\nThe\nrepository\nfield is the full URL to the chart repository. Note that you\nmust also use\nhelm repo add\nto add that repo locally.\nYou might use the name of the repo instead of URL\n$\nhelm repo add fantastic-charts https://charts.helm.sh/incubator\ndependencies\n:\n-\nname\n:\nawesomeness\nversion\n:\n1.0.0\nrepository\n:\n\"@fantastic-charts\"\nOnce you have defined dependencies, you can run\nhelm dependency update\nand it\nwill use your dependency file to download all the specified charts into your\ncharts/\ndirectory for you.\n$\nhelm dep up foochart\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"local\" chart repository\n...Successfully got an update from the \"stable\" chart repository\n...Successfully got an update from the \"example\" chart repository\n...Successfully got an update from the \"another\" chart repository\nUpdate Complete. Happy Helming!\nSaving 2 charts\nDownloading apache from repo https://example.com/charts\nDownloading mysql from repo https://another.example.com/charts\nWhen\nhelm dependency update\nretrieves charts, it will store them as chart\narchives in the\ncharts/\ndirectory. So for the example above, one would expect\nto see the following files in the charts directory:\ncharts/\napache-1.2.3.tgz\nmysql-3.2.1.tgz\nAlias field in dependencies\nIn addition to the other fields above, each requirements entry may contain the\noptional field\nalias\n.\nAdding an alias for a dependency chart would put a chart in dependencies using\nalias as name of new dependency.\nOne can use\nalias\nin cases where they need to access a chart with other\nname(s).\n# parentchart/Chart.yaml\ndependencies\n:\n-\nname\n:\nsubchart\nrepository\n:\nhttp://localhost:10191\nversion\n:\n0.1.0\nalias\n:\nnew-subchart-1\n-\nname\n:\nsubchart\nrepository\n:\nhttp://localhost:10191\nversion\n:\n0.1.0\nalias\n:\nnew-subchart-2\n-\nname\n:\nsubchart\nrepository\n:\nhttp://localhost:10191\nversion\n:\n0.1.0\nIn the above example we will get 3 dependencies in all for\nparentchart\n:\nsubchart\nnew-subchart-1\nnew-subchart-2\nThe manual way of achieving this is by copy/pasting the same chart in the\ncharts/\ndirectory multiple times with different names.\nTags and Condition fields in dependencies\nIn addition to the other fields above, each requirements entry may contain the\noptional fields\ntags\nand\ncondition\n.\nAll charts are loaded by default. If\ntags\nor\ncondition\nfields are present,\nthey will be evaluated and used to control loading for the chart(s) they are\napplied to.\nCondition - The condition field holds one or more YAML paths (delimited by\ncommas). If this path exists in the top parent's values and resolves to a\nboolean value, the chart will be enabled or disabled based on that boolean\nvalue. Only the first valid path found in the list is evaluated and if no paths\nexist then the condition has no effect.\nTags - The tags field is a YAML list of labels to associate with this chart. In\nthe top parent's values, all charts with tags can be enabled or disabled by\nspecifying the tag and a boolean value.\n# parentchart/Chart.yaml\ndependencies\n:\n-\nname\n:\nsubchart1\nrepository\n:\nhttp://localhost:10191\nversion\n:\n0.1.0\ncondition\n:\nsubchart1.enabled,global.subchart1.enabled\ntags\n:\n- front-end\n- subchart1\n-\nname\n:\nsubchart2\nrepository\n:\nhttp://localhost:10191\nversion\n:\n0.1.0\ncondition\n:\nsubchart2.enabled,global.subchart2.enabled\ntags\n:\n- back-end\n- subchart2\n# parentchart/values.yaml\nsubchart1\n:\nenabled\n:\ntrue\ntags\n:\nfront-end\n:\nfalse\nback-end\n:\ntrue\nIn the above example all charts with the tag\nfront-end\nwould be disabled but\nsince the\nsubchart1.enabled\npath evaluates to 'true' in the parent's values,\nthe condition will override the\nfront-end\ntag and\nsubchart1\nwill be enabled.\nSince\nsubchart2\nis tagged with\nback-end\nand that tag evaluates to\ntrue\n,\nsubchart2\nwill be enabled. Also note that although\nsubchart2\nhas a condition\nspecified, there is no corresponding path and value in the parent's values so\nthat condition has no effect.\nUsing the CLI with Tags and Conditions\nThe\n--set\nparameter can be used as usual to alter tag and condition values.\nhelm install --set tags.front-end=true --set subchart2.enabled=false\nTags and Condition Resolution\nConditions (when set in values) always override tags.\nThe first condition\npath that exists wins and subsequent ones for that chart are ignored.\nTags are evaluated as 'if any of the chart's tags are true then enable the\nchart'.\nTags and conditions values must be set in the top parent's values.\nThe\ntags:\nkey in values must be a top level key. Globals and nested\ntags:\ntables are not currently supported.\nImporting Child Values via dependencies\nIn some cases it is desirable to allow a child chart's values to propagate to\nthe parent chart and be shared as common defaults. An additional benefit of\nusing the\nexports\nformat is that it will enable future tooling to introspect\nuser-settable values.\nThe keys containing the values to be imported can be specified in the parent\nchart's\ndependencies\nin the field\nimport-values\nusing a YAML list. Each item\nin the list is a key which is imported from the child chart's\nexports\nfield.\nTo import values not contained in the\nexports\nkey, use the\nchild-parent\nformat. Examples of both formats\nare described below.\nUsing the exports format\nIf a child chart's\nvalues.yaml\nfile contains an\nexports\nfield at the root,\nits contents may be imported directly into the parent's values by specifying the\nkeys to import as in the example below:\n# parent's Chart.yaml file\ndependencies\n:\n-\nname\n:\nsubchart\nrepository\n:\nhttp://localhost:10191\nversion\n:\n0.1.0\nimport-values\n:\n- data\n# child's values.yaml file\nexports\n:\ndata\n:\nmyint\n:\n99\nSince we are specifying the key\ndata\nin our import list, Helm looks in the\nexports\nfield of the child chart for\ndata\nkey and imports its contents.\nThe final parent values would contain our exported field:\n# parent's values\nmyint\n:\n99\nPlease note the parent key\ndata\nis not contained in the parent's final values.\nIf you need to specify the parent key, use the 'child-parent' format.\nUsing the child-parent format\nTo access values that are not contained in the\nexports\nkey of the child\nchart's values, you will need to specify the source key of the values to be\nimported (\nchild\n) and the destination path in the parent chart's values\n(\nparent\n).\nThe\nimport-values\nin the example below instructs Helm to take any values found\nat\nchild:\npath and copy them to the parent's values at the path specified in\nparent:\n# parent's Chart.yaml file\ndependencies\n:\n-\nname\n:\nsubchart1\nrepository\n:\nhttp://localhost:10191\nversion\n:\n0.1.0\n...\nimport-values\n:\n-\nchild\n:\ndefault.data\nparent\n:\nmyimports\nIn the above example, values found at\ndefault.data\nin the subchart1's values\nwill be imported to the\nmyimports\nkey in the parent chart's values as detailed\nbelow:\n# parent's values.yaml file\nmyimports\n:\nmyint\n:\n0\nmybool\n:\nfalse\nmystring\n:\n\"helm rocks!\"\n# subchart1's values.yaml file\ndefault\n:\ndata\n:\nmyint\n:\n999\nmybool\n:\ntrue\nThe parent chart's resulting values would be:\n# parent's final values\nmyimports\n:\nmyint\n:\n999\nmybool\n:\ntrue\nmystring\n:\n\"helm rocks!\"\nThe parent's final values now contains the\nmyint\nand\nmybool\nfields imported\nfrom subchart1.\nManaging Dependencies manually via the\ncharts/\ndirectory\nIf more control over dependencies is desired, these dependencies can be\nexpressed explicitly by copying the dependency charts into the\ncharts/\ndirectory.\nA dependency should be an unpacked chart directory but its name cannot start\nwith\n_\nor\n.\n. Such files are ignored by the chart loader.\nFor example, if the WordPress chart depends on the Apache chart, the Apache\nchart (of the correct version) is supplied in the WordPress chart's\ncharts/\ndirectory:\nwordpress\n:\nChart.yaml\n# ...\ncharts/\napache/\nChart.yaml\n# ...\nmysql/\nChart.yaml\n# ...\nThe example above shows how the WordPress chart expresses its dependency on\nApache and MySQL by including those charts inside of its\ncharts/\ndirectory.\nTIP:\nTo drop a dependency into your\ncharts/\ndirectory, use the\nhelm pull\ncommand\nOperational aspects of using dependencies\nThe above sections explain how to specify chart dependencies, but how does this\naffect chart installation using\nhelm install\nand\nhelm upgrade\n?\nSuppose that a chart named \"A\" creates the following Kubernetes objects\nnamespace \"A-Namespace\"\nstatefulset \"A-StatefulSet\"\nservice \"A-Service\"\nFurthermore, A is dependent on chart B that creates objects\nnamespace \"B-Namespace\"\nreplicaset \"B-ReplicaSet\"\nservice \"B-Service\"\nAfter installation/upgrade of chart A a single Helm release is created/modified.\nThe release will create/update all of the above Kubernetes objects in the\nfollowing order:\nA-Namespace\nB-Namespace\nA-Service\nB-Service\nB-ReplicaSet\nA-StatefulSet\nThis is because when Helm installs/upgrades charts, the Kubernetes objects from\nthe charts and all its dependencies are\naggregated into a single set; then\nsorted by type followed by name; and then\ncreated/updated in that order.\nHence a single release is created with all the objects for the chart and its\ndependencies.\nThe install order of Kubernetes types is given by the enumeration InstallOrder\nin kind_sorter.go (see\nthe Helm source\nfile\n).\nTemplates and Values\nHelm Chart templates are written in the\nGo template\nlanguage\n, with the addition of 50 or so\nadd-on template functions\nfrom the Sprig\nlibrary\nand a few other\nspecialized\nfunctions\n.\nAll template files are stored in a chart's\ntemplates/\nfolder. When Helm\nrenders the charts, it will pass every file in that directory through the\ntemplate engine.\nValues for the templates are supplied two ways:\nChart developers may supply a file called\nvalues.yaml\ninside of a chart.\nThis file can contain default values.\nChart users may supply a YAML file that contains values. This can be provided\non the command line with\nhelm install\n.\nWhen a user supplies custom values, these values will override the values in the\nchart's\nvalues.yaml\nfile.\nTemplate Files\nTemplate files follow the standard conventions for writing Go templates (see\nthe text/template Go package\ndocumentation\nfor details). An example\ntemplate file might look something like this:\napiVersion\n:\nv1\nkind\n:\nReplicationController\nmetadata\n:\nname\n:\ndeis-database\nnamespace\n:\ndeis\nlabels\n:\napp.kubernetes.io/managed-by\n:\ndeis\nspec\n:\nreplicas\n:\n1\nselector\n:\napp.kubernetes.io/name\n:\ndeis-database\ntemplate\n:\nmetadata\n:\nlabels\n:\napp.kubernetes.io/name\n:\ndeis-database\nspec\n:\nserviceAccount\n:\ndeis-database\ncontainers\n:\n-\nname\n:\ndeis-database\nimage\n:\n{{\n.Values.imageRegistry }}/postgres:{{ .Values.dockerTag }}\nimagePullPolicy\n:\n{{\n.Values.pullPolicy }}\nports\n:\n-\ncontainerPort\n:\n5432\nenv\n:\n-\nname\n:\nDATABASE_STORAGE\nvalue\n:\n{{\ndefault \"minio\" .Values.storage }}\nThe above example, based loosely on\nhttps://github.com/deis/charts\n, is a template\nfor a Kubernetes replication controller. It can use the following four template\nvalues (usually defined in a\nvalues.yaml\nfile):\nimageRegistry\n: The source registry for the Docker image.\ndockerTag\n: The tag for the docker image.\npullPolicy\n: The Kubernetes pull policy.\nstorage\n: The storage backend, whose default is set to\n\"minio\"\nAll of these values are defined by the template author. Helm does not require or\ndictate parameters.\nTo see many working charts, check out the CNCF\nArtifact\nHub\n.\nPredefined Values\nValues that are supplied via a\nvalues.yaml\nfile (or via the\n--set\nflag) are\naccessible from the\n.Values\nobject in a template. But there are other\npre-defined pieces of data you can access in your templates.\nThe following values are pre-defined, are available to every template, and\ncannot be overridden. As with all values, the names are\ncase sensitive\n.\nRelease.Name\n: The name of the release (not the chart)\nRelease.Namespace\n: The namespace the chart was released to.\nRelease.Service\n: The service that conducted the release.\nRelease.IsUpgrade\n: This is set to true if the current operation is an\nupgrade or rollback.\nRelease.IsInstall\n: This is set to true if the current operation is an\ninstall.\nChart\n: The contents of the\nChart.yaml\n. Thus, the chart version is\nobtainable as\nChart.Version\nand the maintainers are in\nChart.Maintainers\n.\nFiles\n: A map-like object containing all non-special files in the chart. This\nwill not give you access to templates, but will give you access to additional\nfiles that are present (unless they are excluded using\n.helmignore\n). Files\ncan be accessed using\n{{ index .Files \"file.name\" }}\nor using the\n{{.Files.Get name }}\nfunction. You can also access the contents of the file\nas\n[]byte\nusing\n{{ .Files.GetBytes }}\nCapabilities\n: A map-like object that contains information about the versions\nof Kubernetes (\n{{ .Capabilities.KubeVersion }}\n) and the supported Kubernetes\nAPI versions (\n{{ .Capabilities.APIVersions.Has \"batch/v1\" }}\n)\nNOTE:\nAny unknown\nChart.yaml\nfields will be dropped. They will not be\naccessible inside of the\nChart\nobject. Thus,\nChart.yaml\ncannot be used to\npass arbitrarily structured data into the template. The values file can be used\nfor that, though.\nValues files\nConsidering the template in the previous section, a\nvalues.yaml\nfile that\nsupplies the necessary values would look like this:\nimageRegistry\n:\n\"quay.io/deis\"\ndockerTag\n:\n\"latest\"\npullPolicy\n:\n\"Always\"\nstorage\n:\n\"s3\"\nA values file is formatted in YAML. A chart may include a default\nvalues.yaml\nfile. The Helm install command allows a user to override values by supplying\nadditional YAML values:\n$\nhelm install --generate-name --values\n=\nmyvals.yaml wordpress\nWhen values are passed in this way, they will be merged into the default values\nfile. For example, consider a\nmyvals.yaml\nfile that looks like this:\nstorage\n:\n\"gcs\"\nWhen this is merged with the\nvalues.yaml\nin the chart, the resulting generated\ncontent will be:\nimageRegistry\n:\n\"quay.io/deis\"\ndockerTag\n:\n\"latest\"\npullPolicy\n:\n\"Always\"\nstorage\n:\n\"gcs\"\nNote that only the last field was overridden.\nNOTE:\nThe default values file included inside of a chart\nmust\nbe named\nvalues.yaml\n. But files specified on the command line can be named anything.\nNOTE:\nIf the\n--set\nflag is used on\nhelm install\nor\nhelm upgrade\n, those\nvalues are simply converted to YAML on the client side.\nNOTE:\nIf any required entries in the values file exist, they can be declared\nas required in the chart template by using the\n'required' function\nAny of these values are then accessible inside of templates using the\n.Values\nobject:\napiVersion\n:\nv1\nkind\n:\nReplicationController\nmetadata\n:\nname\n:\ndeis-database\nnamespace\n:\ndeis\nlabels\n:\napp.kubernetes.io/managed-by\n:\ndeis\nspec\n:\nreplicas\n:\n1\nselector\n:\napp.kubernetes.io/name\n:\ndeis-database\ntemplate\n:\nmetadata\n:\nlabels\n:\napp.kubernetes.io/name\n:\ndeis-database\nspec\n:\nserviceAccount\n:\ndeis-database\ncontainers\n:\n-\nname\n:\ndeis-database\nimage\n:\n{{\n.Values.imageRegistry }}/postgres:{{ .Values.dockerTag }}\nimagePullPolicy\n:\n{{\n.Values.pullPolicy }}\nports\n:\n-\ncontainerPort\n:\n5432\nenv\n:\n-\nname\n:\nDATABASE_STORAGE\nvalue\n:\n{{\ndefault \"minio\" .Values.storage }}\nScope, Dependencies, and Values\nValues files can declare values for the top-level chart, as well as for any of\nthe charts that are included in that chart's\ncharts/\ndirectory. Or, to phrase\nit differently, a values file can supply values to the chart as well as to any\nof its dependencies. For example, the demonstration WordPress chart above has\nboth\nmysql\nand\napache\nas dependencies. The values file could supply values\nto all of these components:\ntitle\n:\n\"My WordPress Site\"\n# Sent to the WordPress template\nmysql\n:\nmax_connections\n:\n100\n# Sent to MySQL\npassword\n:\n\"secret\"\napache\n:\nport\n:\n8080\n# Passed to Apache\nCharts at a higher level have access to all of the variables defined beneath. So\nthe WordPress chart can access the MySQL password as\n.Values.mysql.password\n.\nBut lower level charts cannot access things in parent charts, so MySQL will not\nbe able to access the\ntitle\nproperty. Nor, for that matter, can it access\napache.port\n.\nValues are namespaced, but namespaces are pruned. So for the WordPress chart, it\ncan access the MySQL password field as\n.Values.mysql.password\n. But for the\nMySQL chart, the scope of the values has been reduced and the namespace prefix\nremoved, so it will see the password field simply as\n.Values.password\n.\nGlobal Values\nAs of 2.0.0-Alpha.2, Helm supports special \"global\" value. Consider this\nmodified version of the previous example:\ntitle\n:\n\"My WordPress Site\"\n# Sent to the WordPress template\nglobal\n:\napp\n:\nMyWordPress\nmysql\n:\nmax_connections\n:\n100\n# Sent to MySQL\npassword\n:\n\"secret\"\napache\n:\nport\n:\n8080\n# Passed to Apache\nThe above adds a\nglobal\nsection with the value\napp: MyWordPress\n. This value\nis available to\nall\ncharts as\n.Values.global.app\n.\nFor example, the\nmysql\ntemplates may access\napp\nas\n{{ .Values.global.app}}\n, and so can the\napache\nchart. Effectively, the values\nfile above is regenerated like this:\ntitle\n:\n\"My WordPress Site\"\n# Sent to the WordPress template\nglobal\n:\napp\n:\nMyWordPress\nmysql\n:\nglobal\n:\napp\n:\nMyWordPress\nmax_connections\n:\n100\n# Sent to MySQL\npassword\n:\n\"secret\"\napache\n:\nglobal\n:\napp\n:\nMyWordPress\nport\n:\n8080\n# Passed to Apache\nThis provides a way of sharing one top-level variable with all subcharts, which\nis useful for things like setting\nmetadata\nproperties like labels.\nIf a subchart declares a global variable, that global will be passed\ndownward\n(to the subchart's subcharts), but not\nupward\nto the parent chart. There is no\nway for a subchart to influence the values of the parent chart.\nAlso, global variables of parent charts take precedence over the global\nvariables from subcharts.\nSchema Files\nSometimes, a chart maintainer might want to define a structure on their values.\nThis can be done by defining a schema in the\nvalues.schema.json\nfile. A schema\nis represented as a\nJSON Schema\n. It might look\nsomething like this:\n{\n\"$schema\"\n:\n\"https://json-schema.org/draft-07/schema#\"\n,\n\"properties\"\n: {\n\"image\"\n: {\n\"description\"\n:\n\"Container Image\"\n,\n\"properties\"\n: {\n\"repo\"\n: {\n\"type\"\n:\n\"string\"\n},\n\"tag\"\n: {\n\"type\"\n:\n\"string\"\n}\n},\n\"type\"\n:\n\"object\"\n},\n\"name\"\n: {\n\"description\"\n:\n\"Service name\"\n,\n\"type\"\n:\n\"string\"\n},\n\"port\"\n: {\n\"description\"\n:\n\"Port\"\n,\n\"minimum\"\n:\n0\n,\n\"type\"\n:\n\"integer\"\n},\n\"protocol\"\n: {\n\"type\"\n:\n\"string\"\n}\n},\n\"required\"\n: [\n\"protocol\"\n,\n\"port\"\n],\n\"title\"\n:\n\"Values\"\n,\n\"type\"\n:\n\"object\"\n}\nThis schema will be applied to the values to validate it. Validation occurs when\nany of the following commands are invoked:\nhelm install\nhelm upgrade\nhelm lint\nhelm template\nAn example of a\nvalues.yaml\nfile that meets the requirements of this schema\nmight look something like this:\nname\n:\nfrontend\nprotocol\n:\nhttps\nport\n:\n443\nNote that the schema is applied to the final\n.Values\nobject, and not just to\nthe\nvalues.yaml\nfile. This means that the following\nyaml\nfile is valid,\ngiven that the chart is installed with the appropriate\n--set\noption shown\nbelow.\nname\n:\nfrontend\nprotocol\n:\nhttps\nhelm install --set port=443\nFurthermore, the final\n.Values\nobject is checked against\nall\nsubchart\nschemas. This means that restrictions on a subchart can't be circumvented by a\nparent chart. This also works backwards - if a subchart has a requirement that\nis not met in the subchart's\nvalues.yaml\nfile, the parent chart\nmust\nsatisfy\nthose restrictions in order to be valid.\nReferences\nWhen it comes to writing templates, values, and schema files, there are several\nstandard references that will help you out.\nGo templates\nExtra template functions\nThe YAML format\nJSON Schema\nCustom Resource Definitions (CRDs)\nKubernetes provides a mechanism for declaring new types of Kubernetes objects.\nUsing CustomResourceDefinitions (CRDs), Kubernetes developers can declare custom\nresource types.\nIn Helm 3, CRDs are treated as a special kind of object. They are installed\nbefore the rest of the chart, and are subject to some limitations.\nCRD YAML files should be placed in the\ncrds/\ndirectory inside of a chart.\nMultiple CRDs (separated by YAML start and end markers) may be placed in the\nsame file. Helm will attempt to load\nall\nof the files in the CRD directory\ninto Kubernetes.\nCRD files\ncannot be templated\n. They must be plain YAML documents.\nWhen Helm installs a new chart, it will upload the CRDs, pause until the CRDs\nare made available by the API server, and then start the template engine, render\nthe rest of the chart, and upload it to Kubernetes. Because of this ordering,\nCRD information is available in the\n.Capabilities\nobject in Helm templates,\nand Helm templates may create new instances of objects that were declared in\nCRDs.\nFor example, if your chart had a CRD for\nCronTab\nin the\ncrds/\ndirectory, you\nmay create instances of the\nCronTab\nkind in the\ntemplates/\ndirectory:\ncrontabs/\nChart.yaml\ncrds/\ncrontab.yaml\ntemplates/\nmycrontab.yaml\nThe\ncrontab.yaml\nfile must contain the CRD with no template directives:\nkind\n:\nCustomResourceDefinition\nmetadata\n:\nname\n:\ncrontabs.stable.example.com\nspec\n:\ngroup\n:\nstable.example.com\nversions\n:\n-\nname\n:\nv1\nserved\n:\ntrue\nstorage\n:\ntrue\nscope\n:\nNamespaced\nnames\n:\nplural\n:\ncrontabs\nsingular\n:\ncrontab\nkind\n:\nCronTab\nThen the template\nmycrontab.yaml\nmay create a new\nCronTab\n(using templates\nas usual):\napiVersion\n:\nstable.example.com\nkind\n:\nCronTab\nmetadata\n:\nname\n:\n{{\n.Values.name }}\nspec\n:\n# ...\nHelm will make sure that the\nCronTab\nkind has been installed and is available\nfrom the Kubernetes API server before it proceeds installing the things in\ntemplates/\n.\nLimitations on CRDs\nUnlike most objects in Kubernetes, CRDs are installed globally. For that reason,\nHelm takes a very cautious approach in managing CRDs. CRDs are subject to the\nfollowing limitations:\nCRDs are never reinstalled. If Helm determines that the CRDs in the\ncrds/\ndirectory are already present (regardless of version), Helm will not attempt\nto install or upgrade.\nCRDs are never installed on upgrade or rollback. Helm will only create CRDs on\ninstallation operations.\nCRDs are never deleted. Deleting a CRD automatically deletes all of the CRD's\ncontents across all namespaces in the cluster. Consequently, Helm will not\ndelete CRDs.\nOperators who want to upgrade or delete CRDs are encouraged to do this manually\nand with great care.\nUsing Helm to Manage Charts\nThe\nhelm\ntool has several commands for working with charts.\nIt can create a new chart for you:\n$\nhelm create mychart\nCreated mychart/\nOnce you have edited a chart,\nhelm\ncan package it into a chart archive for\nyou:\n$\nhelm package mychart\nArchived mychart-0.1.-.tgz\nYou can also use\nhelm\nto help you find issues with your chart's formatting or\ninformation:\n$\nhelm lint mychart\nNo issues found\nChart Repositories\nA\nchart repository\nis an HTTP server that houses one or more packaged charts.\nWhile\nhelm\ncan be used to manage local chart directories, when it comes to\nsharing charts, the preferred mechanism is a chart repository.\nAny HTTP server that can serve YAML files and tar files and can answer GET\nrequests can be used as a repository server. The Helm team has tested some\nservers, including Google Cloud Storage with website mode enabled, and S3 with\nwebsite mode enabled.\nA repository is characterized primarily by the presence of a special file called\nindex.yaml\nthat has a list of all of the packages supplied by the repository,\ntogether with metadata that allows retrieving and verifying those packages.\nOn the client side, repositories are managed with the\nhelm repo\ncommands.\nHowever, Helm does not provide tools for uploading charts to remote repository\nservers. This is because doing so would add substantial requirements to an\nimplementing server, and thus raise the barrier for setting up a repository.\nChart Starter Packs\nThe\nhelm create\ncommand takes an optional\n--starter\noption that lets you\nspecify a \"starter chart\". Also, the starter option has a short alias\n-p\n.\nExamples of usage:\nhelm create my-chart --starter starter-name\nhelm create my-chart -p starter-name\nhelm create my-chart -p /absolute/path/to/starter-name\nStarters are just regular charts, but are located in\n$XDG_DATA_HOME/helm/starters\n. As a chart developer, you may author charts that\nare specifically designed to be used as starters. Such charts should be designed\nwith the following considerations in mind:\nThe\nChart.yaml\nwill be overwritten by the generator.\nUsers will expect to modify such a chart's contents, so documentation should\nindicate how users can do so.\nAll occurrences of\n<CHARTNAME>\nwill be replaced with the specified chart name so that starter charts can be used as templates, except for some variable files. For example, if you use custom files in the\nvars\ndirectory or certain\nREADME.md\nfiles,\n<CHARTNAME>\nwill NOT override inside them. Additionally, the chart description is not inherited.\nCurrently the only way to add a chart to\n$XDG_DATA_HOME/helm/starters\nis to\nmanually copy it there. In your chart's documentation, you may want to explain\nthat process.\nNext\nChart Hooks →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nChart Hooks\nHelm provides a\nhook\nmechanism to allow chart developers to intervene at\ncertain points in a release's life cycle. For example, you can use hooks to:\nLoad a ConfigMap or Secret during install before any other charts are loaded.\nExecute a Job to back up a database before installing a new chart, and then\nexecute a second job after the upgrade in order to restore data.\nRun a Job before deleting a release to gracefully take a service out of\nrotation before removing it.\nHooks work like regular templates, but they have special annotations that cause\nHelm to utilize them differently. In this section, we cover the basic usage\npattern for hooks.\nThe Available Hooks\nThe following hooks are defined:\nAnnotation Value\nDescription\npre-install\nExecutes after templates are rendered, but before any resources are created in Kubernetes\npost-install\nExecutes after all resources are loaded into Kubernetes\npre-delete\nExecutes on a deletion request before any resources are deleted from Kubernetes\npost-delete\nExecutes on a deletion request after all of the release's resources have been deleted\npre-upgrade\nExecutes on an upgrade request after templates are rendered, but before any resources are updated\npost-upgrade\nExecutes on an upgrade request after all resources have been upgraded\npre-rollback\nExecutes on a rollback request after templates are rendered, but before any resources are rolled back\npost-rollback\nExecutes on a rollback request after all resources have been modified\ntest\nExecutes when the Helm test subcommand is invoked (\nview test docs\n)\nNote that the\ncrd-install\nhook has been removed in favor of the\ncrds/\ndirectory in Helm 3.\nHooks and the Release Lifecycle\nHooks allow you, the chart developer, an opportunity to perform operations at\nstrategic points in a release lifecycle. For example, consider the lifecycle for\na\nhelm install\n. By default, the lifecycle looks like this:\nUser runs\nhelm install foo\nThe Helm library install API is called\nAfter some verification, the library renders the\nfoo\ntemplates\nThe library loads the resulting resources into Kubernetes\nThe library returns the release object (and other data) to the client\nThe client exits\nHelm defines two hooks for the\ninstall\nlifecycle:\npre-install\nand\npost-install\n. If the developer of the\nfoo\nchart implements both hooks, the\nlifecycle is altered like this:\nUser runs\nhelm install foo\nThe Helm library install API is called\nCRDs in the\ncrds/\ndirectory are installed\nAfter some verification, the library renders the\nfoo\ntemplates\nThe library prepares to execute the\npre-install\nhooks (loading hook\nresources into Kubernetes)\nThe library sorts hooks by weight (assigning a weight of 0 by default),\nby resource kind and finally by name in ascending order.\nThe library then loads the hook with the lowest weight first (negative to\npositive)\nThe library waits until the hook is \"Ready\" (except for CRDs)\nThe library loads the resulting resources into Kubernetes. Note that if the\n--wait\nflag is set, the library will wait until all resources are in a\nready state and will not run the\npost-install\nhook until they are ready.\nThe library executes the\npost-install\nhook (loading hook resources)\nThe library waits until the hook is \"Ready\"\nThe library returns the release object (and other data) to the client\nThe client exits\nWhat does it mean to wait until a hook is ready? This depends on the resource\ndeclared in the hook. If the resource is a\nJob\nor\nPod\nkind, Helm will wait\nuntil it successfully runs to completion. And if the hook fails, the release\nwill fail. This is a\nblocking operation\n, so the Helm client will pause while\nthe Job is run.\nFor all other kinds, as soon as Kubernetes marks the resource as loaded (added\nor updated), the resource is considered \"Ready\". When many resources are\ndeclared in a hook, the resources are executed serially. If they have hook\nweights (see below), they are executed in weighted order.\nStarting from Helm 3.2.0 hook resources with same weight are installed in the same\norder as normal non-hook resources. Otherwise, ordering is\nnot guaranteed. (In Helm 2.3.0 and after, they are sorted alphabetically. That\nbehavior, though, is not considered binding and could change in the future.) It\nis considered good practice to add a hook weight, and set it to\n0\nif weight is\nnot important.\nHook resources are not managed with corresponding releases\nThe resources that a hook creates are currently not tracked or managed as part\nof the release. Once Helm verifies that the hook has reached its ready state, it\nwill leave the hook resource alone. Garbage collection of hook resources when\nthe corresponding release is deleted may be added to Helm 3 in the future, so\nany hook resources that must never be deleted should be annotated with\nhelm.sh/resource-policy: keep\n.\nPractically speaking, this means that if you create resources in a hook, you\ncannot rely upon\nhelm uninstall\nto remove the resources. To destroy such\nresources, you need to either\nadd a custom\nhelm.sh/hook-delete-policy\nannotation\nto the hook template file, or\nset the time\nto live (TTL) field of a Job\nresource\n.\nWriting a Hook\nHooks are just Kubernetes manifest files with special annotations in the\nmetadata\nsection. Because they are template files, you can use all of the\nnormal template features, including reading\n.Values\n,\n.Release\n, and\n.Template\n.\nFor example, this template, stored in\ntemplates/post-install-job.yaml\n,\ndeclares a job to be run on\npost-install\n:\napiVersion\n:\nbatch/v1\nkind\n:\nJob\nmetadata\n:\nname\n:\n\"{{ .Release.Name }}\"\nlabels\n:\napp.kubernetes.io/managed-by\n:\n{{\n.Release.Service | quote }}\napp.kubernetes.io/instance\n:\n{{\n.Release.Name | quote }}\napp.kubernetes.io/version\n:\n{{\n.Chart.AppVersion }}\nhelm.sh/chart\n:\n\"{{ .Chart.Name }}-{{ .Chart.Version }}\"\nannotations\n:\n# This is what defines this resource as a hook. Without this line, the\n# job is considered part of the release.\n\"helm.sh/hook\":\npost-install\n\"helm.sh/hook-weight\":\n\"-5\"\n\"helm.sh/hook-delete-policy\":\nhook-succeeded\nspec\n:\ntemplate\n:\nmetadata\n:\nname\n:\n\"{{ .Release.Name }}\"\nlabels\n:\napp.kubernetes.io/managed-by\n:\n{{\n.Release.Service | quote }}\napp.kubernetes.io/instance\n:\n{{\n.Release.Name | quote }}\nhelm.sh/chart\n:\n\"{{ .Chart.Name }}-{{ .Chart.Version }}\"\nspec\n:\nrestartPolicy\n:\nNever\ncontainers\n:\n-\nname\n:\npost-install-job\nimage\n:\n\"alpine:3.3\"\ncommand\n:\n[\n\"/bin/sleep\"\n,\n\"{{ default \"\n10\n\" .Values.sleepyTime }}\"\n]\nWhat makes this template a hook is the annotation:\nannotations\n:\n\"helm.sh/hook\":\npost-install\nOne resource can implement multiple hooks:\nannotations\n:\n\"helm.sh/hook\":\npost-install,post-upgrade\nSimilarly, there is no limit to the number of different resources that may\nimplement a given hook. For example, one could declare both a secret and a\nconfig map as a pre-install hook.\nWhen subcharts declare hooks, those are also evaluated. There is no way for a\ntop-level chart to disable the hooks declared by subcharts.\nIt is possible to define a weight for a hook which will help build a\ndeterministic executing order. Weights are defined using the following\nannotation:\nannotations\n:\n\"helm.sh/hook-weight\":\n\"5\"\nHook weights can be positive or negative numbers but must be represented as\nstrings. When Helm starts the execution cycle of hooks of a particular Kind it\nwill sort those hooks in ascending order.\nHook deletion policies\nIt is possible to define policies that determine when to delete corresponding\nhook resources. Hook deletion policies are defined using the following\nannotation:\nannotations\n:\n\"helm.sh/hook-delete-policy\":\nbefore-hook-creation,hook-succeeded\nYou can choose one or more defined annotation values:\nAnnotation Value\nDescription\nbefore-hook-creation\nDelete the previous resource before a new hook is launched (default)\nhook-succeeded\nDelete the resource after the hook is successfully executed\nhook-failed\nDelete the resource if the hook failed during execution\nIf no hook deletion policy annotation is specified, the\nbefore-hook-creation\nbehavior applies by default.\nPrev\n← Charts\nNext\nChart Tests →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nChart Tests\nA chart contains a number of Kubernetes resources and components that work\ntogether. As a chart author, you may want to write some tests that validate that\nyour chart works as expected when it is installed. These tests also help the\nchart consumer understand what your chart is supposed to do.\nA\ntest\nin a helm chart lives under the\ntemplates/\ndirectory and is a job\ndefinition that specifies a container with a given command to run. The container\nshould exit successfully (exit 0) for a test to be considered a success. The job\ndefinition must contain the helm test hook annotation:\nhelm.sh/hook: test\n.\nNote that until Helm v3, the job definition needed to contain one of these helm\ntest hook annotations:\nhelm.sh/hook: test-success\nor\nhelm.sh/hook: test-failure\n.\nhelm.sh/hook: test-success\nis still accepted as a backwards-compatible\nalternative to\nhelm.sh/hook: test\n.\nExample tests:\nValidate that your configuration from the values.yaml file was properly\ninjected.\nMake sure your username and password work correctly\nMake sure an incorrect username and password does not work\nAssert that your services are up and correctly load balancing\netc.\nYou can run the pre-defined tests in Helm on a release using the command\nhelm test <RELEASE_NAME>\n. For a chart consumer, this is a great way to check that\ntheir release of a chart (or application) works as expected.\nExample Test\nThe\nhelm create\ncommand will automatically create a number of folders and files. To try the helm test functionality, first create a demo helm chart.\n$\nhelm create demo\nYou will now be able to see the following structure in your demo helm chart.\ndemo/\nChart.yaml\nvalues.yaml\ncharts/\ntemplates/\ntemplates/tests/test-connection.yaml\nIn\ndemo/templates/tests/test-connection.yaml\nyou'll see a test you can try. You can see the helm test pod definition here:\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\n\"{{ include \"\ndemo.fullname\" . }}-test-connection\"\nlabels\n:\n{{- include \"demo.labels\" . | nindent 4 }}\nannotations\n:\n\"helm.sh/hook\":\ntest\nspec\n:\ncontainers\n:\n-\nname\n:\nwget\nimage\n:\nbusybox\ncommand\n:\n[\n'wget'\n]\nargs\n:\n[\n'{{ include \"demo.fullname\" . }}:{{ .Values.service.port }}'\n]\nrestartPolicy\n:\nNever\nSteps to Run a Test Suite on a Release\nFirst, install the chart on your cluster to create a release. You may have to\nwait for all pods to become active; if you test immediately after this install,\nit is likely to show a transitive failure, and you will want to re-test.\n$\nhelm install demo demo --namespace default\n$\nhelm\ntest\ndemo\nNAME: demo\nLAST DEPLOYED: Mon Feb 14 20:03:16 2022\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE:     demo-test-connection\nLast Started:   Mon Feb 14 20:35:19 2022\nLast Completed: Mon Feb 14 20:35:23 2022\nPhase:          Succeeded\n[...]\nNotes\nYou can define as many tests as you would like in a single yaml file or spread\nacross several yaml files in the\ntemplates/\ndirectory.\nYou are welcome to nest your test suite under a\ntests/\ndirectory like\n<chart-name>/templates/tests/\nfor more isolation.\nA test is a\nHelm hook\n, so annotations like\nhelm.sh/hook-weight\nand\nhelm.sh/hook-delete-policy\nmay be used with test\nresources.\nPrev\n← Chart Hooks\nNext\nLibrary Charts →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nLibrary Charts\nA library chart is a type of\nHelm chart\nthat defines chart primitives or definitions which can be shared by Helm\ntemplates in other charts. This allows users to share snippets of code that can\nbe re-used across charts, avoiding repetition and keeping charts\nDRY\n.\nThe library chart was introduced in Helm 3 to formally recognize common or\nhelper charts that have been used by chart maintainers since Helm 2. By\nincluding it as a chart type, it provides:\nA means to explicitly distinguish between common and application charts\nLogic to prevent installation of a common chart\nNo rendering of templates in a common chart which may contain release\nartifacts\nAllow for dependent charts to use the importer's context\nA chart maintainer can define a common chart as a library chart and now be\nconfident that Helm will handle the chart in a standard consistent fashion. It\nalso means that definitions in an application chart can be shared by changing\nthe chart type.\nCreate a Simple Library Chart\nAs mentioned previously, a library chart is a type of\nHelm chart\n. This means that you can start off by creating a\nscaffold chart:\n$\nhelm create mylibchart\nCreating mylibchart\nYou will first remove all the files in\ntemplates\ndirectory as we will create\nour own templates definitions in this example.\n$\nrm -rf mylibchart/templates/*\nThe values file will not be required either.\n$\nrm -f mylibchart/values.yaml\nBefore we jump into creating common code, lets do a quick review of some\nrelevant Helm concepts. A\nnamed template\n(sometimes called a partial\nor a subtemplate) is simply a template defined inside of a file, and given a\nname. In the\ntemplates/\ndirectory, any file that begins with an underscore(_)\nis not expected to output a Kubernetes manifest file. So by convention, helper\ntemplates and partials are placed in a\n_*.tpl\nor\n_*.yaml\nfiles.\nIn this example, we will code a common ConfigMap which creates an empty\nConfigMap resource. We will define the common ConfigMap in file\nmylibchart/templates/_configmap.yaml\nas follows:\n{{- define \"mylibchart.configmap.tpl\" -}}\napiVersion\n:\nv1\nkind\n:\nConfigMap\nmetadata\n:\nname\n:\n{{\n.Release.Name | printf \"%s-%s\" .Chart.Name }}\ndata\n:\n{}\n{{- end -}}\n{{- define \"mylibchart.configmap\" -}}\n{{- include \"mylibchart.util.merge\" (append . \"mylibchart.configmap.tpl\") -}}\n{{- end -}}\nThe ConfigMap construct is defined in named template\nmylibchart.configmap.tpl\n.\nIt is a simple ConfigMap with an empty resource,\ndata\n. Within this file there\nis another named template called\nmylibchart.configmap\n. This named template\nincludes another named template\nmylibchart.util.merge\nwhich will take 2 named\ntemplates as arguments, the template calling\nmylibchart.configmap\nand\nmylibchart.configmap.tpl\n.\nThe helper function\nmylibchart.util.merge\nis a named template in\nmylibchart/templates/_util.yaml\n. It is a handy util from\nThe Common Helm\nHelper Chart\nbecause it merges the 2 templates\nand overrides any common parts in both:\n{{- /*\nmylibchart.util.merge will merge two YAML templates and output the result.\nThis takes an array of three values\n:\n- the top context\n- the template name of the overrides (destination)\n- the template name of the base (source)\n*/}}\n{{- define \"mylibchart.util.merge\" -}}\n{{- $top := first . -}}\n{{- $overrides := fromYaml (include (index . 1) $top) | default (dict ) -}}\n{{- $tpl := fromYaml (include (index . 2) $top) | default (dict ) -}}\n{{- toYaml (merge $overrides $tpl) -}}\n{{- end -}}\nThis is important when a chart wants to use common code that it needs to\ncustomize with its configuration.\nFinally, lets change the chart type to\nlibrary\n. This requires editing\nmylibchart/Chart.yaml\nas follows:\napiVersion\n:\nv2\nname\n:\nmylibchart\ndescription\n:\nA Helm chart for Kubernetes\n# A chart can be either an 'application' or a 'library' chart.\n#\n# Application charts are a collection of templates that can be packaged into versioned archives\n# to be deployed.\n#\n# Library charts provide useful utilities or functions for the chart developer. They're included as\n# a dependency of application charts to inject those utilities and functions into the rendering\n# pipeline. Library charts do not define any templates and therefore cannot be deployed.\n# type: application\ntype\n:\nlibrary\n# This is the chart version. This version number should be incremented each time you make changes\n# to the chart and its templates, including the app version.\nversion\n:\n0.1.0\n# This is the version number of the application being deployed. This version number should be\n# incremented each time you make changes to the application and it is recommended to use it with quotes.\nappVersion\n:\n\"1.16.0\"\nThe library chart is now ready to be shared and its ConfigMap definition to be\nre-used.\nBefore moving on, it is worth checking if Helm recognizes the chart as a library\nchart:\n$\nhelm install mylibchart mylibchart/\nError: library charts are not installable\nUse the Simple Library Chart\nIt is time to use the library chart. This means creating a scaffold chart again:\n$\nhelm create mychart\nCreating mychart\nLets clean out the template files again as we want to create a ConfigMap only:\n$\nrm -rf mychart/templates/*\nWhen we want to create a simple ConfigMap in a Helm template, it could look\nsimilar to the following:\napiVersion\n:\nv1\nkind\n:\nConfigMap\nmetadata\n:\nname\n:\n{{\n.Release.Name | printf \"%s-%s\" .Chart.Name }}\ndata\n:\nmyvalue\n:\n\"Hello World\"\nWe are however going to re-use the common code already created in\nmylibchart\n.\nThe ConfigMap can be created in the file\nmychart/templates/configmap.yaml\nas\nfollows:\n{{- include \"mylibchart.configmap\" (list . \"mychart.configmap\") -}}\n{{- define \"mychart.configmap\" -}}\ndata\n:\nmyvalue\n:\n\"Hello World\"\n{{- end -}}\nYou can see that it simplifies the work we have to do by inheriting the common\nConfigMap definition which adds standard properties for ConfigMap. In our\ntemplate we add the configuration, in this case the data key\nmyvalue\nand its\nvalue. The configuration override the empty resource of the common ConfigMap.\nThis is feasible because of the helper function\nmylibchart.util.merge\nwe\nmentioned in the previous section.\nTo be able to use the common code, we need to add\nmylibchart\nas a dependency.\nAdd the following to the end of the file\nmychart/Chart.yaml\n:\n# My common code in my library chart\ndependencies\n:\n-\nname\n:\nmylibchart\nversion\n:\n0.1.0\nrepository\n:\nfile://../mylibchart\nThis includes the library chart as a dynamic dependency from the filesystem\nwhich is at the same parent path as our application chart. As we are including\nthe library chart as a dynamic dependency, we need to run\nhelm dependency update\n. It will copy the library chart into your\ncharts/\ndirectory.\n$\nhelm dependency update mychart/\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"stable\" chart repository\nUpdate Complete. ⎈Happy Helming!⎈\nSaving 1 charts\nDeleting outdated charts\nWe are now ready to deploy our chart. Before installing, it is worth checking\nthe rendered template first.\n$\nhelm install mydemo mychart/ --debug --dry-run\ninstall.go:159: [debug] Original chart version: \"\"\ninstall.go:176: [debug] CHART PATH: /root/test/helm-charts/mychart\nNAME: mydemo\nLAST DEPLOYED: Tue Mar  3 17:48:47 2020\nNAMESPACE: default\nSTATUS: pending-install\nREVISION: 1\nTEST SUITE: None\nUSER-SUPPLIED VALUES:\n{}\nCOMPUTED VALUES:\naffinity: {}\nfullnameOverride: \"\"\nimage:\npullPolicy: IfNotPresent\nrepository: nginx\nimagePullSecrets: []\ningress:\nannotations: {}\nenabled: false\nhosts:\n- host: chart-example.local\npaths: []\ntls: []\nmylibchart:\nglobal: {}\nnameOverride: \"\"\nnodeSelector: {}\npodSecurityContext: {}\nreplicaCount: 1\nresources: {}\nsecurityContext: {}\nservice:\nport: 80\ntype: ClusterIP\nserviceAccount:\nannotations: {}\ncreate: true\nname: null\ntolerations: []\nHOOKS:\nMANIFEST:\n---\n#\nSource: mychart/templates/configmap.yaml\napiVersion: v1\ndata:\nmyvalue: Hello World\nkind: ConfigMap\nmetadata:\nlabels:\napp: mychart\nchart: mychart-0.1.0\nrelease: mydemo\nname: mychart-mydemo\nThis looks like the ConfigMap we want with data override of\nmyvalue: Hello World\n. Lets install it:\n$\nhelm install mydemo mychart/\nNAME: mydemo\nLAST DEPLOYED: Tue Mar  3 17:52:40 2020\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nWe can retrieve the release and see that the actual template was loaded.\n$\nhelm get manifest mydemo\n---\n#\nSource: mychart/templates/configmap.yaml\napiVersion: v1\ndata:\nmyvalue: Hello World\nkind: ConfigMap\nmetadata:\nlabels:\napp: mychart\nchart: mychart-0.1.0\nrelease: mydemo\nname: mychart-mydemo\nLibrary Chart Benefits\nBecause of their inability to act as standalone charts, library charts can leverage the following functionality:\nThe\n.Files\nobject references the file paths on the parent chart, rather than the path local to the library chart\nThe\n.Values\nobject is the same as the parent chart, in contrast to application\nsubcharts\nwhich receive the section of values configured under their header in the parent.\nThe Common Helm Helper Chart\nNote: The Common Helm Helper Chart repo on Github is no longer actively maintained, and the repo has been deprecated and archived.\nThis\nchart\nwas\nthe original pattern for common charts. It provides utilities that reflect best\npractices of Kubernetes chart development. Best of all it can be used off the\nbat by you when developing your charts to give you handy shared code.\nHere is a quick way to use it. For more details, have a look at the\nREADME\n.\nCreate a scaffold chart again:\n$\nhelm create demo\nCreating demo\nLets use the common code from the helper chart. First, edit deployment\ndemo/templates/deployment.yaml\nas follows:\n{{- template \"common.deployment\" (list . \"demo.deployment\") -}}\n{{- define \"demo.deployment\" -}}\n## Define overrides for your Deployment resource here, e.g.\napiVersion\n:\napps/v1\nspec\n:\nreplicas\n:\n{{\n.Values.replicaCount }}\nselector\n:\nmatchLabels\n:\n{{- include \"demo.selectorLabels\" . | nindent 6 }}\ntemplate\n:\nmetadata\n:\nlabels\n:\n{{- include \"demo.selectorLabels\" . | nindent 8 }}\n{{- end -}}\nAnd now the service file,\ndemo/templates/service.yaml\nas follows:\n{{- template \"common.service\" (list . \"demo.service\") -}}\n{{- define \"demo.service\" -}}\n## Define overrides for your Service resource here, e.g.\n# metadata:\n#   labels:\n#     custom: label\n# spec:\n#   ports:\n#   - port: 8080\n{{- end -}}\nThese templates show how inheriting the common code from the helper chart\nsimplifies your coding down to your configuration or customization of the\nresources.\nTo be able to use the common code, we need to add\ncommon\nas a dependency. Add\nthe following to the end of the file\ndemo/Chart.yaml\n:\ndependencies\n:\n-\nname\n:\ncommon\nversion\n:\n\"^0.0.5\"\nrepository\n:\n\"https://charts.helm.sh/incubator/\"\nNote: You will need to add the\nincubator\nrepo to the Helm repository list\n(\nhelm repo add\n).\nAs we are including the chart as a dynamic dependency, we need to run\nhelm dependency update\n. It will copy the helper chart into your\ncharts/\ndirectory.\nAs helper chart is using some Helm 2 constructs, you will need to add the\nfollowing to\ndemo/values.yaml\nto enable the\nnginx\nimage to be loaded as this\nwas updated in Helm 3 scaffold chart:\nimage\n:\ntag\n:\n1.16.0\nYou can test that the chart templates are correct prior to deploying using the\nhelm lint\nand\nhelm template\ncommands.\nIf it's good to go, deploy away using\nhelm install\n!\nPrev\n← Chart Tests\nNext\nHelm Provenance and Integrity →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nHelm Provenance and Integrity\nHelm has provenance tools which help chart users verify the integrity and origin\nof a package. Using industry-standard tools based on PKI, GnuPG, and\nwell-respected package managers, Helm can generate and verify signature files.\nOverview\nIntegrity is established by comparing a chart to a provenance record. Provenance\nrecords are stored in\nprovenance files\n, which are stored alongside a packaged\nchart. For example, if a chart is named\nmyapp-1.2.3.tgz\n, its provenance file\nwill be\nmyapp-1.2.3.tgz.prov\n.\nProvenance files are generated at packaging time (\nhelm package --sign ...\n),\nand can be checked by multiple commands, notably\nhelm install --verify\n.\nThe Workflow\nThis section describes a potential workflow for using provenance data\neffectively.\nPrerequisites:\nA valid PGP keypair in a binary (not ASCII-armored) format\nThe\nhelm\ncommand line tool\nGnuPG command line tools (optional)\nKeybase command line tools (optional)\nNOTE:\nIf your PGP private key has a passphrase, you will be prompted to\nenter that passphrase for any commands that support the\n--sign\noption.\nCreating a new chart is the same as before:\n$\nhelm create mychart\nCreating mychart\nOnce ready to package, add the\n--sign\nflag to\nhelm package\n. Also, specify\nthe name under which the signing key is known and the keyring containing the\ncorresponding private key:\n$\nhelm package --sign --key\n'John Smith'\n--keyring path/to/keyring.secret mychart\nNote:\nThe value of the\n--key\nargument must be a substring of the desired\nkey's\nuid\n(in the output of\ngpg --list-keys\n), for example the name or email.\nThe fingerprint\ncannot\nbe used.\nTIP:\nfor GnuPG users, your secret keyring is in\n~/.gnupg/secring.gpg\n. You\ncan use\ngpg --list-secret-keys\nto list the keys you have.\nWarning:\nthe GnuPG v2 store your secret keyring using a new format\nkbx\non\nthe default location\n~/.gnupg/pubring.kbx\n. Please use the following command\nto convert your keyring to the legacy gpg format:\n$\ngpg --export >~/.gnupg/pubring.gpg\n$\ngpg --export-secret-keys >~/.gnupg/secring.gpg\nAt this point, you should see both\nmychart-0.1.0.tgz\nand\nmychart-0.1.0.tgz.prov\n. Both files should eventually be uploaded to your\ndesired chart repository.\nYou can verify a chart using\nhelm verify\n:\n$\nhelm verify mychart-0.1.0.tgz\nA failed verification looks like this:\n$\nhelm verify topchart-0.1.0.tgz\nError: sha256 sum does not match for topchart-0.1.0.tgz: \"sha256:1939fbf7c1023d2f6b865d137bbb600e0c42061c3235528b1e8c82f4450c12a7\" != \"sha256:5a391a90de56778dd3274e47d789a2c84e0e106e1a37ef8cfa51fd60ac9e623a\"\nTo verify during an install, use the\n--verify\nflag.\n$\nhelm install --generate-name --verify mychart-0.1.0.tgz\nIf the keyring containing the public key associated with the signed chart is not\nin the default location, you may need to point to the keyring with\n--keyring PATH\nas in the\nhelm package\nexample.\nIf verification fails, the install will be aborted before the chart is even\nrendered.\nUsing Keybase.io credentials\nThe\nKeybase.io\nservice makes it easy to establish a chain\nof trust for a cryptographic identity. Keybase credentials can be used to sign\ncharts.\nPrerequisites:\nA configured Keybase.io account\nGnuPG installed locally\nThe\nkeybase\nCLI installed locally\nSigning packages\nThe first step is to import your keybase keys into your local GnuPG keyring:\n$\nkeybase pgp\nexport\n-s | gpg --import\nThis will convert your Keybase key into the OpenPGP format, and then import it\nlocally into your\n~/.gnupg/secring.gpg\nfile.\nYou can double check by running\ngpg --list-secret-keys\n.\n$\ngpg --list-secret-keys\n/Users/mattbutcher/.gnupg/secring.gpg\n-------------------------------------\nsec   2048R/1FC18762 2016-07-25\nuid                  technosophos (keybase.io/technosophos) <technosophos@keybase.io>\nssb   2048R/D125E546 2016-07-25\nNote that your secret key will have an identifier string:\ntechnosophos (keybase.io/technosophos) <technosophos@keybase.io>\nThat is the full name of your key.\nNext, you can package and sign a chart with\nhelm package\n. Make sure you use at\nleast part of that name string in\n--key\n.\n$\nhelm package --sign --key technosophos --keyring ~/.gnupg/secring.gpg mychart\nAs a result, the\npackage\ncommand should produce both a\n.tgz\nfile and a\n.tgz.prov\nfile.\nVerifying packages\nYou can also use a similar technique to verify a chart signed by someone else's\nKeybase key. Say you want to verify a package signed by\nkeybase.io/technosophos\n. To do this, use the\nkeybase\ntool:\n$\nkeybase follow technosophos\n$\nkeybase pgp pull\nThe first command above tracks the user\ntechnosophos\n. Next\nkeybase pgp pull\ndownloads the OpenPGP keys of all of the accounts you follow, placing them in\nyour GnuPG keyring (\n~/.gnupg/pubring.gpg\n).\nAt this point, you can now use\nhelm verify\nor any of the commands with a\n--verify\nflag:\n$\nhelm verify somechart-1.2.3.tgz\nReasons a chart may not verify\nThese are common reasons for failure.\nThe\n.prov\nfile is missing or corrupt. This indicates that something is\nmisconfigured or that the original maintainer did not create a provenance\nfile.\nThe key used to sign the file is not in your keyring. This indicate that the\nentity who signed the chart is not someone you've already signaled that you\ntrust.\nThe verification of the\n.prov\nfile failed. This indicates that something is\nwrong with either the chart or the provenance data.\nThe file hashes in the provenance file do not match the hash of the archive\nfile. This indicates that the archive has been tampered with.\nIf a verification fails, there is reason to distrust the package.\nThe Provenance File\nThe provenance file contains a chart’s YAML file plus several pieces of\nverification information. Provenance files are designed to be automatically\ngenerated.\nThe following pieces of provenance data are added:\nThe chart file (\nChart.yaml\n) is included to give both humans and tools an\neasy view into the contents of the chart.\nThe signature (SHA256, just like Docker) of the chart package (the\n.tgz\nfile) is included, and may be used to verify the integrity of the chart\npackage.\nThe entire body is signed using the algorithm used by OpenPGP (see\nKeybase.io\nfor an emerging way of making crypto\nsigning and verification easy).\nThe combination of this gives users the following assurances:\nThe package itself has not been tampered with (checksum package\n.tgz\n).\nThe entity who released this package is known (via the GnuPG/PGP signature).\nThe format of the file looks something like this:\nHash: SHA512\napiVersion: v2\nappVersion: \"1.16.0\"\ndescription: Sample chart\nname: mychart\ntype: application\nversion: 0.1.0\n...\nfiles:\nmychart-0.1.0.tgz: sha256:d31d2f08b885ec696c37c7f7ef106709aaf5e8575b6d3dc5d52112ed29a9cb92\n-----BEGIN PGP SIGNATURE-----\nwsBcBAEBCgAQBQJdy0ReCRCEO7+YH8GHYgAAfhUIADx3pHHLLINv0MFkiEYpX/Kd\nnvHFBNps7hXqSocsg0a9Fi1LRAc3OpVh3knjPfHNGOy8+xOdhbqpdnB+5ty8YopI\nmYMWp6cP/Mwpkt7/gP1ecWFMevicbaFH5AmJCBihBaKJE4R1IX49/wTIaLKiWkv2\ncR64bmZruQPSW83UTNULtdD7kuTZXeAdTMjAK0NECsCz9/eK5AFggP4CDf7r2zNi\nhZsNrzloIlBZlGGns6mUOTO42J/+JojnOLIhI3Psd0HBD2bTlsm/rSfty4yZUs7D\nqtgooNdohoyGSzR5oapd7fEvauRQswJxOA0m0V+u9/eyLR0+JcYB8Udi1prnWf8=\n=aHfz\n-----END PGP SIGNATURE-----\nNote that the YAML section contains two documents (separated by\n...\\n\n). The\nfirst file is the content of\nChart.yaml\n. The second is the checksums, a map of\nfilenames to SHA-256 digests of that file's content at packaging time.\nThe signature block is a standard PGP signature, which provides\ntamper\nresistance\n.\nChart Repositories\nChart repositories serve as a centralized collection of Helm charts.\nChart repositories must make it possible to serve provenance files over HTTP via\na specific request, and must make them available at the same URI path as the\nchart.\nFor example, if the base URL for a package is\nhttps://example.com/charts/mychart-1.2.3.tgz\n, the provenance file, if it\nexists, MUST be accessible at\nhttps://example.com/charts/mychart-1.2.3.tgz.prov\n.\nFrom the end user's perspective,\nhelm install --verify myrepo/mychart-1.2.3\nshould result in the download of both the chart and the provenance file with no\nadditional user configuration or action.\nSignatures in OCI-based registries\nWhen publishing charts to an\nOCI-based registry\n, the\nhelm-sigstore\nplugin\ncan be used\nto publish provenance to\nsigstore\n.\nAs described in the\ndocumentation\n, the\nprocess of creating provenance and signing with a GPG key are common, but the\nhelm sigstore upload\ncommand can be used to publish the provenance to an\nimmutable transparency log.\nEstablishing Authority and Authenticity\nWhen dealing with chain-of-trust systems, it is important to be able to\nestablish the authority of a signer. Or, to put this plainly, the system above\nhinges on the fact that you trust the person who signed the chart. That, in\nturn, means you need to trust the public key of the signer.\nOne of the design decisions with Helm has been that the Helm project would not\ninsert itself into the chain of trust as a necessary party. We don't want to be\n\"the certificate authority\" for all chart signers. Instead, we strongly favor a\ndecentralized model, which is part of the reason we chose OpenPGP as our\nfoundational technology. So when it comes to establishing authority, we have\nleft this step more-or-less undefined in Helm 2 (a decision carried forward in\nHelm 3).\nHowever, we have some pointers and recommendations for those interested in using\nthe provenance system:\nThe\nKeybase\nplatform provides a public centralized\nrepository for trust information.\nYou can use Keybase to store your keys or to get the public keys of others.\nKeybase also has fabulous documentation available\nWhile we haven't tested it, Keybase's \"secure website\" feature could be used\nto serve Helm charts.\nThe basic idea is that an official \"chart reviewer\" signs charts with her or\nhis key, and the resulting provenance file is then uploaded to the chart\nrepository.\nThere has been some work on the idea that a list of valid signing keys may\nbe included in the\nindex.yaml\nfile of a repository.\nPrev\n← Library Charts\nNext\nThe Chart Repository Guide →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nThe Chart Repository Guide\nThis section explains how to create and work with Helm chart repositories. At a\nhigh level, a chart repository is a location where packaged charts can be stored\nand shared.\nThe distributed community Helm chart repository is located at\nArtifact Hub\nand welcomes\nparticipation. But Helm also makes it possible to create and run your own chart\nrepository. This guide explains how to do so. If you are considering creating a\nchart repository, you may want to consider using an\nOCI registry\ninstead.\nPrerequisites\nGo through the\nQuickstart\nGuide\nRead through the\nCharts\ndocument\nCreate a chart repository\nA\nchart repository\nis an HTTP server that houses an\nindex.yaml\nfile and\noptionally some packaged charts. When you're ready to share your charts, the\npreferred way to do so is by uploading them to a chart repository.\nAs of Helm 2.2.0, client-side SSL auth to a repository is supported. Other\nauthentication protocols may be available as plugins.\nBecause a chart repository can be any HTTP server that can serve YAML and tar\nfiles and can answer GET requests, you have a plethora of options when it comes\ndown to hosting your own chart repository. For example, you can use a Google\nCloud Storage (GCS) bucket, Amazon S3 bucket, GitHub Pages, or even create your\nown web server.\nThe chart repository structure\nA chart repository consists of packaged charts and a special file called\nindex.yaml\nwhich contains an index of all of the charts in the repository.\nFrequently, the charts that\nindex.yaml\ndescribes are also hosted on the same\nserver, as are the\nprovenance files\n.\nFor example, the layout of the repository\nhttps://example.com/charts\nmight\nlook like this:\ncharts/\n|\n|- index.yaml\n|\n|- alpine-0.1.2.tgz\n|\n|- alpine-0.1.2.tgz.prov\nIn this case, the index file would contain information about one chart, the\nAlpine chart, and provide the download URL\nhttps://example.com/charts/alpine-0.1.2.tgz\nfor that chart.\nIt is not required that a chart package be located on the same server as the\nindex.yaml\nfile. However, doing so is often the easiest.\nThe index file\nThe index file is a yaml file called\nindex.yaml\n. It contains some metadata\nabout the package, including the contents of a chart's\nChart.yaml\nfile. A\nvalid chart repository must have an index file. The index file contains\ninformation about each chart in the chart repository. The\nhelm repo index\ncommand will generate an index file based on a given local directory that\ncontains packaged charts.\nThis is an example of an index file:\napiVersion\n:\nv1\nentries\n:\nalpine\n:\n-\ncreated\n:\n2016-10-06T16:23:20.499814565\n-06\n:\n00\ndescription\n:\nDeploy a basic Alpine Linux pod\ndigest\n:\n99c76e403d752c84ead610644d4b1c2f2b453a74b921f422b9dcb8a7c8b559cd\nhome\n:\nhttps://helm.sh/helm\nname\n:\nalpine\nsources\n:\n- https://github.com/helm/helm\nurls\n:\n- https://technosophos.github.io/tscharts/alpine-0.2.0.tgz\nversion\n:\n0.2.0\n-\ncreated\n:\n2016-10-06T16:23:20.499543808\n-06\n:\n00\ndescription\n:\nDeploy a basic Alpine Linux pod\ndigest\n:\n515c58e5f79d8b2913a10cb400ebb6fa9c77fe813287afbacf1a0b897cd78727\nhome\n:\nhttps://helm.sh/helm\nname\n:\nalpine\nsources\n:\n- https://github.com/helm/helm\nurls\n:\n- https://technosophos.github.io/tscharts/alpine-0.1.0.tgz\nversion\n:\n0.1.0\nnginx\n:\n-\ncreated\n:\n2016-10-06T16:23:20.499543808\n-06\n:\n00\ndescription\n:\nCreate a basic nginx HTTP server\ndigest\n:\naaff4545f79d8b2913a10cb400ebb6fa9c77fe813287afbacf1a0b897cdffffff\nhome\n:\nhttps://helm.sh/helm\nname\n:\nnginx\nsources\n:\n- https://github.com/helm/charts\nurls\n:\n- https://technosophos.github.io/tscharts/nginx-1.1.0.tgz\nversion\n:\n1.1.0\ngenerated\n:\n2016-10-06T16:23:20.499029981\n-06\n:\n00\nHosting Chart Repositories\nThis part shows several ways to serve a chart repository.\nGoogle Cloud Storage\nThe first step is to\ncreate your GCS bucket\n. We'll call ours\nfantastic-charts\n.\nNext, make your bucket public by\nediting the bucket permissions\n.\nInsert this line item to\nmake your bucket public\n:\nCongratulations, now you have an empty GCS bucket ready to serve charts!\nYou may upload your chart repository using the Google Cloud Storage command\nline tool, or using the GCS web UI. A public GCS bucket can be accessed via\nsimple HTTPS at this address:\nhttps://bucket-name.storage.googleapis.com/\n.\nCloudsmith\nYou can also set up chart repositories using Cloudsmith. Read more about\nchart repositories with Cloudsmith\nhere\nJFrog Artifactory\nSimilarly, you can also set up chart repositories using JFrog Artifactory. Read more about\nchart repositories with JFrog Artifactory\nhere\nGitHub Pages example\nIn a similar way you can create charts repository using GitHub Pages.\nGitHub allows you to serve static web pages in two different ways:\nBy configuring a project to serve the contents of its\ndocs/\ndirectory\nBy configuring a project to serve a particular branch\nWe'll take the second approach, though the first is just as easy.\nThe first step will be to\ncreate your gh-pages branch\n. You can do that\nlocally as.\n$\ngit checkout -b gh-pages\nOr via web browser using\nBranch\nbutton on your GitHub repository:\nNext, you'll want to make sure your\ngh-pages branch\nis set as GitHub Pages,\nclick on your repo\nSettings\nand scroll down to\nGitHub pages\nsection and\nset as per below:\nBy default\nSource\nusually gets set to\ngh-pages branch\n. If this is not\nset by default, then select it.\nYou can use a\ncustom domain\nthere if you wish so.\nAnd check that\nEnforce HTTPS\nis ticked, so the\nHTTPS\nwill be used when\ncharts are served.\nIn such setup you can use your default branch to store your charts code, and\ngh-pages branch\nas charts repository, e.g.:\nhttps://USERNAME.github.io/REPONAME\n. The demonstration\nTS\nCharts\nrepository is accessible at\nhttps://technosophos.github.io/tscharts/\n.\nIf you have decided to use GitHub pages to host the chart repository, check out\nChart Releaser Action\n.\nChart Releaser Action is a GitHub Action workflow to turn a GitHub project into\na self-hosted Helm chart repo, using\nhelm/chart-releaser\nCLI tool.\nOrdinary web servers\nTo configure an ordinary web server to serve Helm charts, you merely need to do\nthe following:\nPut your index and charts in a directory that the server can serve\nMake sure the\nindex.yaml\nfile can be accessed with no authentication\nrequirement\nMake sure\nyaml\nfiles are served with the correct content type (\ntext/yaml\nor\ntext/x-yaml\n)\nFor example, if you want to serve your charts out of\n$WEBROOT/charts\n, make\nsure there is a\ncharts/\ndirectory in your web root, and put the index file and\ncharts inside of that folder.\nChartMuseum Repository Server\nChartMuseum is an open-source Helm Chart Repository server written in Go\n(Golang), with support for cloud storage backends, including\nGoogle Cloud\nStorage\n,\nAmazon\nS3\n,\nMicrosoft Azure Blob\nStorage\n,\nAlibaba\nCloud OSS Storage\n,\nOpenstack Object\nStorage\n,\nOracle Cloud\nInfrastructure Object Storage\n,\nBaidu Cloud\nBOS Storage\n,\nTencent Cloud Object\nStorage\n,\nDigitalOcean\nSpaces\n,\nMinio\n, and\netcd\n.\nYou can also use the\nChartMuseum\nserver to host a chart repository from a local file system.\nGitLab Package Registry\nWith GitLab you can publish Helm charts in your project’s Package Registry.\nRead more about setting up a helm package repository with GitLab\nhere\n.\nManaging Chart Repositories\nNow that you have a chart repository, the last part of this guide explains how\nto maintain charts in that repository.\nStore charts in your chart repository\nNow that you have a chart repository, let's upload a chart and an index file to\nthe repository. Charts in a chart repository must be packaged (\nhelm package chart-name/\n) and versioned correctly (following\nSemVer 2\nguidelines).\nThese next steps compose an example workflow, but you are welcome to use\nwhatever workflow you fancy for storing and updating charts in your chart\nrepository.\nOnce you have a packaged chart ready, create a new directory, and move your\npackaged chart to that directory.\n$\nhelm package docs/examples/alpine/\n$\nmkdir fantastic-charts\n$\nmv alpine-0.1.0.tgz fantastic-charts/\n$\nhelm repo index fantastic-charts --url https://fantastic-charts.storage.googleapis.com\nThe last command takes the path of the local directory that you just created and\nthe URL of your remote chart repository and composes an\nindex.yaml\nfile inside\nthe given directory path.\nNow you can upload the chart and the index file to your chart repository using a\nsync tool or manually. If you're using Google Cloud Storage, check out this\nexample workflow\nusing the gsutil client. For GitHub, you can simply put the charts in the\nappropriate destination branch.\nAdd new charts to an existing repository\nEach time you want to add a new chart to your repository, you must regenerate\nthe index. The\nhelm repo index\ncommand will completely rebuild the\nindex.yaml\nfile from scratch, including only the charts that it finds locally.\nHowever, you can use the\n--merge\nflag to incrementally add new charts to an\nexisting\nindex.yaml\nfile (a great option when working with a remote repository\nlike GCS). Run\nhelm repo index --help\nto learn more,\nMake sure that you upload both the revised\nindex.yaml\nfile and the chart. And\nif you generated a provenance file, upload that too.\nShare your charts with others\nWhen you're ready to share your charts, simply let someone know what the URL of\nyour repository is.\nFrom there, they will add the repository to their helm client via the\nhelm repo add [NAME] [URL]\ncommand with any name they would like to use to reference the\nrepository.\n$\nhelm repo add fantastic-charts https://fantastic-charts.storage.googleapis.com\n$\nhelm repo list\nfantastic-charts    https://fantastic-charts.storage.googleapis.com\nIf the charts are backed by HTTP basic authentication, you can also supply the\nusername and password here:\n$\nhelm repo add fantastic-charts https://fantastic-charts.storage.googleapis.com --username my-username --password my-password\n$\nhelm repo list\nfantastic-charts    https://fantastic-charts.storage.googleapis.com\nNote:\nA repository will not be added if it does not contain a valid\nindex.yaml\n.\nNote:\nIf your helm repository is e.g. using a self signed\ncertificate, you can use\nhelm repo add --insecure-skip-tls-verify ...\nin order\nto skip the CA verification.\nAfter that, your users will be able to search through your charts. After you've\nupdated the repository, they can use the\nhelm repo update\ncommand to get the\nlatest chart information.\nUnder the hood, the\nhelm repo add\nand\nhelm repo update\ncommands are\nfetching the index.yaml file and storing them in the\n$XDG_CACHE_HOME/helm/repository/cache/\ndirectory. This is where the\nhelm search\nfunction finds information about charts.\nPrev\n← Helm Provenance and Integrity\nNext\nUse OCI-based registries →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nUse OCI-based registries\nBeginning in Helm 3, you can use container registries with\nOCI\nsupport to store and share chart packages. Beginning in Helm v3.8.0, OCI support is enabled by default.\nOCI support prior to v3.8.0\nOCI support graduated from experimental to general availability with Helm v3.8.0. In prior versions of Helm, OCI support behaved differently. If you were using OCI support prior to Helm v3.8.0, its important to understand what has changed with different versions of Helm.\nEnabling OCI support prior to v3.8.0\nPrior to Helm v3.8.0, OCI support is\nexperimental\nand must be enabled.\nTo enable OCI experimental support for Helm versions prior to v3.8.0, set\nHELM_EXPERIMENTAL_OCI\nin your environment. For example:\nexport HELM_EXPERIMENTAL_OCI=1\nOCI feature deprecation and behavior changes with v3.8.0\nThe release of\nHelm v3.8.0\n, the following features and behaviors are different from previous versions of Helm:\nWhen setting a chart in the dependencies as OCI, the version can be set to a range like other dependencies.\nSemVer tags that include build information can be pushed and used. OCI registries don't support\n+\nas a tag character. Helm translates the\n+\nto\n_\nwhen stored as a tag.\nThe\nhelm registry login\ncommand now follows the same structure as the Docker CLI for storing credentials. The same location for registry configuration can be passed to both Helm and the Docker CLI.\nOCI feature deprecation and behavior changes with v3.7.0\nThe release of\nHelm v3.7.0\nincluded the implementation of\nHIP 6\nfor OCI support. As a result, the following features and behaviors are different from previous versions of Helm:\nThe\nhelm chart\nsubcommand has been removed.\nThe chart cache has been removed (no\nhelm chart list\netc.).\nOCI registry references are now always prefixed with\noci://\n.\nThe basename of the registry reference must\nalways\nmatch the chart's name.\nThe tag of the registry reference must\nalways\nmatch the chart's semantic version (i.e. no\nlatest\ntags).\nThe chart layer media type was switched from\napplication/tar+gzip\nto\napplication/vnd.cncf.helm.chart.content.v1.tar+gzip\n.\nUsing an OCI-based registry\nHelm repositories in OCI-based registries\nA\nHelm repository\nis a way to house and distribute packaged Helm charts. An OCI-based registry can contain zero or more Helm repositories and each of those repositories can contain zero or more packaged Helm charts.\nUse hosted registries\nThere are several hosted container registries with OCI support that you can use for your Helm charts. For example:\nAmazon ECR\nAzure Container Registry\nCloudsmith\nDocker Hub\nGoogle Artifact Registry\nHarbor\nIBM Cloud Container Registry\nJFrog Artifactory\nFollow the hosted container registry provider's documentation to create and configure a registry with OCI support.\nNote:\nYou can run\nDocker Registry\nor\nzot\n, which are OCI-based registries, on your development computer. Running an OCI-based registry on your development computer should only be used for testing purposes.\nUsing sigstore to sign OCI-based charts\nThe\nhelm-sigstore\nplugin allows using\nSigstore\nto sign Helm charts with the same tools used to sign container images. This provides an alternative to the\nGPG-based provenance\nsupported by classic\nchart repositories\n.\nFor more details on using the\nhelm sigstore\nplugin, see\nthat project's documentation\n.\nCommands for working with registries\nThe\nregistry\nsubcommand\nlogin\nlogin to a registry (with manual password entry)\n$\nhelm registry login -u myuser localhost:5000\nPassword:\nLogin succeeded\nlogout\nlogout from a registry\n$\nhelm registry\nlogout\nlocalhost:5000\nLogout succeeded\nThe\npush\nsubcommand\nUpload a chart to an OCI-based registry:\n$\nhelm push mychart-0.1.0.tgz oci://localhost:5000/helm-charts\nPushed: localhost:5000/helm-charts/mychart:0.1.0\nDigest: sha256:ec5f08ee7be8b557cd1fc5ae1a0ac985e8538da7c93f51a51eff4b277509a723\nThe\npush\nsubcommand can only be used against\n.tgz\nfiles\ncreated ahead of time using\nhelm package\n.\nWhen using\nhelm push\nto upload a chart an OCI registry, the reference\nmust be prefixed with\noci://\nand must not contain the basename or tag.\nThe registry reference basename is inferred from the chart's name,\nand the tag is inferred from the chart's semantic version. This is\ncurrently a strict requirement.\nCertain registries require the repository and/or namespace (if specified)\nto be created beforehand. Otherwise, an error will be produced during the\nhelm push\noperation.\nIf you have created a\nprovenance file\n(\n.prov\n), and it is present next to the chart\n.tgz\nfile, it will\nautomatically be uploaded to the registry upon\npush\n. This results in\nan extra layer on\nthe Helm chart manifest\n.\nUsers of the\nhelm-push plugin\n(for uploading charts to\nChartMuseum\n)\nmay experience issues, since the plugin conflicts with the new, built-in\npush\n.\nAs of version v0.10.0, the plugin has been renamed to\ncm-push\n.\nOther subcommands\nSupport for the\noci://\nprotocol is also available in various other subcommands.\nHere is a complete list:\nhelm pull\nhelm show\nhelm template\nhelm install\nhelm upgrade\nThe basename (chart name) of the registry reference\nis\nincluded for any type of action involving chart download\n(vs.\nhelm push\nwhere it is omitted).\nHere are a few examples of using the subcommands listed above against\nOCI-based charts:\n$ helm pull oci://localhost:5000/helm-charts/mychart --version 0.1.0\nPulled: localhost:5000/helm-charts/mychart:0.1.0\nDigest: sha256:0be7ec9fb7b962b46d81e4bb74fdcdb7089d965d3baca9f85d64948b05b402ff\n$ helm show all oci://localhost:5000/helm-charts/mychart --version 0.1.0\napiVersion: v2\nappVersion: 1.16.0\ndescription: A Helm chart for Kubernetes\nname: mychart\n...\n$ helm template myrelease oci://localhost:5000/helm-charts/mychart --version 0.1.0\n---\n# Source: mychart/templates/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\n...\n$ helm install myrelease oci://localhost:5000/helm-charts/mychart --version 0.1.0\nNAME: myrelease\nLAST DEPLOYED: Wed Oct 27 15:11:40 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n...\n$ helm upgrade myrelease oci://localhost:5000/helm-charts/mychart --version 0.2.0\nRelease \"myrelease\" has been upgraded. Happy Helming!\nNAME: myrelease\nLAST DEPLOYED: Wed Oct 27 15:12:05 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 2\nNOTES:\n...\nSpecifying dependencies\nDependencies of a chart can be pulled from a registry using the\ndependency update\nsubcommand.\nThe\nrepository\nfor a given entry in\nChart.yaml\nis specified as the registry reference without the basename:\ndependencies:\n- name: mychart\nversion: \"2.7.0\"\nrepository: \"oci://localhost:5000/myrepo\"\nThis will fetch\noci://localhost:5000/myrepo/mychart:2.7.0\nwhen\ndependency update\nis executed.\nHelm chart manifest\nExample Helm chart manifest as represented in a registry\n(note the\nmediaType\nfields):\n{\n\"schemaVersion\"\n:\n2\n,\n\"config\"\n: {\n\"mediaType\"\n:\n\"application/vnd.cncf.helm.config.v1+json\"\n,\n\"digest\"\n:\n\"sha256:8ec7c0f2f6860037c19b54c3cfbab48d9b4b21b485a93d87b64690fdb68c2111\"\n,\n\"size\"\n:\n117\n},\n\"layers\"\n: [\n{\n\"mediaType\"\n:\n\"application/vnd.cncf.helm.chart.content.v1.tar+gzip\"\n,\n\"digest\"\n:\n\"sha256:1b251d38cfe948dfc0a5745b7af5ca574ecb61e52aed10b19039db39af6e1617\"\n,\n\"size\"\n:\n2487\n}\n]\n}\nThe following example contains a\nprovenance file\n(note the extra layer):\n{\n\"schemaVersion\"\n:\n2\n,\n\"config\"\n: {\n\"mediaType\"\n:\n\"application/vnd.cncf.helm.config.v1+json\"\n,\n\"digest\"\n:\n\"sha256:8ec7c0f2f6860037c19b54c3cfbab48d9b4b21b485a93d87b64690fdb68c2111\"\n,\n\"size\"\n:\n117\n},\n\"layers\"\n: [\n{\n\"mediaType\"\n:\n\"application/vnd.cncf.helm.chart.content.v1.tar+gzip\"\n,\n\"digest\"\n:\n\"sha256:1b251d38cfe948dfc0a5745b7af5ca574ecb61e52aed10b19039db39af6e1617\"\n,\n\"size\"\n:\n2487\n},\n{\n\"mediaType\"\n:\n\"application/vnd.cncf.helm.chart.provenance.v1.prov\"\n,\n\"digest\"\n:\n\"sha256:3e207b409db364b595ba862cdc12be96dcdad8e36c59a03b7b3b61c946a5741a\"\n,\n\"size\"\n:\n643\n}\n]\n}\nMigrating from chart repos\nMigrating from classic\nchart repositories\n(index.yaml-based repos) is as simple using\nhelm pull\n, then using\nhelm push\nto upload the resulting\n.tgz\nfiles to a registry.\nPrev\n← The Chart Repository Guide\nNext\nHelm Architecture →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nHelm Architecture\nHelm Architecture\nThis document describes the Helm architecture at a high level.\nThe Purpose of Helm\nHelm is a tool for managing Kubernetes packages called\ncharts\n. Helm can do the\nfollowing:\nCreate new charts from scratch\nPackage charts into chart archive (tgz) files\nInteract with chart repositories where charts are stored\nInstall and uninstall charts into an existing Kubernetes cluster\nManage the release cycle of charts that have been installed with Helm\nFor Helm, there are three important concepts:\nThe\nchart\nis a bundle of information necessary to create an instance of a\nKubernetes application.\nThe\nconfig\ncontains configuration information that can be merged into a\npackaged chart to create a releasable object.\nA\nrelease\nis a running instance of a\nchart\n, combined with a specific\nconfig\n.\nComponents\nHelm is an executable which is implemented into two distinct parts:\nThe Helm Client\nis a command-line client for end users. The client is\nresponsible for the following:\nLocal chart development\nManaging repositories\nManaging releases\nInterfacing with the Helm library\nSending charts to be installed\nRequesting upgrading or uninstalling of existing releases\nThe Helm Library\nprovides the logic for executing all Helm operations. It\ninterfaces with the Kubernetes API server and provides the following capability:\nCombining a chart and configuration to build a release\nInstalling charts into Kubernetes, and providing the subsequent release object\nUpgrading and uninstalling charts by interacting with Kubernetes\nThe standalone Helm library encapsulates the Helm logic so that it can be\nleveraged by different clients.\nImplementation\nThe Helm client and library is written in the Go programming language.\nThe library uses the Kubernetes client library to communicate with Kubernetes.\nCurrently, that library uses REST+JSON. It stores information in Secrets located\ninside of Kubernetes. It does not need its own database.\nConfiguration files are, when possible, written in YAML.\nPrev\n← Use OCI-based registries\nNext\nAdvanced Helm Techniques →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nAdvanced Helm Techniques\nThis section explains various advanced features and techniques for using Helm.\nThe information in this section is intended for \"power users\" of Helm that wish\nto do advanced customization and manipulation of their charts and releases. Each\nof these advanced features comes with their own tradeoffs and caveats, so each\none must be used carefully and with deep knowledge of Helm. Or in other words,\nremember the\nPeter Parker\nprinciple\nPost Rendering\nPost rendering gives chart installers the ability to manually manipulate,\nconfigure, and/or validate rendered manifests before they are installed by Helm.\nThis allows users with advanced configuration needs to be able to use tools like\nkustomize\nto apply configuration changes without the\nneed to fork a public chart or requiring chart maintainers to specify every last\nconfiguration option for a piece of software. There are also use cases for\ninjecting common tools and side cars in enterprise environments or analysis of\nthe manifests before deployment.\nPrerequisites\nHelm 3.1+\nUsage\nA post-renderer can be any executable that accepts rendered Kubernetes manifests\non STDIN and returns valid Kubernetes manifests on STDOUT. It should return an\nnon-0 exit code in the event of a failure. This is the only \"API\" between the\ntwo components. It allows for great flexibility in what you can do with your\npost-render process.\nA post renderer can be used with\ninstall\n,\nupgrade\n, and\ntemplate\n. To use a\npost-renderer, use the\n--post-renderer\nflag with a path to the renderer\nexecutable you wish to use:\n$ helm install mychart stable/wordpress --post-renderer ./path/to/executable\nIf the path does not contain any separators, it will search in $PATH, otherwise\nit will resolve any relative paths to a fully qualified path\nIf you wish to use multiple post-renderers, call all of them in a script or\ntogether in whatever binary tool you have built. In bash, this would be as\nsimple as\nrenderer1 | renderer2 | renderer3\n.\nYou can see an example of using\nkustomize\nas a post renderer\nhere\n.\nCaveats\nWhen using post renderers, there are several important things to keep in mind.\nThe most important of these is that when using a post-renderer, all people\nmodifying that release\nMUST\nuse the same renderer in order to have\nrepeatable builds. This feature is purposefully built to allow any user to\nswitch out which renderer they are using or to stop using a renderer, but this\nshould be done deliberately to avoid accidental modification or data loss.\nOne other important note is around security. If you are using a post-renderer,\nyou should ensure it is coming from a reliable source (as is the case for any\nother arbitrary executable). Using non-trusted or non-verified renderers is NOT\nrecommended as they have full access to rendered templates, which often contain\nsecret data.\nCustom Post Renderers\nThe post render step offers even more flexibility when used in the Go SDK. Any\npost renderer only needs to implement the following Go interface:\ntype\nPostRenderer\ninterface\n{\n// Run expects a single buffer filled with Helm rendered manifests. It\n// expects the modified results to be returned on a separate buffer or an\n// error if there was an issue or failure while running the post render step\nRun\n(renderedManifests\n*\nbytes.Buffer) (modifiedManifests\n*\nbytes.Buffer, err\nerror\n)\n}\nFor more information on using the Go SDK, See the\nGo SDK section\nGo SDK\nHelm 3 debuted a completely restructured Go SDK for a better experience when\nbuilding software and tools that leverage Helm. Full documentation can be found\nin the\nGo SDK Section\n.\nStorage backends\nHelm 3 changed the default release information storage to Secrets in the\nnamespace of the release. Helm 2 by default stores release information as\nConfigMaps in the namespace of the Tiller instance. The subsections which follow\nshow how to configure different backends. This configuration is based on the\nHELM_DRIVER\nenvironment variable. It can be set to one of the values:\n[configmap, secret, sql]\n.\nConfigMap storage backend\nTo enable the ConfigMap backend, you'll need to set the environmental variable\nHELM_DRIVER\nto\nconfigmap\n.\nYou can set it in a shell as follows:\nexport\nHELM_DRIVER\n=\nconfigmap\nIf you want to switch from the default backend to the ConfigMap backend, you'll\nhave to do the migration for this on your own. You can retrieve release\ninformation with the following command:\nkubectl get secret --all-namespaces -l\n\"owner=helm\"\nPRODUCTION NOTES\n: The release information includes the contents of charts and\nvalues files, and therefore might contain sensitive data (like\npasswords, private keys, and other credentials) that needs to be protected from\nunauthorized access. When managing Kubernetes authorization, for instance with\nRBAC\n, it is\npossible to grant broader access to ConfigMap resources, while restricting\naccess to Secret resources. For instance, the default\nuser-facing\nrole\n\"view\" grants access to most resources, but not to Secrets. Furthermore, secrets\ndata can be configured for\nencrypted\nstorage\n.\nPlease keep that in mind if you decide to switch to the ConfigMap backend, as it\ncould expose your application's sensitive data.\nSQL storage backend\nThere is a\nbeta\nSQL storage backend that stores release information in an SQL\ndatabase.\nUsing such a storage backend is particularly useful if your release information\nweighs more than 1MB (in which case, it can't be stored in ConfigMaps/Secrets\nbecause of internal limits in Kubernetes' underlying etcd key-value store).\nTo enable the SQL backend, you'll need to deploy a SQL database and set the\nenvironmental variable\nHELM_DRIVER\nto\nsql\n. The DB details are set with the\nenvironmental variable\nHELM_DRIVER_SQL_CONNECTION_STRING\n.\nYou can set it in a shell as follows:\nexport\nHELM_DRIVER\n=\nsql\nexport\nHELM_DRIVER_SQL_CONNECTION_STRING\n=\npostgresql://helm-postgres:5432/helm?user\n=\nhelm&\npassword\n=\nchangeme\nNote: Only PostgreSQL is supported at this moment.\nPRODUCTION NOTES\n: It is recommended to:\nMake your database production ready. For PostgreSQL, refer to the\nServer Administration\ndocs for more details\nEnable\npermission management\nto\nmirror Kubernetes RBAC for release information\nIf you want to switch from the default backend to the SQL backend, you'll have\nto do the migration for this on your own. You can retrieve release information\nwith the following command:\nkubectl get secret --all-namespaces -l\n\"owner=helm\"\nPrev\n← Helm Architecture\nNext\nKubernetes Distribution Guide →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Distribution Guide\nHelm should work with any\nconformant version of\nKubernetes\n(whether\ncertified\nor not).\nThis document captures information about using Helm in specific Kubernetes\nenvironments. Please contribute more details about any distros (sorted\nalphabetically) if desired.\nAKS\nHelm works with\nAzure Kubernetes\nService\n.\nDC/OS\nHelm has been tested and is working on Mesospheres DC/OS 1.11 Kubernetes\nplatform, and requires no additional configuration.\nEKS\nHelm works with Amazon Elastic Kubernetes Service (Amazon EKS):\nUsing Helm with Amazon\nEKS\n.\nGKE\nGoogle's GKE hosted Kubernetes platform is known to work with Helm, and requires\nno additional configuration.\nscripts/local-cluster\nand Hyperkube\nHyperkube configured via\nscripts/local-cluster.sh\nis known to work. For raw\nHyperkube you may need to do some manual configuration.\nIKS\nHelm works with\nIBM Cloud Kubernetes\nService\n.\nKIND (Kubernetes IN Docker)\nHelm is regularly tested on\nKIND\n.\nKubeOne\nHelm works in clusters that are set up by KubeOne without caveats.\nKubermatic\nHelm works in user clusters that are created by Kubermatic without caveats.\nSince seed cluster can be set up in different ways Helm support depends on their\nconfiguration.\nMicroK8s\nHelm can be enabled in\nMicroK8s\nusing the command:\nmicrok8s.enable helm3\nMinikube\nHelm is tested and known to work with\nMinikube\n. It requires no additional\nconfiguration.\nOpenshift\nHelm works straightforward on OpenShift Online, OpenShift Dedicated, OpenShift\nContainer Platform (version >= 3.6) or OpenShift Origin (version >= 3.6). To\nlearn more read\nthis\nblog\npost.\nPlatform9\nHelm is pre-installed with\nPlatform9 Managed\nKubernetes\n.\nPlatform9 provides access to all official Helm charts through the App Catalog UI\nand native Kubernetes CLI. Additional repositories can be manually added.\nFurther details are available in this\nPlatform9 App Catalog\narticle\n.\nUbuntu with\nkubeadm\nKubernetes bootstrapped with\nkubeadm\nis known to work on the following Linux\ndistributions:\nUbuntu 16.04\nFedora release 25\nSome versions of Helm (v2.0.0-beta2) require you to\nexport KUBECONFIG=/etc/kubernetes/admin.conf\nor create a\n~/.kube/config\n.\nVMware Tanzu Kubernetes Grid\nHelm runs on VMware Tanzu Kubernetes Grid, TKG, without needing configuration changes.\nThe Tanzu CLI can manage installing packages for\nhelm-controller\nallowing for declaratively managing Helm chart releases.\nFurther details available in the TKG documentation for\nCLI-Managed Packages\n.\nPrev\n← Advanced Helm Techniques\nNext\nRole-based Access Control →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nRole-based Access Control\nIn Kubernetes, granting roles to a user or an application-specific service\naccount is a best practice to ensure that your application is operating in the\nscope that you have specified. Read more about service account permissions\nin\nthe official Kubernetes\ndocs\n.\nFrom Kubernetes 1.6 onwards, Role-based Access Control is enabled by default.\nRBAC allows you to specify which types of actions are permitted depending on the\nuser and their role in your organization.\nWith RBAC, you can\ngrant privileged operations (creating cluster-wide resources, like new roles)\nto administrators\nlimit a user's ability to create resources (pods, persistent volumes,\ndeployments) to specific namespaces, or in cluster-wide scopes (resource\nquotas, roles, custom resource definitions)\nlimit a user's ability to view resources either in specific namespaces or at a\ncluster-wide scope.\nThis guide is for administrators who want to restrict the scope of a user's\ninteraction with the Kubernetes API.\nManaging user accounts\nAll Kubernetes clusters have two categories of users: service accounts managed\nby Kubernetes, and normal users.\nNormal users are assumed to be managed by an outside, independent service. An\nadministrator distributing private keys, a user store like Keystone or Google\nAccounts, even a file with a list of usernames and passwords. In this regard,\nKubernetes does not have objects which represent normal user accounts. Normal\nusers cannot be added to a cluster through an API call.\nIn contrast, service accounts are users managed by the Kubernetes API. They are\nbound to specific namespaces, and created automatically by the API server or\nmanually through API calls. Service accounts are tied to a set of credentials\nstored as Secrets, which are mounted into pods allowing in-cluster processes to\ntalk to the Kubernetes API.\nAPI requests are tied to either a normal user or a service account, or are\ntreated as anonymous requests. This means every process inside or outside the\ncluster, from a human user typing\nkubectl\non a workstation, to kubelets on\nnodes, to members of the control plane, must authenticate when making requests\nto the API server, or be treated as an anonymous user.\nRoles, ClusterRoles, RoleBindings, and ClusterRoleBindings\nIn Kubernetes, user accounts and service accounts can only view and edit\nresources they have been granted access to. This access is granted through the\nuse of Roles and RoleBindings. Roles and RoleBindings are bound to a particular\nnamespace, which grant users the ability to view and/or edit resources within\nthat namespace the Role provides them access to.\nAt a cluster scope, these are called ClusterRoles and ClusterRoleBindings.\nGranting a user a ClusterRole grants them access to view and/or edit resources\nacross the entire cluster. It is also required to view and/or edit resources at\nthe cluster scope (namespaces, resource quotas, nodes).\nClusterRoles can be bound to a particular namespace through reference in a\nRoleBinding. The\nadmin\n,\nedit\nand\nview\ndefault ClusterRoles are commonly\nused in this manner.\nThese are a few ClusterRoles available by default in Kubernetes. They are\nintended to be user-facing roles. They include super-user roles\n(\ncluster-admin\n), and roles with more granular access (\nadmin\n,\nedit\n,\nview\n).\nDefault ClusterRole\nDefault ClusterRoleBinding\nDescription\ncluster-admin\nsystem:masters\ngroup\nAllows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself.\nadmin\nNone\nAllows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.\nedit\nNone\nAllows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.\nview\nNone\nAllows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.\nRestricting a user account's access using RBAC\nNow that we understand the basics of Role-based Access Control, let's discuss\nhow an administrator can restrict a user's scope of access.\nExample: Grant a user read/write access to a particular namespace\nTo restrict a user's access to a particular namespace, we can use either the\nedit\nor the\nadmin\nrole. If your charts create or interact with Roles and\nRolebindings, you'll want to use the\nadmin\nClusterRole.\nAdditionally, you may also create a RoleBinding with\ncluster-admin\naccess.\nGranting a user\ncluster-admin\naccess at the namespace scope provides full\ncontrol over every resource in the namespace, including the namespace itself.\nFor this example, we will create a user with the\nedit\nRole. First, create the\nnamespace:\n$\nkubectl create namespace foo\nNow, create a RoleBinding in that namespace, granting the user the\nedit\nrole.\n$\nkubectl create rolebinding sam-edit\n--clusterrole edit \\​\n--user sam \\​\n--namespace foo\nExample: Grant a user read/write access at the cluster scope\nIf a user wishes to install a chart that installs cluster-scope resources\n(namespaces, roles, custom resource definitions, etc.), they will require\ncluster-scope write access.\nTo do that, grant the user either\nadmin\nor\ncluster-admin\naccess.\nGranting a user\ncluster-admin\naccess grants them access to absolutely every\nresource available in Kubernetes, including node access with\nkubectl drain\nand\nother administrative tasks. It is highly recommended to consider providing the\nuser\nadmin\naccess instead, or to create a custom ClusterRole tailored to their\nneeds.\n$\nkubectl create clusterrolebinding sam-view\n--clusterrole view \\​\n--user sam\n$\nkubectl create clusterrolebinding sam-secret-reader\n--clusterrole secret-reader \\​\n--user sam\nExample: Grant a user read-only access to a particular namespace\nYou might've noticed that there is no ClusterRole available for viewing secrets.\nThe\nview\nClusterRole does not grant a user read access to Secrets due to\nescalation concerns. Helm stores release metadata as Secrets by default.\nIn order for a user to run\nhelm list\n, they need to be able to read these\nsecrets. For that, we will create a special\nsecret-reader\nClusterRole.\nCreate the file\ncluster-role-secret-reader.yaml\nand write the following\ncontent into the file:\napiVersion\n:\nrbac.authorization.k8s.io/v1​\nkind\n:\nClusterRole​\nmetadata:​\nname\n:\nsecret-reader​\nrules:​\n-\napiGroups\n:\n[\n\"\"\n]​\nresources\n:\n[\n\"secrets\"\n]​\nverbs\n:\n[\n\"get\"\n,\n\"watch\"\n,\n\"list\"\n]\nThen, create the ClusterRole using\n$\nkubectl create -f clusterrole-secret-reader.yaml​\nOnce that's done, we can grant a user read access to most resources, and then\ngrant them read access to secrets:\n$\nkubectl create namespace foo\n$\nkubectl create rolebinding sam-view\n--clusterrole view \\​\n--user sam \\​\n--namespace foo\n$\nkubectl create rolebinding sam-secret-reader\n--clusterrole secret-reader \\​\n--user sam \\​\n--namespace foo\nExample: Grant a user read-only access at the cluster scope\nIn certain scenarios, it may be beneficial to grant a user cluster-scope access.\nFor example, if a user wants to run the command\nhelm list --all-namespaces\n,\nthe API requires the user has cluster-scope read access.\nTo do that, grant the user both\nview\nand\nsecret-reader\naccess as described\nabove, but with a ClusterRoleBinding.\n$\nkubectl create clusterrolebinding sam-view\n--clusterrole view \\​\n--user sam\n$\nkubectl create clusterrolebinding sam-secret-reader\n--clusterrole secret-reader \\​\n--user sam\nAdditional Thoughts\nThe examples shown above utilize the default ClusterRoles provided with\nKubernetes. For more fine-grained control over what resources users are granted\naccess to, have a look at\nthe Kubernetes\ndocumentation\non\ncreating your own custom Roles and ClusterRoles.\nPrev\n← Kubernetes Distribution Guide\nNext\nThe Helm Plugins Guide →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nThe Helm Plugins Guide\nA Helm plugin is a tool that can be accessed through the\nhelm\nCLI, but which\nis not part of the built-in Helm codebase.\nExisting plugins can be found on\nrelated\nsection or by searching\nGitHub\n.\nThis guide explains how to use and create plugins.\nAn Overview\nHelm plugins are add-on tools that integrate seamlessly with Helm. They provide\na way to extend the core feature set of Helm, but without requiring every new\nfeature to be written in Go and added to the core tool.\nHelm plugins have the following features:\nThey can be added and removed from a Helm installation without impacting the\ncore Helm tool.\nThey can be written in any programming language.\nThey integrate with Helm, and will show up in\nhelm help\nand other places.\nHelm plugins live in\n$HELM_PLUGINS\n. You can find the current value of this,\nincluding the default value when not set in the environment, using the\nhelm env\ncommand.\nThe Helm plugin model is partially based on Git's plugin model. To that end,\nyou may sometimes hear\nhelm\nreferred to as the\nporcelain\nlayer, with plugins\nbeing the\nplumbing\n. This is a shorthand way of suggesting that Helm provides\nthe user experience and top level processing logic, while the plugins do the\n\"detail work\" of performing a desired action.\nInstalling a Plugin\nPlugins are installed using the\n$ helm plugin install <path|url>\ncommand. You\ncan pass in a path to a plugin on your local file system or a url of a remote\nVCS repo. The\nhelm plugin install\ncommand clones or copies the plugin at the\npath/url given into\n$HELM_PLUGINS\n. If you are installing from a VCS you can specify\nthe version with the\n--version\nargument.\n$\nhelm plugin install https://github.com/adamreese/helm-env\nIf you have a plugin tar distribution, simply untar the plugin into the\n$HELM_PLUGINS\ndirectory. You can also install tarball plugins\ndirectly from url by issuing\nhelm plugin install https://domain/path/to/plugin.tar.gz\nThe Plugin File Structure\nIn many ways, a plugin is similar to a chart. Each plugin has a top-level\ndirectory containing a\nplugin.yaml\nfile. Additonal files may be present but\nonly the\nplugin.yaml\nfile is required.\n$\nHELM_PLUGINS/\n|- last/\n|- plugin.yaml\nThe plugin.yaml File\nThe plugin.yaml file is required for a plugin. It contains the following fields:\nname\n:\nThe name of the plugin (REQUIRED)\nversion\n:\nA SemVer 2 version (REQUIRED)\nusage\n:\nSingle line usage text shown in help\ndescription\n:\nLong description shown in places like helm help\nignoreFlags\n:\nIgnore flags passed in from Helm\nplatformCommand\n:\n# Configure command to run based on the platform\n-\nos\n:\nOS match, can be empty or ommited to match all OS'\narch\n:\nArchitecture match, can be empty or ommited to match all architectures\ncommand\n:\nPlugin command to execute\nargs\n:\nPlugin command arguments\ncommand\n:\n(DEPRECATED) Plugin command, use platformCommand instead\nplatformHooks\n:\n# Configure plugin lifecycle hooks based on the platform\ninstall\n:\n# Install lifecycle commands\n-\nos\n:\nOS match, can be empty or ommited to match all OS'\narch\n:\nArchitecture match, can be empty or ommited to match all architectures\ncommand\n:\nPlugin install command to execute\nargs\n:\nPlugin install command arguments\nupdate\n:\n# Update lifecycle commands\n-\nos\n:\nOS match, can be empty or ommited to match all OS'\narch\n:\nArchitecture match, can be empty or ommited to match all architectures\ncommand\n:\nPlugin update command to execute\nargs\n:\nPlugin update command arguments\ndelete\n:\n# Delete lifecycle commands\n-\nos\n:\nOS match, can be empty or ommited to match all OS'\narch\n:\nArchitecture match, can be empty or ommited to match all architectures\ncommand\n:\nPlugin delete command to execute\nargs\n:\nPlugin delete command arguments\nhooks\n:\n# (Deprecated) Plugin lifecycle hooks, use platformHooks instead\ninstall\n:\nCommand to install plugin\nupdate\n:\nCommand to update plugin\ndelete\n:\nCommand to delete plugin\ndownloaders\n:\n# Configure downloaders capability\n-\ncommand\n:\nCommand to invoke\nprotocols\n:\n- Protocol schema supported\nThe\nname\nField\nThe\nname\nis the name of the plugin. When Helm executes this plugin, this is\nthe name it will use (e.g.\nhelm NAME\nwill invoke this plugin).\nname\nshould match the directory name.\nIn our example above, that means the\nplugin with\nname: last\nshould be contained in a directory named\nlast\n.\nRestrictions on\nname\n:\nname\ncannot duplicate one of the existing\nhelm\ntop-level commands.\nname\nmust be restricted to the characters ASCII a-z, A-Z, 0-9,\n_\nand\n-\n.\nThe\nversion\nField\nThe\nversion\nis the SemVer 2 version of the plugin.\nusage\nand\ndescription\nare\nboth used to generate the help text of a command.\nThe\nignoreFlags\nField\nThe\nignoreFlags\nswitch tells Helm to\nnot\npass flags to the plugin. So if a\nplugin is called with\nhelm myplugin --foo\nand\nignoreFlags: true\n, then\n--foo\nis silently discarded.\nThe\nplatformCommand\nField\nThe\nplatformCommand\nconfigures the command that the plugin will execute when\nit is called. You can't set both\nplatformCommand\n&\ncommand\nas this will result\nin an error. The following rules will apply in deciding which command to use:\nIf\nplatformCommand\nis present, it will be used.\nIf both\nos\nand\narch\nmatch the current platform, search will stop and the\ncommand will be used.\nIf\nos\nmatches and\narch\nis empty, the command will be used.\nIf\nos\nand\narch\nare both empty, the command will be used.\nIf there is no match, Helm will exit with an error.\nIf\nplatformCommand\nis not present and the deprecated\ncommand\nis present\nit will be used.\nIf the command is empty, Helm will exit with an error.\nThe\nplatformHooks\nField\nThe\nplatformHooks\nconfigures the commands that the plugin will execute for lifecycle\nevents. You can't set both\nplatformHooks\n&\nhooks\nas this will resultin an error.\nThe following rules will apply in deciding which hook command to use:\nIf\nplatformHooks\nis present, it will be used and the commands for the lifecycle\nevent will be processed.\nIf both\nos\nand\narch\nmatch the current platform, search will stop and the\ncommand will be used.\nIf\nos\nmatches and\narch\nis empty, the command will be used.\nIf\nos\nand\narch\nare both empty, the command will be used.\nIf there is no match, Helm will skip the event.\nIf\nplatformHooks\nis not present and the deprecated\nhooks\nis present the command\nfor the lifecycle event will be used.\nIf the command is empty, Helm will skip the event.\nBuilding a Plugin\nHere is the plugin YAML for a simple plugin that helps get the last release name:\nname\n:\nlast\nversion\n:\n0.1.0\nusage\n:\nget the last release name\ndescription\n:\nget the last release name\nignoreFlags\n:\nfalse\nplatformCommand\n:\n-\ncommand\n:\n${HELM_BIN}\nargs\n:\n- list\n- --short\n- --max=1\n- --date\n- -r\nPlugins may require additional scripts and executables.\nScripts can be included in the plugin directory and executables can be downloaded\nvia a hook. The following is an example plugin:\n$\nHELM_PLUGINS/\n|- myplugin/\n|- scripts/\n|- install.ps1\n|- install.sh\n|- plugin.yaml\nname\n:\nmyplugin\nversion\n:\n0.1.0\nusage\n:\nexample plugin\ndescription\n:\nexample plugin\nignoreFlags\n:\nfalse\nplatformCommand\n:\n-\ncommand\n:\n${HELM_PLUGIN_DIR}/bin/myplugin\n-\nos\n:\nwindows\ncommand\n:\n${HELM_PLUGIN_DIR}\\bin\\myplugin.exe\nplatformHooks\n:\ninstall\n:\n-\ncommand\n:\n${HELM_PLUGIN_DIR}/scripts/install.sh\n-\nos\n:\nwindows\ncommand\n:\npwsh\nargs\n:\n- -c\n- ${HELM_PLUGIN_DIR}\\scripts\\install.ps1\nupdate\n:\n-\ncommand\n:\n${HELM_PLUGIN_DIR}/scripts/install.sh\nargs\n:\n- -u\n-\nos\n:\nwindows\ncommand\n:\npwsh\nargs\n:\n- -c\n- ${HELM_PLUGIN_DIR}\\scripts\\install.ps1\n- -Update\nEnvironment variables are interpolated before the plugin is executed. The\npattern above illustrates the preferred way to indicate where the plugin program\nlives.\nPlugin Commands\nThere are some strategies for working with plugin commands:\nIf a plugin includes an executable, the executable for a\nplatformCommand:\nor\nshould be packaged in the plugin directory or installed via a hook.\nThe\nplatformCommand:\nor\ncommand:\nline will have any environment variables\nexpanded before execution.\n$HELM_PLUGIN_DIR\nwill point to the plugin\ndirectory.\nThe command itself is not executed in a shell. So you can't oneline a shell\nscript.\nHelm injects lots of configuration into environment variables. Take a look at\nthe environment to see what information is available.\nHelm makes no assumptions about the language of the plugin. You can write it\nin whatever you prefer.\nCommands are responsible for implementing specific help text for\n-h\nand\n--help\n. Helm will use\nusage\nand\ndescription\nfor\nhelm help\nand\nhelm help myplugin\n, but will not handle\nhelm myplugin --help\n.\nTesting a Local Plugin\nFirst you need to find your\nHELM_PLUGINS\npath to do it run the folowing command:\nhelm env\nChange your current directory to the director that\nHELM_PLUGINS\nis set to.\nNow you can add a symbolic link to your build output of your plugin in this example\nwe did it for\nmapkubeapis\n.\nln -s ~/GitHub/helm-mapkubeapis ./helm-mapkubeapis\nDownloader Plugins\nBy default, Helm is able to pull Charts using HTTP/S. As of Helm 2.4.0, plugins\ncan have a special capability to download Charts from arbitrary sources.\nPlugins shall declare this special capability in the\nplugin.yaml\nfile (top\nlevel):\ndownloaders\n:\n-\ncommand\n:\n\"bin/mydownloader\"\nprotocols\n:\n-\n\"myprotocol\"\n-\n\"myprotocols\"\nIf such plugin is installed, Helm can interact with the repository using the\nspecified protocol scheme by invoking the\ncommand\n. The special repository\nshall be added similarly to the regular ones:\nhelm repo add favorite myprotocol://example.com/\nThe rules for the special repos are the same to the\nregular ones: Helm must be able to download the\nindex.yaml\nfile in order to\ndiscover and cache the list of available Charts.\nThe defined command will be invoked with the following scheme:\ncommand certFile keyFile caFile full-URL\n. The SSL credentials are coming from the repo\ndefinition, stored in\n$HELM_REPOSITORY_CONFIG\n(i.e.,\n$HELM_CONFIG_HOME/repositories.yaml\n). A Downloader plugin\nis expected to dump the raw content to stdout and report errors on stderr.\nThe downloader command also supports sub-commands or arguments, allowing you to\nspecify for example\nbin/mydownloader subcommand -d\nin the\nplugin.yaml\n. This\nis useful if you want to use the same executable for the main plugin command and\nthe downloader command, but with a different sub-command for each.\nEnvironment Variables\nWhen Helm executes a plugin, it passes the outer environment to the plugin, and\nalso injects some additional environment variables.\nVariables like\nKUBECONFIG\nare set for the plugin if they are set in the outer\nenvironment.\nThe following variables are guaranteed to be set:\nHELM_PLUGINS\n: The path to the plugins directory.\nHELM_PLUGIN_NAME\n: The name of the plugin, as invoked by\nhelm\n. So\nhelm myplug\nwill have the short name\nmyplug\n.\nHELM_PLUGIN_DIR\n: The directory that contains the plugin.\nHELM_BIN\n: The path to the\nhelm\ncommand (as executed by the user).\nHELM_DEBUG\n: Indicates if the debug flag was set by helm.\nHELM_REGISTRY_CONFIG\n: The location for the registry configuration (if\nusing). Note that the use of Helm with registries is an experimental feature.\nHELM_REPOSITORY_CACHE\n: The path to the repository cache files.\nHELM_REPOSITORY_CONFIG\n: The path to the repository configuration file.\nHELM_NAMESPACE\n: The namespace given to the\nhelm\ncommand (generally using\nthe\n-n\nflag).\nHELM_KUBECONTEXT\n: The name of the Kubernetes config context given to the\nhelm\ncommand.\nAdditionally, if a Kubernetes configuration file was explicitly specified, it\nwill be set as the\nKUBECONFIG\nvariable\nA Note on Flag Parsing\nWhen executing a plugin, Helm will parse global flags for its own use. None of\nthese flags are passed on to the plugin.\n--burst-limit\n: This is converted to\n$HELM_BURST_LIMIT\n--debug\n: If this is specified,\n$HELM_DEBUG\nis set to\n1\n--kube-apiserver\n: This is converted to\n$HELM_KUBEAPISERVER\n--kube-as-group\n: These are converted to\n$HELM_KUBEASGROUPS\n--kube-as-user\n: This is converted to\n$HELM_KUBEASUSER\n--kube-ca-file\n: This is converted to\n$HELM_KUBECAFILE\n--kube-context\n: This is converted to\n$HELM_KUBECONTEXT\n--kube-insecure-skip-tls-verify\n: This is converted to\n$HELM_KUBEINSECURE_SKIP_TLS_VERIFY\n--kube-tls-server-name\n: This is converted to\n$HELM_KUBETLS_SERVER_NAME\n--kube-token\n: This is converted to\n$HELM_KUBETOKEN\n--kubeconfig\n: This is converted to\n$KUBECONFIG\n--namespace\nand\n-n\n: This is converted to\n$HELM_NAMESPACE\n--qps\n: This is converted to\n$HELM_QPS\n--registry-config\n: This is converted to\n$HELM_REGISTRY_CONFIG\n--repository-cache\n: This is converted to\n$HELM_REPOSITORY_CACHE\n--repository-config\n: This is converted to\n$HELM_REPOSITORY_CONFIG\nPlugins\nshould\ndisplay help text and then exit for\n-h\nand\n--help\n. In all\nother cases, plugins may use flags as appropriate.\nProviding shell auto-completion\nAs of Helm 3.2, a plugin can optionally provide support for shell\nauto-completion as part of Helm's existing auto-completion mechanism.\nStatic auto-completion\nIf a plugin provides its own flags and/or sub-commands, it can inform Helm of\nthem by having a\ncompletion.yaml\nfile located in the plugin's root directory.\nThe\ncompletion.yaml\nfile has the form:\nname\n:\n<pluginName>\nflags\n:\n- <flag 1>\n- <flag 2>\nvalidArgs\n:\n- <arg value 1>\n- <arg value 2>\ncommands\n:\nname\n:\n<commandName>\nflags\n:\n- <flag 1>\n- <flag 2>\nvalidArgs\n:\n- <arg value 1>\n- <arg value 2>\ncommands\n:\n<and so on, recursively>\nNotes:\nAll sections are optional but should be provided if applicable.\nFlags should not include the\n-\nor\n--\nprefix.\nBoth short and long flags can and should be specified. A short flag need not\nbe associated with its corresponding long form, but both forms should be\nlisted.\nFlags need not be ordered in any way, but need to be listed at the correct\npoint in the sub-command hierarchy of the file.\nHelm's existing global flags are already handled by Helm's auto-completion\nmechanism, therefore plugins need not specify the following flags\n--debug\n,\n--namespace\nor\n-n\n,\n--kube-context\n, and\n--kubeconfig\n, or any other\nglobal flag.\nThe\nvalidArgs\nlist provides a static list of possible completions for the\nfirst parameter following a sub-command. It is not always possible to\nprovide such a list in advance (see the\nDynamic\nCompletion\nsection below), in which case the\nvalidArgs\nsection can be omitted.\nThe\ncompletion.yaml\nfile is entirely optional. If it is not provided, Helm\nwill simply not provide shell auto-completion for the plugin (unless\nDynamic\nCompletion\nis supported by the plugin). Also, adding a\ncompletion.yaml\nfile is backwards-compatible and will not impact the behavior\nof the plugin when using older helm versions.\nAs an example, for the\nfullstatus plugin\nwhich has no\nsub-commands but accepts the same flags as the\nhelm status\ncommand, the\ncompletion.yaml\nfile is:\nname\n:\nfullstatus\nflags\n:\n- o\n- output\n- revision\nA more intricate example for the\n2to3 plugin\n, has a\ncompletion.yaml\nfile of:\nname\n:\n2to3\ncommands\n:\n-\nname\n:\ncleanup\nflags\n:\n- config-cleanup\n- dry-run\n- l\n- label\n- release-cleanup\n- s\n- release-storage\n- tiller-cleanup\n- t\n- tiller-ns\n- tiller-out-cluster\n-\nname\n:\nconvert\nflags\n:\n- delete-v2-releases\n- dry-run\n- l\n- label\n- s\n- release-storage\n- release-versions-max\n- t\n- tiller-ns\n- tiller-out-cluster\n-\nname\n:\nmove\ncommands\n:\n-\nname\n:\nconfig\nflags\n:\n- dry-run\nDynamic completion\nAlso starting with Helm 3.2, plugins can provide their own dynamic shell\nauto-completion. Dynamic shell auto-completion is the completion of parameter\nvalues or flag values that cannot be defined in advance. For example,\ncompletion of the names of helm releases currently available on the cluster.\nFor the plugin to support dynamic auto-completion, it must provide an\nexecutable\nfile called\nplugin.complete\nin its root directory. When the\nHelm completion script requires dynamic completions for the plugin, it will\nexecute the\nplugin.complete\nfile, passing it the command-line that needs to be\ncompleted. The\nplugin.complete\nexecutable will need to have the logic to\ndetermine what the proper completion choices are and output them to standard\noutput to be consumed by the Helm completion script.\nThe\nplugin.complete\nfile is entirely optional. If it is not provided, Helm\nwill simply not provide dynamic auto-completion for the plugin. Also, adding a\nplugin.complete\nfile is backwards-compatible and will not impact the behavior\nof the plugin when using older helm versions.\nThe output of the\nplugin.complete\nscript should be a new-line separated list\nsuch as:\nrel1\nrel2\nrel3\nWhen\nplugin.complete\nis called, the plugin environment is set just like when\nthe plugin's main script is called. Therefore, the variables\n$HELM_NAMESPACE\n,\n$HELM_KUBECONTEXT\n, and all other plugin variables will already be set, and\ntheir corresponding global flags will be removed.\nThe\nplugin.complete\nfile can be in any executable form; it can be a shell\nscript, a Go program, or any other type of program that Helm can execute. The\nplugin.complete\nfile\nmust\nhave executable permissions for the user. The\nplugin.complete\nfile\nmust\nexit with a success code (value 0).\nIn some cases, dynamic completion will require to obtain information from the\nKubernetes cluster. For example, the\nhelm fullstatus\nplugin requires a\nrelease name as input. In the\nfullstatus\nplugin, for its\nplugin.complete\nscript to provide completion for current release names, it can simply run\nhelm list -q\nand output the result.\nIf it is desired to use the same executable for plugin execution and for plugin\ncompletion, the\nplugin.complete\nscript can be made to call the main plugin\nexecutable with some special parameter or flag; when the main plugin executable\ndetects the special parameter or flag, it will know to run the completion. In\nour example,\nplugin.complete\ncould be implemented like this:\n#!/usr/bin/env sh\n# \"$@\" is the entire command-line that requires completion.\n# It is important to double-quote the \"$@\" variable to preserve a possibly empty last parameter.\n$HELM_PLUGIN_DIR\n/status.sh --complete\n\"\n$@\n\"\nThe\nfullstatus\nplugin's real script (\nstatus.sh\n) must then look for the\n--complete\nflag and if found, printout the proper completions.\nTips and tricks\nThe shell will automatically filter out completion choices that don't match\nuser input. A plugin can therefore return all relevant completions without\nremoving the ones that don't match the user input. For example, if the\ncommand-line is\nhelm fullstatus ngin<TAB>\n, the\nplugin.complete\nscript can\nprint\nall\nrelease names (of the\ndefault\nnamespace), not just the ones\nstarting with\nngin\n; the shell will only retain the ones starting with\nngin\n.\nTo simplify dynamic completion support, especially if you have a complex\nplugin, you can have your\nplugin.complete\nscript call your main plugin\nscript and request completion choices. See the\nDynamic\nCompletion\nsection above for an example.\nTo debug dynamic completion and the\nplugin.complete\nfile, one can run the\nfollowing to see the completion results :\nhelm __complete <pluginName> <arguments to complete>\n. For example:\nhelm __complete fullstatus --output js<ENTER>\n,\nhelm __complete fullstatus -o json \"\"<ENTER>\nPrev\n← Role-based Access Control\nNext\nMigrating Helm v2 to v3 →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nMigrating Helm v2 to v3\nThis guide shows how to migrate Helm v2 to v3. Helm v2 needs to be installed\nand managing releases in one or more clusters.\nOverview of Helm 3 Changes\nThe full list of changes from Helm 2 to 3 are documented in the\nFAQ\nsection\n. The following is a\nsummary of some of those changes that a user should be aware of before and\nduring migration:\nRemoval of Tiller:\nReplaces client/server with client/library architecture (\nhelm\nbinary\nonly)\nSecurity is now on per user basis (delegated to Kubernetes user cluster\nsecurity)\nReleases are now stored as in-cluster secrets and the release object\nmetadata has changed\nReleases are persisted on a release namespace basis and not in the Tiller\nnamespace anymore\nChart repository updated:\nhelm search\nnow supports both local repository searches and making search\nqueries against Artifact Hub\nChart apiVersion bumped to \"v2\" for following specification changes:\nDynamically linked chart dependencies moved to\nChart.yaml\n(\nrequirements.yaml\nremoved and requirements --> dependencies)\nLibrary charts (helper/common charts) can now be added as dynamically\nlinked chart dependencies\nCharts have a\ntype\nmetadata field to define the chart to be of an\napplication\nor\nlibrary\nchart. It is application by default which means\nit is renderable and installable\nHelm 2 charts (apiVersion=v1) are still installable\nXDG directory specification added:\nHelm home removed and replaced with XDG directory specification for storing\nconfiguration files\nNo longer need to initialize Helm\nhelm init\nand\nhelm home\nremoved\nAdditional changes:\nHelm install/set-up is simplified:\nHelm client (helm binary) only (no Tiller)\nRun-as-is paradigm\nlocal\nor\nstable\nrepositories are not set-up by default\ncrd-install\nhook removed and replaced with\ncrds\ndirectory in chart\nwhere all CRDs defined in it will be installed before any rendering of the\nchart\ntest-failure\nhook annotation value removed, and\ntest-success\ndeprecated. Use\ntest\ninstead\nCommands removed/replaced/added:\ndelete --> uninstall : removes all release history by default\n(previously needed\n--purge\n)\nfetch --> pull\nhome (removed)\ninit (removed)\ninstall: requires release name or\n--generate-name\nargument\ninspect --> show\nreset (removed)\nserve (removed)\ntemplate:\n-x\n/\n--execute\nargument renamed to\n-s\n/\n--show-only\nupgrade: Added argument\n--history-max\nwhich limits the maximum number\nof revisions saved per release (0 for no limit)\nHelm 3 Go library has undergone a lot of changes and is incompatible with\nthe Helm 2 library\nRelease binaries are now hosted on\nget.helm.sh\nMigration Use Cases\nThe migration use cases are as follows:\nHelm v2 and v3 managing the same cluster:\nThis use case is only recommended if you intend to phase out Helm v2\ngradually and do not require v3 to manage any releases deployed by v2. All\nnew releases being deployed should be performed by v3 and existing v2\ndeployed releases are updated/removed by v2 only\nHelm v2 and v3 can quite happily manage the same cluster. The Helm versions\ncan be installed on the same or separate systems\nIf installing Helm v3 on the same system, you need to perform an additional\nstep to ensure that both client versions can co-exist until ready to remove\nHelm v2 client. Rename or put the Helm v3 binary in a different folder to\navoid conflict\nOtherwise there are no conflicts between both versions because of the\nfollowing distinctions:\nv2 and v3 release (history) storage are independent of each other. The\nchanges include the Kubernetes resource for storage and the release\nobject metadata contained in the resource. Releases will also be on a per\nuser namespace instead of using the Tiller namespace (for example, v2\ndefault Tiller namespace kube-system). v2 uses \"ConfigMaps\" or \"Secrets\"\nunder the Tiller namespace and\nTILLER\nownership. v3 uses \"Secrets\" in\nthe user namespace and\nhelm\nownership. Releases are incremental in both\nv2 and v3\nThe only issue could be if Kubernetes cluster scoped resources (e.g.\nclusterroles.rbac\n) are defined in a chart. The v3 deployment would then\nfail even if unique in the namespace as the resources would clash\nv3 configuration no longer uses\n$HELM_HOME\nand uses XDG directory\nspecification instead. It is also created on the fly as need be. It is\ntherefore independent of v2 configuration. This is applicable only when\nboth versions are installed on the same system\nMigrating Helm v2 to Helm v3:\nThis use case applies when you want Helm v3 to manage existing Helm v2\nreleases\nIt should be noted that a Helm v2 client:\ncan manage 1 to many Kubernetes clusters\ncan connect to 1 to many Tiller instances for a cluster\nThis means that you have to be aware of this when migrating as releases\nare deployed into clusters by Tiller and its namespace. You have to\ntherefore be aware of migrating for each cluster and each Tiller instance\nthat is managed by the Helm v2 client instance\nThe recommended data migration path is as follows:\nBackup v2 data\nMigrate Helm v2 configuration\nMigrate Helm v2 releases\nWhen confident that Helm v3 is managing all Helm v2 data (for all\nclusters and Tiller instances of the Helm v2 client instance) as\nexpected, then clean up Helm v2 data\nThe migration process is automated by the Helm v3\n2to3\nplugin\nReference\nHelm v3\n2to3\nplugin\nBlog\npost\nexplaining\n2to3\nplugin usage with examples\nPrev\n← The Helm Plugins Guide\nNext\nDeprecated Kubernetes APIs →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nDeprecated Kubernetes APIs\nKubernetes is an API-driven system and the API evolves over time to reflect the\nevolving understanding of the problem space. This is common practice across\nsystems and their APIs. An important part of evolving APIs is a good deprecation\npolicy and process to inform users of how changes to APIs are implemented. In\nother words, consumers of your API need to know in advance and in what release\nan API will be removed or changed. This removes the element of surprise and\nbreaking changes to consumers.\nThe\nKubernetes deprecation\npolicy\ndocuments how Kubernetes handles the changes to its API versions. The policy for\ndeprecation states the timeframe that API versions will be supported following a\ndeprecation announcement. It is therefore important to be aware of deprecation\nannouncements and know when API versions will be removed, to help minimize the\neffect.\nThis is an example of an announcement\nfor the removal of deprecated API\nversions in Kubernetes\n1.16\nand was\nadvertised a few months prior to the release. These API versions would have been\nannounced for deprecation prior to this again. This shows that there is a good\npolicy in place which informs consumers of API version support.\nHelm templates specify a\nKubernetes API\ngroup\nwhen defining a Kubernetes object, similar to a Kubernetes manifest file. It is\nspecified in the\napiVersion\nfield of the template and it identifies the API\nversion of the Kubernetes object. This means that Helm users and chart\nmaintainers need to be aware when Kubernetes API versions have been deprecated\nand in what Kubernetes version they will be removed.\nChart Maintainers\nYou should audit your charts checking for Kubernetes API versions that are\ndeprecated or are removed in a Kubernetes version. The API versions found as due\nto be or that are now out of support, should be updated to the supported version\nand a new version of the chart released. The API version is defined by the\nkind\nand\napiVersion\nfields. For example, here is a removed\nDeployment\nobject API version in Kubernetes 1.16:\napiVersion\n:\napps/v1beta1\nkind\n:\nDeployment\nHelm Users\nYou should audit the charts that you use (similar to\nchart\nmaintainers\n) and identify any charts where API versions are\ndeprecated or removed in a Kubernetes version. For the charts identified, you\nneed to check for the latest version of the chart (which has supported API\nversions) or update the chart yourself.\nAdditionally, you also need to audit any charts deployed (i.e. Helm releases)\nchecking again for any deprecated or removed API versions. This can be done by\ngetting details of a release using the\nhelm get manifest\ncommand.\nThe means for updating a Helm release to supported APIs depends on your findings\nas follows:\nIf you find deprecated API versions only then:\nPerform a\nhelm upgrade\nwith a version of the chart with supported\nKubernetes API versions\nAdd a description in the upgrade, something along the lines to not perform a\nrollback to a Helm version prior to this current version\nIf you find any API version(s) that is/are removed in a Kubernetes version\nthen:\nIf you are running a Kubernetes version where the API version(s) are still\navailable (for example, you are on Kubernetes 1.15 and found you use APIs\nthat will be removed in Kubernetes 1.16):\nFollow the step 1 procedure\nOtherwise (for example, you are already running a Kubernetes version where\nsome API versions reported by\nhelm get manifest\nare no longer available):\nYou need to edit the release manifest that is stored in the cluster to\nupdate the API versions to supported APIs. See\nUpdating API Versions of a\nRelease Manifest\nfor more\ndetails\nNote: In all cases of updating a Helm release with supported APIs, you should\nnever rollback the release to a version prior to the release version with the\nsupported APIs.\nRecommendation: The best practice is to upgrade releases using deprecated API\nversions to supported API versions, prior to upgrading to a kubernetes cluster\nthat removes those API versions.\nIf you don't update a release as suggested previously, you will have an error\nsimilar to the following when trying to upgrade a release in a Kubernetes\nversion where its API version(s) is/are removed:\nError: UPGRADE FAILED: current release manifest contains removed kubernetes api(s)\nfor this kubernetes version and it is therefore unable to build the kubernetes\nobjects for performing the diff. error from kubernetes: unable to recognize \"\":\nno matches for kind \"Deployment\" in version \"apps/v1beta1\"\nHelm fails in this scenario because it attempts to create a diff patch between\nthe current deployed release (which contains the Kubernetes APIs that are\nremoved in this Kubernetes version) against the chart you are passing with the\nupdated/supported API versions. The underlying reason for failure is that when\nKubernetes removes an API version, the Kubernetes Go client library can no\nlonger parse the deprecated objects and Helm therefore fails when calling the\nlibrary. Helm unfortunately is unable to recover from this situation and is no\nlonger able to manage such a release. See\nUpdating API Versions of a Release\nManifest\nfor more details on how\nto recover from this scenario.\nUpdating API Versions of a Release Manifest\nThe manifest is a property of the Helm release object which is stored in the\ndata field of a Secret (default) or ConfigMap in the cluster. The data field\ncontains a gzipped object which is base 64 encoded (there is an additional base\n64 encoding for a Secret). There is a Secret/ConfigMap per release\nversion/revision in the namespace of the release.\nYou can use the Helm\nmapkubeapis\nplugin to perform the update of a release to supported APIs. Check out the\nreadme for more details.\nAlternatively, you can follow these manual steps to perform an update of the API\nversions of a release manifest. Depending on your configuration you will follow\nthe steps for the Secret or ConfigMap backend.\nGet the name of the Secret or Configmap associated with the latest deployed\nrelease:\nSecrets backend:\nkubectl get secret -l owner=helm,status=deployed,name=<release_name> --namespace <release_namespace> | awk '{print $1}' | grep -v NAME\nConfigMap backend:\nkubectl get configmap -l owner=helm,status=deployed,name=<release_name> --namespace <release_namespace> | awk '{print $1}' | grep -v NAME\nGet latest deployed release details:\nSecrets backend:\nkubectl get secret <release_secret_name> -n <release_namespace> -o yaml > release.yaml\nConfigMap backend:\nkubectl get configmap <release_configmap_name> -n <release_namespace> -o yaml > release.yaml\nBackup the release in case you need to restore if something goes wrong:\ncp release.yaml release.bak\nIn case of emergency, restore:\nkubectl apply -f release.bak -n <release_namespace>\nDecode the release object:\nSecrets backend:\ncat release.yaml | grep -oP '(?<=release: ).*' | base64 -d | base64 -d | gzip -d > release.data.decoded\nConfigMap backend:\ncat release.yaml | grep -oP '(?<=release: ).*' | base64 -d | gzip -d > release.data.decoded\nChange API versions of the manifests. Can use any tool (e.g. editor) to make\nthe changes. This is in the\nmanifest\nfield of your decoded release object\n(\nrelease.data.decoded\n)\nEncode the release object:\nSecrets backend:\ncat release.data.decoded | gzip | base64 | base64\nConfigMap backend:\ncat release.data.decoded | gzip | base64\nReplace\ndata.release\nproperty value in the deployed release file\n(\nrelease.yaml\n) with the new encoded release object\nApply file to namespace:\nkubectl apply -f release.yaml -n <release_namespace>\nPerform a\nhelm upgrade\nwith a version of the chart with supported Kubernetes\nAPI versions\nAdd a description in the upgrade, something along the lines to not perform a\nrollback to a Helm version prior to this current version\nPrev\n← Migrating Helm v2 to v3\nNext\nHelm Version Support Policy →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nHelm Version Support Policy\nThis document describes the maximum version skew supported between Helm and\nKubernetes.\nSupported Versions\nHelm versions are expressed as\nx.y.z\n, where\nx\nis the major version,\ny\nis\nthe minor version, and\nz\nis the patch version, following\nSemantic\nVersioning\nterminology.\nThe Helm project maintains a release branch for the most recent minor release.\nApplicable fixes, including security fixes, are cherry-picked into the release\nbranch, depending on severity and feasibility. More details can be found in\nHelm's release policy\n.\nSupported Version Skew\nWhen a new version of Helm is released, it is compiled against a particular\nminor version of Kubernetes. For example, Helm 3.0.0 interacts with Kubernetes\nusing the Kubernetes 1.16.2 client, so it is compatible with Kubernetes 1.16.\nAs of Helm 3, Helm is assumed to be compatible with\nn-3\nversions of Kubernetes\nit was compiled against. Due to Kubernetes' changes between minor versions, Helm\n2's support policy is slightly stricter, assuming to be compatible with\nn-1\nversions of Kubernetes.\nFor example, if you are using a version of Helm 3 that was compiled against the\nKubernetes 1.17 client APIs, then it should be safe to use with Kubernetes 1.17,\n1.16, 1.15, and 1.14. If you are using a version of Helm 2 that was compiled\nagainst the Kubernetes 1.16 client APIs, then it should be safe to use with\nKubernetes 1.16 and 1.15.\nIt is not recommended to use Helm with a version of Kubernetes that is newer\nthan the version it was compiled against, as Helm does not make any forward\ncompatiblility guarantees.\nIf you choose to use Helm with a version of Kubernetes that it does not support,\nyou are using it at your own risk.\nPlease refer to the table below to determine what version of Helm is compatible\nwith your cluster.\nHelm Version\nSupported Kubernetes Versions\n3.17.x\n1.32.x - 1.29.x\n3.16.x\n1.31.x - 1.28.x\n3.15.x\n1.30.x - 1.27.x\n3.14.x\n1.29.x - 1.26.x\n3.13.x\n1.28.x - 1.25.x\n3.12.x\n1.27.x - 1.24.x\n3.11.x\n1.26.x - 1.23.x\n3.10.x\n1.25.x - 1.22.x\n3.9.x\n1.24.x - 1.21.x\n3.8.x\n1.23.x - 1.20.x\n3.7.x\n1.22.x - 1.19.x\n3.6.x\n1.21.x - 1.18.x\n3.5.x\n1.20.x - 1.17.x\n3.4.x\n1.19.x - 1.16.x\n3.3.x\n1.18.x - 1.15.x\n3.2.x\n1.18.x - 1.15.x\n3.1.x\n1.17.x - 1.14.x\n3.0.x\n1.16.x - 1.13.x\n2.16.x\n1.16.x - 1.15.x\n2.15.x\n1.15.x - 1.14.x\n2.14.x\n1.14.x - 1.13.x\n2.13.x\n1.13.x - 1.12.x\n2.12.x\n1.12.x - 1.11.x\n2.11.x\n1.11.x - 1.10.x\n2.10.x\n1.10.x - 1.9.x\n2.9.x\n1.10.x - 1.9.x\n2.8.x\n1.9.x - 1.8.x\n2.7.x\n1.8.x - 1.7.x\n2.6.x\n1.7.x - 1.6.x\n2.5.x\n1.6.x - 1.5.x\n2.4.x\n1.6.x - 1.5.x\n2.3.x\n1.5.x - 1.4.x\n2.2.x\n1.5.x - 1.4.x\n2.1.x\n1.5.x - 1.4.x\n2.0.x\n1.4.x - 1.3.x\nPrev\n← Deprecated Kubernetes APIs\nNext\nPermissions management for SQL storage backend →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nPermissions management for SQL storage backend\nThis document aims to provide guidance to users for setting up and managing\npermissions when using the SQL storage backend.\nIntroduction\nTo handle permissions, Helm leverages the RBAC feature of Kubernetes. When using\nthe SQL storage backend, Kubernetes' roles can't be used to determine whether or\nnot an user can access a given resource. This document shows how to create and\nmanage these permissions.\nInitialization\nThe first time the Helm CLI will make connect to your database, the client will\nmake sure that it was previously initialized. If it is not, it will take care of\nthe necessary setup automatically. This initialization requires admin privileges\non the public schema, or at least to be able to:\ncreate a table\ngrant privileges on the public schema\nAfter the migration was run against your database, all the other roles can use\nthe client.\nGrant privileges to a non admin user in PostgreSQL\nTo manage permissions, the SQL backend driver leverages the\nRLS\n(Row Security\nLevel) feature of PostgreSQL. RLS allows all users to be able to read/write\nfrom/to the same table, without being able to manipulate the same rows if they\nare not explicitly allowed to. By default, any role that has not been\nexplicitly granted with the right privileges will always return an empty list\nwhen running\nhelm list\nand will not be able to retrieve or modify any resource\nin the cluster.\nLet's see how to grant a given role access to specific namespaces:\nCREATE\nPOLICY\n<\nname\n>\nON\nreleases_v1\nFOR\nALL\nTO\n<\nrole\n>\nUSING\n(namespace\n=\n'default'\n);\nThis command will grant the permissions to read and write all resources that\nmeet the\nnamespace = 'default'\ncondition to the role\nrole\n. After creating\nthis policy, the user being connected to the database on the behalf of the role\nrole\nwill therefore be able to see all the releases living in the\ndefault\nnamespace when running\nhelm list\n, and to modify and delete them.\nPrivileges can be managed granularly with RLS, and one might be interested in\nrestraining access given the different columns of the table:\nkey\ntype\nbody\nname\nnamespace\nversion\nstatus\nowner\ncreatedAt\nmodifiedAt\nPrev\n← Helm Version Support Policy\nNext\nRelease schedule policy →\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nRelease schedule policy\nFor the benefit of its users, Helm defines and announces release dates in\nadvance. This document describes the policy governing Helm's release schedule.\nRelease calendar\nA public calendar showing the upcoming Helm releases can be found\nhere\n.\nSemantic versioning\nHelm versions are expressed as\nx.y.z\n, where\nx\nis the major version,\ny\nis\nthe minor version, and\nz\nis the patch version, following\nSemantic\nVersioning\nterminology.\nPatch releases\nPatch releases provide users with bug fixes and security fixes. They do not\ncontain new features.\nA new patch release relating to the latest minor/major release will normally be\ndone once a month on the second Wednesday of each month.\nA patch release to fix a high priority regression or security issue can be done\nwhenever needed.\nA patch release will be cancelled for any of the following reasons:\nif there is no new content since the previous release\nif the patch release date falls within one week before the first release candidate (RC1) of an upcoming minor release\nif the patch release date falls within four weeks following a minor release\nMinor releases\nMinor releases contain security and bug fixes as well as new features. They\nare backwards compatible with respect to the API and the CLI usage.\nTo align with Kubernetes releases, a minor helm release will be done every\n4 months (3 releases a year).\nExtra minor releases can be done if needed but will not affect the timeline of\nan announced future release, unless the announced release is less than 7 days\naway.\nAt the same time as a release is published, the date of the next minor release\nwill be announced and posted to Helm's main web page.\nMajor releases\nMajor releases contain breaking changes. Such releases are rare but are\nsometimes necessary to allow helm to continue to evolve in important new\ndirections.\nMajor releases can be difficult to plan. With that in mind, a final release\ndate will only be chosen and announced once the first beta version of such a\nrelease is available.\nPrev\n← Permissions management for SQL storage backend\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of HELM.\nStart by understanding the core concepts from this content.\n\nDOC START:\nThe Chart Best Practices Guide\nThis guide covers the Helm Team's considered best practices for creating charts.\nIt focuses on how charts should be structured.\nWe focus primarily on best practices for charts that may be publicly deployed.\nWe know that many charts are for internal-use only, and authors of such charts\nmay find that their internal interests override our suggestions here.\nQuicklinks\nGeneral Conventions\nGeneral conventions for charts.\nValues\nFocuses on how you should structure and use your values.\nTemplates\nA closer look at best practices surrounding templates.\nDependencies\nCovers best practices for Chart dependencies.\nLabels and Annotations\nCovers best practices for using labels and annotations in your Chart.\nPods and PodTemplates\nDiscusses formatting the Pod and PodTemplate portions in Chart manifests.\nCustom Resource Definitions\nHow to handle creating and using CRDs.\nRole-Based Access Control\nDiscusses the creation and formatting of RBAC resources in Chart manifests.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the HELM official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "helm", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nDocumentation\nKubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerized applications. The open source project is hosted by the Cloud Native Computing Foundation (\nCNCF\n).\nUnderstand Kubernetes\nLearn about Kubernetes and its fundamental concepts.\nWhy Kubernetes?\nComponents of a cluster\nThe Kubernetes API\nObjects In Kubernetes\nContainers\nWorkloads and Pods\nView Concepts\nTry Kubernetes\nFollow tutorials to learn how to deploy applications in Kubernetes.\nHello Minikube\nWalkthrough the basics\nStateless Example: PHP Guestbook with Redis\nStateful Example: Wordpress with Persistent Volumes\nView Tutorials\nSet up a K8s cluster\nGet Kubernetes running based on your resources and needs.\nLearning environment\nProduction environment\nInstall the kubeadm setup tool\nSecuring a cluster\nkubeadm command reference\nSet up Kubernetes\nLearn how to use Kubernetes\nLook up common tasks and how to perform them using a short sequence of steps.\nkubectl Quick Reference\nInstall kubectl\nConfigure access to clusters\nUse the Web UI Dashboard\nConfigure a Pod to Use a ConfigMap\nGetting help\nView Tasks\nLook up reference information\nBrowse terminology, command line syntax, API resource types, and setup tool documentation.\nGlossary\nkubectl command line tool\nLabels, annotations and taints\nKubernetes API reference\nOverview of API\nFeature Gates\nView Reference\nContribute to Kubernetes\nFind out how you can help make Kubernetes better.\nContribute to Kubernetes\nContribute to documentation\nSuggest content improvements\nOpening a pull request\nDocumenting a feature for a release\nLocalizing the docs\nParticipating in SIG Docs\nViewing Site Analytics\nSee Ways to Contribute\nTraining\nGet certified in Kubernetes and make your cloud native projects successful!\nView training\nDownload Kubernetes\nInstall Kubernetes or upgrade to the newest version.\nDownload Kubernetes\nAbout the documentation\nThis website contains documentation for the current and previous 4 versions of Kubernetes.\nSee available versions\nLast modified April 20, 2024 at 8:53 PM PST:\nReady docs landing page for vanilla Docsy (4da04a797b)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nDocumentation\nKubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerized applications. The open source project is hosted by the Cloud Native Computing Foundation (\nCNCF\n).\nUnderstand Kubernetes\nLearn about Kubernetes and its fundamental concepts.\nWhy Kubernetes?\nComponents of a cluster\nThe Kubernetes API\nObjects In Kubernetes\nContainers\nWorkloads and Pods\nView Concepts\nTry Kubernetes\nFollow tutorials to learn how to deploy applications in Kubernetes.\nHello Minikube\nWalkthrough the basics\nStateless Example: PHP Guestbook with Redis\nStateful Example: Wordpress with Persistent Volumes\nView Tutorials\nSet up a K8s cluster\nGet Kubernetes running based on your resources and needs.\nLearning environment\nProduction environment\nInstall the kubeadm setup tool\nSecuring a cluster\nkubeadm command reference\nSet up Kubernetes\nLearn how to use Kubernetes\nLook up common tasks and how to perform them using a short sequence of steps.\nkubectl Quick Reference\nInstall kubectl\nConfigure access to clusters\nUse the Web UI Dashboard\nConfigure a Pod to Use a ConfigMap\nGetting help\nView Tasks\nLook up reference information\nBrowse terminology, command line syntax, API resource types, and setup tool documentation.\nGlossary\nkubectl command line tool\nLabels, annotations and taints\nKubernetes API reference\nOverview of API\nFeature Gates\nView Reference\nContribute to Kubernetes\nFind out how you can help make Kubernetes better.\nContribute to Kubernetes\nContribute to documentation\nSuggest content improvements\nOpening a pull request\nDocumenting a feature for a release\nLocalizing the docs\nParticipating in SIG Docs\nViewing Site Analytics\nSee Ways to Contribute\nTraining\nGet certified in Kubernetes and make your cloud native projects successful!\nView training\nDownload Kubernetes\nInstall Kubernetes or upgrade to the newest version.\nDownload Kubernetes\nAbout the documentation\nThis website contains documentation for the current and previous 4 versions of Kubernetes.\nSee available versions\nLast modified April 20, 2024 at 8:53 PM PST:\nReady docs landing page for vanilla Docsy (4da04a797b)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nDocumentation\nAvailable Documentation Versions\nAvailable Documentation Versions\nThis website contains documentation for the current version of Kubernetes\nand the four previous versions of Kubernetes.\nThe availability of documentation for a Kubernetes version is separate from whether\nthat release is currently supported.\nRead\nSupport period\nto learn about\nwhich versions of Kubernetes are officially supported, and for how long.\nLatest version\nv1.33\n(this documentation)\nOlder versions\nv1.32\nv1.31\nv1.30\nv1.29\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified April 12, 2024 at 10:34 AM PST:\nUpdate button label to 'See available versions' in docs homepage (#45571) (84cb81aca1)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nThis section lists the different ways to set up and run Kubernetes.\nWhen you install Kubernetes, choose an installation type based on: ease of maintenance, security,\ncontrol, available resources, and expertise required to operate and manage a cluster.\nYou can\ndownload Kubernetes\nto deploy a Kubernetes cluster\non a local machine, into the cloud, or for your own datacenter.\nSeveral\nKubernetes components\nsuch as\nkube-apiserver\nor\nkube-proxy\ncan also be\ndeployed as\ncontainer images\nwithin the cluster.\nIt is\nrecommended\nto run Kubernetes components as container images wherever\nthat is possible, and to have Kubernetes manage those components.\nComponents that run containers - notably, the kubelet - can't be included in this category.\nIf you don't want to manage a Kubernetes cluster yourself, you could pick a managed service, including\ncertified platforms\n.\nThere are also other standardized and custom solutions across a wide range of cloud and\nbare metal environments.\nLearning environment\nIf you're learning Kubernetes, use the tools supported by the Kubernetes community,\nor tools in the ecosystem to set up a Kubernetes cluster on a local machine.\nSee\nInstall tools\n.\nProduction environment\nWhen evaluating a solution for a\nproduction environment\n, consider which aspects of\noperating a Kubernetes cluster (or\nabstractions\n) you want to manage yourself and which you\nprefer to hand off to a provider.\nFor a cluster you're managing yourself, the officially supported tool\nfor deploying Kubernetes is\nkubeadm\n.\nWhat's next\nDownload Kubernetes\nDownload and\ninstall tools\nincluding\nkubectl\nSelect a\ncontainer runtime\nfor your new cluster\nLearn about\nbest practices\nfor cluster setup\nKubernetes is designed for its\ncontrol plane\nto\nrun on Linux. Within your cluster you can run applications on Linux or other operating systems, including\nWindows.\nLearn to\nset up clusters with Windows nodes\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified November 23, 2022 at 3:06 PM PST:\nAdded glossary tooltip to kube-apiserver and kube-prox (c430d9a72c)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nTasks\nInstall Tools\nInstall Tools\nSet up Kubernetes tools on your computer.\nkubectl\nThe Kubernetes command-line tool,\nkubectl\n, allows\nyou to run commands against Kubernetes clusters.\nYou can use kubectl to deploy applications, inspect and manage cluster resources,\nand view logs. For more information including a complete list of kubectl operations, see the\nkubectl\nreference documentation\n.\nkubectl is installable on a variety of Linux platforms, macOS and Windows.\nFind your preferred operating system below.\nInstall kubectl on Linux\nInstall kubectl on macOS\nInstall kubectl on Windows\nkind\nkind\nlets you run Kubernetes on\nyour local computer. This tool requires that you have either\nDocker\nor\nPodman\ninstalled.\nThe kind\nQuick Start\npage\nshows you what you need to do to get up and running with kind.\nView kind Quick Start Guide\nminikube\nLike\nkind\n,\nminikube\nis a tool that lets you run Kubernetes\nlocally.\nminikube\nruns an all-in-one or a multi-node local Kubernetes cluster on your personal\ncomputer (including Windows, macOS and Linux PCs) so that you can try out\nKubernetes, or for daily development work.\nYou can follow the official\nGet Started!\nguide if your focus is\non getting the tool installed.\nView minikube Get Started! Guide\nOnce you have\nminikube\nworking, you can use it to\nrun a sample application\n.\nkubeadm\nYou can use the\nkubeadm\ntool to create and manage Kubernetes clusters.\nIt performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way.\nInstalling kubeadm\nshows you how to install kubeadm.\nOnce installed, you can use it to\ncreate a cluster\n.\nView kubeadm Install Guide\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified July 12, 2023 at 1:25 AM PST:\nRevise docs home page (9520b96a61)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nProduction environment\nCreate a production-quality Kubernetes cluster\nA production-quality Kubernetes cluster requires planning and preparation.\nIf your Kubernetes cluster is to run critical workloads, it must be configured to be resilient.\nThis page explains steps you can take to set up a production-ready cluster,\nor to promote an existing cluster for production use.\nIf you're already familiar with production setup and want the links, skip to\nWhat's next\n.\nProduction considerations\nTypically, a production Kubernetes cluster environment has more requirements than a\npersonal learning, development, or test environment Kubernetes. A production environment may require\nsecure access by many users, consistent availability, and the resources to adapt\nto changing demands.\nAs you decide where you want your production Kubernetes environment to live\n(on premises or in a cloud) and the amount of management you want to take\non or hand to others, consider how your requirements for a Kubernetes cluster\nare influenced by the following issues:\nAvailability\n: A single-machine Kubernetes\nlearning environment\nhas a single point of failure. Creating a highly available cluster means considering:\nSeparating the control plane from the worker nodes.\nReplicating the control plane components on multiple nodes.\nLoad balancing traffic to the cluster’s\nAPI server\n.\nHaving enough worker nodes available, or able to quickly become available, as changing workloads warrant it.\nScale\n: If you expect your production Kubernetes environment to receive a stable amount of\ndemand, you might be able to set up for the capacity you need and be done. However,\nif you expect demand to grow over time or change dramatically based on things like\nseason or special events, you need to plan how to scale to relieve increased\npressure from more requests to the control plane and worker nodes or scale down to reduce unused\nresources.\nSecurity and access management\n: You have full admin privileges on your own\nKubernetes learning cluster. But shared clusters with important workloads, and\nmore than one or two users, require a more refined approach to who and what can\naccess cluster resources. You can use role-based access control\n(\nRBAC\n) and other\nsecurity mechanisms to make sure that users and workloads can get access to the\nresources they need, while keeping workloads, and the cluster itself, secure.\nYou can set limits on the resources that users and workloads can access\nby managing\npolicies\nand\ncontainer resources\n.\nBefore building a Kubernetes production environment on your own, consider\nhanding off some or all of this job to\nTurnkey Cloud Solutions\nproviders or other\nKubernetes Partners\n.\nOptions include:\nServerless\n: Just run workloads on third-party equipment without managing\na cluster at all. You will be charged for things like CPU usage, memory, and\ndisk requests.\nManaged control plane\n: Let the provider manage the scale and availability\nof the cluster's control plane, as well as handle patches and upgrades.\nManaged worker nodes\n: Configure pools of nodes to meet your needs,\nthen the provider makes sure those nodes are available and ready to implement\nupgrades when needed.\nIntegration\n: There are providers that integrate Kubernetes with other\nservices you may need, such as storage, container registries, authentication\nmethods, and development tools.\nWhether you build a production Kubernetes cluster yourself or work with\npartners, review the following sections to evaluate your needs as they relate\nto your cluster’s\ncontrol plane\n,\nworker nodes\n,\nuser access\n, and\nworkload resources\n.\nProduction cluster setup\nIn a production-quality Kubernetes cluster, the control plane manages the\ncluster from services that can be spread across multiple computers\nin different ways. Each worker node, however, represents a single entity that\nis configured to run Kubernetes pods.\nProduction control plane\nThe simplest Kubernetes cluster has the entire control plane and worker node\nservices running on the same machine. You can grow that environment by adding\nworker nodes, as reflected in the diagram illustrated in\nKubernetes Components\n.\nIf the cluster is meant to be available for a short period of time, or can be\ndiscarded if something goes seriously wrong, this might meet your needs.\nIf you need a more permanent, highly available cluster, however, you should\nconsider ways of extending the control plane. By design, one-machine control\nplane services running on a single machine are not highly available.\nIf keeping the cluster up and running\nand ensuring that it can be repaired if something goes wrong is important,\nconsider these steps:\nChoose deployment tools\n: You can deploy a control plane using tools such\nas kubeadm, kops, and kubespray. See\nInstalling Kubernetes with deployment tools\nto learn tips for production-quality deployments using each of those deployment\nmethods. Different\nContainer Runtimes\nare available to use with your deployments.\nManage certificates\n: Secure communications between control plane services\nare implemented using certificates. Certificates are automatically generated\nduring deployment or you can generate them using your own certificate authority.\nSee\nPKI certificates and requirements\nfor details.\nConfigure load balancer for apiserver\n: Configure a load balancer\nto distribute external API requests to the apiserver service instances running on different nodes. See\nCreate an External Load Balancer\nfor details.\nSeparate and backup etcd service\n: The etcd services can either run on the\nsame machines as other control plane services or run on separate machines, for\nextra security and availability. Because etcd stores cluster configuration data,\nbacking up the etcd database should be done regularly to ensure that you can\nrepair that database if needed.\nSee the\netcd FAQ\nfor details on configuring and using etcd.\nSee\nOperating etcd clusters for Kubernetes\nand\nSet up a High Availability etcd cluster with kubeadm\nfor details.\nCreate multiple control plane systems\n: For high availability, the\ncontrol plane should not be limited to a single machine. If the control plane\nservices are run by an init service (such as systemd), each service should run on at\nleast three machines. However, running control plane services as pods in\nKubernetes ensures that the replicated number of services that you request\nwill always be available.\nThe scheduler should be fault tolerant,\nbut not highly available. Some deployment tools set up\nRaft\nconsensus algorithm to do leader election of Kubernetes services. If the\nprimary goes away, another service elects itself and take over.\nSpan multiple zones\n: If keeping your cluster available at all times is\ncritical, consider creating a cluster that runs across multiple data centers,\nreferred to as zones in cloud environments. Groups of zones are referred to as regions.\nBy spreading a cluster across\nmultiple zones in the same region, it can improve the chances that your\ncluster will continue to function even if one zone becomes unavailable.\nSee\nRunning in multiple zones\nfor details.\nManage on-going features\n: If you plan to keep your cluster over time,\nthere are tasks you need to do to maintain its health and security. For example,\nif you installed with kubeadm, there are instructions to help you with\nCertificate Management\nand\nUpgrading kubeadm clusters\n.\nSee\nAdminister a Cluster\nfor a longer list of Kubernetes administrative tasks.\nTo learn about available options when you run control plane services, see\nkube-apiserver\n,\nkube-controller-manager\n,\nand\nkube-scheduler\ncomponent pages. For highly available control plane examples, see\nOptions for Highly Available topology\n,\nCreating Highly Available clusters with kubeadm\n,\nand\nOperating etcd clusters for Kubernetes\n.\nSee\nBacking up an etcd cluster\nfor information on making an etcd backup plan.\nProduction worker nodes\nProduction-quality workloads need to be resilient and anything they rely\non needs to be resilient (such as CoreDNS). Whether you manage your own\ncontrol plane or have a cloud provider do it for you, you still need to\nconsider how you want to manage your worker nodes (also referred to\nsimply as\nnodes\n).\nConfigure nodes\n: Nodes can be physical or virtual machines. If you want to\ncreate and manage your own nodes, you can install a supported operating system,\nthen add and run the appropriate\nNode services\n. Consider:\nThe demands of your workloads when you set up nodes by having appropriate memory, CPU, and disk speed and storage capacity available.\nWhether generic computer systems will do or you have workloads that need GPU processors, Windows nodes, or VM isolation.\nValidate nodes\n: See\nValid node setup\nfor information on how to ensure that a node meets the requirements to join\na Kubernetes cluster.\nAdd nodes to the cluster\n: If you are managing your own cluster you can\nadd nodes by setting up your own machines and either adding them manually or\nhaving them register themselves to the cluster’s apiserver. See the\nNodes\nsection for information on how to set up Kubernetes to add nodes in these ways.\nScale nodes\n: Have a plan for expanding the capacity your cluster will\neventually need. See\nConsiderations for large clusters\nto help determine how many nodes you need, based on the number of pods and\ncontainers you need to run. If you are managing nodes yourself, this can mean\npurchasing and installing your own physical equipment.\nAutoscale nodes\n: Read\nNode Autoscaling\nto learn about the\ntools available to automatically manage your nodes and the capacity they\nprovide.\nSet up node health checks\n: For important workloads, you want to make sure\nthat the nodes and pods running on those nodes are healthy. Using the\nNode Problem Detector\ndaemon, you can ensure your nodes are healthy.\nProduction user management\nIn production, you may be moving from a model where you or a small group of\npeople are accessing the cluster to where there may potentially be dozens or\nhundreds of people. In a learning environment or platform prototype, you might have a single\nadministrative account for everything you do. In production, you will want\nmore accounts with different levels of access to different namespaces.\nTaking on a production-quality cluster means deciding how you\nwant to selectively allow access by other users. In particular, you need to\nselect strategies for validating the identities of those who try to access your\ncluster (authentication) and deciding if they have permissions to do what they\nare asking (authorization):\nAuthentication\n: The apiserver can authenticate users using client\ncertificates, bearer tokens, an authenticating proxy, or HTTP basic auth.\nYou can choose which authentication methods you want to use.\nUsing plugins, the apiserver can leverage your organization’s existing\nauthentication methods, such as LDAP or Kerberos. See\nAuthentication\nfor a description of these different methods of authenticating Kubernetes users.\nAuthorization\n: When you set out to authorize your regular users, you will probably choose\nbetween RBAC and ABAC authorization. See\nAuthorization Overview\nto review different modes for authorizing user accounts (as well as service account access to\nyour cluster):\nRole-based access control\n(\nRBAC\n): Lets you\nassign access to your cluster by allowing specific sets of permissions to authenticated users.\nPermissions can be assigned for a specific namespace (Role) or across the entire cluster\n(ClusterRole). Then using RoleBindings and ClusterRoleBindings, those permissions can be attached\nto particular users.\nAttribute-based access control\n(\nABAC\n): Lets you\ncreate policies based on resource attributes in the cluster and will allow or deny access\nbased on those attributes. Each line of a policy file identifies versioning properties (apiVersion\nand kind) and a map of spec properties to match the subject (user or group), resource property,\nnon-resource property (/version or /apis), and readonly. See\nExamples\nfor details.\nAs someone setting up authentication and authorization on your production Kubernetes cluster, here are some things to consider:\nSet the authorization mode\n: When the Kubernetes API server\n(\nkube-apiserver\n)\nstarts, supported authorization modes must be set using an\n--authorization-config\nfile or the\n--authorization-mode\nflag. For example, that flag in the\nkube-adminserver.yaml\nfile (in\n/etc/kubernetes/manifests\n)\ncould be set to Node,RBAC. This would allow Node and RBAC authorization for authenticated requests.\nCreate user certificates and role bindings (RBAC)\n: If you are using RBAC\nauthorization, users can create a CertificateSigningRequest (CSR) that can be\nsigned by the cluster CA. Then you can bind Roles and ClusterRoles to each user.\nSee\nCertificate Signing Requests\nfor details.\nCreate policies that combine attributes (ABAC)\n: If you are using ABAC\nauthorization, you can assign combinations of attributes to form policies to\nauthorize selected users or groups to access particular resources (such as a\npod), namespace, or apiGroup. For more information, see\nExamples\n.\nConsider Admission Controllers\n: Additional forms of authorization for\nrequests that can come in through the API server include\nWebhook Token Authentication\n.\nWebhooks and other special authorization types need to be enabled by adding\nAdmission Controllers\nto the API server.\nSet limits on workload resources\nDemands from production workloads can cause pressure both inside and outside\nof the Kubernetes control plane. Consider these items when setting up for the\nneeds of your cluster's workloads:\nSet namespace limits\n: Set per-namespace quotas on things like memory and CPU. See\nManage Memory, CPU, and API Resources\nfor details.\nPrepare for DNS demand\n: If you expect workloads to massively scale up,\nyour DNS service must be ready to scale up as well. See\nAutoscale the DNS service in a Cluster\n.\nCreate additional service accounts\n: User accounts determine what users can\ndo on a cluster, while a service account defines pod access within a particular\nnamespace. By default, a pod takes on the default service account from its namespace.\nSee\nManaging Service Accounts\nfor information on creating a new service account. For example, you might want to:\nAdd secrets that a pod could use to pull images from a particular container registry. See\nConfigure Service Accounts for Pods\nfor an example.\nAssign RBAC permissions to a service account. See\nServiceAccount permissions\nfor details.\nWhat's next\nDecide if you want to build your own production Kubernetes or obtain one from\navailable\nTurnkey Cloud Solutions\nor\nKubernetes Partners\n.\nIf you choose to build your own cluster, plan how you want to\nhandle\ncertificates\nand set up high availability for features such as\netcd\nand the\nAPI server\n.\nChoose from\nkubeadm\n,\nkops\nor\nKubespray\ndeployment methods.\nConfigure user management by determining your\nAuthentication\nand\nAuthorization\nmethods.\nPrepare for application workloads by setting up\nresource limits\n,\nDNS autoscaling\nand\nservice accounts\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified April 18, 2025 at 3:26 PM PST:\nRemove references to archived HNC project (c5441a5da7)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nContainer Runtimes\nContainer Runtimes\nNote:\nDockershim has been removed from the Kubernetes project as of release 1.24. Read the\nDockershim Removal FAQ\nfor further details.\nYou need to install a\ncontainer runtime\ninto each node in the cluster so that Pods can run there. This page outlines\nwhat is involved and describes related tasks for setting up nodes.\nKubernetes 1.33 requires that you use a runtime that\nconforms with the\nContainer Runtime Interface\n(CRI).\nSee\nCRI version support\nfor more information.\nThis page provides an outline of how to use several common container runtimes with\nKubernetes.\ncontainerd\nCRI-O\nDocker Engine\nMirantis Container Runtime\nNote:\nKubernetes releases before v1.24 included a direct integration with Docker Engine,\nusing a component named\ndockershim\n. That special direct integration is no longer\npart of Kubernetes (this removal was\nannounced\nas part of the v1.20 release).\nYou can read\nCheck whether Dockershim removal affects you\nto understand how this removal might affect you. To learn about migrating from using dockershim, see\nMigrating from dockershim\n.\nIf you are running a version of Kubernetes other than v1.33,\ncheck the documentation for that version.\nInstall and configure prerequisites\nNetwork configuration\nBy default, the Linux kernel does not allow IPv4 packets to be routed\nbetween interfaces. Most Kubernetes cluster networking implementations\nwill change this setting (if needed), but some might expect the\nadministrator to do it for them. (Some might also expect other sysctl\nparameters to be set, kernel modules to be loaded, etc; consult the\ndocumentation for your specific network implementation.)\nEnable IPv4 packet forwarding\nTo manually enable IPv4 packet forwarding:\n# sysctl params required by setup, params persist across reboots\ncat\n<<EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nEOF\n# Apply sysctl params without reboot\nsudo sysctl --system\nVerify that\nnet.ipv4.ip_forward\nis set to 1 with:\nsysctl net.ipv4.ip_forward\ncgroup drivers\nOn Linux,\ncontrol groups\nare used to constrain resources that are allocated to processes.\nBoth the\nkubelet\nand the\nunderlying container runtime need to interface with control groups to enforce\nresource management for pods and containers\nand set resources such as cpu/memory requests and limits. To interface with control\ngroups, the kubelet and the container runtime need to use a\ncgroup driver\n.\nIt's critical that the kubelet and the container runtime use the same cgroup\ndriver and are configured the same.\nThere are two cgroup drivers available:\ncgroupfs\nsystemd\ncgroupfs driver\nThe\ncgroupfs\ndriver is the\ndefault cgroup driver in the kubelet\n.\nWhen the\ncgroupfs\ndriver is used, the kubelet and the container runtime directly interface with\nthe cgroup filesystem to configure cgroups.\nThe\ncgroupfs\ndriver is\nnot\nrecommended when\nsystemd\nis the\ninit system because systemd expects a single cgroup manager on\nthe system. Additionally, if you use\ncgroup v2\n, use the\nsystemd\ncgroup driver instead of\ncgroupfs\n.\nsystemd cgroup driver\nWhen\nsystemd\nis chosen as the init\nsystem for a Linux distribution, the init process generates and consumes a root control group\n(\ncgroup\n) and acts as a cgroup manager.\nsystemd has a tight integration with cgroups and allocates a cgroup per systemd\nunit. As a result, if you use\nsystemd\nas the init system with the\ncgroupfs\ndriver, the system gets two different cgroup managers.\nTwo cgroup managers result in two views of the available and in-use resources in\nthe system. In some cases, nodes that are configured to use\ncgroupfs\nfor the\nkubelet and container runtime, but use\nsystemd\nfor the rest of the processes become\nunstable under resource pressure.\nThe approach to mitigate this instability is to use\nsystemd\nas the cgroup driver for\nthe kubelet and the container runtime when systemd is the selected init system.\nTo set\nsystemd\nas the cgroup driver, edit the\nKubeletConfiguration\noption of\ncgroupDriver\nand set it to\nsystemd\n. For example:\napiVersion\n:\nkubelet.config.k8s.io/v1beta1\nkind\n:\nKubeletConfiguration\n...\ncgroupDriver\n:\nsystemd\nNote:\nStarting with v1.22 and later, when creating a cluster with kubeadm, if the user does not set\nthe\ncgroupDriver\nfield under\nKubeletConfiguration\n, kubeadm defaults it to\nsystemd\n.\nIf you configure\nsystemd\nas the cgroup driver for the kubelet, you must also\nconfigure\nsystemd\nas the cgroup driver for the container runtime. Refer to\nthe documentation for your container runtime for instructions. For example:\ncontainerd\nCRI-O\nIn Kubernetes 1.33, with the\nKubeletCgroupDriverFromCRI\nfeature gate\nenabled and a container runtime that supports the\nRuntimeConfig\nCRI RPC,\nthe kubelet automatically detects the appropriate cgroup driver from the runtime,\nand ignores the\ncgroupDriver\nsetting within the kubelet configuration.\nCaution:\nChanging the cgroup driver of a Node that has joined a cluster is a sensitive operation.\nIf the kubelet has created Pods using the semantics of one cgroup driver, changing the container\nruntime to another cgroup driver can cause errors when trying to re-create the Pod sandbox\nfor such existing Pods. Restarting the kubelet may not solve such errors.\nIf you have automation that makes it feasible, replace the node with another using the updated\nconfiguration, or reinstall it using automation.\nMigrating to the\nsystemd\ndriver in kubeadm managed clusters\nIf you wish to migrate to the\nsystemd\ncgroup driver in existing kubeadm managed clusters,\nfollow\nconfiguring a cgroup driver\n.\nCRI version support\nYour container runtime must support at least v1alpha2 of the container runtime interface.\nKubernetes\nstarting v1.26\nonly works\nwith v1 of the CRI API. Earlier versions default\nto v1 version, however if a container runtime does not support the v1 API, the kubelet falls back to\nusing the (deprecated) v1alpha2 API instead.\nContainer runtimes\nNote:\nThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the\ncontent guide\nbefore submitting a change.\nMore information.\ncontainerd\nThis section outlines the necessary steps to use containerd as CRI runtime.\nTo install containerd on your system, follow the instructions on\ngetting started with containerd\n.\nReturn to this step once you've created a valid\nconfig.toml\nconfiguration file.\nLinux\nWindows\nYou can find this file under the path\n/etc/containerd/config.toml\n.\nYou can find this file under the path\nC:\\Program Files\\containerd\\config.toml\n.\nOn Linux the default CRI socket for containerd is\n/run/containerd/containerd.sock\n.\nOn Windows the default CRI endpoint is\nnpipe://./pipe/containerd-containerd\n.\nConfiguring the\nsystemd\ncgroup driver\nTo use the\nsystemd\ncgroup driver in\n/etc/containerd/config.toml\nwith\nrunc\n, set\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n  ...\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n    SystemdCgroup = true\nThe\nsystemd\ncgroup driver is recommended if you use\ncgroup v2\n.\nNote:\nIf you installed containerd from a package (for example, RPM or\n.deb\n), you may find\nthat the CRI integration plugin is disabled by default.\nYou need CRI support enabled to use containerd with Kubernetes. Make sure that\ncri\nis not included in the\ndisabled_plugins\nlist within\n/etc/containerd/config.toml\n;\nif you made changes to that file, also restart\ncontainerd\n.\nIf you experience container crash loops after the initial cluster installation or after\ninstalling a CNI, the containerd configuration provided with the package might contain\nincompatible configuration parameters. Consider resetting the containerd configuration\nwith\ncontainerd config default > /etc/containerd/config.toml\nas specified in\ngetting-started.md\nand then set the configuration parameters specified above accordingly.\nIf you apply this change, make sure to restart containerd:\nsudo systemctl restart containerd\nWhen using kubeadm, manually configure the\ncgroup driver for kubelet\n.\nIn Kubernetes v1.28, you can enable automatic detection of the\ncgroup driver as an alpha feature. See\nsystemd cgroup driver\nfor more details.\nOverriding the sandbox (pause) image\nIn your\ncontainerd config\nyou can overwrite the\nsandbox image by setting the following config:\n[plugins.\n\"io.containerd.grpc.v1.cri\"\n]\nsandbox_image =\n\"registry.k8s.io/pause:3.10\"\nYou might need to restart\ncontainerd\nas well once you've updated the config file:\nsystemctl restart containerd\n.\nCRI-O\nThis section contains the necessary steps to install CRI-O as a container runtime.\nTo install CRI-O, follow\nCRI-O Install Instructions\n.\ncgroup driver\nCRI-O uses the systemd cgroup driver per default, which is likely to work fine\nfor you. To switch to the\ncgroupfs\ncgroup driver, either edit\n/etc/crio/crio.conf\nor place a drop-in configuration in\n/etc/crio/crio.conf.d/02-cgroup-manager.conf\n, for example:\n[crio.runtime]\nconmon_cgroup =\n\"pod\"\ncgroup_manager =\n\"cgroupfs\"\nYou should also note the changed\nconmon_cgroup\n, which has to be set to the value\npod\nwhen using CRI-O with\ncgroupfs\n. It is generally necessary to keep the\ncgroup driver configuration of the kubelet (usually done via kubeadm) and CRI-O\nin sync.\nIn Kubernetes v1.28, you can enable automatic detection of the\ncgroup driver as an alpha feature. See\nsystemd cgroup driver\nfor more details.\nFor CRI-O, the CRI socket is\n/var/run/crio/crio.sock\nby default.\nOverriding the sandbox (pause) image\nIn your\nCRI-O config\nyou can set the following\nconfig value:\n[crio.image]\npause_image=\n\"registry.k8s.io/pause:3.10\"\nThis config option supports live configuration reload to apply this change:\nsystemctl reload crio\nor by sending\nSIGHUP\nto the\ncrio\nprocess.\nDocker Engine\nNote:\nThese instructions assume that you are using the\ncri-dockerd\nadapter to integrate\nDocker Engine with Kubernetes.\nOn each of your nodes, install Docker for your Linux distribution as per\nInstall Docker Engine\n.\nInstall\ncri-dockerd\n, following the directions in the install section of the documentation.\nFor\ncri-dockerd\n, the CRI socket is\n/run/cri-dockerd.sock\nby default.\nMirantis Container Runtime\nMirantis Container Runtime\n(MCR) is a commercially\navailable container runtime that was formerly known as Docker Enterprise Edition.\nYou can use Mirantis Container Runtime with Kubernetes using the open source\ncri-dockerd\ncomponent, included with MCR.\nTo learn more about how to install Mirantis Container Runtime,\nvisit\nMCR Deployment Guide\n.\nCheck the systemd unit named\ncri-docker.socket\nto find out the path to the CRI\nsocket.\nOverriding the sandbox (pause) image\nThe\ncri-dockerd\nadapter accepts a command line argument for\nspecifying which container image to use as the Pod infrastructure container (“pause image”).\nThe command line argument to use is\n--pod-infra-container-image\n.\nWhat's next\nAs well as a container runtime, your cluster will need a working\nnetwork plugin\n.\nItems on this page refer to third party products or projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. See the\nCNCF website guidelines\nfor more details.\nYou should read the\ncontent guide\nbefore proposing a change that adds an extra third-party link.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified June 11, 2025 at 10:32 AM PST:\nupdate mcr from 20.10 to 25.0 in cri en pages (1081a625a8)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nInstalling Kubernetes with deployment tools\nThere are many methods and tools for setting up your own production Kubernetes cluster.\nFor example:\nkubeadm\nCluster API\n: A Kubernetes sub-project focused on\nproviding declarative APIs and tooling to simplify provisioning, upgrading, and operating\nmultiple Kubernetes clusters.\nkops\n: An automated cluster provisioning tool.\nFor tutorials, best practices, configuration options and information on\nreaching out to the community, please check the\nkOps\nwebsite\nfor details.\nkubespray\n:\nA composition of\nAnsible\nplaybooks,\ninventory\n,\nprovisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration\nmanagement tasks. You can reach out to the community on Slack channel\n#kubespray\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified December 09, 2024 at 11:46 PM PST:\nUpdate Link of Inventory in Install k8s doc (7692020f50)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nBootstrapping clusters with kubeadm\nInstalling kubeadm\nTroubleshooting kubeadm\nCreating a cluster with kubeadm\nCustomizing components with the kubeadm API\nOptions for Highly Available Topology\nCreating Highly Available Clusters with kubeadm\nSet up a High Availability etcd Cluster with kubeadm\nConfiguring each kubelet in your cluster using kubeadm\nDual-stack support with kubeadm\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified June 12, 2019 at 5:27 PM PST:\nRestructure the left navigation pane of setup (#14826) (55ac801bc4)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nInstalling kubeadm\nInstalling kubeadm\nThis page shows how to install the\nkubeadm\ntoolbox.\nFor information on how to create a cluster with kubeadm once you have performed this installation process,\nsee the\nCreating a cluster with kubeadm\npage.\nThis installation guide is for Kubernetes v1.33. If you want to use a different Kubernetes version, please refer to the following pages instead:\nInstalling kubeadm (Kubernetes v1.32)\nInstalling kubeadm (Kubernetes v1.31)\nInstalling kubeadm (Kubernetes v1.30)\nInstalling kubeadm (Kubernetes v1.29)\nBefore you begin\nA compatible Linux host. The Kubernetes project provides generic instructions for Linux distributions\nbased on Debian and Red Hat, and those distributions without a package manager.\n2 GB or more of RAM per machine (any less will leave little room for your apps).\n2 CPUs or more for control plane machines.\nFull network connectivity between all machines in the cluster (public or private network is fine).\nUnique hostname, MAC address, and product_uuid for every node. See\nhere\nfor more details.\nCertain ports are open on your machines. See\nhere\nfor more details.\nNote:\nThe\nkubeadm\ninstallation is done via binaries that use dynamic linking and assumes that your target system provides\nglibc\n.\nThis is a reasonable assumption on many Linux distributions (including Debian, Ubuntu, Fedora, CentOS, etc.)\nbut it is not always the case with custom and lightweight distributions which don't include\nglibc\nby default, such as Alpine Linux.\nThe expectation is that the distribution either includes\nglibc\nor a\ncompatibility layer\nthat provides the expected symbols.\nCheck your OS version\nNote:\nThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the\ncontent guide\nbefore submitting a change.\nMore information.\nLinux\nWindows\nThe kubeadm project supports LTS kernels. See\nList of LTS kernels\n.\nYou can get the kernel version using the command\nuname -r\nFor more information, see\nLinux Kernel Requirements\n.\nThe kubeadm project supports recent kernel versions. For a list of recent kernels, see\nWindows Server Release Information\n.\nYou can get the kernel version (also called the OS version) using the command\nsysteminfo\nFor more information, see\nWindows OS version compatibility\n.\nA Kubernetes cluster created by kubeadm depends on software that use kernel features.\nThis software includes, but is not limited to the\ncontainer runtime\n,\nthe\nkubelet\n, and a\nContainer Network Interface\nplugin.\nTo help you avoid unexpected errors as a result of an unsupported kernel version, kubeadm runs the\nSystemVerification\npre-flight check. This check fails if the kernel version is not supported.\nYou may choose to skip the check, if you know that your kernel\nprovides the required features, even though kubeadm does not support its version.\nVerify the MAC address and product_uuid are unique for every node\nYou can get the MAC address of the network interfaces using the command\nip link\nor\nifconfig -a\nThe product_uuid can be checked by using the command\nsudo cat /sys/class/dmi/id/product_uuid\nIt is very likely that hardware devices will have unique addresses, although some virtual machines may have\nidentical values. Kubernetes uses these values to uniquely identify the nodes in the cluster.\nIf these values are not unique to each node, the installation process\nmay\nfail\n.\nCheck network adapters\nIf you have more than one network adapter, and your Kubernetes components are not reachable on the default\nroute, we recommend you add IP route(s) so Kubernetes cluster addresses go via the appropriate adapter.\nCheck required ports\nThese\nrequired ports\nneed to be open in order for Kubernetes components to communicate with each other.\nYou can use tools like\nnetcat\nto check if a port is open. For example:\nnc 127.0.0.1\n6443\n-zv -w\n2\nThe pod network plugin you use may also require certain ports to be\nopen. Since this differs with each pod network plugin, please see the\ndocumentation for the plugins about what port(s) those need.\nSwap configuration\nThe default behavior of a kubelet is to fail to start if swap memory is detected on a node.\nThis means that swap should either be disabled or tolerated by kubelet.\nTo tolerate swap, add\nfailSwapOn: false\nto kubelet configuration or as a command line argument.\nNote: even if\nfailSwapOn: false\nis provided, workloads wouldn't have swap access by default.\nThis can be changed by setting a\nswapBehavior\n, again in the kubelet configuration file. To use swap,\nset a\nswapBehavior\nother than the default\nNoSwap\nsetting.\nSee\nSwap memory management\nfor more details.\nTo disable swap,\nsudo swapoff -a\ncan be used to disable swapping temporarily.\nTo make this change persistent across reboots, make sure swap is disabled in\nconfig files like\n/etc/fstab\n,\nsystemd.swap\n, depending how it was configured on your system.\nInstalling a container runtime\nTo run containers in Pods, Kubernetes uses a\ncontainer runtime\n.\nBy default, Kubernetes uses the\nContainer Runtime Interface\n(CRI)\nto interface with your chosen container runtime.\nIf you don't specify a runtime, kubeadm automatically tries to detect an installed\ncontainer runtime by scanning through a list of known endpoints.\nIf multiple or no container runtimes are detected kubeadm will throw an error\nand will request that you specify which one you want to use.\nSee\ncontainer runtimes\nfor more information.\nNote:\nDocker Engine does not implement the\nCRI\nwhich is a requirement for a container runtime to work with Kubernetes.\nFor that reason, an additional service\ncri-dockerd\nhas to be installed. cri-dockerd is a project based on the legacy built-in\nDocker Engine support that was\nremoved\nfrom the kubelet in version 1.24.\nThe tables below include the known endpoints for supported operating systems:\nLinux\nWindows\nLinux container runtimes\nRuntime\nPath to Unix domain socket\ncontainerd\nunix:///var/run/containerd/containerd.sock\nCRI-O\nunix:///var/run/crio/crio.sock\nDocker Engine (using cri-dockerd)\nunix:///var/run/cri-dockerd.sock\nWindows container runtimes\nRuntime\nPath to Windows named pipe\ncontainerd\nnpipe:////./pipe/containerd-containerd\nDocker Engine (using cri-dockerd)\nnpipe:////./pipe/cri-dockerd\nInstalling kubeadm, kubelet and kubectl\nYou will install these packages on all of your machines:\nkubeadm\n: the command to bootstrap the cluster.\nkubelet\n: the component that runs on all of the machines in your cluster\nand does things like starting pods and containers.\nkubectl\n: the command line util to talk to your cluster.\nkubeadm\nwill not\ninstall or manage\nkubelet\nor\nkubectl\nfor you, so you will\nneed to ensure they match the version of the Kubernetes control plane you want\nkubeadm to install for you. If you do not, there is a risk of a version skew occurring that\ncan lead to unexpected, buggy behaviour. However,\none\nminor version skew between the\nkubelet and the control plane is supported, but the kubelet version may never exceed the API\nserver version. For example, the kubelet running 1.7.0 should be fully compatible with a 1.8.0 API server,\nbut not vice versa.\nFor information about installing\nkubectl\n, see\nInstall and set up kubectl\n.\nWarning:\nThese instructions exclude all Kubernetes packages from any system upgrades.\nThis is because kubeadm and Kubernetes require\nspecial attention to upgrade\n.\nFor more information on version skews, see:\nKubernetes\nversion and version-skew policy\nKubeadm-specific\nversion skew policy\nNote:\nThe legacy package repositories (\napt.kubernetes.io\nand\nyum.kubernetes.io\n) have been\ndeprecated and frozen starting from September 13, 2023\n.\nUsing the\nnew package repositories hosted at\npkgs.k8s.io\nis strongly recommended and required in order to install Kubernetes versions released after September 13, 2023.\nThe deprecated legacy repositories, and their contents, might be removed at any time in the future and without\na further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.\nNote:\nThere's a dedicated package repository for each Kubernetes minor version. If you want to install\na minor version other than v1.33, please see the installation guide for\nyour desired minor version.\nDebian-based distributions\nRed Hat-based distributions\nWithout a package manager\nThese instructions are for Kubernetes v1.33.\nUpdate the\napt\npackage index and install packages needed to use the Kubernetes\napt\nrepository:\nsudo apt-get update\n# apt-transport-https may be a dummy package; if so, you can skip that package\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\nDownload the public signing key for the Kubernetes package repositories.\nThe same signing key is used for all repositories so you can disregard the version in the URL:\n# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.\n# sudo mkdir -p -m 755 /etc/apt/keyrings\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nNote:\nIn releases older than Debian 12 and Ubuntu 22.04, directory\n/etc/apt/keyrings\ndoes not\nexist by default, and it should be created before the curl command.\nAdd the appropriate Kubernetes\napt\nrepository. Please note that this repository have packages\nonly for Kubernetes 1.33; for other Kubernetes minor versions, you need to\nchange the Kubernetes minor version in the URL to match your desired minor version\n(you should also check that you are reading the documentation for the version of Kubernetes\nthat you plan to install).\n# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list\necho\n'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /'\n| sudo tee /etc/apt/sources.list.d/kubernetes.list\nUpdate the\napt\npackage index, install kubelet, kubeadm and kubectl, and pin their version:\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n(Optional) Enable the kubelet service before running kubeadm:\nsudo systemctl\nenable\n--now kubelet\nSet SELinux to\npermissive\nmode:\nThese instructions are for Kubernetes 1.33.\n# Set SELinux in permissive mode (effectively disabling it)\nsudo setenforce\n0\nsudo sed -i\n's/^SELINUX=enforcing$/SELINUX=permissive/'\n/etc/selinux/config\nCaution:\nSetting SELinux in permissive mode by running\nsetenforce 0\nand\nsed ...\neffectively disables it. This is required to allow containers to access the host\nfilesystem; for example, some cluster network plugins require that. You have to\ndo this until SELinux support is improved in the kubelet.\nYou can leave SELinux enabled if you know how to configure it but it may require\nsettings that are not supported by kubeadm.\nAdd the Kubernetes\nyum\nrepository. The\nexclude\nparameter in the\nrepository definition ensures that the packages related to Kubernetes are\nnot upgraded upon running\nyum update\nas there's a special procedure that\nmust be followed for upgrading Kubernetes. Please note that this repository\nhave packages only for Kubernetes 1.33; for other\nKubernetes minor versions, you need to change the Kubernetes minor version\nin the URL to match your desired minor version (you should also check that\nyou are reading the documentation for the version of Kubernetes that you\nplan to install).\n# This overwrites any existing configuration in /etc/yum.repos.d/kubernetes.repo\ncat\n<<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key\nexclude=kubelet kubeadm kubectl cri-tools kubernetes-cni\nEOF\nInstall kubelet, kubeadm and kubectl:\nsudo yum install -y kubelet kubeadm kubectl --disableexcludes\n=\nkubernetes\n(Optional) Enable the kubelet service before running kubeadm:\nsudo systemctl\nenable\n--now kubelet\nInstall CNI plugins (required for most pod network):\nCNI_PLUGINS_VERSION\n=\n\"v1.3.0\"\nARCH\n=\n\"amd64\"\nDEST\n=\n\"/opt/cni/bin\"\nsudo mkdir -p\n\"\n$DEST\n\"\ncurl -L\n\"https://github.com/containernetworking/plugins/releases/download/\n${\nCNI_PLUGINS_VERSION\n}\n/cni-plugins-linux-\n${\nARCH\n}\n-\n${\nCNI_PLUGINS_VERSION\n}\n.tgz\"\n| sudo tar -C\n\"\n$DEST\n\"\n-xz\nDefine the directory to download command files:\nNote:\nThe\nDOWNLOAD_DIR\nvariable must be set to a writable directory.\nIf you are running Flatcar Container Linux, set\nDOWNLOAD_DIR=\"/opt/bin\"\n.\nDOWNLOAD_DIR\n=\n\"/usr/local/bin\"\nsudo mkdir -p\n\"\n$DOWNLOAD_DIR\n\"\nOptionally install crictl (required for interaction with the Container Runtime Interface (CRI), optional for kubeadm):\nCRICTL_VERSION\n=\n\"v1.31.0\"\nARCH\n=\n\"amd64\"\ncurl -L\n\"https://github.com/kubernetes-sigs/cri-tools/releases/download/\n${\nCRICTL_VERSION\n}\n/crictl-\n${\nCRICTL_VERSION\n}\n-linux-\n${\nARCH\n}\n.tar.gz\"\n| sudo tar -C\n$DOWNLOAD_DIR\n-xz\nInstall\nkubeadm\n,\nkubelet\nand add a\nkubelet\nsystemd service:\nRELEASE\n=\n\"\n$(\ncurl -sSL https://dl.k8s.io/release/stable.txt\n)\n\"\nARCH\n=\n\"amd64\"\ncd\n$DOWNLOAD_DIR\nsudo curl -L --remote-name-all https://dl.k8s.io/release/\n${\nRELEASE\n}\n/bin/linux/\n${\nARCH\n}\n/\n{\nkubeadm,kubelet\n}\nsudo chmod +x\n{\nkubeadm,kubelet\n}\nRELEASE_VERSION\n=\n\"v0.16.2\"\ncurl -sSL\n\"https://raw.githubusercontent.com/kubernetes/release/\n${\nRELEASE_VERSION\n}\n/cmd/krel/templates/latest/kubelet/kubelet.service\"\n| sed\n\"s:/usr/bin:\n${\nDOWNLOAD_DIR\n}\n:g\"\n| sudo tee /usr/lib/systemd/system/kubelet.service\nsudo mkdir -p /usr/lib/systemd/system/kubelet.service.d\ncurl -sSL\n\"https://raw.githubusercontent.com/kubernetes/release/\n${\nRELEASE_VERSION\n}\n/cmd/krel/templates/latest/kubeadm/10-kubeadm.conf\"\n| sed\n\"s:/usr/bin:\n${\nDOWNLOAD_DIR\n}\n:g\"\n| sudo tee /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf\nNote:\nPlease refer to the note in the\nBefore you begin\nsection for Linux distributions\nthat do not include\nglibc\nby default.\nInstall\nkubectl\nby following the instructions on\nInstall Tools page\n.\nOptionally, enable the kubelet service before running kubeadm:\nsudo systemctl\nenable\n--now kubelet\nNote:\nThe Flatcar Container Linux distribution mounts the\n/usr\ndirectory as a read-only filesystem.\nBefore bootstrapping your cluster, you need to take additional steps to configure a writable directory.\nSee the\nKubeadm Troubleshooting guide\nto learn how to set up a writable directory.\nThe kubelet is now restarting every few seconds, as it waits in a crashloop for\nkubeadm to tell it what to do.\nConfiguring a cgroup driver\nBoth the container runtime and the kubelet have a property called\n\"cgroup driver\"\n, which is important\nfor the management of cgroups on Linux machines.\nWarning:\nMatching the container runtime and kubelet cgroup drivers is required or otherwise the kubelet process will fail.\nSee\nConfiguring a cgroup driver\nfor more details.\nTroubleshooting\nIf you are running into difficulties with kubeadm, please consult our\ntroubleshooting docs\n.\nWhat's next\nUsing kubeadm to Create a Cluster\nItems on this page refer to third party products or projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. See the\nCNCF website guidelines\nfor more details.\nYou should read the\ncontent guide\nbefore proposing a change that adds an extra third-party link.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified April 09, 2025 at 9:57 AM PST:\nAdd links to Kubernetes docs about Linux and Windows requirements, and update text. (6e8297ea91)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nTroubleshooting kubeadm\nTroubleshooting kubeadm\nAs with any program, you might run into an error installing or running kubeadm.\nThis page lists some common failure scenarios and have provided steps that can help you understand and fix the problem.\nIf your problem is not listed below, please follow the following steps:\nIf you think your problem is a bug with kubeadm:\nGo to\ngithub.com/kubernetes/kubeadm\nand search for existing issues.\nIf no issue exists, please\nopen one\nand follow the issue template.\nIf you are unsure about how kubeadm works, you can ask on\nSlack\nin\n#kubeadm\n,\nor open a question on\nStackOverflow\n. Please include\nrelevant tags like\n#kubernetes\nand\n#kubeadm\nso folks can help you.\nNot possible to join a v1.18 Node to a v1.17 cluster due to missing RBAC\nIn v1.18 kubeadm added prevention for joining a Node in the cluster if a Node with the same name already exists.\nThis required adding RBAC for the bootstrap-token user to be able to GET a Node object.\nHowever this causes an issue where\nkubeadm join\nfrom v1.18 cannot join a cluster created by kubeadm v1.17.\nTo workaround the issue you have two options:\nExecute\nkubeadm init phase bootstrap-token\non a control-plane node using kubeadm v1.18.\nNote that this enables the rest of the bootstrap-token permissions as well.\nor\nApply the following RBAC manually using\nkubectl apply -f ...\n:\napiVersion\n:\nrbac.authorization.k8s.io/v1\nkind\n:\nClusterRole\nmetadata\n:\nname\n:\nkubeadm:get-nodes\nrules\n:\n-\napiGroups\n:\n-\n\"\"\nresources\n:\n- nodes\nverbs\n:\n- get\n---\napiVersion\n:\nrbac.authorization.k8s.io/v1\nkind\n:\nClusterRoleBinding\nmetadata\n:\nname\n:\nkubeadm:get-nodes\nroleRef\n:\napiGroup\n:\nrbac.authorization.k8s.io\nkind\n:\nClusterRole\nname\n:\nkubeadm:get-nodes\nsubjects\n:\n-\napiGroup\n:\nrbac.authorization.k8s.io\nkind\n:\nGroup\nname\n:\nsystem:bootstrappers:kubeadm:default-node-token\nebtables\nor some similar executable not found during installation\nIf you see the following warnings while running\nkubeadm init\n[preflight] WARNING: ebtables not found in system path\n[preflight] WARNING: ethtool not found in system path\nThen you may be missing\nebtables\n,\nethtool\nor a similar executable on your node.\nYou can install them with the following commands:\nFor Ubuntu/Debian users, run\napt install ebtables ethtool\n.\nFor CentOS/Fedora users, run\nyum install ebtables ethtool\n.\nkubeadm blocks waiting for control plane during installation\nIf you notice that\nkubeadm init\nhangs after printing out the following line:\n[apiclient] Created API client, waiting for the control plane to become ready\nThis may be caused by a number of problems. The most common are:\nnetwork connection problems. Check that your machine has full network connectivity before continuing.\nthe cgroup driver of the container runtime differs from that of the kubelet. To understand how to\nconfigure it properly, see\nConfiguring a cgroup driver\n.\ncontrol plane containers are crashlooping or hanging. You can check this by running\ndocker ps\nand investigating each container by running\ndocker logs\n. For other container runtime, see\nDebugging Kubernetes nodes with crictl\n.\nkubeadm blocks when removing managed containers\nThe following could happen if the container runtime halts and does not remove\nany Kubernetes-managed containers:\nsudo kubeadm reset\n[preflight] Running pre-flight checks\n[reset] Stopping the kubelet service\n[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n[reset] Removing kubernetes-managed containers\n(block)\nA possible solution is to restart the container runtime and then re-run\nkubeadm reset\n.\nYou can also use\ncrictl\nto debug the state of the container runtime. See\nDebugging Kubernetes nodes with crictl\n.\nPods in\nRunContainerError\n,\nCrashLoopBackOff\nor\nError\nstate\nRight after\nkubeadm init\nthere should not be any pods in these states.\nIf there are pods in one of these states\nright after\nkubeadm init\n, please open an\nissue in the kubeadm repo.\ncoredns\n(or\nkube-dns\n) should be in the\nPending\nstate\nuntil you have deployed the network add-on.\nIf you see Pods in the\nRunContainerError\n,\nCrashLoopBackOff\nor\nError\nstate\nafter deploying the network add-on and nothing happens to\ncoredns\n(or\nkube-dns\n),\nit's very likely that the Pod Network add-on that you installed is somehow broken.\nYou might have to grant it more RBAC privileges or use a newer version. Please file\nan issue in the Pod Network providers' issue tracker and get the issue triaged there.\ncoredns\nis stuck in the\nPending\nstate\nThis is\nexpected\nand part of the design. kubeadm is network provider-agnostic, so the admin\nshould\ninstall the pod network add-on\nof choice. You have to install a Pod Network\nbefore CoreDNS may be deployed fully. Hence the\nPending\nstate before the network is set up.\nHostPort\nservices do not work\nThe\nHostPort\nand\nHostIP\nfunctionality is available depending on your Pod Network\nprovider. Please contact the author of the Pod Network add-on to find out whether\nHostPort\nand\nHostIP\nfunctionality are available.\nCalico, Canal, and Flannel CNI providers are verified to support HostPort.\nFor more information, see the\nCNI portmap documentation\n.\nIf your network provider does not support the portmap CNI plugin, you may need to use the\nNodePort feature of services\nor use\nHostNetwork=true\n.\nPods are not accessible via their Service IP\nMany network add-ons do not yet enable\nhairpin mode\nwhich allows pods to access themselves via their Service IP. This is an issue related to\nCNI\n. Please contact the network\nadd-on provider to get the latest status of their support for hairpin mode.\nIf you are using VirtualBox (directly or via Vagrant), you will need to\nensure that\nhostname -i\nreturns a routable IP address. By default, the first\ninterface is connected to a non-routable host-only network. A work around\nis to modify\n/etc/hosts\n, see this\nVagrantfile\nfor an example.\nTLS certificate errors\nThe following error indicates a possible certificate mismatch.\n# kubectl get pods\nUnable to connect to the server: x509: certificate signed by unknown authority (possibly because of \"crypto/rsa: verification error\" while trying to verify candidate authority certificate \"kubernetes\")\nVerify that the\n$HOME/.kube/config\nfile contains a valid certificate, and\nregenerate a certificate if necessary. The certificates in a kubeconfig file\nare base64 encoded. The\nbase64 --decode\ncommand can be used to decode the certificate\nand\nopenssl x509 -text -noout\ncan be used for viewing the certificate information.\nUnset the\nKUBECONFIG\nenvironment variable using:\nunset\nKUBECONFIG\nOr set it to the default\nKUBECONFIG\nlocation:\nexport\nKUBECONFIG\n=\n/etc/kubernetes/admin.conf\nAnother workaround is to overwrite the existing\nkubeconfig\nfor the \"admin\" user:\nmv\n$HOME\n/.kube\n$HOME\n/.kube.bak\nmkdir\n$HOME\n/.kube\nsudo cp -i /etc/kubernetes/admin.conf\n$HOME\n/.kube/config\nsudo chown\n$(\nid -u\n)\n:\n$(\nid -g\n)\n$HOME\n/.kube/config\nKubelet client certificate rotation fails\nBy default, kubeadm configures a kubelet with automatic rotation of client certificates by using the\n/var/lib/kubelet/pki/kubelet-client-current.pem\nsymlink specified in\n/etc/kubernetes/kubelet.conf\n.\nIf this rotation process fails you might see errors such as\nx509: certificate has expired or is not yet valid\nin kube-apiserver logs. To fix the issue you must follow these steps:\nBackup and delete\n/etc/kubernetes/kubelet.conf\nand\n/var/lib/kubelet/pki/kubelet-client*\nfrom the failed node.\nFrom a working control plane node in the cluster that has\n/etc/kubernetes/pki/ca.key\nexecute\nkubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE > kubelet.conf\n.\n$NODE\nmust be set to the name of the existing failed node in the cluster.\nModify the resulted\nkubelet.conf\nmanually to adjust the cluster name and server endpoint,\nor pass\nkubeconfig user --config\n(see\nGenerating kubeconfig files for additional users\n). If your cluster does not have\nthe\nca.key\nyou must sign the embedded certificates in the\nkubelet.conf\nexternally.\nCopy this resulted\nkubelet.conf\nto\n/etc/kubernetes/kubelet.conf\non the failed node.\nRestart the kubelet (\nsystemctl restart kubelet\n) on the failed node and wait for\n/var/lib/kubelet/pki/kubelet-client-current.pem\nto be recreated.\nManually edit the\nkubelet.conf\nto point to the rotated kubelet client certificates, by replacing\nclient-certificate-data\nand\nclient-key-data\nwith:\nclient-certificate\n:\n/var/lib/kubelet/pki/kubelet-client-current.pem\nclient-key\n:\n/var/lib/kubelet/pki/kubelet-client-current.pem\nRestart the kubelet.\nMake sure the node becomes\nReady\n.\nDefault NIC When using flannel as the pod network in Vagrant\nThe following error might indicate that something was wrong in the pod network:\nError from server\n(\nNotFound\n)\n: the server could not find the requested resource\nIf you're using flannel as the pod network inside Vagrant, then you will have to\nspecify the default interface name for flannel.\nVagrant typically assigns two interfaces to all VMs. The first, for which all hosts\nare assigned the IP address\n10.0.2.15\n, is for external traffic that gets NATed.\nThis may lead to problems with flannel, which defaults to the first interface on a host.\nThis leads to all hosts thinking they have the same public IP address. To prevent this,\npass the\n--iface eth1\nflag to flannel so that the second interface is chosen.\nNon-public IP used for containers\nIn some situations\nkubectl logs\nand\nkubectl run\ncommands may return with the\nfollowing errors in an otherwise functional cluster:\nError from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host\nThis may be due to Kubernetes using an IP that can not communicate with other IPs on\nthe seemingly same subnet, possibly by policy of the machine provider.\nDigitalOcean assigns a public IP to\neth0\nas well as a private one to be used internally\nas anchor for their floating IP feature, yet\nkubelet\nwill pick the latter as the node's\nInternalIP\ninstead of the public one.\nUse\nip addr show\nto check for this scenario instead of\nifconfig\nbecause\nifconfig\nwill\nnot display the offending alias IP address. Alternatively an API endpoint specific to\nDigitalOcean allows to query for the anchor IP from the droplet:\ncurl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address\nThe workaround is to tell\nkubelet\nwhich IP to use using\n--node-ip\n.\nWhen using DigitalOcean, it can be the public one (assigned to\neth0\n) or\nthe private one (assigned to\neth1\n) should you want to use the optional\nprivate network. The\nkubeletExtraArgs\nsection of the kubeadm\nNodeRegistrationOptions\nstructure\ncan be used for this.\nThen restart\nkubelet\n:\nsystemctl daemon-reload\nsystemctl restart kubelet\ncoredns\npods have\nCrashLoopBackOff\nor\nError\nstate\nIf you have nodes that are running SELinux with an older version of Docker, you might experience a scenario\nwhere the\ncoredns\npods are not starting. To solve that, you can try one of the following options:\nUpgrade to a\nnewer version of Docker\n.\nDisable SELinux\n.\nModify the\ncoredns\ndeployment to set\nallowPrivilegeEscalation\nto\ntrue\n:\nkubectl -n kube-system get deployment coredns -o yaml |\n\\\nsed\n's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g'\n|\n\\\nkubectl apply -f -\nAnother cause for CoreDNS to have\nCrashLoopBackOff\nis when a CoreDNS Pod deployed in Kubernetes detects a loop.\nA number of workarounds\nare available to avoid Kubernetes trying to restart the CoreDNS Pod every time CoreDNS detects the loop and exits.\nWarning:\nDisabling SELinux or setting\nallowPrivilegeEscalation\nto\ntrue\ncan compromise\nthe security of your cluster.\netcd pods restart continually\nIf you encounter the following error:\nrpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused \"process_linux.go:110: decoding init error from pipe caused \\\"read parent: connection reset by peer\\\"\"\nThis issue appears if you run CentOS 7 with Docker 1.13.1.84.\nThis version of Docker can prevent the kubelet from executing into the etcd container.\nTo work around the issue, choose one of these options:\nRoll back to an earlier version of Docker, such as 1.13.1-75\nyum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64\nInstall one of the more recent recommended versions, such as 18.06:\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nyum install docker-ce-18.06.1.ce-3.el7.x86_64\nNot possible to pass a comma separated list of values to arguments inside a\n--component-extra-args\nflag\nkubeadm init\nflags such as\n--component-extra-args\nallow you to pass custom arguments to a control-plane\ncomponent like the kube-apiserver. However, this mechanism is limited due to the underlying type used for parsing\nthe values (\nmapStringString\n).\nIf you decide to pass an argument that supports multiple, comma-separated values such as\n--apiserver-extra-args \"enable-admission-plugins=LimitRanger,NamespaceExists\"\nthis flag will fail with\nflag: malformed pair, expect string=string\n. This happens because the list of arguments for\n--apiserver-extra-args\nexpects\nkey=value\npairs and in this case\nNamespacesExists\nis considered\nas a key that is missing a value.\nAlternatively, you can try separating the\nkey=value\npairs like so:\n--apiserver-extra-args \"enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists\"\nbut this will result in the key\nenable-admission-plugins\nonly having the value of\nNamespaceExists\n.\nA known workaround is to use the kubeadm\nconfiguration file\n.\nkube-proxy scheduled before node is initialized by cloud-controller-manager\nIn cloud provider scenarios, kube-proxy can end up being scheduled on new worker nodes before\nthe cloud-controller-manager has initialized the node addresses. This causes kube-proxy to fail\nto pick up the node's IP address properly and has knock-on effects to the proxy function managing\nload balancers.\nThe following error can be seen in kube-proxy Pods:\nserver.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []\nproxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP\nA known solution is to patch the kube-proxy DaemonSet to allow scheduling it on control-plane\nnodes regardless of their conditions, keeping it off of other nodes until their initial guarding\nconditions abate:\nkubectl -n kube-system patch ds kube-proxy -p='{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"tolerations\": [\n          {\n            \"key\": \"CriticalAddonsOnly\",\n            \"operator\": \"Exists\"\n          },\n          {\n            \"effect\": \"NoSchedule\",\n            \"key\": \"node-role.kubernetes.io/control-plane\"\n          }\n        ]\n      }\n    }\n  }\n}'\nThe tracking issue for this problem is\nhere\n.\n/usr\nis mounted read-only on nodes\nOn Linux distributions such as Fedora CoreOS or Flatcar Container Linux, the directory\n/usr\nis mounted as a read-only filesystem.\nFor\nflex-volume support\n,\nKubernetes components like the kubelet and kube-controller-manager use the default path of\n/usr/libexec/kubernetes/kubelet-plugins/volume/exec/\n, yet the flex-volume directory\nmust be writeable\nfor the feature to work.\nNote:\nFlexVolume was deprecated in the Kubernetes v1.23 release.\nTo workaround this issue, you can configure the flex-volume directory using the kubeadm\nconfiguration file\n.\nOn the primary control-plane Node (created using\nkubeadm init\n), pass the following\nfile using\n--config\n:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nInitConfiguration\nnodeRegistration\n:\nkubeletExtraArgs\n:\n-\nname\n:\n\"volume-plugin-dir\"\nvalue\n:\n\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"\n---\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\ncontrollerManager\n:\nextraArgs\n:\n-\nname\n:\n\"flex-volume-plugin-dir\"\nvalue\n:\n\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"\nOn joining Nodes:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nJoinConfiguration\nnodeRegistration\n:\nkubeletExtraArgs\n:\n-\nname\n:\n\"volume-plugin-dir\"\nvalue\n:\n\"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\"\nAlternatively, you can modify\n/etc/fstab\nto make the\n/usr\nmount writeable, but please\nbe advised that this is modifying a design principle of the Linux distribution.\nkubeadm upgrade plan\nprints out\ncontext deadline exceeded\nerror message\nThis error message is shown when upgrading a Kubernetes cluster with\nkubeadm\nin\nthe case of running an external etcd. This is not a critical bug and happens because\nolder versions of kubeadm perform a version check on the external etcd cluster.\nYou can proceed with\nkubeadm upgrade apply ...\n.\nThis issue is fixed as of version 1.19.\nkubeadm reset\nunmounts\n/var/lib/kubelet\nIf\n/var/lib/kubelet\nis being mounted, performing a\nkubeadm reset\nwill effectively unmount it.\nTo workaround the issue, re-mount the\n/var/lib/kubelet\ndirectory after performing the\nkubeadm reset\noperation.\nThis is a regression introduced in kubeadm 1.15. The issue is fixed in 1.20.\nCannot use the metrics-server securely in a kubeadm cluster\nIn a kubeadm cluster, the\nmetrics-server\ncan be used insecurely by passing the\n--kubelet-insecure-tls\nto it. This is not recommended for production clusters.\nIf you want to use TLS between the metrics-server and the kubelet there is a problem,\nsince kubeadm deploys a self-signed serving certificate for the kubelet. This can cause the following errors\non the side of the metrics-server:\nx509: certificate signed by unknown authority\nx509: certificate is valid for IP-foo not IP-bar\nSee\nEnabling signed kubelet serving certificates\nto understand how to configure the kubelets in a kubeadm cluster to have properly signed serving certificates.\nAlso see\nHow to run the metrics-server securely\n.\nUpgrade fails due to etcd hash not changing\nOnly applicable to upgrading a control plane node with a kubeadm binary v1.28.3 or later,\nwhere the node is currently managed by kubeadm versions v1.28.0, v1.28.1 or v1.28.2.\nHere is the error message you may encounter:\n[upgrade/etcd] Failed to upgrade etcd: couldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced: static Pod hash for component etcd on Node kinder-upgrade-control-plane-1 did not change after 5m0s: timed out waiting for the condition\n[upgrade/etcd] Waiting for previous etcd to become available\nI0907 10:10:09.109104    3704 etcd.go:588] [etcd] attempting to see if all cluster endpoints ([https://172.17.0.6:2379/ https://172.17.0.4:2379/ https://172.17.0.3:2379/]) are available 1/10\n[upgrade/etcd] Etcd was rolled back and is now available\nstatic Pod hash for component etcd on Node kinder-upgrade-control-plane-1 did not change after 5m0s: timed out waiting for the condition\ncouldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.rollbackOldManifests\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:525\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.upgradeComponent\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:254\nk8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.performEtcdStaticPodUpgrade\n\tcmd/kubeadm/app/phases/upgrade/staticpods.go:338\n...\nThe reason for this failure is that the affected versions generate an etcd manifest file with\nunwanted defaults in the PodSpec. This will result in a diff from the manifest comparison,\nand kubeadm will expect a change in the Pod hash, but the kubelet will never update the hash.\nThere are two way to workaround this issue if you see it in your cluster:\nThe etcd upgrade can be skipped between the affected versions and v1.28.3 (or later) by using:\nkubeadm upgrade\n{\napply|node\n}\n[\nversion\n]\n--etcd-upgrade\n=\nfalse\nThis is not recommended in case a new etcd version was introduced by a later v1.28 patch version.\nBefore upgrade, patch the manifest for the etcd static pod, to remove the problematic defaulted attributes:\ndiff --git a/etc/kubernetes/manifests/etcd_defaults.yaml b/etc/kubernetes/manifests/etcd_origin.yaml\nindex d807ccbe0aa..46b35f00e15 100644\n--- a/etc/kubernetes/manifests/etcd_defaults.yaml\n+++ b/etc/kubernetes/manifests/etcd_origin.yaml\n@@ -43,7 +43,6 @@ spec:\nscheme: HTTP\ninitialDelaySeconds: 10\nperiodSeconds: 10\n-      successThreshold: 1\ntimeoutSeconds: 15\nname: etcd\nresources:\n@@ -59,26 +58,18 @@ spec:\nscheme: HTTP\ninitialDelaySeconds: 10\nperiodSeconds: 10\n-      successThreshold: 1\ntimeoutSeconds: 15\n-    terminationMessagePath: /dev/termination-log\n-    terminationMessagePolicy: File\nvolumeMounts:\n- mountPath: /var/lib/etcd\nname: etcd-data\n- mountPath: /etc/kubernetes/pki/etcd\nname: etcd-certs\n-  dnsPolicy: ClusterFirst\n-  enableServiceLinks: true\nhostNetwork: true\npriority: 2000001000\npriorityClassName: system-node-critical\n-  restartPolicy: Always\n-  schedulerName: default-scheduler\nsecurityContext:\nseccompProfile:\ntype: RuntimeDefault\n-  terminationGracePeriodSeconds: 30\nvolumes:\n- hostPath:\npath: /etc/kubernetes/pki/etcd\nMore information can be found in the\ntracking issue\nfor this bug.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified July 05, 2024 at 4:06 PM PST:\nkubeadm: use v1beta4 in all docs examples (efc1133fa4)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nCreating a cluster with kubeadm\nCreating a cluster with kubeadm\nUsing\nkubeadm\n, you can create a minimum viable Kubernetes cluster that conforms to best practices.\nIn fact, you can use\nkubeadm\nto set up a cluster that will pass the\nKubernetes Conformance tests\n.\nkubeadm\nalso supports other cluster lifecycle functions, such as\nbootstrap tokens\nand cluster upgrades.\nThe\nkubeadm\ntool is good if you need:\nA simple way for you to try out Kubernetes, possibly for the first time.\nA way for existing users to automate setting up a cluster and test their application.\nA building block in other ecosystem and/or installer tools with a larger\nscope.\nYou can install and use\nkubeadm\non various machines: your laptop, a set\nof cloud servers, a Raspberry Pi, and more. Whether you're deploying into the\ncloud or on-premises, you can integrate\nkubeadm\ninto provisioning systems such\nas Ansible or Terraform.\nBefore you begin\nTo follow this guide, you need:\nOne or more machines running a deb/rpm-compatible Linux OS; for example: Ubuntu or CentOS.\n2 GiB or more of RAM per machine--any less leaves little room for your apps.\nAt least 2 CPUs on the machine that you use as a control-plane node.\nFull network connectivity among all machines in the cluster. You can use either a\npublic or a private network.\nYou also need to use a version of\nkubeadm\nthat can deploy the version\nof Kubernetes that you want to use in your new cluster.\nKubernetes' version and version skew support policy\napplies to\nkubeadm\nas well as to Kubernetes overall.\nCheck that policy to learn about what versions of Kubernetes and\nkubeadm\nare supported. This page is written for Kubernetes v1.33.\nThe\nkubeadm\ntool's overall feature state is General Availability (GA). Some sub-features are\nstill under active development. The implementation of creating the cluster may change\nslightly as the tool evolves, but the overall implementation should be pretty stable.\nNote:\nAny commands under\nkubeadm alpha\nare, by definition, supported on an alpha level.\nObjectives\nInstall a single control-plane Kubernetes cluster\nInstall a Pod network on the cluster so that your Pods can\ntalk to each other\nInstructions\nPreparing the hosts\nComponent installation\nInstall a\ncontainer runtime\nand kubeadm on all the hosts. For detailed instructions and other prerequisites, see\nInstalling kubeadm\n.\nNote:\nIf you have already installed kubeadm, see the first two steps of the\nUpgrading Linux nodes\ndocument for instructions on how to upgrade kubeadm.\nWhen you upgrade, the kubelet restarts every few seconds as it waits in a crashloop for\nkubeadm to tell it what to do. This crashloop is expected and normal.\nAfter you initialize your control-plane, the kubelet runs normally.\nNetwork setup\nkubeadm similarly to other Kubernetes components tries to find a usable IP on\nthe network interfaces associated with a default gateway on a host. Such\nan IP is then used for the advertising and/or listening performed by a component.\nTo find out what this IP is on a Linux host you can use:\nip route show\n# Look for a line starting with \"default via\"\nNote:\nIf two or more default gateways are present on the host, a Kubernetes component will\ntry to use the first one it encounters that has a suitable global unicast IP address.\nWhile making this choice, the exact ordering of gateways might vary between different\noperating systems and kernel versions.\nKubernetes components do not accept custom network interface as an option,\ntherefore a custom IP address must be passed as a flag to all components instances\nthat need such a custom configuration.\nNote:\nIf the host does not have a default gateway and if a custom IP address is not passed\nto a Kubernetes component, the component may exit with an error.\nTo configure the API server advertise address for control plane nodes created with both\ninit\nand\njoin\n, the flag\n--apiserver-advertise-address\ncan be used.\nPreferably, this option can be set in the\nkubeadm API\nas\nInitConfiguration.localAPIEndpoint\nand\nJoinConfiguration.controlPlane.localAPIEndpoint\n.\nFor kubelets on all nodes, the\n--node-ip\noption can be passed in\n.nodeRegistration.kubeletExtraArgs\ninside a kubeadm configuration file\n(\nInitConfiguration\nor\nJoinConfiguration\n).\nFor dual-stack see\nDual-stack support with kubeadm\n.\nThe IP addresses that you assign to control plane components become part of their X.509 certificates'\nsubject alternative name fields. Changing these IP addresses would require\nsigning new certificates and restarting the affected components, so that the change in\ncertificate files is reflected. See\nManual certificate renewal\nfor more details on this topic.\nWarning:\nThe Kubernetes project recommends against this approach (configuring all component instances\nwith custom IP addresses). Instead, the Kubernetes maintainers recommend to setup the host network,\nso that the default gateway IP is the one that Kubernetes components auto-detect and use.\nOn Linux nodes, you can use commands such as\nip route\nto configure networking; your operating\nsystem might also provide higher level network management tools. If your node's default gateway\nis a public IP address, you should configure packet filtering or other security measures that\nprotect the nodes and your cluster.\nPreparing the required container images\nThis step is optional and only applies in case you wish\nkubeadm init\nand\nkubeadm join\nto not download the default container images which are hosted at\nregistry.k8s.io\n.\nKubeadm has commands that can help you pre-pull the required images\nwhen creating a cluster without an internet connection on its nodes.\nSee\nRunning kubeadm without an internet connection\nfor more details.\nKubeadm allows you to use a custom image repository for the required images.\nSee\nUsing custom images\nfor more details.\nInitializing your control-plane node\nThe control-plane node is the machine where the control plane components run, including\netcd\n(the cluster database) and the\nAPI Server\n(which the\nkubectl\ncommand line tool\ncommunicates with).\n(Recommended) If you have plans to upgrade this single control-plane\nkubeadm\ncluster\nto\nhigh availability\nyou should specify the\n--control-plane-endpoint\nto set the shared endpoint for all control-plane nodes.\nSuch an endpoint can be either a DNS name or an IP address of a load-balancer.\nChoose a Pod network add-on, and verify whether it requires any arguments to\nbe passed to\nkubeadm init\n. Depending on which\nthird-party provider you choose, you might need to set the\n--pod-network-cidr\nto\na provider-specific value. See\nInstalling a Pod network add-on\n.\n(Optional)\nkubeadm\ntries to detect the container runtime by using a list of well\nknown endpoints. To use different container runtime or if there are more than one installed\non the provisioned node, specify the\n--cri-socket\nargument to\nkubeadm\n. See\nInstalling a runtime\n.\nTo initialize the control-plane node run:\nkubeadm init <args>\nConsiderations about apiserver-advertise-address and ControlPlaneEndpoint\nWhile\n--apiserver-advertise-address\ncan be used to set the advertised address for this particular\ncontrol-plane node's API server,\n--control-plane-endpoint\ncan be used to set the shared endpoint\nfor all control-plane nodes.\n--control-plane-endpoint\nallows both IP addresses and DNS names that can map to IP addresses.\nPlease contact your network administrator to evaluate possible solutions with respect to such mapping.\nHere is an example mapping:\n192.168.0.102 cluster-endpoint\nWhere\n192.168.0.102\nis the IP address of this node and\ncluster-endpoint\nis a custom DNS name that maps to this IP.\nThis will allow you to pass\n--control-plane-endpoint=cluster-endpoint\nto\nkubeadm init\nand pass the same DNS name to\nkubeadm join\n. Later you can modify\ncluster-endpoint\nto point to the address of your load-balancer in a\nhigh availability scenario.\nTurning a single control plane cluster created without\n--control-plane-endpoint\ninto a highly available cluster\nis not supported by kubeadm.\nMore information\nFor more information about\nkubeadm init\narguments, see the\nkubeadm reference guide\n.\nTo configure\nkubeadm init\nwith a configuration file see\nUsing kubeadm init with a configuration file\n.\nTo customize control plane components, including optional IPv6 assignment to liveness probe\nfor control plane components and etcd server, provide extra arguments to each component as documented in\ncustom arguments\n.\nTo reconfigure a cluster that has already been created see\nReconfiguring a kubeadm cluster\n.\nTo run\nkubeadm init\nagain, you must first\ntear down the cluster\n.\nIf you join a node with a different architecture to your cluster, make sure that your deployed DaemonSets\nhave container image support for this architecture.\nkubeadm init\nfirst runs a series of prechecks to ensure that the machine\nis ready to run Kubernetes. These prechecks expose warnings and exit on errors.\nkubeadm init\nthen downloads and installs the cluster control plane components. This may take several minutes.\nAfter it finishes you should see:\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a Pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  /docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>\nTo make kubectl work for your non-root user, run these commands, which are\nalso part of the\nkubeadm init\noutput:\nmkdir -p\n$HOME\n/.kube\nsudo cp -i /etc/kubernetes/admin.conf\n$HOME\n/.kube/config\nsudo chown\n$(\nid -u\n)\n:\n$(\nid -g\n)\n$HOME\n/.kube/config\nAlternatively, if you are the\nroot\nuser, you can run:\nexport\nKUBECONFIG\n=\n/etc/kubernetes/admin.conf\nWarning:\nThe kubeconfig file\nadmin.conf\nthat\nkubeadm init\ngenerates contains a certificate with\nSubject: O = kubeadm:cluster-admins, CN = kubernetes-admin\n. The group\nkubeadm:cluster-admins\nis bound to the built-in\ncluster-admin\nClusterRole.\nDo not share the\nadmin.conf\nfile with anyone.\nkubeadm init\ngenerates another kubeconfig file\nsuper-admin.conf\nthat contains a certificate with\nSubject: O = system:masters, CN = kubernetes-super-admin\n.\nsystem:masters\nis a break-glass, super user group that bypasses the authorization layer (for example RBAC).\nDo not share the\nsuper-admin.conf\nfile with anyone. It is recommended to move the file to a safe location.\nSee\nGenerating kubeconfig files for additional users\non how to use\nkubeadm kubeconfig user\nto generate kubeconfig files for additional users.\nMake a record of the\nkubeadm join\ncommand that\nkubeadm init\noutputs. You\nneed this command to\njoin nodes to your cluster\n.\nThe token is used for mutual authentication between the control-plane node and the joining\nnodes. The token included here is secret. Keep it safe, because anyone with this\ntoken can add authenticated nodes to your cluster. These tokens can be listed,\ncreated, and deleted with the\nkubeadm token\ncommand. See the\nkubeadm reference guide\n.\nInstalling a Pod network add-on\nCaution:\nThis section contains important information about networking setup and\ndeployment order.\nRead all of this advice carefully before proceeding.\nYou must deploy a\nContainer Network Interface\n(CNI) based Pod network add-on so that your Pods can communicate with each other.\nCluster DNS (CoreDNS) will not start up before a network is installed.\nTake care that your Pod network must not overlap with any of the host\nnetworks: you are likely to see problems if there is any overlap.\n(If you find a collision between your network plugin's preferred Pod\nnetwork and some of your host networks, you should think of a suitable\nCIDR block to use instead, then use that during\nkubeadm init\nwith\n--pod-network-cidr\nand as a replacement in your network plugin's YAML).\nBy default,\nkubeadm\nsets up your cluster to use and enforce use of\nRBAC\n(role based access\ncontrol).\nMake sure that your Pod network plugin supports RBAC, and so do any manifests\nthat you use to deploy it.\nIf you want to use IPv6--either dual-stack, or single-stack IPv6 only\nnetworking--for your cluster, make sure that your Pod network plugin\nsupports IPv6.\nIPv6 support was added to CNI in\nv0.6.0\n.\nNote:\nKubeadm should be CNI agnostic and the validation of CNI providers is out of the scope of our current e2e testing.\nIf you find an issue related to a CNI plugin you should log a ticket in its respective issue\ntracker instead of the kubeadm or kubernetes issue trackers.\nSeveral external projects provide Kubernetes Pod networks using CNI, some of which also\nsupport\nNetwork Policy\n.\nSee a list of add-ons that implement the\nKubernetes networking model\n.\nPlease refer to the\nInstalling Addons\npage for a non-exhaustive list of networking addons supported by Kubernetes.\nYou can install a Pod network add-on with the following command on the\ncontrol-plane node or a node that has the kubeconfig credentials:\nkubectl apply -f <add-on.yaml>\nNote:\nOnly a few CNI plugins support Windows. More details and setup instructions can be found\nin\nAdding Windows worker nodes\n.\nYou can install only one Pod network per cluster.\nOnce a Pod network has been installed, you can confirm that it is working by\nchecking that the CoreDNS Pod is\nRunning\nin the output of\nkubectl get pods --all-namespaces\n.\nAnd once the CoreDNS Pod is up and running, you can continue by joining your nodes.\nIf your network is not working or CoreDNS is not in the\nRunning\nstate, check out the\ntroubleshooting guide\nfor\nkubeadm\n.\nManaged node labels\nBy default, kubeadm enables the\nNodeRestriction\nadmission controller that restricts what labels can be self-applied by kubelets on node registration.\nThe admission controller documentation covers what labels are permitted to be used with the kubelet\n--node-labels\noption.\nThe\nnode-role.kubernetes.io/control-plane\nlabel is such a restricted label and kubeadm manually applies it using\na privileged client after a node has been created. To do that manually you can do the same by using\nkubectl label\nand ensure it is using a privileged kubeconfig such as the kubeadm managed\n/etc/kubernetes/admin.conf\n.\nControl plane node isolation\nBy default, your cluster will not schedule Pods on the control plane nodes for security\nreasons. If you want to be able to schedule Pods on the control plane nodes,\nfor example for a single machine Kubernetes cluster, run:\nkubectl taint nodes --all node-role.kubernetes.io/control-plane-\nThe output will look something like:\nnode \"test-01\" untainted\n...\nThis will remove the\nnode-role.kubernetes.io/control-plane:NoSchedule\ntaint\nfrom any nodes that have it, including the control plane nodes, meaning that the\nscheduler will then be able to schedule Pods everywhere.\nAdditionally, you can execute the following command to remove the\nnode.kubernetes.io/exclude-from-external-load-balancers\nlabel\nfrom the control plane node, which excludes it from the list of backend servers:\nkubectl label nodes --all node.kubernetes.io/exclude-from-external-load-balancers-\nAdding more control plane nodes\nSee\nCreating Highly Available Clusters with kubeadm\nfor steps on creating a high availability kubeadm cluster by adding more control plane nodes.\nAdding worker nodes\nThe worker nodes are where your workloads run.\nThe following pages show how to add Linux and Windows worker nodes to the cluster by using\nthe\nkubeadm join\ncommand:\nAdding Linux worker nodes\nAdding Windows worker nodes\n(Optional) Controlling your cluster from machines other than the control-plane node\nIn order to get a kubectl on some other computer (e.g. laptop) to talk to your\ncluster, you need to copy the administrator kubeconfig file from your control-plane node\nto your workstation like this:\nscp root@<control-plane-host>:/etc/kubernetes/admin.conf .\nkubectl --kubeconfig ./admin.conf get nodes\nNote:\nThe example above assumes SSH access is enabled for root. If that is not the\ncase, you can copy the\nadmin.conf\nfile to be accessible by some other user\nand\nscp\nusing that other user instead.\nThe\nadmin.conf\nfile gives the user\nsuperuser\nprivileges over the cluster.\nThis file should be used sparingly. For normal users, it's recommended to\ngenerate an unique credential to which you grant privileges. You can do\nthis with the\nkubeadm kubeconfig user --client-name <CN>\ncommand. That command will print out a KubeConfig file to STDOUT which you\nshould save to a file and distribute to your user. After that, grant\nprivileges by using\nkubectl create (cluster)rolebinding\n.\n(Optional) Proxying API Server to localhost\nIf you want to connect to the API Server from outside the cluster, you can use\nkubectl proxy\n:\nscp root@<control-plane-host>:/etc/kubernetes/admin.conf .\nkubectl --kubeconfig ./admin.conf proxy\nYou can now access the API Server locally at\nhttp://localhost:8001/api/v1\nClean up\nIf you used disposable servers for your cluster, for testing, you can\nswitch those off and do no further clean up. You can use\nkubectl config delete-cluster\nto delete your local references to the\ncluster.\nHowever, if you want to deprovision your cluster more cleanly, you should\nfirst\ndrain the node\nand make sure that the node is empty, then deconfigure the node.\nRemove the node\nTalking to the control-plane node with the appropriate credentials, run:\nkubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets\nBefore removing the node, reset the state installed by\nkubeadm\n:\nkubeadm reset\nThe reset process does not reset or clean up iptables rules or IPVS tables.\nIf you wish to reset iptables, you must do so manually:\niptables -F\n&&\niptables -t nat -F\n&&\niptables -t mangle -F\n&&\niptables -X\nIf you want to reset the IPVS tables, you must run the following command:\nipvsadm -C\nNow remove the node:\nkubectl delete node <node name>\nIf you wish to start over, run\nkubeadm init\nor\nkubeadm join\nwith the\nappropriate arguments.\nClean up the control plane\nYou can use\nkubeadm reset\non the control plane host to trigger a best-effort\nclean up.\nSee the\nkubeadm reset\nreference documentation for more information about this subcommand and its\noptions.\nVersion skew policy\nWhile kubeadm allows version skew against some components that it manages, it is recommended that you\nmatch the kubeadm version with the versions of the control plane components, kube-proxy and kubelet.\nkubeadm's skew against the Kubernetes version\nkubeadm can be used with Kubernetes components that are the same version as kubeadm\nor one version older. The Kubernetes version can be specified to kubeadm by using the\n--kubernetes-version\nflag of\nkubeadm init\nor the\nClusterConfiguration.kubernetesVersion\nfield when using\n--config\n. This option will control the versions\nof kube-apiserver, kube-controller-manager, kube-scheduler and kube-proxy.\nExample:\nkubeadm is at 1.33\nkubernetesVersion\nmust be at 1.33 or 1.32\nkubeadm's skew against the kubelet\nSimilarly to the Kubernetes version, kubeadm can be used with a kubelet version that is\nthe same version as kubeadm or three versions older.\nExample:\nkubeadm is at 1.33\nkubelet on the host must be at 1.33, 1.32,\n1.31 or 1.30\nkubeadm's skew against kubeadm\nThere are certain limitations on how kubeadm commands can operate on existing nodes or whole clusters\nmanaged by kubeadm.\nIf new nodes are joined to the cluster, the kubeadm binary used for\nkubeadm join\nmust match\nthe last version of kubeadm used to either create the cluster with\nkubeadm init\nor to upgrade\nthe same node with\nkubeadm upgrade\n. Similar rules apply to the rest of the kubeadm commands\nwith the exception of\nkubeadm upgrade\n.\nExample for\nkubeadm join\n:\nkubeadm version 1.33 was used to create a cluster with\nkubeadm init\nJoining nodes must use a kubeadm binary that is at version 1.33\nNodes that are being upgraded must use a version of kubeadm that is the same MINOR\nversion or one MINOR version newer than the version of kubeadm used for managing the\nnode.\nExample for\nkubeadm upgrade\n:\nkubeadm version 1.32 was used to create or upgrade the node\nThe version of kubeadm used for upgrading the node must be at 1.32\nor 1.33\nTo learn more about the version skew between the different Kubernetes component see\nthe\nVersion Skew Policy\n.\nLimitations\nCluster resilience\nThe cluster created here has a single control-plane node, with a single etcd database\nrunning on it. This means that if the control-plane node fails, your cluster may lose\ndata and may need to be recreated from scratch.\nWorkarounds:\nRegularly\nback up etcd\n. The\netcd data directory configured by kubeadm is at\n/var/lib/etcd\non the control-plane node.\nUse multiple control-plane nodes. You can read\nOptions for Highly Available topology\nto pick a cluster\ntopology that provides\nhigh-availability\n.\nPlatform compatibility\nkubeadm deb/rpm packages and binaries are built for amd64, arm (32-bit), arm64, ppc64le, and s390x\nfollowing the\nmulti-platform proposal\n.\nMultiplatform container images for the control plane and addons are also supported since v1.12.\nOnly some of the network providers offer solutions for all platforms. Please consult the list of\nnetwork providers above or the documentation from each provider to figure out whether the provider\nsupports your chosen platform.\nTroubleshooting\nIf you are running into difficulties with kubeadm, please consult our\ntroubleshooting docs\n.\nWhat's next\nVerify that your cluster is running properly with\nSonobuoy\nSee\nUpgrading kubeadm clusters\nfor details about upgrading your cluster using\nkubeadm\n.\nLearn about advanced\nkubeadm\nusage in the\nkubeadm reference documentation\nLearn more about Kubernetes\nconcepts\nand\nkubectl\n.\nSee the\nCluster Networking\npage for a bigger list\nof Pod network add-ons.\nSee the\nlist of add-ons\nto\nexplore other add-ons, including tools for logging, monitoring, network policy, visualization &\ncontrol of your Kubernetes cluster.\nConfigure how your cluster handles logs for cluster events and from\napplications running in Pods.\nSee\nLogging Architecture\nfor\nan overview of what is involved.\nFeedback\nFor bugs, visit the\nkubeadm GitHub issue tracker\nFor support, visit the\n#kubeadm\nSlack channel\nGeneral SIG Cluster Lifecycle development Slack channel:\n#sig-cluster-lifecycle\nSIG Cluster Lifecycle\nSIG information\nSIG Cluster Lifecycle mailing list:\nkubernetes-sig-cluster-lifecycle\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 16, 2024 at 9:28 AM PST:\nTweak and clean up four kubeadm files (67c5917e32)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nCustomizing components with the kubeadm API\nCustomizing components with the kubeadm API\nThis page covers how to customize the components that kubeadm deploys. For control plane components\nyou can use flags in the\nClusterConfiguration\nstructure or patches per-node. For the kubelet\nand kube-proxy you can use\nKubeletConfiguration\nand\nKubeProxyConfiguration\n, accordingly.\nAll of these options are possible via the kubeadm configuration API.\nFor more details on each field in the configuration you can navigate to our\nAPI reference pages\n.\nNote:\nCustomizing the CoreDNS deployment of kubeadm is currently not supported. You must manually\npatch the\nkube-system/coredns\nConfigMap\nand recreate the CoreDNS\nPods\nafter that. Alternatively,\nyou can skip the default CoreDNS deployment and deploy your own variant.\nFor more details on that see\nUsing init phases with kubeadm\n.\nNote:\nTo reconfigure a cluster that has already been created see\nReconfiguring a kubeadm cluster\n.\nCustomizing the control plane with flags in\nClusterConfiguration\nThe kubeadm\nClusterConfiguration\nobject exposes a way for users to override the default\nflags passed to control plane components such as the APIServer, ControllerManager, Scheduler and Etcd.\nThe components are defined using the following structures:\napiServer\ncontrollerManager\nscheduler\netcd\nThese structures contain a common\nextraArgs\nfield, that consists of\nname\n/\nvalue\npairs.\nTo override a flag for a control plane component:\nAdd the appropriate\nextraArgs\nto your configuration.\nAdd flags to the\nextraArgs\nfield.\nRun\nkubeadm init\nwith\n--config <YOUR CONFIG YAML>\n.\nNote:\nYou can generate a\nClusterConfiguration\nobject with default values by running\nkubeadm config print init-defaults\nand saving the output to a file of your choice.\nNote:\nThe\nClusterConfiguration\nobject is currently global in kubeadm clusters. This means that any flags that you add,\nwill apply to all instances of the same component on different nodes. To apply individual configuration per component\non different nodes you can use\npatches\n.\nNote:\nDuplicate flags (keys), or passing the same flag\n--foo\nmultiple times, is currently not supported.\nTo workaround that you must use\npatches\n.\nAPIServer flags\nFor details, see the\nreference documentation for kube-apiserver\n.\nExample usage:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nkubernetesVersion\n:\nv1.16.0\napiServer\n:\nextraArgs\n:\n-\nname\n:\n\"enable-admission-plugins\"\nvalue\n:\n\"AlwaysPullImages,DefaultStorageClass\"\n-\nname\n:\n\"audit-log-path\"\nvalue\n:\n\"/home/johndoe/audit.log\"\nControllerManager flags\nFor details, see the\nreference documentation for kube-controller-manager\n.\nExample usage:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nkubernetesVersion\n:\nv1.16.0\ncontrollerManager\n:\nextraArgs\n:\n-\nname\n:\n\"cluster-signing-key-file\"\nvalue\n:\n\"/home/johndoe/keys/ca.key\"\n-\nname\n:\n\"deployment-controller-sync-period\"\nvalue\n:\n\"50\"\nScheduler flags\nFor details, see the\nreference documentation for kube-scheduler\n.\nExample usage:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nkubernetesVersion\n:\nv1.16.0\nscheduler\n:\nextraArgs\n:\n-\nname\n:\n\"config\"\nvalue\n:\n\"/etc/kubernetes/scheduler-config.yaml\"\nextraVolumes\n:\n-\nname\n:\nschedulerconfig\nhostPath\n:\n/home/johndoe/schedconfig.yaml\nmountPath\n:\n/etc/kubernetes/scheduler-config.yaml\nreadOnly\n:\ntrue\npathType\n:\n\"File\"\nEtcd flags\nFor details, see the\netcd server documentation\n.\nExample usage:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\netcd\n:\nlocal\n:\nextraArgs\n:\n-\nname\n:\n\"election-timeout\"\nvalue\n:\n1000\nCustomizing with patches\nFEATURE STATE:\nKubernetes v1.22 [beta]\nKubeadm allows you to pass a directory with patch files to\nInitConfiguration\nand\nJoinConfiguration\non individual nodes. These patches can be used as the last customization step before component configuration\nis written to disk.\nYou can pass this file to\nkubeadm init\nwith\n--config <YOUR CONFIG YAML>\n:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nInitConfiguration\npatches\n:\ndirectory\n:\n/home/user/somedir\nNote:\nFor\nkubeadm init\nyou can pass a file containing both a\nClusterConfiguration\nand\nInitConfiguration\nseparated by\n---\n.\nYou can pass this file to\nkubeadm join\nwith\n--config <YOUR CONFIG YAML>\n:\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nJoinConfiguration\npatches\n:\ndirectory\n:\n/home/user/somedir\nThe directory must contain files named\ntarget[suffix][+patchtype].extension\n.\nFor example,\nkube-apiserver0+merge.yaml\nor just\netcd.json\n.\ntarget\ncan be one of\nkube-apiserver\n,\nkube-controller-manager\n,\nkube-scheduler\n,\netcd\nand\nkubeletconfiguration\n.\nsuffix\nis an optional string that can be used to determine which patches are applied first\nalpha-numerically.\npatchtype\ncan be one of\nstrategic\n,\nmerge\nor\njson\nand these must match the patching formats\nsupported by kubectl\n.\nThe default\npatchtype\nis\nstrategic\n.\nextension\nmust be either\njson\nor\nyaml\n.\nNote:\nIf you are using\nkubeadm upgrade\nto upgrade your kubeadm nodes you must again provide the same\npatches, so that the customization is preserved after upgrade. To do that you can use the\n--patches\nflag, which must point to the same directory.\nkubeadm upgrade\ncurrently does not support a configuration\nAPI structure that can be used for the same purpose.\nCustomizing the kubelet\nTo customize the kubelet you can add a\nKubeletConfiguration\nnext to the\nClusterConfiguration\nor\nInitConfiguration\nseparated by\n---\nwithin the same configuration file.\nThis file can then be passed to\nkubeadm init\nand kubeadm will apply the same base\nKubeletConfiguration\nto all nodes in the cluster.\nFor applying instance-specific configuration over the base\nKubeletConfiguration\nyou can use the\nkubeletconfiguration\npatch target\n.\nAlternatively, you can use kubelet flags as overrides by passing them in the\nnodeRegistration.kubeletExtraArgs\nfield supported by both\nInitConfiguration\nand\nJoinConfiguration\n.\nSome kubelet flags are deprecated, so check their status in the\nkubelet reference documentation\nbefore using them.\nFor additional details see\nConfiguring each kubelet in your cluster using kubeadm\nCustomizing kube-proxy\nTo customize kube-proxy you can pass a\nKubeProxyConfiguration\nnext your\nClusterConfiguration\nor\nInitConfiguration\nto\nkubeadm init\nseparated by\n---\n.\nFor more details you can navigate to our\nAPI reference pages\n.\nNote:\nkubeadm deploys kube-proxy as a\nDaemonSet\n, which means\nthat the\nKubeProxyConfiguration\nwould apply to all instances of kube-proxy in the cluster.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified July 05, 2024 at 4:06 PM PST:\nkubeadm: use v1beta4 in all docs examples (efc1133fa4)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nOptions for Highly Available Topology\nOptions for Highly Available Topology\nThis page explains the two options for configuring the topology of your highly available (HA) Kubernetes clusters.\nYou can set up an HA cluster:\nWith stacked control plane nodes, where etcd nodes are colocated with control plane nodes\nWith external etcd nodes, where etcd runs on separate nodes from the control plane\nYou should carefully consider the advantages and disadvantages of each topology before setting up an HA cluster.\nNote:\nkubeadm bootstraps the etcd cluster statically. Read the etcd\nClustering Guide\nfor more details.\nStacked etcd topology\nA stacked HA cluster is a\ntopology\nwhere the distributed\ndata storage cluster provided by etcd is stacked on top of the cluster formed by the nodes managed by\nkubeadm that run control plane components.\nEach control plane node runs an instance of the\nkube-apiserver\n,\nkube-scheduler\n, and\nkube-controller-manager\n.\nThe\nkube-apiserver\nis exposed to worker nodes using a load balancer.\nEach control plane node creates a local etcd member and this etcd member communicates only with\nthe\nkube-apiserver\nof this node. The same applies to the local\nkube-controller-manager\nand\nkube-scheduler\ninstances.\nThis topology couples the control planes and etcd members on the same nodes. It is simpler to set up than a cluster\nwith external etcd nodes, and simpler to manage for replication.\nHowever, a stacked cluster runs the risk of failed coupling. If one node goes down, both an etcd member and a control\nplane instance are lost, and redundancy is compromised. You can mitigate this risk by adding more control plane nodes.\nYou should therefore run a minimum of three stacked control plane nodes for an HA cluster.\nThis is the default topology in kubeadm. A local etcd member is created automatically\non control plane nodes when using\nkubeadm init\nand\nkubeadm join --control-plane\n.\nExternal etcd topology\nAn HA cluster with external etcd is a\ntopology\nwhere the distributed data storage cluster provided by etcd is external to the cluster formed by\nthe nodes that run control plane components.\nLike the stacked etcd topology, each control plane node in an external etcd topology runs\nan instance of the\nkube-apiserver\n,\nkube-scheduler\n, and\nkube-controller-manager\n.\nAnd the\nkube-apiserver\nis exposed to worker nodes using a load balancer. However,\netcd members run on separate hosts, and each etcd host communicates with the\nkube-apiserver\nof each control plane node.\nThis topology decouples the control plane and etcd member. It therefore provides an HA setup where\nlosing a control plane instance or an etcd member has less impact and does not affect\nthe cluster redundancy as much as the stacked HA topology.\nHowever, this topology requires twice the number of hosts as the stacked HA topology.\nA minimum of three hosts for control plane nodes and three hosts for etcd nodes are\nrequired for an HA cluster with this topology.\nWhat's next\nSet up a highly available cluster with kubeadm\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 16, 2024 at 9:28 AM PST:\nTweak and clean up four kubeadm files (67c5917e32)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nCreating Highly Available Clusters with kubeadm\nCreating Highly Available Clusters with kubeadm\nThis page explains two different approaches to setting up a highly available Kubernetes\ncluster using kubeadm:\nWith stacked control plane nodes. This approach requires less infrastructure. The etcd members\nand control plane nodes are co-located.\nWith an external etcd cluster. This approach requires more infrastructure. The\ncontrol plane nodes and etcd members are separated.\nBefore proceeding, you should carefully consider which approach best meets the needs of your applications\nand environment.\nOptions for Highly Available topology\noutlines the advantages and disadvantages of each.\nIf you encounter issues with setting up the HA cluster, please report these\nin the kubeadm\nissue tracker\n.\nSee also the\nupgrade documentation\n.\nCaution:\nThis page does not address running your cluster on a cloud provider. In a cloud\nenvironment, neither approach documented here works with Service objects of type\nLoadBalancer, or with dynamic PersistentVolumes.\nBefore you begin\nThe prerequisites depend on which topology you have selected for your cluster's\ncontrol plane:\nStacked etcd\nExternal etcd\nYou need:\nThree or more machines that meet\nkubeadm's minimum requirements\nfor\nthe control-plane nodes. Having an odd number of control plane nodes can help\nwith leader selection in the case of machine or zone failure.\nincluding a\ncontainer runtime\n, already set up and working\nThree or more machines that meet\nkubeadm's minimum\nrequirements\nfor the workers\nincluding a container runtime, already set up and working\nFull network connectivity between all machines in the cluster (public or\nprivate network)\nSuperuser privileges on all machines using\nsudo\nYou can use a different tool; this guide uses\nsudo\nin the examples.\nSSH access from one device to all nodes in the system\nkubeadm\nand\nkubelet\nalready installed on all machines.\nSee\nStacked etcd topology\nfor context.\nYou need:\nThree or more machines that meet\nkubeadm's minimum requirements\nfor\nthe control-plane nodes. Having an odd number of control plane nodes can help\nwith leader selection in the case of machine or zone failure.\nincluding a\ncontainer runtime\n, already set up and working\nThree or more machines that meet\nkubeadm's minimum\nrequirements\nfor the workers\nincluding a container runtime, already set up and working\nFull network connectivity between all machines in the cluster (public or\nprivate network)\nSuperuser privileges on all machines using\nsudo\nYou can use a different tool; this guide uses\nsudo\nin the examples.\nSSH access from one device to all nodes in the system\nkubeadm\nand\nkubelet\nalready installed on all machines.\nAnd you also need:\nThree or more additional machines, that will become etcd cluster members.\nHaving an odd number of members in the etcd cluster is a requirement for achieving\noptimal voting quorum.\nThese machines again need to have\nkubeadm\nand\nkubelet\ninstalled.\nThese machines also require a container runtime, that is already set up and working.\nSee\nExternal etcd topology\nfor context.\nContainer images\nEach host should have access read and fetch images from the Kubernetes container image registry,\nregistry.k8s.io\n. If you want to deploy a highly-available cluster where the hosts do not have\naccess to pull images, this is possible. You must ensure by some other means that the correct\ncontainer images are already available on the relevant hosts.\nCommand line interface\nTo manage Kubernetes once your cluster is set up, you should\ninstall kubectl\non your PC. It is also useful\nto install the\nkubectl\ntool on each control plane node, as this can be\nhelpful for troubleshooting.\nFirst steps for both methods\nCreate load balancer for kube-apiserver\nNote:\nThere are many configurations for load balancers. The following example is only one\noption. Your cluster requirements may need a different configuration.\nCreate a kube-apiserver load balancer with a name that resolves to DNS.\nIn a cloud environment you should place your control plane nodes behind a TCP\nforwarding load balancer. This load balancer distributes traffic to all\nhealthy control plane nodes in its target list. The health check for\nan apiserver is a TCP check on the port the kube-apiserver listens on\n(default value\n:6443\n).\nIt is not recommended to use an IP address directly in a cloud environment.\nThe load balancer must be able to communicate with all control plane nodes\non the apiserver port. It must also allow incoming traffic on its\nlistening port.\nMake sure the address of the load balancer always matches\nthe address of kubeadm's\nControlPlaneEndpoint\n.\nRead the\nOptions for Software Load Balancing\nguide for more details.\nAdd the first control plane node to the load balancer, and test the\nconnection:\nnc -zv -w\n2\n<LOAD_BALANCER_IP> <PORT>\nA connection refused error is expected because the API server is not yet\nrunning. A timeout, however, means the load balancer cannot communicate\nwith the control plane node. If a timeout occurs, reconfigure the load\nbalancer to communicate with the control plane node.\nAdd the remaining control plane nodes to the load balancer target group.\nStacked control plane and etcd nodes\nSteps for the first control plane node\nInitialize the control plane:\nsudo kubeadm init --control-plane-endpoint\n\"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\"\n--upload-certs\nYou can use the\n--kubernetes-version\nflag to set the Kubernetes version to use.\nIt is recommended that the versions of kubeadm, kubelet, kubectl and Kubernetes match.\nThe\n--control-plane-endpoint\nflag should be set to the address or DNS and port of the load balancer.\nThe\n--upload-certs\nflag is used to upload the certificates that should be shared\nacross all the control-plane instances to the cluster. If instead, you prefer to copy certs across\ncontrol-plane nodes manually or using automation tools, please remove this flag and refer to\nManual\ncertificate distribution\nsection below.\nNote:\nThe\nkubeadm init\nflags\n--config\nand\n--certificate-key\ncannot be mixed, therefore if you want\nto use the\nkubeadm configuration\nyou must add the\ncertificateKey\nfield in the appropriate config locations\n(under\nInitConfiguration\nand\nJoinConfiguration: controlPlane\n).\nNote:\nSome CNI network plugins require additional configuration, for example specifying the pod IP CIDR, while others do not.\nSee the\nCNI network documentation\n.\nTo add a pod CIDR pass the flag\n--pod-network-cidr\n, or if you are using a kubeadm configuration file\nset the\npodSubnet\nfield under the\nnetworking\nobject of\nClusterConfiguration\n.\nThe output looks similar to:\n...\nYou can now join any number of control-plane node by running the following\ncommand\non each as a root:\nkubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.\nThen you can join any number of worker nodes by running the following on each as root:\nkubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866\nCopy this output to a text file. You will need it later to join control plane and worker nodes to\nthe cluster.\nWhen\n--upload-certs\nis used with\nkubeadm init\n, the certificates of the primary control plane\nare encrypted and uploaded in the\nkubeadm-certs\nSecret.\nTo re-upload the certificates and generate a new decryption key, use the following command on a\ncontrol plane\nnode that is already joined to the cluster:\nsudo kubeadm init phase upload-certs --upload-certs\nYou can also specify a custom\n--certificate-key\nduring\ninit\nthat can later be used by\njoin\n.\nTo generate such a key you can use the following command:\nkubeadm certs certificate-key\nThe certificate key is a hex encoded string that is an AES key of size 32 bytes.\nNote:\nThe\nkubeadm-certs\nSecret and the decryption key expire after two hours.\nCaution:\nAs stated in the command output, the certificate key gives access to cluster sensitive data, keep it secret!\nApply the CNI plugin of your choice:\nFollow these instructions\nto install the CNI provider. Make sure the configuration corresponds to the Pod CIDR specified in the\nkubeadm configuration file (if applicable).\nNote:\nYou must pick a network plugin that suits your use case and deploy it before you move on to next step.\nIf you don't do this, you will not be able to launch your cluster properly.\nType the following and watch the pods of the control plane components get started:\nkubectl get pod -n kube-system -w\nSteps for the rest of the control plane nodes\nFor each additional control plane node you should:\nExecute the join command that was previously given to you by the\nkubeadm init\noutput on the first node.\nIt should look something like this:\nsudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07\nThe\n--control-plane\nflag tells\nkubeadm join\nto create a new control plane.\nThe\n--certificate-key ...\nwill cause the control plane certificates to be downloaded\nfrom the\nkubeadm-certs\nSecret in the cluster and be decrypted using the given key.\nYou can join multiple control-plane nodes in parallel.\nExternal etcd nodes\nSetting up a cluster with external etcd nodes is similar to the procedure used for stacked etcd\nwith the exception that you should setup etcd first, and you should pass the etcd information\nin the kubeadm config file.\nSet up the etcd cluster\nFollow these\ninstructions\nto set up the etcd cluster.\nSet up SSH as described\nhere\n.\nCopy the following files from any etcd node in the cluster to the first control plane node:\nexport\nCONTROL_PLANE\n=\n\"ubuntu@10.0.0.7\"\nscp /etc/kubernetes/pki/etcd/ca.crt\n\"\n${\nCONTROL_PLANE\n}\n\"\n:\nscp /etc/kubernetes/pki/apiserver-etcd-client.crt\n\"\n${\nCONTROL_PLANE\n}\n\"\n:\nscp /etc/kubernetes/pki/apiserver-etcd-client.key\n\"\n${\nCONTROL_PLANE\n}\n\"\n:\nReplace the value of\nCONTROL_PLANE\nwith the\nuser@host\nof the first control-plane node.\nSet up the first control plane node\nCreate a file called\nkubeadm-config.yaml\nwith the following contents:\n---\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nkubernetesVersion\n:\nstable\ncontrolPlaneEndpoint\n:\n\"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\"\n# change this (see below)\netcd\n:\nexternal\n:\nendpoints\n:\n- https://ETCD_0_IP:2379\n# change ETCD_0_IP appropriately\n- https://ETCD_1_IP:2379\n# change ETCD_1_IP appropriately\n- https://ETCD_2_IP:2379\n# change ETCD_2_IP appropriately\ncaFile\n:\n/etc/kubernetes/pki/etcd/ca.crt\ncertFile\n:\n/etc/kubernetes/pki/apiserver-etcd-client.crt\nkeyFile\n:\n/etc/kubernetes/pki/apiserver-etcd-client.key\nNote:\nThe difference between stacked etcd and external etcd here is that the external etcd setup requires\na configuration file with the etcd endpoints under the\nexternal\nobject for\netcd\n.\nIn the case of the stacked etcd topology, this is managed automatically.\nReplace the following variables in the config template with the appropriate values for your cluster:\nLOAD_BALANCER_DNS\nLOAD_BALANCER_PORT\nETCD_0_IP\nETCD_1_IP\nETCD_2_IP\nThe following steps are similar to the stacked etcd setup:\nRun\nsudo kubeadm init --config kubeadm-config.yaml --upload-certs\non this node.\nWrite the output join commands that are returned to a text file for later use.\nApply the CNI plugin of your choice.\nNote:\nYou must pick a network plugin that suits your use case and deploy it before you move on to next step.\nIf you don't do this, you will not be able to launch your cluster properly.\nSteps for the rest of the control plane nodes\nThe steps are the same as for the stacked etcd setup:\nMake sure the first control plane node is fully initialized.\nJoin each control plane node with the join command you saved to a text file. It's recommended\nto join the control plane nodes one at a time.\nDon't forget that the decryption key from\n--certificate-key\nexpires after two hours, by default.\nCommon tasks after bootstrapping control plane\nInstall workers\nWorker nodes can be joined to the cluster with the command you stored previously\nas the output from the\nkubeadm init\ncommand:\nsudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866\nManual certificate distribution\nIf you choose to not use\nkubeadm init\nwith the\n--upload-certs\nflag this means that\nyou are going to have to manually copy the certificates from the primary control plane node to the\njoining control plane nodes.\nThere are many ways to do this. The following example uses\nssh\nand\nscp\n:\nSSH is required if you want to control all nodes from a single machine.\nEnable ssh-agent on your main device that has access to all other nodes in\nthe system:\neval\n$(\nssh-agent\n)\nAdd your SSH identity to the session:\nssh-add ~/.ssh/path_to_private_key\nSSH between nodes to check that the connection is working correctly.\nWhen you SSH to any node, add the\n-A\nflag. This flag allows the node that you\nhave logged into via SSH to access the SSH agent on your PC. Consider alternative\nmethods if you do not fully trust the security of your user session on the node.\nssh -A 10.0.0.7\nWhen using sudo on any node, make sure to preserve the environment so SSH\nforwarding works:\nsudo -E -s\nAfter configuring SSH on all the nodes you should run the following script on the first\ncontrol plane node after running\nkubeadm init\n. This script will copy the certificates from\nthe first control plane node to the other control plane nodes:\nIn the following example, replace\nCONTROL_PLANE_IPS\nwith the IP addresses of the\nother control plane nodes.\nUSER\n=\nubuntu\n# customizable\nCONTROL_PLANE_IPS\n=\n\"10.0.0.7 10.0.0.8\"\nfor\nhost in\n${\nCONTROL_PLANE_IPS\n}\n;\ndo\nscp /etc/kubernetes/pki/ca.crt\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/ca.key\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/sa.key\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/sa.pub\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/front-proxy-ca.crt\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/front-proxy-ca.key\n\"\n${\nUSER\n}\n\"\n@\n$host\n:\nscp /etc/kubernetes/pki/etcd/ca.crt\n\"\n${\nUSER\n}\n\"\n@\n$host\n:etcd-ca.crt\n# Skip the next line if you are using external etcd\nscp /etc/kubernetes/pki/etcd/ca.key\n\"\n${\nUSER\n}\n\"\n@\n$host\n:etcd-ca.key\ndone\nCaution:\nCopy only the certificates in the above list. kubeadm will take care of generating the rest of the certificates\nwith the required SANs for the joining control-plane instances. If you copy all the certificates by mistake,\nthe creation of additional nodes could fail due to a lack of required SANs.\nThen on each joining control plane node you have to run the following script before running\nkubeadm join\n.\nThis script will move the previously copied certificates from the home directory to\n/etc/kubernetes/pki\n:\nUSER\n=\nubuntu\n# customizable\nmkdir -p /etc/kubernetes/pki/etcd\nmv /home/\n${\nUSER\n}\n/ca.crt /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/ca.key /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/sa.pub /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/sa.key /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/front-proxy-ca.crt /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/front-proxy-ca.key /etc/kubernetes/pki/\nmv /home/\n${\nUSER\n}\n/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt\n# Skip the next line if you are using external etcd\nmv /home/\n${\nUSER\n}\n/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified March 08, 2025 at 3:13 PM PST:\nUpdate high-availability.md (358fef3e0a)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nSet up a High Availability etcd Cluster with kubeadm\nSet up a High Availability etcd Cluster with kubeadm\nBy default, kubeadm runs a local etcd instance on each control plane node.\nIt is also possible to treat the etcd cluster as external and provision\netcd instances on separate hosts. The differences between the two approaches are covered in the\nOptions for Highly Available topology\npage.\nThis task walks through the process of creating a high availability external\netcd cluster of three members that can be used by kubeadm during cluster creation.\nBefore you begin\nThree hosts that can talk to each other over TCP ports 2379 and 2380. This\ndocument assumes these default ports. However, they are configurable through\nthe kubeadm config file.\nEach host must have systemd and a bash compatible shell installed.\nEach host must\nhave a container runtime, kubelet, and kubeadm installed\n.\nEach host should have access to the Kubernetes container image registry (\nregistry.k8s.io\n) or list/pull the required etcd image using\nkubeadm config images list/pull\n. This guide will set up etcd instances as\nstatic pods\nmanaged by a kubelet.\nSome infrastructure to copy files between hosts. For example\nssh\nand\nscp\ncan satisfy this requirement.\nSetting up the cluster\nThe general approach is to generate all certs on one node and only distribute\nthe\nnecessary\nfiles to the other nodes.\nNote:\nkubeadm contains all the necessary cryptographic machinery to generate\nthe certificates described below; no other cryptographic tooling is required for\nthis example.\nNote:\nThe examples below use IPv4 addresses but you can also configure kubeadm, the kubelet and etcd\nto use IPv6 addresses. Dual-stack is supported by some Kubernetes options, but not by etcd. For more details\non Kubernetes dual-stack support see\nDual-stack support with kubeadm\n.\nConfigure the kubelet to be a service manager for etcd.\nNote:\nYou must do this on every host where etcd should be running.\nSince etcd was created first, you must override the service priority by creating a new unit file\nthat has higher precedence than the kubeadm-provided kubelet unit file.\ncat\n<< EOF > /etc/systemd/system/kubelet.service.d/kubelet.conf\n# Replace \"systemd\" with the cgroup driver of your container runtime. The default value in the kubelet is \"cgroupfs\".\n# Replace the value of \"containerRuntimeEndpoint\" for a different container runtime if needed.\n#\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nauthentication:\nanonymous:\nenabled: false\nwebhook:\nenabled: false\nauthorization:\nmode: AlwaysAllow\ncgroupDriver: systemd\naddress: 127.0.0.1\ncontainerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock\nstaticPodPath: /etc/kubernetes/manifests\nEOF\ncat\n<< EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf\n[Service]\nExecStart=\nExecStart=/usr/bin/kubelet --config=/etc/systemd/system/kubelet.service.d/kubelet.conf\nRestart=always\nEOF\nsystemctl daemon-reload\nsystemctl restart kubelet\nCheck the kubelet status to ensure it is running.\nsystemctl status kubelet\nCreate configuration files for kubeadm.\nGenerate one kubeadm configuration file for each host that will have an etcd\nmember running on it using the following script.\n# Update HOST0, HOST1 and HOST2 with the IPs of your hosts\nexport\nHOST0\n=\n10.0.0.6\nexport\nHOST1\n=\n10.0.0.7\nexport\nHOST2\n=\n10.0.0.8\n# Update NAME0, NAME1 and NAME2 with the hostnames of your hosts\nexport\nNAME0\n=\n\"infra0\"\nexport\nNAME1\n=\n\"infra1\"\nexport\nNAME2\n=\n\"infra2\"\n# Create temp directories to store files that will end up on other hosts\nmkdir -p /tmp/\n${\nHOST0\n}\n/ /tmp/\n${\nHOST1\n}\n/ /tmp/\n${\nHOST2\n}\n/\nHOSTS\n=(\n${\nHOST0\n}\n${\nHOST1\n}\n${\nHOST2\n}\n)\nNAMES\n=(\n${\nNAME0\n}\n${\nNAME1\n}\n${\nNAME2\n}\n)\nfor\ni in\n\"\n${\n!HOSTS[@]\n}\n\"\n;\ndo\nHOST\n=\n${\nHOSTS\n[\n$i\n]\n}\nNAME\n=\n${\nNAMES\n[\n$i\n]\n}\ncat\n<< EOF > /tmp/${HOST}/kubeadmcfg.yaml\n---\napiVersion: \"kubeadm.k8s.io/v1beta4\"\nkind: InitConfiguration\nnodeRegistration:\nname: ${NAME}\nlocalAPIEndpoint:\nadvertiseAddress: ${HOST}\n---\napiVersion: \"kubeadm.k8s.io/v1beta4\"\nkind: ClusterConfiguration\netcd:\nlocal:\nserverCertSANs:\n- \"${HOST}\"\npeerCertSANs:\n- \"${HOST}\"\nextraArgs:\n- name: initial-cluster\nvalue: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380\n- name: initial-cluster-state\nvalue: new\n- name: name\nvalue: ${NAME}\n- name: listen-peer-urls\nvalue: https://${HOST}:2380\n- name: listen-client-urls\nvalue: https://${HOST}:2379\n- name: advertise-client-urls\nvalue: https://${HOST}:2379\n- name: initial-advertise-peer-urls\nvalue: https://${HOST}:2380\nEOF\ndone\nGenerate the certificate authority.\nIf you already have a CA then the only action that is copying the CA's\ncrt\nand\nkey\nfile to\n/etc/kubernetes/pki/etcd/ca.crt\nand\n/etc/kubernetes/pki/etcd/ca.key\n. After those files have been copied,\nproceed to the next step, \"Create certificates for each member\".\nIf you do not already have a CA then run this command on\n$HOST0\n(where you\ngenerated the configuration files for kubeadm).\nkubeadm init phase certs etcd-ca\nThis creates two files:\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/etcd/ca.key\nCreate certificates for each member.\nkubeadm init phase certs etcd-server --config\n=\n/tmp/\n${\nHOST2\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-peer --config\n=\n/tmp/\n${\nHOST2\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-healthcheck-client --config\n=\n/tmp/\n${\nHOST2\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs apiserver-etcd-client --config\n=\n/tmp/\n${\nHOST2\n}\n/kubeadmcfg.yaml\ncp -R /etc/kubernetes/pki /tmp/\n${\nHOST2\n}\n/\n# cleanup non-reusable certificates\nfind /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete\nkubeadm init phase certs etcd-server --config\n=\n/tmp/\n${\nHOST1\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-peer --config\n=\n/tmp/\n${\nHOST1\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-healthcheck-client --config\n=\n/tmp/\n${\nHOST1\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs apiserver-etcd-client --config\n=\n/tmp/\n${\nHOST1\n}\n/kubeadmcfg.yaml\ncp -R /etc/kubernetes/pki /tmp/\n${\nHOST1\n}\n/\nfind /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete\nkubeadm init phase certs etcd-server --config\n=\n/tmp/\n${\nHOST0\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-peer --config\n=\n/tmp/\n${\nHOST0\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs etcd-healthcheck-client --config\n=\n/tmp/\n${\nHOST0\n}\n/kubeadmcfg.yaml\nkubeadm init phase certs apiserver-etcd-client --config\n=\n/tmp/\n${\nHOST0\n}\n/kubeadmcfg.yaml\n# No need to move the certs because they are for HOST0\n# clean up certs that should not be copied off this host\nfind /tmp/\n${\nHOST2\n}\n-name ca.key -type f -delete\nfind /tmp/\n${\nHOST1\n}\n-name ca.key -type f -delete\nCopy certificates and kubeadm configs.\nThe certificates have been generated and now they must be moved to their\nrespective hosts.\nUSER\n=\nubuntu\nHOST\n=\n${\nHOST1\n}\nscp -r /tmp/\n${\nHOST\n}\n/*\n${\nUSER\n}\n@\n${\nHOST\n}\n:\nssh\n${\nUSER\n}\n@\n${\nHOST\n}\nUSER@HOST $ sudo -Es\nroot@HOST $ chown -R root:root pki\nroot@HOST $ mv pki /etc/kubernetes/\nEnsure all expected files exist.\nThe complete list of required files on\n$HOST0\nis:\n/tmp/${HOST0}\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── ca.key\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key\nOn\n$HOST1\n:\n$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key\nOn\n$HOST2\n:\n$HOME\n└── kubeadmcfg.yaml\n---\n/etc/kubernetes/pki\n├── apiserver-etcd-client.crt\n├── apiserver-etcd-client.key\n└── etcd\n    ├── ca.crt\n    ├── healthcheck-client.crt\n    ├── healthcheck-client.key\n    ├── peer.crt\n    ├── peer.key\n    ├── server.crt\n    └── server.key\nCreate the static pod manifests.\nNow that the certificates and configs are in place it's time to create the\nmanifests. On each host run the\nkubeadm\ncommand to generate a static manifest\nfor etcd.\nroot@HOST0 $ kubeadm init phase etcd\nlocal\n--config\n=\n/tmp/\n${\nHOST0\n}\n/kubeadmcfg.yaml\nroot@HOST1 $ kubeadm init phase etcd\nlocal\n--config\n=\n$HOME\n/kubeadmcfg.yaml\nroot@HOST2 $ kubeadm init phase etcd\nlocal\n--config\n=\n$HOME\n/kubeadmcfg.yaml\nOptional: Check the cluster health.\nIf\netcdctl\nisn't available, you can run this tool inside a container image.\nYou would do that directly with your container runtime using a tool such as\ncrictl run\nand not through Kubernetes\nETCDCTL_API\n=\n3\netcdctl\n\\\n--cert /etc/kubernetes/pki/etcd/peer.crt\n\\\n--key /etc/kubernetes/pki/etcd/peer.key\n\\\n--cacert /etc/kubernetes/pki/etcd/ca.crt\n\\\n--endpoints https://\n${\nHOST0\n}\n:2379 endpoint health\n...\nhttps://\n[\nHOST0 IP\n]\n:2379 is healthy: successfully committed proposal:\ntook\n=\n16.283339ms\nhttps://\n[\nHOST1 IP\n]\n:2379 is healthy: successfully committed proposal:\ntook\n=\n19.44402ms\nhttps://\n[\nHOST2 IP\n]\n:2379 is healthy: successfully committed proposal:\ntook\n=\n35.926451ms\nSet\n${HOST0}\nto the IP address of the host you are testing.\nWhat's next\nOnce you have an etcd cluster with 3 working members, you can continue setting up a\nhighly available control plane using the\nexternal etcd method with kubeadm\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 02, 2024 at 12:45 AM PST:\nRemove deprecated note on \"Set up a High Availability etcd Cluster with kubeadm\" page (#48120) (40419a5ef5)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nConfiguring each kubelet in your cluster using kubeadm\nConfiguring each kubelet in your cluster using kubeadm\nNote:\nDockershim has been removed from the Kubernetes project as of release 1.24. Read the\nDockershim Removal FAQ\nfor further details.\nFEATURE STATE:\nKubernetes v1.11 [stable]\nThe lifecycle of the kubeadm CLI tool is decoupled from the\nkubelet\n, which is a daemon that runs\non each node within the Kubernetes cluster. The kubeadm CLI tool is executed by the user when Kubernetes is\ninitialized or upgraded, whereas the kubelet is always running in the background.\nSince the kubelet is a daemon, it needs to be maintained by some kind of an init\nsystem or service manager. When the kubelet is installed using DEBs or RPMs,\nsystemd is configured to manage the kubelet. You can use a different service\nmanager instead, but you need to configure it manually.\nSome kubelet configuration details need to be the same across all kubelets involved in the cluster, while\nother configuration aspects need to be set on a per-kubelet basis to accommodate the different\ncharacteristics of a given machine (such as OS, storage, and networking). You can manage the configuration\nof your kubelets manually, but kubeadm now provides a\nKubeletConfiguration\nAPI type for\nmanaging your kubelet configurations centrally\n.\nKubelet configuration patterns\nThe following sections describe patterns to kubelet configuration that are simplified by\nusing kubeadm, rather than managing the kubelet configuration for each Node manually.\nPropagating cluster-level configuration to each kubelet\nYou can provide the kubelet with default values to be used by\nkubeadm init\nand\nkubeadm join\ncommands. Interesting examples include using a different container runtime or setting the default subnet\nused by services.\nIf you want your services to use the subnet\n10.96.0.0/12\nas the default for services, you can pass\nthe\n--service-cidr\nparameter to kubeadm:\nkubeadm init --service-cidr 10.96.0.0/12\nVirtual IPs for services are now allocated from this subnet. You also need to set the DNS address used\nby the kubelet, using the\n--cluster-dns\nflag. This setting needs to be the same for every kubelet\non every manager and Node in the cluster. The kubelet provides a versioned, structured API object\nthat can configure most parameters in the kubelet and push out this configuration to each running\nkubelet in the cluster. This object is called\nKubeletConfiguration\n.\nThe\nKubeletConfiguration\nallows the user to specify flags such as the cluster DNS IP addresses expressed as\na list of values to a camelCased key, illustrated by the following example:\napiVersion\n:\nkubelet.config.k8s.io/v1beta1\nkind\n:\nKubeletConfiguration\nclusterDNS\n:\n-\n10.96.0.10\nFor more details on the\nKubeletConfiguration\nhave a look at\nthis section\n.\nProviding instance-specific configuration details\nSome hosts require specific kubelet configurations due to differences in hardware, operating system,\nnetworking, or other host-specific parameters. The following list provides a few examples.\nThe path to the DNS resolution file, as specified by the\n--resolv-conf\nkubelet\nconfiguration flag, may differ among operating systems, or depending on whether you are using\nsystemd-resolved\n. If this path is wrong, DNS resolution will fail on the Node whose kubelet\nis configured incorrectly.\nThe Node API object\n.metadata.name\nis set to the machine's hostname by default,\nunless you are using a cloud provider. You can use the\n--hostname-override\nflag to override the\ndefault behavior if you need to specify a Node name different from the machine's hostname.\nCurrently, the kubelet cannot automatically detect the cgroup driver used by the container runtime,\nbut the value of\n--cgroup-driver\nmust match the cgroup driver used by the container runtime to ensure\nthe health of the kubelet.\nTo specify the container runtime you must set its endpoint with the\n--container-runtime-endpoint=<path>\nflag.\nThe recommended way of applying such instance-specific configuration is by using\nKubeletConfiguration\npatches\n.\nConfigure kubelets using kubeadm\nIt is possible to configure the kubelet that kubeadm will start if a custom\nKubeletConfiguration\nAPI object is passed with a configuration file like so\nkubeadm ... --config some-config-file.yaml\n.\nBy calling\nkubeadm config print init-defaults --component-configs KubeletConfiguration\nyou can\nsee all the default values for this structure.\nIt is also possible to apply instance-specific patches over the base\nKubeletConfiguration\n.\nHave a look at\nCustomizing the kubelet\nfor more details.\nWorkflow when using\nkubeadm init\nWhen you call\nkubeadm init\n, the kubelet configuration is marshalled to disk\nat\n/var/lib/kubelet/config.yaml\n, and also uploaded to a\nkubelet-config\nConfigMap in the\nkube-system\nnamespace of the cluster. A kubelet configuration file is also written to\n/etc/kubernetes/kubelet.conf\nwith the baseline cluster-wide configuration for all kubelets in the cluster. This configuration file\npoints to the client certificates that allow the kubelet to communicate with the API server. This\naddresses the need to\npropagate cluster-level configuration to each kubelet\n.\nTo address the second pattern of\nproviding instance-specific configuration details\n,\nkubeadm writes an environment file to\n/var/lib/kubelet/kubeadm-flags.env\n, which contains a list of\nflags to pass to the kubelet when it starts. The flags are presented in the file like this:\nKUBELET_KUBEADM_ARGS\n=\n\"--flag1=value1 --flag2=value2 ...\"\nIn addition to the flags used when starting the kubelet, the file also contains dynamic\nparameters such as the cgroup driver and whether to use a different container runtime socket\n(\n--cri-socket\n).\nAfter marshalling these two files to disk, kubeadm attempts to run the following two\ncommands, if you are using systemd:\nsystemctl daemon-reload\n&&\nsystemctl restart kubelet\nIf the reload and restart are successful, the normal\nkubeadm init\nworkflow continues.\nWorkflow when using\nkubeadm join\nWhen you run\nkubeadm join\n, kubeadm uses the Bootstrap Token credential to perform\na TLS bootstrap, which fetches the credential needed to download the\nkubelet-config\nConfigMap and writes it to\n/var/lib/kubelet/config.yaml\n. The dynamic\nenvironment file is generated in exactly the same way as\nkubeadm init\n.\nNext,\nkubeadm\nruns the following two commands to load the new configuration into the kubelet:\nsystemctl daemon-reload\n&&\nsystemctl restart kubelet\nAfter the kubelet loads the new configuration, kubeadm writes the\n/etc/kubernetes/bootstrap-kubelet.conf\nKubeConfig file, which contains a CA certificate and Bootstrap\nToken. These are used by the kubelet to perform the TLS Bootstrap and obtain a unique\ncredential, which is stored in\n/etc/kubernetes/kubelet.conf\n.\nWhen the\n/etc/kubernetes/kubelet.conf\nfile is written, the kubelet has finished performing the TLS Bootstrap.\nKubeadm deletes the\n/etc/kubernetes/bootstrap-kubelet.conf\nfile after completing the TLS Bootstrap.\nThe kubelet drop-in file for systemd\nkubeadm\nships with configuration for how systemd should run the kubelet.\nNote that the kubeadm CLI command never touches this drop-in file.\nThis configuration file installed by the\nkubeadm\npackage\nis written to\n/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf\nand is used by systemd.\nIt augments the basic\nkubelet.service\n.\nIf you want to override that further, you can make a directory\n/etc/systemd/system/kubelet.service.d/\n(not\n/usr/lib/systemd/system/kubelet.service.d/\n) and put your own customizations into a file there.\nFor example, you might add a new local file\n/etc/systemd/system/kubelet.service.d/local-overrides.conf\nto override the unit settings configured by\nkubeadm\n.\nHere is what you are likely to find in\n/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf\n:\nNote:\nThe contents below are just an example. If you don't want to use a package manager\nfollow the guide outlined in the (\nWithout a package manager\n)\nsection.\n[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\"\nEnvironment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\"\n# This is a file that \"kubeadm init\" and \"kubeadm join\" generate at runtime, populating\n# the KUBELET_KUBEADM_ARGS variable dynamically\nEnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env\n# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably,\n# the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead.\n# KUBELET_EXTRA_ARGS should be sourced from this file.\nEnvironmentFile=-/etc/default/kubelet\nExecStart=\nExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS\nThis file specifies the default locations for all of the files managed by kubeadm for the kubelet.\nThe KubeConfig file to use for the TLS Bootstrap is\n/etc/kubernetes/bootstrap-kubelet.conf\n,\nbut it is only used if\n/etc/kubernetes/kubelet.conf\ndoes not exist.\nThe KubeConfig file with the unique kubelet identity is\n/etc/kubernetes/kubelet.conf\n.\nThe file containing the kubelet's ComponentConfig is\n/var/lib/kubelet/config.yaml\n.\nThe dynamic environment file that contains\nKUBELET_KUBEADM_ARGS\nis sourced from\n/var/lib/kubelet/kubeadm-flags.env\n.\nThe file that can contain user-specified flag overrides with\nKUBELET_EXTRA_ARGS\nis sourced from\n/etc/default/kubelet\n(for DEBs), or\n/etc/sysconfig/kubelet\n(for RPMs).\nKUBELET_EXTRA_ARGS\nis last in the flag chain and has the highest priority in the event of conflicting settings.\nKubernetes binaries and package contents\nThe DEB and RPM packages shipped with the Kubernetes releases are:\nPackage name\nDescription\nkubeadm\nInstalls the\n/usr/bin/kubeadm\nCLI tool and the\nkubelet drop-in file\nfor the kubelet.\nkubelet\nInstalls the\n/usr/bin/kubelet\nbinary.\nkubectl\nInstalls the\n/usr/bin/kubectl\nbinary.\ncri-tools\nInstalls the\n/usr/bin/crictl\nbinary from the\ncri-tools git repository\n.\nkubernetes-cni\nInstalls the\n/opt/cni/bin\nbinaries from the\nplugins git repository\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified March 12, 2024 at 8:18 PM PST:\nFix path of kubelet systemd config file (6b2e5dfac6)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nInstalling Kubernetes with deployment tools\nBootstrapping clusters with kubeadm\nDual-stack support with kubeadm\nDual-stack support with kubeadm\nFEATURE STATE:\nKubernetes v1.23 [stable]\nYour Kubernetes cluster includes\ndual-stack\nnetworking, which means that cluster networking lets you use either address family.\nIn a cluster, the control plane can assign both an IPv4 address and an IPv6 address to a single\nPod\nor a\nService\n.\nBefore you begin\nYou need to have installed the\nkubeadm\ntool,\nfollowing the steps from\nInstalling kubeadm\n.\nFor each server that you want to use as a\nnode\n,\nmake sure it allows IPv6 forwarding.\nEnable IPv6 packet forwarding\nTo check if IPv6 packet forwarding is enabled:\nsysctl net.ipv6.conf.all.forwarding\nIf the output is\nnet.ipv6.conf.all.forwarding = 1\nit is already enabled.\nOtherwise it is not enabled yet.\nTo manually enable IPv6 packet forwarding:\n# sysctl params required by setup, params persist across reboots\ncat\n<<EOF | sudo tee -a /etc/sysctl.d/k8s.conf\nnet.ipv6.conf.all.forwarding = 1\nEOF\n# Apply sysctl params without reboot\nsudo sysctl --system\nYou need to have an IPv4 and and IPv6 address range to use. Cluster operators typically\nuse private address ranges for IPv4. For IPv6, a cluster operator typically chooses a global\nunicast address block from within\n2000::/3\n, using a range that is assigned to the operator.\nYou don't have to route the cluster's IP address ranges to the public internet.\nThe size of the IP address allocations should be suitable for the number of Pods and\nServices that you are planning to run.\nNote:\nIf you are upgrading an existing cluster with the\nkubeadm upgrade\ncommand,\nkubeadm\ndoes not support making modifications to the pod IP address range\n(“cluster CIDR”) nor to the cluster's Service address range (“Service CIDR”).\nCreate a dual-stack cluster\nTo create a dual-stack cluster with\nkubeadm init\nyou can pass command line arguments\nsimilar to the following example:\n# These address ranges are examples\nkubeadm init --pod-network-cidr\n=\n10.244.0.0/16,2001:db8:42:0::/56 --service-cidr\n=\n10.96.0.0/16,2001:db8:42:1::/112\nTo make things clearer, here is an example kubeadm\nconfiguration file\nkubeadm-config.yaml\nfor the primary dual-stack control plane node.\n---\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nnetworking\n:\npodSubnet\n:\n10.244.0.0\n/16,2001:db8:42:0::/56\nserviceSubnet\n:\n10.96.0.0\n/16,2001:db8:42:1::/112\n---\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nInitConfiguration\nlocalAPIEndpoint\n:\nadvertiseAddress\n:\n\"10.100.0.1\"\nbindPort\n:\n6443\nnodeRegistration\n:\nkubeletExtraArgs\n:\n-\nname\n:\n\"node-ip\"\nvalue\n:\n\"10.100.0.2,fd00:1:2:3::2\"\nadvertiseAddress\nin InitConfiguration specifies the IP address that the API Server\nwill advertise it is listening on. The value of\nadvertiseAddress\nequals the\n--apiserver-advertise-address\nflag of\nkubeadm init\n.\nRun kubeadm to initiate the dual-stack control plane node:\nkubeadm init --config\n=\nkubeadm-config.yaml\nThe kube-controller-manager flags\n--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6\nare set with default values. See\nconfigure IPv4/IPv6 dual stack\n.\nNote:\nThe\n--apiserver-advertise-address\nflag does not support dual-stack.\nJoin a node to dual-stack cluster\nBefore joining a node, make sure that the node has IPv6 routable network interface and allows IPv6 forwarding.\nHere is an example kubeadm\nconfiguration file\nkubeadm-config.yaml\nfor joining a worker node to the cluster.\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nJoinConfiguration\ndiscovery\n:\nbootstrapToken\n:\napiServerEndpoint\n:\n10.100.0.1\n:\n6443\ntoken\n:\n\"clvldh.vjjwg16ucnhp94qr\"\ncaCertHashes\n:\n-\n\"sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e\"\n# change auth info above to match the actual token and CA certificate hash for your cluster\nnodeRegistration\n:\nkubeletExtraArgs\n:\n-\nname\n:\n\"node-ip\"\nvalue\n:\n\"10.100.0.2,fd00:1:2:3::3\"\nAlso, here is an example kubeadm\nconfiguration file\nkubeadm-config.yaml\nfor joining another control plane node to the cluster.\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nJoinConfiguration\ncontrolPlane\n:\nlocalAPIEndpoint\n:\nadvertiseAddress\n:\n\"10.100.0.2\"\nbindPort\n:\n6443\ndiscovery\n:\nbootstrapToken\n:\napiServerEndpoint\n:\n10.100.0.1\n:\n6443\ntoken\n:\n\"clvldh.vjjwg16ucnhp94qr\"\ncaCertHashes\n:\n-\n\"sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e\"\n# change auth info above to match the actual token and CA certificate hash for your cluster\nnodeRegistration\n:\nkubeletExtraArgs\n:\n-\nname\n:\n\"node-ip\"\nvalue\n:\n\"10.100.0.2,fd00:1:2:3::4\"\nadvertiseAddress\nin JoinConfiguration.controlPlane specifies the IP address that the\nAPI Server will advertise it is listening on. The value of\nadvertiseAddress\nequals\nthe\n--apiserver-advertise-address\nflag of\nkubeadm join\n.\nkubeadm join --config\n=\nkubeadm-config.yaml\nCreate a single-stack cluster\nNote:\nDual-stack support doesn't mean that you need to use dual-stack addressing.\nYou can deploy a single-stack cluster that has the dual-stack networking feature enabled.\nTo make things more clear, here is an example kubeadm\nconfiguration file\nkubeadm-config.yaml\nfor the single-stack control plane node.\napiVersion\n:\nkubeadm.k8s.io/v1beta4\nkind\n:\nClusterConfiguration\nnetworking\n:\npodSubnet\n:\n10.244.0.0\n/16\nserviceSubnet\n:\n10.96.0.0\n/16\nWhat's next\nValidate IPv4/IPv6 dual-stack\nnetworking\nRead about\nDual-stack\ncluster networking\nLearn more about the kubeadm\nconfiguration format\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 28, 2024 at 1:01 PM PST:\nupdate how to persist enabling ipv6 forwarding across reboot (85e4a6c251)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nProduction environment\nTurnkey Cloud Solutions\nTurnkey Cloud Solutions\nThis page provides a list of Kubernetes certified solution providers. From each\nprovider page, you can learn how to install and setup production\nready clusters.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified January 18, 2024 at 10:32 AM PST:\nUpdate references to CNCF landscape (v2) (26e760da6e)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nBest practices\nBest practices\nConsiderations for large clusters\nRunning in multiple zones\nValidate node setup\nEnforcing Pod Security Standards\nPKI certificates and requirements\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified June 12, 2019 at 5:27 PM PST:\nRestructure the left navigation pane of setup (#14826) (55ac801bc4)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nBest practices\nConsiderations for large clusters\nConsiderations for large clusters\nA cluster is a set of\nnodes\n(physical\nor virtual machines) running Kubernetes agents, managed by the\ncontrol plane\n.\nKubernetes v1.33 supports clusters with up to 5,000 nodes. More specifically,\nKubernetes is designed to accommodate configurations that meet\nall\nof the following criteria:\nNo more than 110 pods per node\nNo more than 5,000 nodes\nNo more than 150,000 total pods\nNo more than 300,000 total containers\nYou can scale your cluster by adding or removing nodes. The way you do this depends\non how your cluster is deployed.\nCloud provider resource quotas\nTo avoid running into cloud provider quota issues, when creating a cluster with many nodes,\nconsider:\nRequesting a quota increase for cloud resources such as:\nComputer instances\nCPUs\nStorage volumes\nIn-use IP addresses\nPacket filtering rule sets\nNumber of load balancers\nNetwork subnets\nLog streams\nGating the cluster scaling actions to bring up new nodes in batches, with a pause\nbetween batches, because some cloud providers rate limit the creation of new instances.\nControl plane components\nFor a large cluster, you need a control plane with sufficient compute and other\nresources.\nTypically you would run one or two control plane instances per failure zone,\nscaling those instances vertically first and then scaling horizontally after reaching\nthe point of falling returns to (vertical) scale.\nYou should run at least one instance per failure zone to provide fault-tolerance. Kubernetes\nnodes do not automatically steer traffic towards control-plane endpoints that are in the\nsame failure zone; however, your cloud provider might have its own mechanisms to do this.\nFor example, using a managed load balancer, you configure the load balancer to send traffic\nthat originates from the kubelet and Pods in failure zone\nA\n, and direct that traffic only\nto the control plane hosts that are also in zone\nA\n. If a single control-plane host or\nendpoint failure zone\nA\ngoes offline, that means that all the control-plane traffic for\nnodes in zone\nA\nis now being sent between zones. Running multiple control plane hosts in\neach zone makes that outcome less likely.\netcd storage\nTo improve performance of large clusters, you can store Event objects in a separate\ndedicated etcd instance.\nWhen creating a cluster, you can (using custom tooling):\nstart and configure additional etcd instance\nconfigure the\nAPI server\nto use it for storing events\nSee\nOperating etcd clusters for Kubernetes\nand\nSet up a High Availability etcd cluster with kubeadm\nfor details on configuring and managing etcd for a large cluster.\nAddon resources\nKubernetes\nresource limits\nhelp to minimize the impact of memory leaks and other ways that pods and containers can\nimpact on other components. These resource limits apply to\naddon\nresources just as they apply to application workloads.\nFor example, you can set CPU and memory limits for a logging component:\n...\ncontainers\n:\n-\nname\n:\nfluentd-cloud-logging\nimage\n:\nfluent/fluentd-kubernetes-daemonset:v1\nresources\n:\nlimits\n:\ncpu\n:\n100m\nmemory\n:\n200Mi\nAddons' default limits are typically based on data collected from experience running\neach addon on small or medium Kubernetes clusters. When running on large\nclusters, addons often consume more of some resources than their default limits.\nIf a large cluster is deployed without adjusting these values, the addon(s)\nmay continuously get killed because they keep hitting the memory limit.\nAlternatively, the addon may run but with poor performance due to CPU time\nslice restrictions.\nTo avoid running into cluster addon resource issues, when creating a cluster with\nmany nodes, consider the following:\nSome addons scale vertically - there is one replica of the addon for the cluster\nor serving a whole failure zone. For these addons, increase requests and limits\nas you scale out your cluster.\nMany addons scale horizontally - you add capacity by running more pods - but with\na very large cluster you may also need to raise CPU or memory limits slightly.\nThe\nVertical Pod Autoscaler\ncan run in\nrecommender\nmode to provide suggested\nfigures for requests and limits.\nSome addons run as one copy per node, controlled by a\nDaemonSet\n: for example, a node-level log aggregator. Similar to\nthe case with horizontally-scaled addons, you may also need to raise CPU or memory\nlimits slightly.\nWhat's next\nVerticalPodAutoscaler\nis a custom resource that you can deploy into your cluster\nto help you manage resource requests and limits for pods.\nLearn more about\nVertical Pod Autoscaler\nand how you can use it to scale cluster\ncomponents, including cluster-critical addons.\nRead about\nNode autoscaling\nThe\naddon resizer\nhelps you in resizing the addons automatically as your cluster's scale changes.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified April 26, 2024 at 3:39 PM PST:\ncluster-autoscaling -> node-autoscaling clean-up (dc530ffd6a)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nBest practices\nRunning in multiple zones\nRunning in multiple zones\nThis page describes running Kubernetes across multiple zones.\nBackground\nKubernetes is designed so that a single Kubernetes cluster can run\nacross multiple failure zones, typically where these zones fit within\na logical grouping called a\nregion\n. Major cloud providers define a region\nas a set of failure zones (also called\navailability zones\n) that provide\na consistent set of features: within a region, each zone offers the same\nAPIs and services.\nTypical cloud architectures aim to minimize the chance that a failure in\none zone also impairs services in another zone.\nControl plane behavior\nAll\ncontrol plane components\nsupport running as a pool of interchangeable resources, replicated per\ncomponent.\nWhen you deploy a cluster control plane, place replicas of\ncontrol plane components across multiple failure zones. If availability is\nan important concern, select at least three failure zones and replicate\neach individual control plane component (API server, scheduler, etcd,\ncluster controller manager) across at least three failure zones.\nIf you are running a cloud controller manager then you should\nalso replicate this across all the failure zones you selected.\nNote:\nKubernetes does not provide cross-zone resilience for the API server\nendpoints. You can use various techniques to improve availability for\nthe cluster API server, including DNS round-robin, SRV records, or\na third-party load balancing solution with health checking.\nNode behavior\nKubernetes automatically spreads the Pods for\nworkload resources (such as\nDeployment\nor\nStatefulSet\n)\nacross different nodes in a cluster. This spreading helps\nreduce the impact of failures.\nWhen nodes start up, the kubelet on each node automatically adds\nlabels\nto the Node object\nthat represents that specific kubelet in the Kubernetes API.\nThese labels can include\nzone information\n.\nIf your cluster spans multiple zones or regions, you can use node labels\nin conjunction with\nPod topology spread constraints\nto control how Pods are spread across your cluster among fault domains:\nregions, zones, and even specific nodes.\nThese hints enable the\nscheduler\nto place\nPods for better expected availability, reducing the risk that a correlated\nfailure affects your whole workload.\nFor example, you can set a constraint to make sure that the\n3 replicas of a StatefulSet are all running in different zones to each\nother, whenever that is feasible. You can define this declaratively\nwithout explicitly defining which availability zones are in use for\neach workload.\nDistributing nodes across zones\nKubernetes' core does not create nodes for you; you need to do that yourself,\nor use a tool such as the\nCluster API\nto\nmanage nodes on your behalf.\nUsing tools such as the Cluster API you can define sets of machines to run as\nworker nodes for your cluster across multiple failure domains, and rules to\nautomatically heal the cluster in case of whole-zone service disruption.\nManual zone assignment for Pods\nYou can apply\nnode selector constraints\nto Pods that you create, as well as to Pod templates in workload resources\nsuch as Deployment, StatefulSet, or Job.\nStorage access for zones\nWhen persistent volumes are created, Kubernetes automatically adds zone labels\nto any PersistentVolumes that are linked to a specific zone.\nThe\nscheduler\nthen ensures,\nthrough its\nNoVolumeZoneConflict\npredicate, that pods which claim a given PersistentVolume\nare only placed into the same zone as that volume.\nPlease note that the method of adding zone labels can depend on your\ncloud provider and the storage provisioner you’re using. Always refer to the specific\ndocumentation for your environment to ensure correct configuration.\nYou can specify a\nStorageClass\nfor PersistentVolumeClaims that specifies the failure domains (zones) that the\nstorage in that class may use.\nTo learn about configuring a StorageClass that is aware of failure domains or zones,\nsee\nAllowed topologies\n.\nNetworking\nBy itself, Kubernetes does not include zone-aware networking. You can use a\nnetwork plugin\nto configure cluster networking, and that network solution might have zone-specific\nelements. For example, if your cloud provider supports Services with\ntype=LoadBalancer\n, the load balancer might only send traffic to Pods running in the\nsame zone as the load balancer element processing a given connection.\nCheck your cloud provider's documentation for details.\nFor custom or on-premises deployments, similar considerations apply.\nService\nand\nIngress\nbehavior, including handling\nof different failure zones, does vary depending on exactly how your cluster is set up.\nFault recovery\nWhen you set up your cluster, you might also need to consider whether and how\nyour setup can restore service if all the failure zones in a region go\noff-line at the same time. For example, do you rely on there being at least\none node able to run Pods in a zone?\nMake sure that any cluster-critical repair work does not rely\non there being at least one healthy node in your cluster. For example: if all nodes\nare unhealthy, you might need to run a repair Job with a special\ntoleration\nso that the repair\ncan complete enough to bring at least one node into service.\nKubernetes doesn't come with an answer for this challenge; however, it's\nsomething to consider.\nWhat's next\nTo learn how the scheduler places Pods in a cluster, honoring the configured constraints,\nvisit\nScheduling and Eviction\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified September 01, 2024 at 1:54 AM PST:\nFix broken links from \"overview/components/#...\" to \"architecture/#...\" (#47724) (7e64c2db82)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nBest practices\nValidate node setup\nValidate node setup\nNode Conformance Test\nNode conformance test\nis a containerized test framework that provides a system\nverification and functionality test for a node. The test validates whether the\nnode meets the minimum requirements for Kubernetes; a node that passes the test\nis qualified to join a Kubernetes cluster.\nNode Prerequisite\nTo run node conformance test, a node must satisfy the same prerequisites as a\nstandard Kubernetes node. At a minimum, the node should have the following\ndaemons installed:\nCRI-compatible container runtimes such as Docker, containerd and CRI-O\nkubelet\nRunning Node Conformance Test\nTo run the node conformance test, perform the following steps:\nWork out the value of the\n--kubeconfig\noption for the kubelet; for example:\n--kubeconfig=/var/lib/kubelet/config.yaml\n.\nBecause the test framework starts a local control plane to test the kubelet,\nuse\nhttp://localhost:8080\nas the URL of the API server.\nThere are some other kubelet command line parameters you may want to use:\n--cloud-provider\n: If you are using\n--cloud-provider=gce\n, you should\nremove the flag to run the test.\nRun the node conformance test with command:\n# $CONFIG_DIR is the pod manifest path of your kubelet.\n# $LOG_DIR is the test output path.\nsudo docker run -it --rm --privileged --net\n=\nhost\n\\\n-v /:/rootfs -v\n$CONFIG_DIR\n:\n$CONFIG_DIR\n-v\n$LOG_DIR\n:/var/result\n\\\nregistry.k8s.io/node-test:0.2\nRunning Node Conformance Test for Other Architectures\nKubernetes also provides node conformance test docker images for other\narchitectures:\nArch\nImage\namd64\nnode-test-amd64\narm\nnode-test-arm\narm64\nnode-test-arm64\nRunning Selected Test\nTo run specific tests, overwrite the environment variable\nFOCUS\nwith the\nregular expression of tests you want to run.\nsudo docker run -it --rm --privileged --net\n=\nhost\n\\\n-v /:/rootfs:ro -v\n$CONFIG_DIR\n:\n$CONFIG_DIR\n-v\n$LOG_DIR\n:/var/result\n\\\n-e\nFOCUS\n=\nMirrorPod\n\\\n# Only run MirrorPod test\nregistry.k8s.io/node-test:0.2\nTo skip specific tests, overwrite the environment variable\nSKIP\nwith the\nregular expression of tests you want to skip.\nsudo docker run -it --rm --privileged --net\n=\nhost\n\\\n-v /:/rootfs:ro -v\n$CONFIG_DIR\n:\n$CONFIG_DIR\n-v\n$LOG_DIR\n:/var/result\n\\\n-e\nSKIP\n=\nMirrorPod\n\\\n# Run all conformance tests but skip MirrorPod test\nregistry.k8s.io/node-test:0.2\nNode conformance test is a containerized version of\nnode e2e test\n.\nBy default, it runs all conformance tests.\nTheoretically, you can run any node e2e test if you configure the container and\nmount required volumes properly. But\nit is strongly recommended to only run conformance\ntest\n, because it requires much more complex configuration to run non-conformance test.\nCaveats\nThe test leaves some docker images on the node, including the node conformance\ntest image and images of containers used in the functionality\ntest.\nThe test leaves dead containers on the node. These containers are created\nduring the functionality test.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 17, 2024 at 6:09 PM PST:\nClean up best-practices: certificates and node-conformance (1d24167b04)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nBest practices\nEnforcing Pod Security Standards\nEnforcing Pod Security Standards\nThis page provides an overview of best practices when it comes to enforcing\nPod Security Standards\n.\nUsing the built-in Pod Security Admission Controller\nFEATURE STATE:\nKubernetes v1.25 [stable]\nThe\nPod Security Admission Controller\nintends to replace the deprecated PodSecurityPolicies.\nConfigure all cluster namespaces\nNamespaces that lack any configuration at all should be considered significant gaps in your cluster\nsecurity model. We recommend taking the time to analyze the types of workloads occurring in each\nnamespace, and by referencing the Pod Security Standards, decide on an appropriate level for\neach of them. Unlabeled namespaces should only indicate that they've yet to be evaluated.\nIn the scenario that all workloads in all namespaces have the same security requirements,\nwe provide an\nexample\nthat illustrates how the PodSecurity labels can be applied in bulk.\nEmbrace the principle of least privilege\nIn an ideal world, every pod in every namespace would meet the requirements of the\nrestricted\npolicy. However, this is not possible nor practical, as some workloads will require elevated\nprivileges for legitimate reasons.\nNamespaces allowing\nprivileged\nworkloads should establish and enforce appropriate access controls.\nFor workloads running in those permissive namespaces, maintain documentation about their unique\nsecurity requirements. If at all possible, consider how those requirements could be further\nconstrained.\nAdopt a multi-mode strategy\nThe\naudit\nand\nwarn\nmodes of the Pod Security Standards admission controller make it easy to\ncollect important security insights about your pods without breaking existing workloads.\nIt is good practice to enable these modes for all namespaces, setting them to the\ndesired\nlevel\nand version you would eventually like to\nenforce\n. The warnings and audit annotations generated in\nthis phase can guide you toward that state. If you expect workload authors to make changes to fit\nwithin the desired level, enable the\nwarn\nmode. If you expect to use audit logs to monitor/drive\nchanges to fit within the desired level, enable the\naudit\nmode.\nWhen you have the\nenforce\nmode set to your desired value, these modes can still be useful in a\nfew different ways:\nBy setting\nwarn\nto the same level as\nenforce\n, clients will receive warnings when attempting\nto create Pods (or resources that have Pod templates) that do not pass validation. This will help\nthem update those resources to become compliant.\nIn Namespaces that pin\nenforce\nto a specific non-latest version, setting the\naudit\nand\nwarn\nmodes to the same level as\nenforce\n, but to the\nlatest\nversion, gives visibility into settings\nthat were allowed by previous versions but are not allowed per current best practices.\nThird-party alternatives\nNote:\nThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the\ncontent guide\nbefore submitting a change.\nMore information.\nOther alternatives for enforcing security profiles are being developed in the Kubernetes\necosystem:\nKubewarden\n.\nKyverno\n.\nOPA Gatekeeper\n.\nThe decision to go with a\nbuilt-in\nsolution (e.g. PodSecurity admission controller) versus a\nthird-party tool is entirely dependent on your own situation. When evaluating any solution,\ntrust of your supply chain is crucial. Ultimately, using\nany\nof the aforementioned approaches\nwill be better than doing nothing.\nItems on this page refer to third party products or projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. See the\nCNCF website guidelines\nfor more details.\nYou should read the\ncontent guide\nbefore proposing a change that adds an extra third-party link.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified September 22, 2022 at 3:12 PM PST:\nBump the feature state of the Pod Security plugin to stable (1ab76fba82)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nGetting started\nBest practices\nPKI certificates and requirements\nPKI certificates and requirements\nKubernetes requires PKI certificates for authentication over TLS.\nIf you install Kubernetes with\nkubeadm\n, the certificates\nthat your cluster requires are automatically generated.\nYou can also generate your own certificates -- for example, to keep your private keys more secure\nby not storing them on the API server.\nThis page explains the certificates that your cluster requires.\nHow certificates are used by your cluster\nKubernetes requires PKI for the following operations:\nServer certificates\nServer certificate for the API server endpoint\nServer certificate for the etcd server\nServer certificates\nfor each kubelet (every\nnode\nruns a kubelet)\nOptional server certificate for the\nfront-proxy\nClient certificates\nClient certificates for each kubelet, used to authenticate to the API server as a client of\nthe Kubernetes API\nClient certificate for each API server, used to authenticate to etcd\nClient certificate for the controller manager to securely communicate with the API server\nClient certificate for the scheduler to securely communicate with the API server\nClient certificates, one for each node, for kube-proxy to authenticate to the API server\nOptional client certificates for administrators of the cluster to authenticate to the API server\nOptional client certificate for the\nfront-proxy\nKubelet's server and client certificates\nTo establish a secure connection and authenticate itself to the kubelet, the API Server\nrequires a client certificate and key pair.\nIn this scenario, there are two approaches for certificate usage:\nShared Certificates: The kube-apiserver can utilize the same certificate and key pair it uses\nto authenticate its clients. This means that the existing certificates, such as\napiserver.crt\nand\napiserver.key\n, can be used for communicating with the kubelet servers.\nSeparate Certificates: Alternatively, the kube-apiserver can generate a new client certificate\nand key pair to authenticate its communication with the kubelet servers. In this case,\na distinct certificate named\nkubelet-client.crt\nand its corresponding private key,\nkubelet-client.key\nare created.\nNote:\nfront-proxy\ncertificates are required only if you run kube-proxy to support\nan extension API server\n.\netcd also implements mutual TLS to authenticate clients and peers.\nWhere certificates are stored\nIf you install Kubernetes with kubeadm, most certificates are stored in\n/etc/kubernetes/pki\n.\nAll paths in this documentation are relative to that directory, with the exception of user account\ncertificates which kubeadm places in\n/etc/kubernetes\n.\nConfigure certificates manually\nIf you don't want kubeadm to generate the required certificates, you can create them using a\nsingle root CA or by providing all certificates. See\nCertificates\nfor details on creating your own certificate authority. See\nCertificate Management with kubeadm\nfor more on managing certificates.\nSingle root CA\nYou can create a single root CA, controlled by an administrator. This root CA can then create\nmultiple intermediate CAs, and delegate all further creation to Kubernetes itself.\nRequired CAs:\nPath\nDefault CN\nDescription\nca.crt,key\nkubernetes-ca\nKubernetes general CA\netcd/ca.crt,key\netcd-ca\nFor all etcd-related functions\nfront-proxy-ca.crt,key\nkubernetes-front-proxy-ca\nFor the\nfront-end proxy\nOn top of the above CAs, it is also necessary to get a public/private key pair for service account\nmanagement,\nsa.key\nand\nsa.pub\n.\nThe following example illustrates the CA key and certificate files shown in the previous table:\n/etc/kubernetes/pki/ca.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/front-proxy-ca.crt\n/etc/kubernetes/pki/front-proxy-ca.key\nAll certificates\nIf you don't wish to copy the CA private keys to your cluster, you can generate all certificates yourself.\nRequired certificates:\nDefault CN\nParent CA\nO (in Subject)\nkind\nhosts (SAN)\nkube-etcd\netcd-ca\nserver, client\n<hostname>\n,\n<Host_IP>\n,\nlocalhost\n,\n127.0.0.1\nkube-etcd-peer\netcd-ca\nserver, client\n<hostname>\n,\n<Host_IP>\n,\nlocalhost\n,\n127.0.0.1\nkube-etcd-healthcheck-client\netcd-ca\nclient\nkube-apiserver-etcd-client\netcd-ca\nclient\nkube-apiserver\nkubernetes-ca\nserver\n<hostname>\n,\n<Host_IP>\n,\n<advertise_IP>\n1\nkube-apiserver-kubelet-client\nkubernetes-ca\nsystem:masters\nclient\nfront-proxy-client\nkubernetes-front-proxy-ca\nclient\nNote:\nInstead of using the super-user group\nsystem:masters\nfor\nkube-apiserver-kubelet-client\na less privileged group can be used. kubeadm uses the\nkubeadm:cluster-admins\ngroup for\nthat purpose.\nwhere\nkind\nmaps to one or more of the x509 key usage, which is also documented in the\n.spec.usages\nof a\nCertificateSigningRequest\ntype:\nkind\nKey usage\nserver\ndigital signature, key encipherment, server auth\nclient\ndigital signature, key encipherment, client auth\nNote:\nHosts/SAN listed above are the recommended ones for getting a working cluster; if required by a\nspecific setup, it is possible to add additional SANs on all the server certificates.\nNote:\nFor kubeadm users only:\nThe scenario where you are copying to your cluster CA certificates without private keys is\nreferred as external CA in the kubeadm documentation.\nIf you are comparing the above list with a kubeadm generated PKI, please be aware that\nkube-etcd\n,\nkube-etcd-peer\nand\nkube-etcd-healthcheck-client\ncertificates are not generated\nin case of external etcd.\nCertificate paths\nCertificates should be placed in a recommended path (as used by\nkubeadm\n).\nPaths should be specified using the given argument regardless of location.\nDefaultCN\nrecommendedkeypath\nrecommendedcertpath\ncommand\nkeyargument\ncertargument\netcd-ca\netcd/ca.key\netcd/ca.crt\nkube-apiserver\n--etcd-cafile\nkube-apiserver-etcd-client\napiserver-etcd-client.key\napiserver-etcd-client.crt\nkube-apiserver\n--etcd-keyfile\n--etcd-certfile\nkubernetes-ca\nca.key\nca.crt\nkube-apiserver\n--client-ca-file\nkubernetes-ca\nca.key\nca.crt\nkube-controller-manager\n--cluster-signing-key-file\n--client-ca-file,--root-ca-file,--cluster-signing-cert-file\nkube-apiserver\napiserver.key\napiserver.crt\nkube-apiserver\n--tls-private-key-file\n--tls-cert-file\nkube-apiserver-kubelet-client\napiserver-kubelet-client.key\napiserver-kubelet-client.crt\nkube-apiserver\n--kubelet-client-key\n--kubelet-client-certificate\nfront-proxy-ca\nfront-proxy-ca.key\nfront-proxy-ca.crt\nkube-apiserver\n--requestheader-client-ca-file\nfront-proxy-ca\nfront-proxy-ca.key\nfront-proxy-ca.crt\nkube-controller-manager\n--requestheader-client-ca-file\nfront-proxy-client\nfront-proxy-client.key\nfront-proxy-client.crt\nkube-apiserver\n--proxy-client-key-file\n--proxy-client-cert-file\netcd-ca\netcd/ca.key\netcd/ca.crt\netcd\n--trusted-ca-file,--peer-trusted-ca-file\nkube-etcd\netcd/server.key\netcd/server.crt\netcd\n--key-file\n--cert-file\nkube-etcd-peer\netcd/peer.key\netcd/peer.crt\netcd\n--peer-key-file\n--peer-cert-file\netcd-ca\netcd/ca.crt\netcdctl\n--cacert\nkube-etcd-healthcheck-client\netcd/healthcheck-client.key\netcd/healthcheck-client.crt\netcdctl\n--key\n--cert\nSame considerations apply for the service account key pair:\nprivate key path\npublic key path\ncommand\nargument\nsa.key\nkube-controller-manager\n--service-account-private-key-file\nsa.pub\nkube-apiserver\n--service-account-key-file\nThe following example illustrates the file paths\nfrom the previous tables\nyou need to provide if you are generating all of your own keys and certificates:\n/etc/kubernetes/pki/etcd/ca.key\n/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/apiserver-etcd-client.key\n/etc/kubernetes/pki/apiserver-etcd-client.crt\n/etc/kubernetes/pki/ca.key\n/etc/kubernetes/pki/ca.crt\n/etc/kubernetes/pki/apiserver.key\n/etc/kubernetes/pki/apiserver.crt\n/etc/kubernetes/pki/apiserver-kubelet-client.key\n/etc/kubernetes/pki/apiserver-kubelet-client.crt\n/etc/kubernetes/pki/front-proxy-ca.key\n/etc/kubernetes/pki/front-proxy-ca.crt\n/etc/kubernetes/pki/front-proxy-client.key\n/etc/kubernetes/pki/front-proxy-client.crt\n/etc/kubernetes/pki/etcd/server.key\n/etc/kubernetes/pki/etcd/server.crt\n/etc/kubernetes/pki/etcd/peer.key\n/etc/kubernetes/pki/etcd/peer.crt\n/etc/kubernetes/pki/etcd/healthcheck-client.key\n/etc/kubernetes/pki/etcd/healthcheck-client.crt\n/etc/kubernetes/pki/sa.key\n/etc/kubernetes/pki/sa.pub\nConfigure certificates for user accounts\nYou must manually configure these administrator accounts and service accounts:\nFilename\nCredential name\nDefault CN\nO (in Subject)\nadmin.conf\ndefault-admin\nkubernetes-admin\n<admin-group>\nsuper-admin.conf\ndefault-super-admin\nkubernetes-super-admin\nsystem:masters\nkubelet.conf\ndefault-auth\nsystem:node:\n<nodeName>\n(see note)\nsystem:nodes\ncontroller-manager.conf\ndefault-controller-manager\nsystem:kube-controller-manager\nscheduler.conf\ndefault-scheduler\nsystem:kube-scheduler\nNote:\nThe value of\n<nodeName>\nfor\nkubelet.conf\nmust\nmatch precisely the value of the node name\nprovided by the kubelet as it registers with the apiserver. For further details, read the\nNode Authorization\n.\nNote:\nIn the above example\n<admin-group>\nis implementation specific. Some tools sign the\ncertificate in the default\nadmin.conf\nto be part of the\nsystem:masters\ngroup.\nsystem:masters\nis a break-glass, super user group can bypass the authorization\nlayer of Kubernetes, such as RBAC. Also some tools do not generate a separate\nsuper-admin.conf\nwith a certificate bound to this super user group.\nkubeadm generates two separate administrator certificates in kubeconfig files.\nOne is in\nadmin.conf\nand has\nSubject: O = kubeadm:cluster-admins, CN = kubernetes-admin\n.\nkubeadm:cluster-admins\nis a custom group bound to the\ncluster-admin\nClusterRole.\nThis file is generated on all kubeadm managed control plane machines.\nAnother is in\nsuper-admin.conf\nthat has\nSubject: O = system:masters, CN = kubernetes-super-admin\n.\nThis file is generated only on the node where\nkubeadm init\nwas called.\nFor each configuration, generate an x509 certificate/key pair with the\ngiven Common Name (CN) and Organization (O).\nRun\nkubectl\nas follows for each configuration:\nKUBECONFIG=<filename> kubectl config set-cluster default-cluster --server=https://<host ip>:6443 --certificate-authority <path-to-kubernetes-ca> --embed-certs\nKUBECONFIG=<filename> kubectl config set-credentials <credential-name> --client-key <path-to-key>.pem --client-certificate <path-to-cert>.pem --embed-certs\nKUBECONFIG=<filename> kubectl config set-context default-system --cluster default-cluster --user <credential-name>\nKUBECONFIG=<filename> kubectl config use-context default-system\nThese files are used as follows:\nFilename\nCommand\nComment\nadmin.conf\nkubectl\nConfigures administrator user for the cluster\nsuper-admin.conf\nkubectl\nConfigures super administrator user for the cluster\nkubelet.conf\nkubelet\nOne required for each node in the cluster.\ncontroller-manager.conf\nkube-controller-manager\nMust be added to manifest in\nmanifests/kube-controller-manager.yaml\nscheduler.conf\nkube-scheduler\nMust be added to manifest in\nmanifests/kube-scheduler.yaml\nThe following files illustrate full paths to the files listed in the previous table:\n/etc/kubernetes/admin.conf\n/etc/kubernetes/super-admin.conf\n/etc/kubernetes/kubelet.conf\n/etc/kubernetes/controller-manager.conf\n/etc/kubernetes/scheduler.conf\nany other IP or DNS name you contact your cluster on (as used by\nkubeadm\nthe load balancer stable IP and/or DNS name,\nkubernetes\n,\nkubernetes.default\n,\nkubernetes.default.svc\n,\nkubernetes.default.svc.cluster\n,\nkubernetes.default.svc.cluster.local\n)\n↩︎\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 17, 2024 at 6:09 PM PST:\nClean up best-practices: certificates and node-conformance (1d24167b04)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nConcepts\nThe Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your\ncluster\n, and helps you obtain a deeper understanding of how Kubernetes works.\nOverview\nKubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\nCluster Architecture\nThe architectural concepts behind Kubernetes.\nContainers\nTechnology for packaging an application along with its runtime dependencies.\nWorkloads\nUnderstand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.\nServices, Load Balancing, and Networking\nConcepts and resources behind networking in Kubernetes.\nStorage\nWays to provide both long-term and temporary storage to Pods in your cluster.\nConfiguration\nResources that Kubernetes provides for configuring Pods.\nSecurity\nConcepts for keeping your cloud-native workload secure.\nPolicies\nManage security and best-practices with policies.\nScheduling, Preemption and Eviction\nCluster Administration\nLower-level detail relevant to creating or administering a Kubernetes cluster.\nWindows in Kubernetes\nKubernetes supports nodes that run Microsoft Windows.\nExtending Kubernetes\nDifferent ways to change the behavior of your Kubernetes cluster.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified June 22, 2020 at 11:01 PM PST:\nAdd descriptions to Concept sections (3ff7312cff)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nConcepts\nOverview\nOverview\nKubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\nThis page is an overview of Kubernetes.\nThe name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation\nresults from counting the eight letters between the \"K\" and the \"s\". Google open-sourced the\nKubernetes project in 2014. Kubernetes combines\nover 15 years of Google's experience\nrunning\nproduction workloads at scale with best-of-breed ideas and practices from the community.\nWhy you need Kubernetes and what it can do\nContainers are a good way to bundle and run your applications. In a production\nenvironment, you need to manage the containers that run the applications and\nensure that there is no downtime. For example, if a container goes down, another\ncontainer needs to start. Wouldn't it be easier if this behavior was handled by a system?\nThat's how Kubernetes comes to the rescue! Kubernetes provides you with a framework\nto run distributed systems resiliently. It takes care of scaling and failover for\nyour application, provides deployment patterns, and more. For example: Kubernetes\ncan easily manage a canary deployment for your system.\nKubernetes provides you with:\nService discovery and load balancing\nKubernetes can expose a container using the DNS name or using their own IP address.\nIf traffic to a container is high, Kubernetes is able to load balance and distribute\nthe network traffic so that the deployment is stable.\nStorage orchestration\nKubernetes allows you to automatically mount a storage system of your choice, such as\nlocal storages, public cloud providers, and more.\nAutomated rollouts and rollbacks\nYou can describe the desired state for your deployed containers using Kubernetes,\nand it can change the actual state to the desired state at a controlled rate.\nFor example, you can automate Kubernetes to create new containers for your\ndeployment, remove existing containers and adopt all their resources to the new container.\nAutomatic bin packing\nYou provide Kubernetes with a cluster of nodes that it can use to run containerized tasks.\nYou tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit\ncontainers onto your nodes to make the best use of your resources.\nSelf-healing\nKubernetes restarts containers that fail, replaces containers, kills containers that don't\nrespond to your user-defined health check, and doesn't advertise them to clients until they\nare ready to serve.\nSecret and configuration management\nKubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens,\nand SSH keys. You can deploy and update secrets and application configuration without\nrebuilding your container images, and without exposing secrets in your stack configuration.\nBatch execution\nIn addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.\nHorizontal scaling\nScale your application up and down with a simple command, with a UI, or automatically based on CPU usage.\nIPv4/IPv6 dual-stack\nAllocation of IPv4 and IPv6 addresses to Pods and Services\nDesigned for extensibility\nAdd features to your Kubernetes cluster without changing upstream source code.\nWhat Kubernetes is not\nKubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system.\nSince Kubernetes operates at the container level rather than at the hardware level,\nit provides some generally applicable features common to PaaS offerings, such as\ndeployment, scaling, load balancing, and lets users integrate their logging, monitoring,\nand alerting solutions. However, Kubernetes is not monolithic, and these default solutions\nare optional and pluggable. Kubernetes provides the building blocks for building developer\nplatforms, but preserves user choice and flexibility where it is important.\nKubernetes:\nDoes not limit the types of applications supported. Kubernetes aims to support an\nextremely diverse variety of workloads, including stateless, stateful, and data-processing\nworkloads. If an application can run in a container, it should run great on Kubernetes.\nDoes not deploy source code and does not build your application. Continuous Integration,\nDelivery, and Deployment (CI/CD) workflows are determined by organization cultures and\npreferences as well as technical requirements.\nDoes not provide application-level services, such as middleware (for example, message buses),\ndata-processing frameworks (for example, Spark), databases (for example, MySQL), caches, nor\ncluster storage systems (for example, Ceph) as built-in services. Such components can run on\nKubernetes, and/or can be accessed by applications running on Kubernetes through portable\nmechanisms, such as the\nOpen Service Broker\n.\nDoes not dictate logging, monitoring, or alerting solutions. It provides some integrations\nas proof of concept, and mechanisms to collect and export metrics.\nDoes not provide nor mandate a configuration language/system (for example, Jsonnet). It provides\na declarative API that may be targeted by arbitrary forms of declarative specifications.\nDoes not provide nor adopt any comprehensive machine configuration, maintenance, management,\nor self-healing systems.\nAdditionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need\nfor orchestration. The technical definition of orchestration is execution of a defined workflow:\nfirst do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable\ncontrol processes that continuously drive the current state towards the provided desired state.\nIt shouldn't matter how you get from A to C. Centralized control is also not required. This\nresults in a system that is easier to use and more powerful, robust, resilient, and extensible.\nHistorical context for Kubernetes\nLet's take a look at why Kubernetes is so useful by going back in time.\nTraditional deployment era:\nEarly on, organizations ran applications on physical servers. There was no way to define\nresource boundaries for applications in a physical server, and this caused resource\nallocation issues. For example, if multiple applications run on a physical server, there\ncan be instances where one application would take up most of the resources, and as a result,\nthe other applications would underperform. A solution for this would be to run each application\non a different physical server. But this did not scale as resources were underutilized, and it\nwas expensive for organizations to maintain many physical servers.\nVirtualized deployment era:\nAs a solution, virtualization was introduced. It allows you\nto run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization\nallows applications to be isolated between VMs and provides a level of security as the\ninformation of one application cannot be freely accessed by another application.\nVirtualization allows better utilization of resources in a physical server and allows\nbetter scalability because an application can be added or updated easily, reduces\nhardware costs, and much more. With virtualization you can present a set of physical\nresources as a cluster of disposable virtual machines.\nEach VM is a full machine running all the components, including its own operating\nsystem, on top of the virtualized hardware.\nContainer deployment era:\nContainers are similar to VMs, but they have relaxed\nisolation properties to share the Operating System (OS) among the applications.\nTherefore, containers are considered lightweight. Similar to a VM, a container\nhas its own filesystem, share of CPU, memory, process space, and more. As they\nare decoupled from the underlying infrastructure, they are portable across clouds\nand OS distributions.\nContainers have become popular because they provide extra benefits, such as:\nAgile application creation and deployment: increased ease and efficiency of\ncontainer image creation compared to VM image use.\nContinuous development, integration, and deployment: provides for reliable\nand frequent container image build and deployment with quick and efficient\nrollbacks (due to image immutability).\nDev and Ops separation of concerns: create application container images at\nbuild/release time rather than deployment time, thereby decoupling\napplications from infrastructure.\nObservability: not only surfaces OS-level information and metrics, but also\napplication health and other signals.\nEnvironmental consistency across development, testing, and production: runs\nthe same on a laptop as it does in the cloud.\nCloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises,\non major public clouds, and anywhere else.\nApplication-centric management: raises the level of abstraction from running an\nOS on virtual hardware to running an application on an OS using logical resources.\nLoosely coupled, distributed, elastic, liberated micro-services: applications are\nbroken into smaller, independent pieces and can be deployed and managed dynamically –\nnot a monolithic stack running on one big single-purpose machine.\nResource isolation: predictable application performance.\nResource utilization: high efficiency and density.\nWhat's next\nTake a look at the\nKubernetes Components\nTake a look at the\nThe Kubernetes API\nTake a look at the\nCluster Architecture\nReady to\nGet Started\n?\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified September 11, 2024 at 3:17 PM PST:\nRemoved duplicated paragraph (7e71096044)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nConcepts\nOverview\nKubernetes Components\nKubernetes Components\nAn overview of the key components that make up a Kubernetes cluster.\nThis page provides a high-level overview of the essential components that make up a Kubernetes cluster.\nThe components of a Kubernetes cluster\nCore Components\nA Kubernetes cluster consists of a control plane and one or more worker nodes.\nHere's a brief overview of the main components:\nControl Plane Components\nManage the overall state of the cluster:\nkube-apiserver\nThe core component server that exposes the Kubernetes HTTP API.\netcd\nConsistent and highly-available key value store for all API server data.\nkube-scheduler\nLooks for Pods not yet bound to a node, and assigns each Pod to a suitable node.\nkube-controller-manager\nRuns\ncontrollers\nto implement Kubernetes API behavior.\ncloud-controller-manager\n(optional)\nIntegrates with underlying cloud provider(s).\nNode Components\nRun on every node, maintaining running pods and providing the Kubernetes runtime environment:\nkubelet\nEnsures that Pods are running, including their containers.\nkube-proxy\n(optional)\nMaintains network rules on nodes to implement\nServices\n.\nContainer runtime\nSoftware responsible for running containers. Read\nContainer Runtimes\nto learn more.\n🛇 This item links to a third party project or product that is not part of Kubernetes itself.\nMore information\nYour cluster may require additional software on each node; for example, you might also\nrun\nsystemd\non a Linux node to supervise local components.\nAddons\nAddons extend the functionality of Kubernetes. A few important examples include:\nDNS\nFor cluster-wide DNS resolution.\nWeb UI\n(Dashboard)\nFor cluster management via a web interface.\nContainer Resource Monitoring\nFor collecting and storing container metrics.\nCluster-level Logging\nFor saving container logs to a central log store.\nFlexibility in Architecture\nKubernetes allows for flexibility in how these components are deployed and managed.\nThe architecture can be adapted to various needs, from small development environments\nto large-scale production deployments.\nFor more detailed information about each component and various ways to configure your\ncluster architecture, see the\nCluster Architecture\npage.\nItems on this page refer to third party products or projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. See the\nCNCF website guidelines\nfor more details.\nYou should read the\ncontent guide\nbefore proposing a change that adds an extra third-party link.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified May 31, 2025 at 8:36 AM PST:\nFix missing periods at the end of sentences (f0b3dc1f07)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nConcepts\nOverview\nObjects In Kubernetes\nObjects In Kubernetes\nKubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Learn about the Kubernetes object model and how to work with these objects.\nThis page explains how Kubernetes objects are represented in the Kubernetes API, and how you can\nexpress them in\n.yaml\nformat.\nUnderstanding Kubernetes objects\nKubernetes objects\nare persistent entities in the Kubernetes system. Kubernetes uses these\nentities to represent the state of your cluster. Specifically, they can describe:\nWhat containerized applications are running (and on which nodes)\nThe resources available to those applications\nThe policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance\nA Kubernetes object is a \"record of intent\"--once you create the object, the Kubernetes system\nwill constantly work to ensure that the object exists. By creating an object, you're effectively\ntelling the Kubernetes system what you want your cluster's workload to look like; this is your\ncluster's\ndesired state\n.\nTo work with Kubernetes objects—whether to create, modify, or delete them—you'll need to use the\nKubernetes API\n. When you use the\nkubectl\ncommand-line\ninterface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use\nthe Kubernetes API directly in your own programs using one of the\nClient Libraries\n.\nObject spec and status\nAlmost every Kubernetes object includes two nested object fields that govern\nthe object's configuration: the object\nspec\nand the object\nstatus\n.\nFor objects that have a\nspec\n, you have to set this when you create the object,\nproviding a description of the characteristics you want the resource to have:\nits\ndesired state\n.\nThe\nstatus\ndescribes the\ncurrent state\nof the object, supplied and updated\nby the Kubernetes system and its components. The Kubernetes\ncontrol plane\ncontinually\nand actively manages every object's actual state to match the desired state you\nsupplied.\nFor example: in Kubernetes, a Deployment is an object that can represent an\napplication running on your cluster. When you create the Deployment, you\nmight set the Deployment\nspec\nto specify that you want three replicas of\nthe application to be running. The Kubernetes system reads the Deployment\nspec and starts three instances of your desired application--updating\nthe status to match your spec. If any of those instances should fail\n(a status change), the Kubernetes system responds to the difference\nbetween spec and status by making a correction--in this case, starting\na replacement instance.\nFor more information on the object spec, status, and metadata, see the\nKubernetes API Conventions\n.\nDescribing a Kubernetes object\nWhen you create an object in Kubernetes, you must provide the object spec that describes its\ndesired state, as well as some basic information about the object (such as a name). When you use\nthe Kubernetes API to create the object (either directly or via\nkubectl\n), that API request must\ninclude that information as JSON in the request body.\nMost often, you provide the information to\nkubectl\nin a file known as a\nmanifest\n.\nBy convention, manifests are YAML (you could also use JSON format).\nTools such as\nkubectl\nconvert the information from a manifest into JSON or another supported\nserialization format when making the API request over HTTP.\nHere's an example manifest that shows the required fields and object spec for a Kubernetes\nDeployment:\napplication/deployment.yaml\napiVersion\n:\napps/v1\nkind\n:\nDeployment\nmetadata\n:\nname\n:\nnginx-deployment\nspec\n:\nselector\n:\nmatchLabels\n:\napp\n:\nnginx\nreplicas\n:\n2\n# tells deployment to run 2 pods matching the template\ntemplate\n:\nmetadata\n:\nlabels\n:\napp\n:\nnginx\nspec\n:\ncontainers\n:\n-\nname\n:\nnginx\nimage\n:\nnginx:1.14.2\nports\n:\n-\ncontainerPort\n:\n80\nOne way to create a Deployment using a manifest file like the one above is to use the\nkubectl apply\ncommand\nin the\nkubectl\ncommand-line interface, passing the\n.yaml\nfile as an argument. Here's an example:\nkubectl apply -f https://k8s.io/examples/application/deployment.yaml\nThe output is similar to this:\ndeployment.apps/nginx-deployment created\nRequired fields\nIn the manifest (YAML or JSON file) for the Kubernetes object you want to create, you'll need to set values for\nthe following fields:\napiVersion\n- Which version of the Kubernetes API you're using to create this object\nkind\n- What kind of object you want to create\nmetadata\n- Data that helps uniquely identify the object, including a\nname\nstring,\nUID\n, and optional\nnamespace\nspec\n- What state you desire for the object\nThe precise format of the object\nspec\nis different for every Kubernetes object, and contains\nnested fields specific to that object. The\nKubernetes API Reference\ncan help you find the spec format for all of the objects you can create using Kubernetes.\nFor example, see the\nspec\nfield\nfor the Pod API reference.\nFor each Pod, the\n.spec\nfield specifies the pod and its desired state (such as the container image name for\neach container within that pod).\nAnother example of an object specification is the\nspec\nfield\nfor the StatefulSet API. For StatefulSet, the\n.spec\nfield specifies the StatefulSet and\nits desired state.\nWithin the\n.spec\nof a StatefulSet is a\ntemplate\nfor Pod objects. That template describes Pods that the StatefulSet controller will create in order to\nsatisfy the StatefulSet specification.\nDifferent kinds of objects can also have different\n.status\n; again, the API reference pages\ndetail the structure of that\n.status\nfield, and its content for each different type of object.\nNote:\nSee\nConfiguration Best Practices\nfor additional\ninformation on writing YAML configuration files.\nServer side field validation\nStarting with Kubernetes v1.25, the API server offers server side\nfield validation\nthat detects unrecognized or duplicate fields in an object. It provides all the functionality\nof\nkubectl --validate\non the server side.\nThe\nkubectl\ntool uses the\n--validate\nflag to set the level of field validation. It accepts the\nvalues\nignore\n,\nwarn\n, and\nstrict\nwhile also accepting the values\ntrue\n(equivalent to\nstrict\n)\nand\nfalse\n(equivalent to\nignore\n). The default validation setting for\nkubectl\nis\n--validate=true\n.\nStrict\nStrict field validation, errors on validation failure\nWarn\nField validation is performed, but errors are exposed as warnings rather than failing the request\nIgnore\nNo server side field validation is performed\nWhen\nkubectl\ncannot connect to an API server that supports field validation it will fall back\nto using client-side validation. Kubernetes 1.27 and later versions always offer field validation;\nolder Kubernetes releases might not. If your cluster is older than v1.27, check the documentation\nfor your version of Kubernetes.\nWhat's next\nIf you're new to Kubernetes, read more about the following:\nPods\nwhich are the most important basic Kubernetes objects.\nDeployment\nobjects.\nControllers\nin Kubernetes.\nkubectl\nand\nkubectl commands\n.\nKubernetes Object Management\nexplains how to use\nkubectl\nto manage objects.\nYou might need to\ninstall kubectl\nif you don't already have it available.\nTo learn about the Kubernetes API in general, visit:\nKubernetes API overview\nTo learn about objects in Kubernetes in more depth, read other pages in this section:\nKubernetes Object Management\nObject Names and IDs\nLabels and Selectors\nNamespaces\nAnnotations\nField Selectors\nFinalizers\nOwners and Dependents\nRecommended Labels\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified August 25, 2024 at 8:24 PM PST:\nReorder overview pages (42da717f16)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of KUBERNETES.\nStart by understanding the core concepts from this content.\n\nDOC START:\nKubernetes Documentation\nConcepts\nOverview\nObjects In Kubernetes\nKubernetes Object Management\nKubernetes Object Management\nThe\nkubectl\ncommand-line tool supports several different ways to create and manage\nKubernetes\nobjects\n. This document provides an overview of the different\napproaches. Read the\nKubectl book\nfor\ndetails of managing objects by Kubectl.\nManagement techniques\nWarning:\nA Kubernetes object should be managed using only one technique. Mixing\nand matching techniques for the same object results in undefined behavior.\nManagement technique\nOperates on\nRecommended environment\nSupported writers\nLearning curve\nImperative commands\nLive objects\nDevelopment projects\n1+\nLowest\nImperative object configuration\nIndividual files\nProduction projects\n1\nModerate\nDeclarative object configuration\nDirectories of files\nProduction projects\n1+\nHighest\nImperative commands\nWhen using imperative commands, a user operates directly on live objects\nin a cluster. The user provides operations to\nthe\nkubectl\ncommand as arguments or flags.\nThis is the recommended way to get started or to run a one-off task in\na cluster. Because this technique operates directly on live\nobjects, it provides no history of previous configurations.\nExamples\nRun an instance of the nginx container by creating a Deployment object:\nkubectl create deployment nginx --image nginx\nTrade-offs\nAdvantages compared to object configuration:\nCommands are expressed as a single action word.\nCommands require only a single step to make changes to the cluster.\nDisadvantages compared to object configuration:\nCommands do not integrate with change review processes.\nCommands do not provide an audit trail associated with changes.\nCommands do not provide a source of records except for what is live.\nCommands do not provide a template for creating new objects.\nImperative object configuration\nIn imperative object configuration, the kubectl command specifies the\noperation (create, replace, etc.), optional flags and at least one file\nname. The file specified must contain a full definition of the object\nin YAML or JSON format.\nSee the\nAPI reference\nfor more details on object definitions.\nWarning:\nThe imperative\nreplace\ncommand replaces the existing\nspec with the newly provided one, dropping all changes to the object missing from\nthe configuration file. This approach should not be used with resource\ntypes whose specs are updated independently of the configuration file.\nServices of type\nLoadBalancer\n, for example, have their\nexternalIPs\nfield updated\nindependently from the configuration by the cluster.\nExamples\nCreate the objects defined in a configuration file:\nkubectl create -f nginx.yaml\nDelete the objects defined in two configuration files:\nkubectl delete -f nginx.yaml -f redis.yaml\nUpdate the objects defined in a configuration file by overwriting\nthe live configuration:\nkubectl replace -f nginx.yaml\nTrade-offs\nAdvantages compared to imperative commands:\nObject configuration can be stored in a source control system such as Git.\nObject configuration can integrate with processes such as reviewing changes before push and audit trails.\nObject configuration provides a template for creating new objects.\nDisadvantages compared to imperative commands:\nObject configuration requires basic understanding of the object schema.\nObject configuration requires the additional step of writing a YAML file.\nAdvantages compared to declarative object configuration:\nImperative object configuration behavior is simpler and easier to understand.\nAs of Kubernetes version 1.5, imperative object configuration is more mature.\nDisadvantages compared to declarative object configuration:\nImperative object configuration works best on files, not directories.\nUpdates to live objects must be reflected in configuration files, or they will be lost during the next replacement.\nDeclarative object configuration\nWhen using declarative object configuration, a user operates on object\nconfiguration files stored locally, however the user does not define the\noperations to be taken on the files. Create, update, and delete operations\nare automatically detected per-object by\nkubectl\n. This enables working on\ndirectories, where different operations might be needed for different objects.\nNote:\nDeclarative object configuration retains changes made by other\nwriters, even if the changes are not merged back to the object configuration file.\nThis is possible by using the\npatch\nAPI operation to write only\nobserved differences, instead of using the\nreplace\nAPI operation to replace the entire object configuration.\nExamples\nProcess all object configuration files in the\nconfigs\ndirectory, and create or\npatch the live objects. You can first\ndiff\nto see what changes are going to be\nmade, and then apply:\nkubectl diff -f configs/\nkubectl apply -f configs/\nRecursively process directories:\nkubectl diff -R -f configs/\nkubectl apply -R -f configs/\nTrade-offs\nAdvantages compared to imperative object configuration:\nChanges made directly to live objects are retained, even if they are not merged back into the configuration files.\nDeclarative object configuration has better support for operating on directories and automatically detecting operation types (create, patch, delete) per-object.\nDisadvantages compared to imperative object configuration:\nDeclarative object configuration is harder to debug and understand results when they are unexpected.\nPartial updates using diffs create complex merge and patch operations.\nWhat's next\nManaging Kubernetes Objects Using Imperative Commands\nImperative Management of Kubernetes Objects Using Configuration Files\nDeclarative Management of Kubernetes Objects Using Configuration Files\nDeclarative Management of Kubernetes Objects Using Kustomize\nKubectl Command Reference\nKubectl Book\nKubernetes API Reference\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified January 08, 2022 at 6:09 PM PST:\nReorganize Working with Kubernetes Objects section (634c17f61c)\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the KUBERNETES official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "kubernetes", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nDocumentation\nLearn how to deploy, use, and operate Istio.\nOverview\nA high-level introduction to Istio and service mesh.\nConcepts\nLearn about the different parts of the Istio system and the abstractions it uses.\nSidecar Mode\nInformation for setting up and operating Istio in sidecar mode.\nAmbient Mode\nInformation for setting up and operating Istio with support for ambient mode.\nTasks\nHow to do single specific targeted activities with the Istio system.\nExamples\nA variety of fully working example uses for Istio that you can experiment with.\nOperations\nConcepts, tools, and techniques to deploy and manage an Istio mesh.\nReleases\nInformation relating to Istio releases.\nReference\nDetailed authoritative reference material such as command-line options, configuration options, and API calling parameters.\nIn addition to the above documentation links, please consider the following resources:\nFrequently Asked Questions\nGlossary\nDocumentation Archive\n, which contains snapshots of the documentation for prior releases.\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nOverview\nQuickstart\nQuickstart\nThanks for your interest in Istio!\nIstio has two primary modes:\nambient mode\nand\nsidecar mode\n.\nAmbient mode\nis the new and improved model, created to address the shortcomings of sidecar mode. In ambient mode, a secure tunnel is installed on each node, and you can opt in to the full feature set with proxies you install, (generally) per-namespace.\nSidecar mode\nis the traditional model of service mesh pioneered by Istio in 2017. In sidecar mode, a proxy is deployed along with every Kubernetes pod or other workload.\nMost of the energy in the Istio community is going towards improvement of ambient mode, although sidecar mode remains fully supported. Any major new feature contributed to the project is expected to work in both modes.\nIn general,\nwe recommend that new users start with ambient mode\n. It is faster, cheaper, and easier to manage. There are\nadvanced use cases\nthat still require the use of sidecar mode, but closing these gaps is on our 2025 roadmap.\nGet started with ambient mode\nGet started with sidecar mode\nSidecar or ambient?\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nOverview\nOverview\nA high-level introduction to Istio and service mesh.\nWhat is Istio?\nFind out what Istio can do for you.\nWhy choose Istio?\nCompare Istio to other service mesh solutions.\nSidecar or ambient?\nLearn about Istio's two dataplane modes and which you should use.\nQuickstart\nLearn how to get started with a simple example installation.\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nOverview\nWhat is Istio?\nWhat is Istio?\n2 minute read\nHow it works\nSee also\nIstio is an open source service mesh that layers transparently onto existing distributed applications. Istio’s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring – with few or no service code changes. It gives you:\nSecure service-to-service communication in a cluster with mutual TLS encryption, strong identity-based authentication and authorization\nAutomatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic\nFine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection\nA pluggable policy layer and configuration API supporting access controls, rate limits and quotas\nAutomatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress\nIstio is designed for extensibility and can handle a diverse range of deployment needs. Istio’s\ncontrol plane\nruns on Kubernetes, and you can add applications deployed in that cluster to your mesh,\nextend the mesh to other clusters\n, or even\nconnect VMs or other endpoints\nrunning outside of Kubernetes.\nA large ecosystem of contributors, partners, integrations, and distributors extend and leverage Istio for a wide variety of scenarios. You can install Istio yourself, or a\nlarge number of vendors\nhave products that integrate Istio and manage it for you.\nHow it works\nIstio uses a proxy to intercept all your network traffic, allowing a broad set of application-aware features based on configuration you set.\nThe control plane takes your desired configuration, and its view of the services, and dynamically programs the proxy servers, updating them as the rules or the environment changes.\nThe data plane is the communication between services. Without a service mesh, the network doesn’t understand the traffic being sent over, and can’t make any decisions based on what type of traffic it is, or who it is from or to.\nIstio supports two data plane modes:\nsidecar mode\n, which deploys an Envoy proxy along with each pod that you start in your cluster, or running alongside services running on VMs.\nambient mode\n, which uses a per-node Layer 4 proxy, and optionally a per-namespace Envoy proxy for Layer 7 features.\nLearn how to choose which mode is right for you\n.\nSee also\nQuickstart\nLearn how to get started with a simple example installation.\nWhy choose Istio?\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nHow it works\nSee also\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nOverview\nWhy choose Istio?\nWhy choose Istio?\n5 minute read\nSimple and powerful\nThe Envoy proxy\nCommunity\nPackages\nAlternatives considered\nWhy not “use eBPF”?\nWhy not use a per-node proxy?\nI have a CNI. Why do I need Istio?\nIstio pioneered the concept of a sidecar-based service mesh when it launched in 2017. Out of the gate, the project included the features that would come to define a service mesh, including standards-based mutual TLS for zero-trust networking, smart traffic routing, and observability through metrics, logs and tracing.\nSince then, the project has driven advances in the mesh space including\nmulti-cluster & multi-network topologies\n,\nextensibility via WebAssembly\n, the\ndevelopment of the Kubernetes Gateway API\n, and moving the mesh infrastructure away from application developers with\nambient mode\n.\nHere are a few reasons we think you should use Istio as your service mesh.\nSimple and powerful\nKubernetes has hundreds of features and dozens of APIs, but you can get started with it with just one command. We’ve built Istio to be the same way. Progressive disclosure means you can use a small set of APIs, and only turn the more powerful knobs if you have the need. Other “simple” service meshes spent years catching up to the feature set Istio had on day 1.\nIt is better to have a feature and not need it, than to need it and not have it!\nThe Envoy proxy\nFrom the beginning, Istio has been powered by the\nEnvoy\nproxy, a high performance service proxy initially built by Lyft. Istio was the first project to adopt Envoy, and\nthe Istio team were the first external committers\n. Envoy would go on to become\nthe load balancer that powers Google Cloud\nas well as the proxy for almost every other service mesh platform.\nIstio inherits all the power and flexibility of Envoy, including world-class extensibility using WebAssembly that was\ndeveloped in Envoy by the Istio team\n.\nCommunity\nIstio is a true community project. In 2023, there were 10 companies who made over 1,000 contributions each to Istio, with no single company exceeding 25%. (\nSee the numbers here\n).\nNo other service mesh project has the breadth of support from the industry as Istio.\nPackages\nWe make stable binary releases available to everyone, with every release, and commit to continue doing so. We publish free and regular security patches for\nour latest release and a number of prior releases\n. Many of our vendors will support older versions, but we believe that engaging a vendor should not be a requirement to be safe in a stable open source project.\nAlternatives considered\nA good design document includes a section on alternatives that were considered, and ultimately rejected.\nWhy not “use eBPF”?\nWe do - where it’s appropriate! Istio can be configured to use\neBPF\nto route traffic from pods to proxies\n. This shows a small performance increase over using\niptables\n.\nWhy not use it for everything? No-one does, because no-one actually can.\neBPF is a virtual machine that runs inside the Linux kernel. It was designed for functions guaranteed to complete in a limited compute envelope to avoid destabilizing kernel behavior, such as those that perform simple L3 traffic routing or application observability. It was not designed for long running or complex functions like those found in Envoy: that’s why operating systems have\nuser space\n! eBPF maintainers have theorized that it could eventually be extended to support running a program as complex as Envoy, but this is a science project and unlikely to have real world practicality.\nOther meshes that claim to “use eBPF” actually use a per-node Envoy proxy, or other user space tools, for much of their functionality.\nWhy not use a per-node proxy?\nEnvoy is not inherently multi-tenant. As a result, we have major security and stability concerns with commingling complex processing rules for L7 traffic from multiple unconstrained tenants in a shared instance. Since Kubernetes, by default can schedule a pod from any namespace onto any node, the node is not an appropriate tenancy boundary. Budgeting and cost attribution are also major issues, as L7 processing costs a lot more than L4.\nIn ambient mode, we strictly limit our ztunnel proxy to L4 processing -\njust like the Linux kernel\n. This reduces the vulnerability surface area significantly, and allows us to safely operate a shared component. Traffic is then forwarded off to Envoy proxies that operate per-namespace, such that no Envoy proxy is ever multi-tenant.\nI have a CNI. Why do I need Istio?\nToday, some CNI plugins are starting to offer service mesh-like functionality as an add-on that sits on top of their own CNI implementation. For example, they may implement their own encryption schemes for traffic between nodes or pods, workload identity, or support some amount of transport-level policy by redirecting traffic to a L7 proxy. These service mesh addons are non-standard, and as such can only work on top of the CNI that ships them. They also offer varying feature sets. For example, solutions built on top of Wireguard cannot be made FIPS-compliant.\nFor this reason, Istio has implemented its zero-trust tunnel (ztunnel) component, which transparently and efficiently provides this functionality using proven, industry-standard encryption protocols.\nLearn more about ztunnel\n.\nIstio is designed to be a service mesh that provides a consistent, highly secure, efficient, and standards-compliant service mesh implementation providing a\npowerful set of L7 policies\n,\nplatform-agnostic workload identity\n, using\nindustry-proven mTLS protocols\n- in any environment, with any CNI, or even across clusters with different CNIs.\nWhat is Istio?\nSidecar or ambient?\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nSimple and powerful\nThe Envoy proxy\nCommunity\nPackages\nAlternatives considered\nWhy not “use eBPF”?\nWhy not use a per-node proxy?\nI have a CNI. Why do I need Istio?\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nOverview\nSidecar or ambient?\nSidecar or ambient?\n5 minute read\nSidecar mode\nAmbient mode\nChoosing between sidecar and ambient\nLayer 4 vs Layer 7 features\nSecurity\nObservability\nTraffic management\nUnsupported features\nSee also\nAn Istio service mesh is logically split into a data plane and a control plane.\nThe\ndata plane\nis the set of proxies that mediate and control all network communication between microservices. They also collect and report telemetry on all mesh traffic.\nThe\ncontrol plane\nmanages and configures the proxies in the data plane.\nIstio supports two main\ndata plane modes\n:\nsidecar mode\n, which deploys an Envoy proxy along with each pod that you start in your cluster, or running alongside services running on VMs.\nambient mode\n, which uses a per-node Layer 4 proxy, and optionally a per-namespace Envoy proxy for Layer 7 features.\nYou can opt certain namespaces or workloads into each mode.\nSidecar mode\nIstio has been built on the sidecar pattern from its first release in 2017. Sidecar mode is well understood and thoroughly battle-tested, but comes with a resource cost and operational overhead.\nEach application you deploy has an Envoy proxy\ninjected\nas a sidecar\nAll proxies can process both Layer 4 and Layer 7\nAmbient mode\nLaunched in 2022, ambient mode was built to address the shortcomings reported by users of sidecar mode. As of Istio 1.22, it is production-ready for single cluster use cases.\nAll traffic is proxied through a Layer 4-only node proxy\nApplications can opt in to routing through an Envoy proxy to get Layer 7 features\nChoosing between sidecar and ambient\nUsers often deploy a mesh to enable a zero-trust security posture as a first-step and then selectively enable L7 capabilities as needed. Ambient mesh allows those users to bypass the cost of L7 processing entirely when it’s not needed.\nSidecar\nAmbient\nTraffic management\nFull Istio feature set\nFull Istio feature set (requires using waypoint)\nSecurity\nFull Istio feature set\nFull Istio feature set: encryption and L4 authorization in ambient mode. Requires waypoints for L7 authorization.\nObservability\nFull Istio feature set\nFull Istio feature set: L4 telemetry in ambient mode; L7 observability when using waypoint\nExtensibility\nFull Istio feature set\nVia\nWebAssembly plugins\n(requires using waypoint)\nThe EnvoyFilter API is not supported.\nAdding workloads to the mesh\nLabel a namespace and restart all pods to have sidecars added\nLabel a namespace - no pod restart required\nIncremental deployment\nBinary: sidecar is injected or it isn't\nGradual: L4 is always on, L7 can be added by configuration\nLifecycle management\nProxies managed by application developer\nPlatform administrator\nUtilization of resources\nWasteful; CPU and memory resources must be provisioned for worst case usage of each individual pod\nWaypoint proxies can be auto-scaled like any other Kubernetes deployment.\nA workload with many replicas can use one waypoint, vs. each one having its own sidecar.\nAverage resource cost\nLarge\nSmall\nAverage latency (p90/p99)\n0.63ms-0.88ms\nAmbient: 0.16ms-0.20ms\nWaypoint: 0.40ms-0.50ms\nL7 processing steps\n2 (source and destination sidecar)\n1 (destination waypoint)\nConfiguration at scale\nRequires\nconfiguration of the scope of each sidecar\nto reduce configuration\nWorks without custom configuration\nSupports \"server-first\" protocols\nRequires configuration\nYes\nSupport for Kubernetes Jobs\nComplicated by long life of sidecar\nTransparent\nSecurity model\nStrongest: each workload has its own keys\nStrong: each node agent has only the keys for workloads on that node\nCompromised application pod\ngives access to mesh keys\nYes\nNo\nSupport\nStable, including multi-cluster\nStable, only single-cluster\nPlatforms supported\nKubernetes (any CNI)\nVirtual machines\nKubernetes (any CNI)\nLayer 4 vs Layer 7 features\nThe overhead for processing protocols at Layer 7 is substantially higher than processing network packets at Layer 4. For a given service, if your requirements can be met at L4, service mesh can be delivered at substantially lower cost.\nSecurity\nL4\nL7\nEncryption\nAll traffic between pods is encrypted using\nmTLS\n.\nN/A—service identity in Istio is based on TLS.\nService-to-service authentication\nSPIFFE\n, via mTLS certificates. Istio issues a short-lived X.509 certificate that encodes the pod's service account identity.\nN/A—service identity in Istio is based on TLS.\nService-to-service authorization\nNetwork-based authorization, plus identity-based policy, e.g.:\nA can accept inbound calls from only \"10.2.0.0/16\";\nA can call B.\nFull policy, e.g.:\nA can GET /foo on B only with valid end-user credentials containing the READ scope.\nEnd-user authentication\nN/A—we can't apply per-user settings.\nLocal authentication of JWTs, support for remote authentication via OAuth and OIDC flows.\nEnd-user authorization\nN/A—see above.\nService-to-service policies can be extended to require\nend-user credentials with specific scopes, issuers, principal, audiences, etc.\nFull user-to-resource access can be implemented using external authorization, allowing per-request policy with decisions from an external service, e.g. OPA.\nObservability\nL4\nL7\nLogging\nBasic network information: network 5-tuple, bytes sent/received, etc.\nSee Envoy docs\n.\nFull request metadata logging\n, in addition to basic network information.\nTracing\nNot today; possible eventually with HBONE.\nEnvoy participates in distributed tracing.\nSee Istio overview on tracing\n.\nMetrics\nTCP only (bytes sent/received, number of packets, etc.).\nL7 RED metrics: rate of requests, rate of errors, request duration (latency).\nTraffic management\nL4\nL7\nLoad balancing\nConnection level only.\nSee TCP traffic shifting task\n.\nPer request, enabling e.g. canary deployments, gRPC traffic, etc.\nSee HTTP traffic shifting task\n.\nCircuit breaking\nTCP only\n.\nHTTP settings\nin addition to TCP.\nOutlier detection\nOn connection establishment/failure.\nOn request success/failure.\nRate limiting\nRate limit on L4 connection data only, on connection establishment\n, with global and local rate limiting options.\nRate limit on L7 request metadata\n, per request.\nTimeouts\nConnection establishment only (connection keep-alive is configured via circuit breaking settings).\nPer request.\nRetries\nRetry connection establishment\nRetry per request failure.\nFault injection\nN/A—fault injection cannot be configured on TCP connections.\nFull application and connection-level faults (\ntimeouts, delays, specific response codes\n).\nTraffic mirroring\nN/A—HTTP only\nPercentage-based mirroring of requests to multiple backends\n.\nUnsupported features\nThe following features are available in sidecar mode, but not yet implemented in ambient mode:\nSidecar-to-waypoint interoperability\nMulti-cluster installations\nMulti-network support\nVM support\nSee also\nDeep Dive into the Network Traffic Path of the Coexistence of Ambient and Sidecar\nDeep Dive into the Traffic Path of the Coexistence of Ambient and Sidecar.\nIstio publishes results of ztunnel security audit\nPasses with flying colors.\nIstio: The Highest-Performance Solution for Network Security\nAmbient mode provides more encrypted throughput than any other project in the Kubernetes ecosystem.\nFast, Secure, and Simple: Istio’s Ambient Mode Reaches General Availability in v1.24\nOur latest release signals ambient mode – service mesh without sidecars – is ready for everyone.\nSay goodbye to your sidecars: Istio's ambient mode reaches Beta in v1.22\nLayer 4 & Layer 7 features are both now ready for production.\nIstio Ambient Waypoint Proxy Made Simple\nIntroducing the new destination oriented waypoint proxy for simplicity and scalability.\nWhy choose Istio?\nQuickstart\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nSidecar mode\nAmbient mode\nChoosing between sidecar and ambient\nLayer 4 vs Layer 7 features\nSecurity\nObservability\nTraffic management\nUnsupported features\nSee also\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nOverview\nQuickstart\nQuickstart\nThanks for your interest in Istio!\nIstio has two primary modes:\nambient mode\nand\nsidecar mode\n.\nAmbient mode\nis the new and improved model, created to address the shortcomings of sidecar mode. In ambient mode, a secure tunnel is installed on each node, and you can opt in to the full feature set with proxies you install, (generally) per-namespace.\nSidecar mode\nis the traditional model of service mesh pioneered by Istio in 2017. In sidecar mode, a proxy is deployed along with every Kubernetes pod or other workload.\nMost of the energy in the Istio community is going towards improvement of ambient mode, although sidecar mode remains fully supported. Any major new feature contributed to the project is expected to work in both modes.\nIn general,\nwe recommend that new users start with ambient mode\n. It is faster, cheaper, and easier to manage. There are\nadvanced use cases\nthat still require the use of sidecar mode, but closing these gaps is on our 2025 roadmap.\nGet started with ambient mode\nGet started with sidecar mode\nSidecar or ambient?\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nConcepts\nConcepts\nLearn about the different parts of the Istio system and the abstractions it uses.\nTraffic Management\nDescribes the various Istio features focused on traffic routing and control.\nSecurity\nDescribes Istio's authorization and authentication functionality.\nObservability\nDescribes the telemetry and monitoring features provided by Istio.\nExtensibility\nDescribes Istio's WebAssembly Plugin system.\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nConcepts\nTraffic Management\nTraffic Management\n22 minute read\nIntroducing Istio traffic management\nVirtual services\nWhy use virtual services?\nVirtual service example\nThe hosts field\nRouting rules\nMatch condition\nDestination\nRouting rule precedence\nMore about routing rules\nDestination rules\nLoad balancing options\nDestination rule example\nGateways\nGateway example\nService entries\nService entry example\nSidecars\nNetwork resilience and testing\nTimeouts\nRetries\nCircuit breakers\nFault injection\nWorking with your applications\nSee also\nIstio’s traffic routing rules let you easily control the flow\nof traffic and API calls between services. Istio simplifies configuration of\nservice-level properties like circuit breakers, timeouts, and retries, and makes\nit easy to set up important tasks like A/B testing, canary rollouts, and staged\nrollouts with percentage-based traffic splits. It also provides out-of-box\nreliability features that help make your application\nmore resilient against failures of dependent services or the network.\nIstio’s traffic management model relies on the\nEnvoy\nproxies that are deployed along with your services. All traffic that your mesh\nservices send and receive (\ndata plane\ntraffic) is proxied through Envoy, making\nit easy to direct and control traffic around your mesh without making any\nchanges to your services.\nIf you’re interested in the details of how the features described in this guide\nwork, you can find out more about Istio’s traffic management implementation in the\narchitecture overview\n. The rest of\nthis guide introduces Istio’s traffic management features.\nIntroducing Istio traffic management\nIn order to direct traffic within your mesh, Istio needs to know where all your\nendpoints are, and which services they belong to. To populate its own\nservice registry\n, Istio connects to a service\ndiscovery system. For example, if you’ve installed Istio on a Kubernetes cluster,\nthen Istio automatically detects the services and endpoints in that cluster.\nUsing this service registry, the Envoy proxies can then direct traffic to the\nrelevant services. Most microservice-based applications have multiple instances\nof each service workload to handle service traffic, sometimes referred to as a\nload balancing pool. By default, the Envoy proxies distribute traffic across\neach service’s load balancing pool using a least requests model, where each\nrequest is routed to the host with fewer active requests from a\nrandom selection of two hosts from the pool; in this way the most heavily loaded\nhost will not receive requests until it is no more loaded than any other host.\nWhile Istio’s basic service discovery and load balancing gives you a working\nservice mesh, it’s far from all that Istio can do. In many cases you might want\nmore fine-grained control over what happens to your mesh traffic.\nYou might want to direct a particular percentage of traffic to a new version of\na service as part of A/B testing, or apply a different load balancing policy to\ntraffic for a particular subset of service instances. You might also want to\napply special rules to traffic coming into or out of your mesh, or add an\nexternal dependency of your mesh to the service registry. You can do all this\nand more by adding your own traffic configuration to Istio using Istio’s traffic\nmanagement API.\nLike other Istio configuration, the API is specified using Kubernetes custom\nresource definitions (\nCRDs\n), which you can configure\nusing YAML, as you’ll see in the examples.\nThe rest of this guide examines each of the traffic management API resources\nand what you can do with them. These resources are:\nVirtual services\nDestination rules\nGateways\nService entries\nSidecars\nThis guide also gives an overview of some of the\nnetwork resilience and testing features\nthat\nare built in to the API resources.\nVirtual services\nVirtual services\n,\nalong with\ndestination rules\n, are the key building blocks of Istio’s traffic\nrouting functionality. A virtual service lets you configure how requests are\nrouted to a service within an Istio service mesh, building on the basic\nconnectivity and discovery provided by Istio and your platform. Each virtual\nservice consists of a set of routing rules that are evaluated in order, letting\nIstio match each given request to the virtual service to a specific real\ndestination within the mesh. Your mesh can require multiple virtual services or\nnone depending on your use case.\nWhy use virtual services?\nVirtual services play a key role in making Istio’s traffic management flexible\nand powerful. They do this by strongly decoupling where clients send their\nrequests from the destination workloads that actually implement them. Virtual\nservices also provide a rich way of specifying different traffic routing rules\nfor sending traffic to those workloads.\nWhy is this so useful? Without virtual services, Envoy distributes\ntraffic using least requests load balancing between all service instances, as\ndescribed in the introduction. You can improve this behavior with what you know\nabout the workloads. For example, some might represent a different version. This\ncan be useful in A/B testing, where you might want to configure traffic routes\nbased on percentages across different service versions, or to direct\ntraffic from your internal users to a particular set of instances.\nWith a virtual service, you can specify traffic behavior for one or more hostnames.\nYou use routing rules in the virtual service that tell Envoy how to send the\nvirtual service’s traffic to appropriate destinations. Route destinations can\nbe different versions of the same service or entirely different services.\nA typical use case is to send traffic to different versions of a service,\nspecified as service subsets. Clients send requests to the virtual service host as if\nit was a single entity, and Envoy then routes the traffic to the different\nversions depending on the virtual service rules: for example, “20% of calls go to\nthe new version” or “calls from these users go to version 2”. This allows you to,\nfor instance, create a canary rollout where you gradually increase the\npercentage of traffic that’s sent to a new service version. The traffic routing\nis completely separate from the instance deployment, meaning that the number of\ninstances implementing the new service version can scale up and down based on\ntraffic load without referring to traffic routing at all. By contrast, container\norchestration platforms like Kubernetes only support traffic distribution based\non instance scaling, which quickly becomes complex. You can read more about how\nvirtual services help with canary deployments in\nCanary Deployments using Istio\n.\nVirtual services also let you:\nAddress multiple application services through a single virtual service. If\nyour mesh uses Kubernetes, for example, you can configure a virtual service\nto handle all services in a specific namespace. Mapping a single\nvirtual service to multiple “real” services is particularly useful in\nfacilitating turning a monolithic application into a composite service built\nout of distinct microservices without requiring the consumers of the service\nto adapt to the transition. Your routing rules can specify “calls to these URIs of\nmonolith.com\ngo to\nmicroservice A\n”, and so on. You can see how this works\nin\none of our examples below\n.\nConfigure traffic rules in combination with\ngateways\nto control ingress\nand egress traffic.\nIn some cases you also need to configure destination rules to use these\nfeatures, as these are where you specify your service subsets. Specifying\nservice subsets and other destination-specific policies in a separate object\nlets you reuse these cleanly between virtual services. You can find out more\nabout destination rules in the next section.\nVirtual service example\nThe following virtual service routes\nrequests to different versions of a service depending on whether the request\ncomes from a particular user.\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n  - reviews\n  http:\n  - match:\n    - headers:\n        end-user:\n          exact: jason\n    route:\n    - destination:\n        host: reviews\n        subset: v2\n  - route:\n    - destination:\n        host: reviews\n        subset: v3\nThe hosts field\nThe\nhosts\nfield lists the virtual service’s hosts - in other words, the user-addressable\ndestination or destinations that these routing rules apply to. This is the\naddress or addresses the client uses when sending requests to the service.\nhosts:\n- reviews\nThe virtual service hostname can be an IP address, a DNS name, or, depending on\nthe platform, a short name (such as a Kubernetes service short name) that resolves,\nimplicitly or explicitly, to a fully qualified domain name (FQDN). You can also\nuse wildcard (\"*\") prefixes, letting you create a single set of routing rules for\nall matching services. Virtual service hosts don’t actually have to be part of the\nIstio service registry, they are simply virtual destinations. This lets you model\ntraffic for virtual hosts that don’t have routable entries inside the mesh.\nRouting rules\nThe\nhttp\nsection contains the virtual service’s routing rules, describing\nmatch conditions and actions for routing HTTP/1.1, HTTP2, and gRPC traffic sent\nto the destination(s) specified in the hosts field (you can also use\ntcp\nand\ntls\nsections to configure routing rules for\nTCP\nand\nunterminated\nTLS\ntraffic). A routing rule consists of the destination where you want the traffic\nto go and zero or more match conditions, depending on your use case.\nMatch condition\nThe first routing rule in the example has a condition and so begins with the\nmatch\nfield. In this case you want this routing to apply to all requests from\nthe user “jason”, so you use the\nheaders\n,\nend-user\n, and\nexact\nfields to select\nthe appropriate requests.\n- match:\n   - headers:\n       end-user:\n         exact: jason\nDestination\nThe route section’s\ndestination\nfield specifies the actual destination for\ntraffic that matches this condition. Unlike the virtual service’s host(s), the\ndestination’s host must be a real destination that exists in Istio’s service\nregistry or Envoy won’t know where to send traffic to it. This can be a mesh\nservice with proxies or a non-mesh service added using a service entry. In this\ncase we’re running on Kubernetes and the host name is a Kubernetes service name:\nroute:\n- destination:\n    host: reviews\n    subset: v2\nNote in this and the other examples on this page, we use a Kubernetes short name for the\ndestination hosts for simplicity. When this rule is evaluated, Istio adds a domain suffix based\non the namespace of the virtual service that contains the routing rule to get\nthe fully qualified name for the host. Using short names in our examples\nalso means that you can copy and try them in any namespace you like.\nUsing short names like this only works if the\ndestination hosts and the virtual service are actually in the same Kubernetes\nnamespace. Because using the Kubernetes short name can result in\nmisconfigurations, we recommend that you specify fully qualified host names in\nproduction environments.\nThe destination section also specifies which subset of this Kubernetes service\nyou want requests that match this rule’s conditions to go to, in this case the\nsubset named v2. You’ll see how you define a service subset in the section on\ndestination rules\nbelow.\nRouting rule precedence\nRouting rules are\nevaluated in sequential order from top to bottom\n, with the\nfirst rule in the virtual service definition being given highest priority. In\nthis case you want anything that doesn’t match the first routing rule to go to a\ndefault destination, specified in the second rule. Because of this, the second\nrule has no match conditions and just directs traffic to the v3 subset.\n- route:\n  - destination:\n      host: reviews\n      subset: v3\nWe recommend providing a default “no condition” or weight-based rule (described\nbelow) like this as the last rule in each virtual service to ensure that traffic\nto the virtual service always has at least one matching route.\nMore about routing rules\nAs you saw above, routing rules are a powerful tool for routing particular\nsubsets of traffic to particular destinations. You can set match conditions on\ntraffic ports, header fields, URIs, and more. For example, this virtual service\nlets users send traffic to two separate services, ratings and reviews, as if\nthey were part of a bigger virtual service at\nhttp://bookinfo.com/.\nThe\nvirtual service rules match traffic based on request URIs and direct requests to\nthe appropriate service.\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n    - bookinfo.com\n  http:\n  - match:\n    - uri:\n        prefix: /reviews\n    route:\n    - destination:\n        host: reviews\n  - match:\n    - uri:\n        prefix: /ratings\n    route:\n    - destination:\n        host: ratings\nFor some match conditions, you can also choose to select them using the exact\nvalue, a prefix, or a regex.\nYou can add multiple match conditions to the same\nmatch\nblock to AND your\nconditions, or add multiple match blocks to the same rule to OR your conditions.\nYou can also have multiple routing rules for any given virtual service. This\nlets you make your routing conditions as complex or simple as you like within a\nsingle virtual service. A full list of match condition fields and their possible\nvalues can be found in the\nHTTPMatchRequest\nreference\n.\nIn addition to using match conditions, you can distribute traffic\nby percentage “weight”. This is useful for A/B testing and canary rollouts:\nspec:\n  hosts:\n  - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n      weight: 75\n    - destination:\n        host: reviews\n        subset: v2\n      weight: 25\nYou can also use routing rules to perform some actions on the traffic, for\nexample:\nAppend or remove headers.\nRewrite the URL.\nSet a\nretry policy\nfor calls to this destination.\nTo learn more about the actions available, see the\nHTTPRoute\nreference\n.\nDestination rules\nAlong with\nvirtual services\n,\ndestination rules\nare a key part of Istio’s traffic routing functionality. You can think of\nvirtual services as how you route your traffic\nto\na given destination, and\nthen you use destination rules to configure what happens to traffic\nfor\nthat\ndestination. Destination rules are applied after virtual service routing rules\nare evaluated, so they apply to the traffic’s “real” destination.\nIn particular, you use destination rules to specify named service subsets, such\nas grouping all a given service’s instances by version. You can then use these\nservice subsets in the routing rules of virtual services to control the\ntraffic to different instances of your services.\nDestination rules also let you customize Envoy’s traffic policies when calling\nthe entire destination service or a particular service subset, such as your\npreferred load balancing model, TLS security mode, or circuit breaker settings.\nYou can see a complete list of destination rule options in the\nDestination Rule reference\n.\nLoad balancing options\nBy default, Istio uses a least requests load balancing policy, where requests\nare distributed among the instances with the least number of requests. Istio also supports the\nfollowing models, which you can specify in destination rules for requests to a\nparticular service or service subset.\nRandom: Requests are forwarded at random to instances in the pool.\nWeighted: Requests are forwarded to instances in the pool according to a\nspecific percentage.\nRound robin: Requests are forwarded to each instance in sequence.\nConsistent hash: Provides soft session affinity based on HTTP headers, cookies or other properties.\nRing hash: Implements consistent hashing to upstream hosts using the\nKetama algorithm\n.\nMaglev: Implements consistent hashing to upstream hosts as described in the\nMaglev paper\n.\nSee the\nEnvoy load balancing documentation\nfor more information about each option.\nDestination rule example\nThe following example destination rule configures three different subsets for\nthe\nmy-svc\ndestination service, with different load balancing policies:\napiVersion: networking.istio.io/v1\nkind: DestinationRule\nmetadata:\n  name: my-destination-rule\nspec:\n  host: my-svc\n  trafficPolicy:\n    loadBalancer:\n      simple: RANDOM\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n    trafficPolicy:\n      loadBalancer:\n        simple: ROUND_ROBIN\n  - name: v3\n    labels:\n      version: v3\nEach subset is defined based on one or more\nlabels\n, which in Kubernetes are\nkey/value pairs that are attached to objects such as Pods. These labels are\napplied in the Kubernetes service’s deployment as\nmetadata\nto identify\ndifferent versions.\nAs well as defining subsets, this destination rule has both a default traffic\npolicy for all subsets in this destination and a subset-specific policy that\noverrides it for just that subset. The default policy, defined above the\nsubsets\nfield, sets a simple random load balancer for the\nv1\nand\nv3\nsubsets. In the\nv2\npolicy, a round-robin load balancer is specified in the corresponding\nsubset’s field.\nGateways\nYou use a\ngateway\nto\nmanage inbound and outbound traffic for your mesh, letting you specify which\ntraffic you want to enter or leave the mesh. Gateway configurations are applied\nto standalone Envoy proxies that are running at the edge of the mesh, rather\nthan sidecar Envoy proxies running alongside your service workloads.\nUnlike other mechanisms for controlling traffic entering your systems, such as\nthe Kubernetes Ingress APIs, Istio gateways let you use the full power and\nflexibility of Istio’s traffic routing. You can do this because Istio’s Gateway\nresource just lets you configure layer 4-6 load balancing properties such as\nports to expose, TLS settings, and so on. Then instead of adding\napplication-layer traffic routing (L7) to the same API resource, you bind a\nregular Istio\nvirtual service\nto the gateway. This lets you\nbasically manage gateway traffic like any other data plane traffic in an Istio\nmesh.\nGateways are primarily used to manage ingress traffic, but you can also\nconfigure egress gateways. An egress gateway lets you configure a dedicated exit\nnode for the traffic leaving the mesh, letting you limit which services can or\nshould access external networks, or to enable\nsecure control of egress traffic\nto add security to your mesh, for example. You can also use a gateway to\nconfigure a purely internal proxy.\nIstio provides some preconfigured gateway proxy deployments\n(\nistio-ingressgateway\nand\nistio-egressgateway\n) that you can use - both are\ndeployed if you use our\ndemo installation\n,\nwhile just the ingress gateway is deployed with our\ndefault profile\n.\nYou can apply your own gateway configurations to these deployments or deploy and\nconfigure your own gateway proxies.\nGateway example\nThe following example shows a possible gateway configuration for external HTTPS\ningress traffic:\napiVersion: networking.istio.io/v1\nkind: Gateway\nmetadata:\n  name: ext-host-gwy\nspec:\n  selector:\n    app: my-gateway-controller\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    hosts:\n    - ext-host.example.com\n    tls:\n      mode: SIMPLE\n      credentialName: ext-host-cert\nThis gateway configuration lets HTTPS traffic from\next-host.example.com\ninto the mesh on\nport 443, but doesn’t specify any routing for the traffic.\nTo specify routing and for the gateway to work as intended, you must also bind\nthe gateway to a virtual service. You do this using the virtual service’s\ngateways\nfield, as shown in the following example:\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: virtual-svc\nspec:\n  hosts:\n  - ext-host.example.com\n  gateways:\n  - ext-host-gwy\nYou can then configure the virtual service with routing rules for the external\ntraffic.\nService entries\nYou use a\nservice entry\nto add\nan entry to the service registry that Istio maintains internally. After you add\nthe service entry, the Envoy proxies can send traffic to the service as if it\nwas a service in your mesh. Configuring service entries allows you to manage\ntraffic for services running outside of the mesh, including the following tasks:\nRedirect and forward traffic for external destinations, such as APIs\nconsumed from the web, or traffic to services in legacy infrastructure.\nDefine\nretry\n,\ntimeout\n, and\nfault injection\npolicies for external destinations.\nRun a mesh service in a Virtual Machine (VM) by\nadding VMs to your mesh\n.\nYou don’t need to add a service entry for every external service that you want\nyour mesh services to use. By default, Istio configures the Envoy proxies to\npassthrough requests to unknown services. However, you can’t use Istio features\nto control the traffic to destinations that aren’t registered in the mesh.\nService entry example\nThe following example mesh-external service entry adds the\next-svc.example.com\nexternal dependency to Istio’s service registry:\napiVersion: networking.istio.io/v1\nkind: ServiceEntry\nmetadata:\n  name: svc-entry\nspec:\n  hosts:\n  - ext-svc.example.com\n  ports:\n  - number: 443\n    name: https\n    protocol: HTTPS\n  location: MESH_EXTERNAL\n  resolution: DNS\nYou specify the external resource using the\nhosts\nfield. You can qualify it\nfully or use a wildcard prefixed domain name.\nYou can configure virtual services and destination rules to control traffic to a\nservice entry in a more granular way, in the same way you configure traffic for\nany other service in the mesh. For example, the following destination rule\nadjusts the TCP connection timeout for requests to the\next-svc.example.com\nexternal service that we configured using the service entry:\napiVersion: networking.istio.io/v1\nkind: DestinationRule\nmetadata:\n  name: ext-res-dr\nspec:\n  host: ext-svc.example.com\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        connectTimeout: 1s\nSee the\nService Entry reference\nfor more possible configuration options.\nSidecars\nBy default, Istio configures every Envoy proxy to accept traffic on all the\nports of its associated workload, and to reach every workload in the mesh when\nforwarding traffic. You can use a\nsidecar\nconfiguration to do the following:\nFine-tune the set of ports and protocols that an Envoy proxy accepts.\nLimit the set of services that the Envoy proxy can reach.\nYou might want to limit sidecar reachability like this in larger applications,\nwhere having every proxy configured to reach every other service in the mesh can\npotentially affect mesh performance due to high memory usage.\nYou can specify that you want a sidecar configuration to apply to all workloads\nin a particular namespace, or choose specific workloads using a\nworkloadSelector\n. For example, the following sidecar configuration configures\nall services in the\nbookinfo\nnamespace to only reach services running in the\nsame namespace and the Istio control plane (needed by Istio’s\negress and telemetry features):\napiVersion: networking.istio.io/v1\nkind: Sidecar\nmetadata:\n  name: default\n  namespace: bookinfo\nspec:\n  egress:\n  - hosts:\n    - \"./*\"\n    - \"istio-system/*\"\nSee the\nSidecar reference\nfor more details.\nNetwork resilience and testing\nAs well as helping you direct traffic around your mesh, Istio provides opt-in\nfailure recovery and fault injection features that you can configure dynamically\nat runtime. Using these features helps your applications operate reliably,\nensuring that the service mesh can tolerate failing nodes and preventing\nlocalized failures from cascading to other nodes.\nTimeouts\nA timeout is the amount of time that an Envoy proxy should wait for replies from\na given service, ensuring that services don’t hang around waiting for replies\nindefinitely and that calls succeed or fail within a predictable timeframe. The\nEnvoy timeout for HTTP requests is disabled in Istio by default.\nFor some applications and services, Istio’s default timeout might not be\nappropriate. For example, a timeout that is too long could result in excessive\nlatency from waiting for replies from failing services, while a timeout that is\ntoo short could result in calls failing unnecessarily while waiting for an\noperation involving multiple services to return. To find and use your optimal timeout\nsettings, Istio lets you easily adjust timeouts dynamically on a per-service\nbasis using\nvirtual services\nwithout having to edit your\nservice code. Here’s a virtual service that specifies a 10 second timeout for\ncalls to the v1 subset of the ratings service:\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - route:\n    - destination:\n        host: ratings\n        subset: v1\n    timeout: 10s\nRetries\nA retry setting specifies the maximum number of times an Envoy proxy attempts to\nconnect to a service if the initial call fails. Retries can enhance service\navailability and application performance by making sure that calls don’t fail\npermanently because of transient problems such as a temporarily overloaded\nservice or network. The interval between retries (25ms+) is variable and\ndetermined automatically by Istio, preventing the called service from being\noverwhelmed with requests. The default retry behavior for HTTP requests is to\nretry twice before returning the error.\nLike timeouts, Istio’s default retry behavior might not suit your application\nneeds in terms of latency (too many retries to a failed service can slow things\ndown) or availability. Also like timeouts, you can adjust your retry settings on\na per-service basis in\nvirtual services\nwithout having to\ntouch your service code. You can also further refine your retry behavior by\nadding per-retry timeouts, specifying the amount of time you want to wait for\neach retry attempt to successfully connect to the service. The following example\nconfigures a maximum of 3 retries to connect to this service subset after an\ninitial call failure, each with a 2 second timeout.\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - route:\n    - destination:\n        host: ratings\n        subset: v1\n    retries:\n      attempts: 3\n      perTryTimeout: 2s\nCircuit breakers\nCircuit breakers are another useful mechanism Istio provides for creating\nresilient microservice-based applications. In a circuit breaker, you set limits\nfor calls to individual hosts within a service, such as the number of concurrent\nconnections or how many times calls to this host have failed. Once that limit\nhas been reached the circuit breaker “trips” and stops further connections to\nthat host. Using a circuit breaker pattern enables fast failure rather than\nclients trying to connect to an overloaded or failing host.\nAs circuit breaking applies to “real” mesh destinations in a load balancing\npool, you configure circuit breaker thresholds in\ndestination rules\n, with the settings applying to each\nindividual host in the service. The following example limits the number of\nconcurrent connections for the\nreviews\nservice workloads of the v1 subset to\n100:\napiVersion: networking.istio.io/v1\nkind: DestinationRule\nmetadata:\n  name: reviews\nspec:\n  host: reviews\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n    trafficPolicy:\n      connectionPool:\n        tcp:\n          maxConnections: 100\nYou can find out more about creating circuit breakers in\nCircuit Breaking\n.\nFault injection\nAfter you’ve configured your network, including failure recovery policies, you\ncan use Istio’s fault injection mechanisms to test the failure recovery capacity\nof your application as a whole. Fault injection is a testing method that\nintroduces errors into a system to ensure that it can withstand and recover from\nerror conditions. Using fault injection can be particularly useful to ensure\nthat your failure recovery policies aren’t incompatible or too restrictive,\npotentially resulting in critical services being unavailable.\nCurrently, the fault injection configuration can not be combined with retry or timeout configuration\non the same virtual service, see\nTraffic Management Problems\n.\nUnlike other mechanisms for introducing errors, such as delaying packets or\nkilling pods at the network layer, Istio lets you inject faults at the\napplication layer. This lets you inject more relevant failures, such as HTTP\nerror codes, to get more relevant results.\nYou can inject two types of faults, both configured using a\nvirtual service\n:\nDelays: Delays are timing failures. They mimic increased network latency or\nan overloaded upstream service.\nAborts: Aborts are crash failures. They mimic failures in upstream services.\nAborts usually manifest in the form of HTTP error codes or TCP connection\nfailures.\nFor example, this virtual service introduces a 5 second delay for 1 out of every 1000\nrequests to the\nratings\nservice.\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: ratings\nspec:\n  hosts:\n  - ratings\n  http:\n  - fault:\n      delay:\n        percentage:\n          value: 0.1\n        fixedDelay: 5s\n    route:\n    - destination:\n        host: ratings\n        subset: v1\nFor detailed instructions on how to configure delays and aborts, see\nFault Injection\n.\nWorking with your applications\nIstio failure recovery features are completely transparent to the\napplication. Applications don’t know if an Envoy sidecar proxy is handling\nfailures for a called service before returning a response. This means that\nif you are also setting failure recovery policies in your application code\nyou need to keep in mind that both work independently, and therefore might\nconflict. For example, suppose you can have two timeouts, one configured in\na virtual service and another in the application. The application sets a 2\nsecond timeout for an API call to a service. However, you configured a 3\nsecond timeout with 1 retry in your virtual service. In this case, the\napplication’s timeout kicks in first, so your Envoy timeout and retry\nattempt has no effect.\nWhile Istio failure recovery features improve the reliability and\navailability of services in the mesh, applications must handle the failure\nor errors and take appropriate fallback actions. For example, when all\ninstances in a load balancing pool have failed, Envoy returns an\nHTTP 503\ncode. The application must implement any fallback logic needed to handle the\nHTTP 503\nerror code.\nSee also\nRouting egress traffic to wildcard destinations\nA generic approach to set up egress gateways that can route traffic to a restricted set of target remote hosts dynamically, including wildcard domains.\nGetting started with the Kubernetes Gateway API\nUsing the Gateway API to configure ingress traffic for your Kubernetes cluster.\nExtending Gateway API support in Istio\nA standard API for service mesh, in Istio and in the broader community.\nDirect encrypted traffic from IBM Cloud Kubernetes Service Ingress to Istio Ingress Gateway\nConfigure the IBM Cloud Kubernetes Service Application Load Balancer to direct traffic to the Istio Ingress gateway with mutual TLS.\nMulticluster Istio configuration and service discovery using Admiral\nAutomating Istio configuration for Istio deployments (clusters) that work as a single mesh.\nIstio as a Proxy for External Services\nConfigure Istio ingress gateway to act as a proxy for external services.\nSecurity\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nIntroducing Istio traffic management\nVirtual services\nWhy use virtual services?\nVirtual service example\nThe hosts field\nRouting rules\nMatch condition\nDestination\nRouting rule precedence\nMore about routing rules\nDestination rules\nLoad balancing options\nDestination rule example\nGateways\nGateway example\nService entries\nService entry example\nSidecars\nNetwork resilience and testing\nTimeouts\nRetries\nCircuit breakers\nFault injection\nWorking with your applications\nSee also\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nConcepts\nSecurity\nSecurity\n23 minute read\nHigh-level architecture\nIstio identity\nIdentity and certificate management\nAuthentication\nMutual TLS authentication\nPermissive mode\nSecure naming\nAuthentication architecture\nAuthentication policies\nPolicy storage\nSelector field\nPeer authentication\nRequest authentication\nPrincipals\nUpdating authentication policies\nAuthorization\nAuthorization architecture\nImplicit enablement\nAuthorization policies\nPolicy Target\nValue matching\nExclusion matching\nallow-nothing\n,\ndeny-all\nand\nallow-all\npolicy\nCustom conditions\nAuthenticated and unauthenticated identity\nUsing Istio authorization on plain TCP protocols\nDependency on mutual TLS\nLearn more\nSee also\nBreaking down a monolithic application into atomic services offers various\nbenefits, including better agility, better scalability and better ability to\nreuse services. However, microservices also have particular security needs:\nTo defend against man-in-the-middle attacks, they need traffic encryption.\nTo provide flexible service access control, they need mutual TLS and\nfine-grained access policies.\nTo determine who did what at what time, they need auditing tools.\nIstio Security provides a comprehensive security solution to solve these issues.\nThis page gives an overview on how you can use Istio security features to secure\nyour services, wherever you run them. In particular, Istio security mitigates\nboth insider and external threats against your data, endpoints, communication,\nand platform.\nSecurity overview\nThe Istio security features provide strong identity, powerful policy,\ntransparent TLS encryption, and authentication, authorization and audit (AAA)\ntools to protect your services and data. The goals of Istio security are:\nSecurity by default: no changes needed to application code and\ninfrastructure\nDefense in depth: integrate with existing security systems to provide\nmultiple layers of defense\nZero-trust network: build security solutions on distrusted networks\nVisit our\nmutual TLS Migration docs\nto start using Istio security features with your deployed services. Visit our\nSecurity Tasks\nfor detailed\ninstructions to use the security features.\nHigh-level architecture\nSecurity in Istio involves multiple components:\nA Certificate Authority (CA) for key and certificate management\nThe configuration API server distributes to the proxies:\nauthentication policies\nauthorization policies\nsecure naming information\nSidecar and perimeter proxies work as\nPolicy Enforcement Points\n(PEPs) to secure communication between clients and servers.\nA set of Envoy proxy extensions to manage telemetry and auditing\nThe control plane handles configuration from the API server and\nconfigures the PEPs in the data plane. The PEPs are implemented using Envoy. The\nfollowing diagram shows the architecture.\nSecurity Architecture\nIn the following sections, we introduce the Istio security features in detail.\nIstio identity\nIdentity is a fundamental concept of any security infrastructure. At the\nbeginning of a workload-to-workload communication, the two parties must exchange\ncredentials with their identity information for mutual authentication purposes.\nOn the client side, the server’s identity is checked against the\nsecure naming\ninformation to see if it is an authorized runner of the workload. On the server\nside, the server can determine what information the client can access based on\nthe\nauthorization policies\n,\naudit who accessed what at what time, charge clients based on the workloads they\nused, and reject any clients who failed to pay their bill from accessing the\nworkloads.\nThe Istio identity model uses the first-class\nservice identity\nto\ndetermine the identity of a request’s origin. This model allows for great\nflexibility and granularity for service identities to represent a human user, an\nindividual workload, or a group of workloads. On platforms without a service\nidentity, Istio can use other identities that can group workload\ninstances, such as service names.\nThe following list shows examples of service identities that you can use on different\nplatforms:\nKubernetes: Kubernetes service account\nGCE: GCP service account\nOn-premises (non-Kubernetes): user account, custom service account,\nservice name, Istio service account, or GCP service account. The custom\nservice account refers to the existing service account just like the\nidentities that the customer’s Identity Directory manages.\nIdentity and certificate management\nIstio securely provisions strong identities\nto every workload with X.509 certificates. Istio agents, running alongside each Envoy proxy,\nwork together with\nistiod\nto automate key and certificate\nrotation at scale. The following diagram shows the identity\nprovisioning flow.\nIdentity Provisioning Workflow\nIstio provisions keys and certificates through the following flow:\nistiod\noffers a gRPC service to take\ncertificate signing requests\n(CSRs).\nWhen started, the Istio agent creates the private key\nand CSR, and then sends the CSR with its credentials to\nistiod\nfor signing.\nThe CA in\nistiod\nvalidates the credentials carried in the CSR.\nUpon successful validation, it signs the CSR to generate the certificate.\nWhen a workload is started, Envoy requests the certificate and key from the Istio agent in the\nsame container via the\nEnvoy secret discovery service (SDS)\nAPI.\nThe Istio agent sends the certificates received from\nistiod\nand the\nprivate key to Envoy via the Envoy SDS API.\nIstio agent monitors the expiration of the workload certificate.\nThe above process repeats periodically for certificate and key rotation.\nAuthentication\nIstio provides two types of authentication:\nPeer authentication: used for service-to-service authentication to verify\nthe client making the connection. Istio offers\nmutual\nTLS\nas a full stack\nsolution for transport authentication, which can be enabled without\nrequiring service code changes. This solution:\nProvides each service with a strong identity representing its role\nto enable interoperability across clusters and clouds.\nSecures service-to-service communication.\nProvides a key management system to automate key and certificate\ngeneration, distribution, and rotation.\nRequest authentication: Used for end-user authentication to verify the\ncredential attached to the request. Istio enables request-level\nauthentication with JSON Web Token (JWT) validation and a streamlined\ndeveloper experience using a custom authentication provider or any OpenID\nConnect providers, for example:\nORY Hydra\nKeycloak\nAuth0\nFirebase Auth\nGoogle Auth\nIn all cases, Istio stores the authentication policies in the\nIstio config store\nvia a custom Kubernetes API.\nIstiod\nkeeps them up-to-date for each proxy,\nalong with the keys where appropriate. Additionally, Istio supports\nauthentication in permissive mode to help you understand how a policy change can\naffect your security posture before it is enforced.\nMutual TLS authentication\nIstio tunnels service-to-service communication through the client- and\nserver-side PEPs, which are implemented as\nEnvoy\nproxies\n. When a workload sends a request\nto another workload using mutual TLS authentication, the request is handled as\nfollows:\nIstio re-routes the outbound traffic from a client to the client’s local\nsidecar Envoy.\nThe client side Envoy starts a mutual TLS handshake with the server side\nEnvoy. During the handshake, the client side Envoy also does a\nsecure naming\ncheck to verify that the service account presented in the server certificate\nis authorized to run the target service.\nThe client side Envoy and the server side Envoy establish a mutual TLS\nconnection, and Istio forwards the traffic from the client side Envoy to the\nserver side Envoy.\nThe server side Envoy authorizes the request. If authorized, it forwards the traffic to the\nbackend service through local TCP connections.\nIstio configures\nTLSv1_2\nas the minimum TLS version for both client and server with\nthe following cipher suites:\nECDHE-ECDSA-AES256-GCM-SHA384\nECDHE-RSA-AES256-GCM-SHA384\nECDHE-ECDSA-AES128-GCM-SHA256\nECDHE-RSA-AES128-GCM-SHA256\nAES256-GCM-SHA384\nAES128-GCM-SHA256\nPermissive mode\nIstio mutual TLS has a permissive mode, which allows a service to accept both\nplaintext traffic and mutual TLS traffic at the same time. This feature greatly\nimproves the mutual TLS onboarding experience.\nMany non-Istio clients communicating with a non-Istio server presents a problem\nfor an operator who wants to migrate that server to Istio with mutual TLS\nenabled. Commonly, the operator cannot install an Istio sidecar for all clients\nat the same time or does not even have the permissions to do so on some clients.\nEven after installing the Istio sidecar on the server, the operator cannot\nenable mutual TLS without breaking existing communications.\nWith the permissive mode enabled, the server accepts both plaintext and mutual\nTLS traffic. The mode provides greater flexibility for the on-boarding process.\nThe server’s installed Istio sidecar takes mutual TLS traffic immediately\nwithout breaking existing plaintext traffic. As a result, the operator can\ngradually install and configure the client’s Istio sidecars to send mutual TLS\ntraffic. Once the configuration of the clients is complete, the operator can\nconfigure the server to mutual TLS only mode. For more information, visit the\nMutual TLS Migration tutorial\n.\nSecure naming\nServer identities are encoded in certificates, but service names are retrieved\nthrough the discovery service or DNS. The secure naming information maps the\nserver identities to the service names. A mapping of identity\nA\nto service\nname\nB\nmeans “\nA\nis authorized to run service\nB\n”. The control plane watches\nthe\napiserver\n, generates the secure naming mappings, and distributes them\nsecurely to the PEPs. The following example explains why secure naming is\ncritical in authentication.\nSuppose the legitimate servers that run the service\ndatastore\nonly use the\ninfra-team\nidentity. A malicious user has the certificate and key for the\ntest-team\nidentity. The malicious user intends to impersonate the service to\ninspect the data sent from the clients. The malicious user deploys a forged\nserver with the certificate and key for the\ntest-team\nidentity. Suppose the\nmalicious user successfully hijacked (through DNS spoofing, BGP/route hijacking,\nARP spoofing, etc.) the traffic sent to the\ndatastore\nand redirected it to the\nforged server.\nWhen a client calls the\ndatastore\nservice, it extracts the\ntest-team\nidentity from the server’s certificate, and checks whether\ntest-team\nis\nallowed to run\ndatastore\nwith the secure naming information. The client\ndetects that\ntest-team\nis not allowed to run the\ndatastore\nservice and the\nauthentication fails.\nNote that, for non HTTP/HTTPS traffic, secure naming doesn’t protect from DNS spoofing,\nin which case the attacker modifies the destination IPs for the service.\nSince TCP traffic does not contain\nHost\ninformation and Envoy can only\nrely on the destination IP for routing, Envoy may route traffic to\nservices on the hijacked IPs. This DNS spoofing can happen even\nbefore the client-side Envoy receives the traffic.\nAuthentication architecture\nYou can specify authentication requirements for workloads receiving requests in\nan Istio mesh using peer and request authentication policies. The mesh operator\nuses\n.yaml\nfiles to specify the policies. The policies are saved in the Istio\nconfiguration storage once deployed. The Istio controller watches the\nconfiguration storage.\nUpon any policy changes, the new policy is translated to the appropriate\nconfiguration telling the PEP how to perform the required authentication\nmechanisms. The control plane may fetch the public key and attach it to the\nconfiguration for JWT validation. Alternatively, Istiod provides the path to the\nkeys and certificates the Istio system manages and installs them to the\napplication pod for mutual TLS. You can find more info in the\nIdentity and certificate management section\n.\nIstio sends configurations to the targeted endpoints asynchronously. Once the\nproxy receives the configuration, the new authentication requirement takes\neffect immediately on that pod.\nClient services, those that send requests, are responsible for following the\nnecessary authentication mechanism. For request authentication, the application is\nresponsible for acquiring and attaching the JWT credential to the request. For\npeer authentication, Istio automatically upgrades all traffic between two PEPs to mutual\nTLS. If authentication policies disable mutual TLS mode, Istio continues to use\nplain text between PEPs. To override this behavior explicitly disable mutual\nTLS mode with\ndestination rules\n.\nYou can find out more about how mutual TLS works in the\nMutual TLS authentication section\n.\nAuthentication Architecture\nIstio outputs identities with both types of authentication, as well as other\nclaims in the credential if applicable, to the next layer:\nauthorization\n.\nAuthentication policies\nThis section provides more details about how Istio authentication policies work.\nAs you’ll remember from the\nArchitecture section\n,\nauthentication policies apply to requests that a service receives. To specify\nclient-side authentication rules in mutual TLS, you need to specify the\nTLSSettings\nin the\nDestinationRule\n. You can find more information in our\nTLS settings reference docs\n.\nLike other Istio configurations, you can specify authentication policies in\n.yaml\nfiles. You deploy policies using\nkubectl\n.\nThe following example authentication policy specifies that transport\nauthentication for the workloads with the\napp:reviews\nlabel must use mutual\nTLS:\napiVersion: security.istio.io/v1\nkind: PeerAuthentication\nmetadata:\n  name: \"example-peer-policy\"\n  namespace: \"foo\"\nspec:\n  selector:\n    matchLabels:\n      app: reviews\n  mtls:\n    mode: STRICT\nPolicy storage\nIstio stores mesh-scope policies in the root namespace. These policies have an\nempty selector apply to all workloads in the mesh. Policies that have a\nnamespace scope are stored in the corresponding namespace. They only apply to\nworkloads within their namespace. If you configure a\nselector\nfield, the\nauthentication policy only applies to workloads matching the conditions you\nconfigured.\nPeer and request authentication policies are stored separately by kind,\nPeerAuthentication\nand\nRequestAuthentication\nrespectively.\nSelector field\nPeer and request authentication policies use\nselector\nfields to specify the\nlabel of the workloads to which the policy applies. The following example shows\nthe selector field of a policy that applies to workloads with the\napp:product-page\nlabel:\nselector:\n  matchLabels:\n    app: product-page\nIf you don’t provide a value for the\nselector\nfield, Istio matches the policy\nto all workloads in the storage scope of the policy. Thus, the\nselector\nfields\nhelp you specify the scope of the policies:\nMesh-wide policy: A policy specified for the root namespace without or\nwith an empty\nselector\nfield.\nNamespace-wide policy: A policy specified for a non-root namespace without\nor with an empty\nselector\nfield.\nWorkload-specific policy: a policy defined in the regular namespace, with\nnon-empty selector field.\nPeer and request authentication policies follow the same hierarchy principles\nfor the\nselector\nfields, but Istio combines and applies them in slightly\ndifferent ways.\nThere can be only one mesh-wide peer authentication policy, and only one\nnamespace-wide peer authentication policy per namespace. When you configure\nmultiple mesh- or namespace-wide peer authentication policies for the same mesh\nor namespace, Istio ignores the newer policies. When more than one\nworkload-specific peer authentication policy matches, Istio picks the oldest\none.\nIstio applies the narrowest matching policy for each workload using the\nfollowing order:\nworkload-specific\nnamespace-wide\nmesh-wide\nIstio can combine all matching request authentication policies to work as if\nthey come from a single request authentication policy. Thus, you can have\nmultiple mesh-wide or namespace-wide policies in a mesh or namespace. However,\nit is still a good practice to avoid having multiple mesh-wide or namespace-wide\nrequest authentication policies.\nPeer authentication\nPeer authentication policies specify the mutual TLS mode Istio enforces on\ntarget workloads. The following modes are supported:\nPERMISSIVE: Workloads accept both mutual TLS and plain text traffic. This\nmode is most useful during migrations when workloads without sidecar cannot\nuse mutual TLS. Once workloads are migrated with sidecar injection, you should\nswitch the mode to STRICT.\nSTRICT: Workloads only accept mutual TLS traffic.\nDISABLE: Mutual TLS is disabled. From a security perspective, you\nshouldn’t use this mode unless you provide your own security solution.\nWhen the mode is unset, the mode of the parent scope is inherited. Mesh-wide\npeer authentication policies with an unset mode use the\nPERMISSIVE\nmode by\ndefault.\nThe following peer authentication policy requires all workloads in namespace\nfoo\nto use mutual TLS:\napiVersion: security.istio.io/v1\nkind: PeerAuthentication\nmetadata:\n  name: \"example-policy\"\n  namespace: \"foo\"\nspec:\n  mtls:\n    mode: STRICT\nWith workload-specific peer authentication policies, you can specify different\nmutual TLS modes for different ports. You can only use ports that workloads have\nclaimed for port-wide mutual TLS configuration. The following example disables\nmutual TLS on port\n80\nfor the\napp:example-app\nworkload, and uses the mutual TLS\nsettings of the namespace-wide peer authentication policy for all other ports:\napiVersion: security.istio.io/v1\nkind: PeerAuthentication\nmetadata:\n  name: \"example-workload-policy\"\n  namespace: \"foo\"\nspec:\n  selector:\n     matchLabels:\n       app: example-app\n  portLevelMtls:\n    80:\n      mode: DISABLE\nThe peer authentication policy above works only because the service\nconfiguration below bound the requests from the\nexample-app\nworkload to port\n80\nof the\nexample-service\n:\napiVersion: v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: foo\nspec:\n  ports:\n  - name: http\n    port: 8000\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: example-app\nRequest authentication\nRequest authentication policies specify the values needed to validate a JSON Web\nToken (JWT). These values include, among others, the following:\nThe location of the token in the request\nThe issuer or the request\nThe public JSON Web Key Set (JWKS)\nIstio checks the presented token, if presented against the rules in the request\nauthentication policy, and rejects requests with invalid tokens. When requests\ncarry no token, they are accepted by default. To reject requests without tokens,\nprovide authorization rules that specify the restrictions for specific\noperations, for example paths or actions.\nRequest authentication policies can specify more than one JWT if each uses a\nunique location. When more than one policy matches a workload, Istio combines\nall rules as if they were specified as a single policy. This behavior is useful\nto program workloads to accept JWT from different providers. However, requests\nwith more than one valid JWT are not supported because the output principal of\nsuch requests is undefined.\nPrincipals\nWhen you use peer authentication policies and mutual TLS, Istio extracts the\nidentity from the peer authentication into the\nsource.principal\n. Similarly,\nwhen you use request authentication policies, Istio assigns the identity from\nthe JWT to the\nrequest.auth.principal\n. Use these principals to set\nauthorization policies and as telemetry output.\nUpdating authentication policies\nYou can change an authentication policy at any time and Istio pushes the new\npolicies to the workloads almost in real time. However, Istio can’t guarantee\nthat all workloads receive the new policy at the same time. The following\nrecommendations help avoid disruption when updating your authentication\npolicies:\nUse intermediate peer authentication policies using the\nPERMISSIVE\nmode\nwhen changing the mode from\nDISABLE\nto\nSTRICT\nand vice-versa. When all\nworkloads switch successfully to the desired mode, you can apply the policy\nwith the final mode. You can use Istio telemetry to verify that workloads\nhave switched successfully.\nWhen migrating request authentication policies from one JWT to another, add\nthe rule for the new JWT to the policy without removing the old rule.\nWorkloads then accept both types of JWT, and you can remove the old rule\nwhen all traffic switches to the new JWT. However, each JWT has to use a\ndifferent location.\nAuthorization\nIstio’s authorization features provide mesh-, namespace-, and workload-wide\naccess control for your workloads in the mesh. This level of control provides\nthe following benefits:\nWorkload-to-workload and end-user-to-workload authorization.\nA simple API: it includes a single\nAuthorizationPolicy\nCRD\n,\nwhich is easy to use and maintain.\nFlexible semantics: operators can define custom conditions on Istio attributes, and use CUSTOM, DENY and ALLOW actions.\nHigh performance: Istio authorization (\nALLOW\nand\nDENY\n) is enforced natively on Envoy.\nHigh compatibility: supports gRPC, HTTP, HTTPS and HTTP/2 natively, as well as any plain TCP protocols.\nAuthorization architecture\nThe authorization policy enforces access control to the inbound traffic in the\nserver side Envoy proxy. Each Envoy proxy runs an authorization engine that authorizes requests at\nruntime. When a request comes to the proxy, the authorization engine evaluates\nthe request context against the current authorization policies, and returns the\nauthorization result, either\nALLOW\nor\nDENY\n. Operators specify Istio\nauthorization policies using\n.yaml\nfiles.\nAuthorization Architecture\nImplicit enablement\nYou don’t need to explicitly enable Istio’s authorization features; they are available after installation.\nTo enforce access control to your workloads, you apply an authorization policy.\nFor workloads without authorization policies applied, Istio allows all requests.\nAuthorization policies support\nALLOW\n,\nDENY\nand\nCUSTOM\nactions. You can apply multiple policies, each with a\ndifferent action, as needed to secure access to your workloads.\nIstio checks for matching policies in layers, in this order:\nCUSTOM\n,\nDENY\n, and then\nALLOW\n. For each type of action,\nIstio first checks if there is a policy with the action applied, and then checks if the request matches the policy’s\nspecification. If a request doesn’t match a policy in one of the layers, the check continues to the next layer.\nThe following graph shows the policy precedence in detail:\nAuthorization Policy Precedence\nWhen you apply multiple authorization policies to the same workload, Istio applies them additively.\nAuthorization policies\nTo configure an authorization policy, you create an\nAuthorizationPolicy\ncustom resource\n.\nAn authorization policy includes a selector, an action, and a list of rules:\nThe\nselector\nfield specifies the target of the policy\nThe\naction\nfield specifies whether to allow or deny the request\nThe\nrules\nspecify when to trigger the action\nThe\nfrom\nfield in the\nrules\nspecifies the sources of the request\nThe\nto\nfield in the\nrules\nspecifies the operations of the request\nThe\nwhen\nfield specifies the conditions needed to apply the rule\nThe following example shows an authorization policy that allows two sources, the\ncluster.local/ns/default/sa/curl\nservice account and the\ndev\nnamespace, to\naccess the workloads with the\napp: httpbin\nand\nversion: v1\nlabels in the\nfoo\nnamespace when requests sent have a valid JWT token.\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n name: httpbin\n namespace: foo\nspec:\n selector:\n   matchLabels:\n     app: httpbin\n     version: v1\n action: ALLOW\n rules:\n - from:\n   - source:\n       principals: [\"cluster.local/ns/default/sa/curl\"]\n   - source:\n       namespaces: [\"dev\"]\n   to:\n   - operation:\n       methods: [\"GET\"]\n   when:\n   - key: request.auth.claims[iss]\n     values: [\"https://accounts.google.com\"]\nThe following example shows an authorization policy that denies requests if the\nsource is not the\nfoo\nnamespace:\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n name: httpbin-deny\n namespace: foo\nspec:\n selector:\n   matchLabels:\n     app: httpbin\n     version: v1\n action: DENY\n rules:\n - from:\n   - source:\n       notNamespaces: [\"foo\"]\nThe deny policy takes precedence over the allow policy. Requests matching allow\npolicies can be denied if they match a deny policy. Istio evaluates deny\npolicies first to ensure that an allow policy can’t bypass a deny policy.\nPolicy Target\nYou can specify a policy’s scope or target with the\nmetadata/namespace\nfield and an optional\nselector\nfield.\nA policy applies to the namespace in the\nmetadata/namespace\nfield. If\nset its value to the root namespace, the policy applies to all namespaces in a\nmesh. The value of the root namespace is configurable, and the default is\nistio-system\n. If set to any other namespace, the policy only applies to the\nspecified namespace.\nYou can use a\nselector\nfield to further restrict policies to apply to specific\nworkloads. The\nselector\nuses labels to select the target workload. The\nselector contains a list of\n{key: value}\npairs, where the\nkey\nis the name of\nthe label. If not set, the authorization policy applies to all workloads in the\nsame namespace as the authorization policy.\nFor example, the\nallow-read\npolicy allows\n\"GET\"\nand\n\"HEAD\"\naccess to the\nworkload with the\napp: products\nlabel in the\ndefault\nnamespace.\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: allow-read\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: products\n  action: ALLOW\n  rules:\n  - to:\n    - operation:\n         methods: [\"GET\", \"HEAD\"]\nValue matching\nMost fields in authorization policies support all the following matching\nschemas:\nExact match: exact string match.\nPrefix match: a string with an ending\n\"*\"\n. For example,\n\"test.abc.*\"\nmatches\n\"test.abc.com\"\n,\n\"test.abc.com.cn\"\n,\n\"test.abc.org\"\n, etc.\nSuffix match: a string with a starting\n\"*\"\n. For example,\n\"*.abc.com\"\nmatches\n\"eng.abc.com\"\n,\n\"test.eng.abc.com\"\n, etc.\nPresence match:\n*\nis used to specify anything but not empty. To specify\nthat a field must be present, use the\nfieldname: [\"*\"]\nformat. This is\ndifferent from leaving a field unspecified, which means match anything,\nincluding empty.\nThere are a few exceptions. For example, the following fields only support exact\nmatch:\nThe\nkey\nfield under the\nwhen\nsection\nThe\nipBlocks\nunder the\nsource\nsection\nThe\nports\nfield under the\nto\nsection\nThe following example policy allows access at paths with the\n/test/*\nprefix\nor the\n*/info\nsuffix.\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: tester\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: products\n  action: ALLOW\n  rules:\n  - to:\n    - operation:\n        paths: [\"/test/*\", \"*/info\"]\nExclusion matching\nTo match negative conditions like\nnotValues\nin the\nwhen\nfield,\nnotIpBlocks\nin the\nsource\nfield,\nnotPorts\nin the\nto\nfield, Istio supports exclusion\nmatching.\nThe following example requires a valid request principals, which is derived from\nJWT authentication, if the request path is not\n/healthz\n. Thus, the policy\nexcludes requests to the\n/healthz\npath from the JWT authentication:\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: disable-jwt-for-healthz\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: products\n  action: ALLOW\n  rules:\n  - to:\n    - operation:\n        notPaths: [\"/healthz\"]\n    from:\n    - source:\n        requestPrincipals: [\"*\"]\nThe following example denies the request to the\n/admin\npath for requests\nwithout request principals:\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: enable-jwt-for-admin\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: products\n  action: DENY\n  rules:\n  - to:\n    - operation:\n        paths: [\"/admin\"]\n    from:\n    - source:\n        notRequestPrincipals: [\"*\"]\nallow-nothing\n,\ndeny-all\nand\nallow-all\npolicy\nThe following example shows an\nALLOW\npolicy that matches nothing. If there are no other\nALLOW\npolicies, requests\nwill always be denied because of the “deny by default” behavior.\nNote the “deny by default” behavior applies only if the workload has at least one authorization policy with the\nALLOW\naction.\nIt is a good security practice to start with the\nallow-nothing\npolicy and incrementally add more\nALLOW\npolicies to open more\naccess to the workload.\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: allow-nothing\nspec:\n  action: ALLOW\n  # the rules field is not specified, and the policy will never match.\nThe following example shows a\nDENY\npolicy that explicitly denies all access. It will always deny the request even if\nthere is another\nALLOW\npolicy allowing the request because the\nDENY\npolicy takes precedence over the\nALLOW\npolicy.\nThis is useful if you want to temporarily disable all access to the workload.\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: deny-all\nspec:\n  action: DENY\n  # the rules field has an empty rule, and the policy will always match.\n  rules:\n  - {}\nThe following example shows an\nALLOW\npolicy that allows full access to the workload. It will make other\nALLOW\npolicies\nuseless as it will always allow the request. It might be useful if you want to temporarily expose full access to the\nworkload. Note the request could still be denied due to\nCUSTOM\nand\nDENY\npolicies.\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: allow-all\nspec:\n  action: ALLOW\n  # This matches everything.\n  rules:\n  - {}\nCustom conditions\nYou can also use the\nwhen\nsection to specify additional conditions. For\nexample, the following\nAuthorizationPolicy\ndefinition includes a condition\nthat\nrequest.headers[version]\nis either\n\"v1\"\nor\n\"v2\"\n. In this case, the\nkey is\nrequest.headers[version]\n, which is an entry in the Istio attribute\nrequest.headers\n, which is a map.\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n name: httpbin\n namespace: foo\nspec:\n selector:\n   matchLabels:\n     app: httpbin\n     version: v1\n action: ALLOW\n rules:\n - from:\n   - source:\n       principals: [\"cluster.local/ns/default/sa/curl\"]\n   to:\n   - operation:\n       methods: [\"GET\"]\n   when:\n   - key: request.headers[version]\n     values: [\"v1\", \"v2\"]\nThe supported\nkey\nvalues of a condition are listed on the\nconditions page\n.\nAuthenticated and unauthenticated identity\nIf you want to make a workload publicly accessible, you need to leave the\nsource\nsection empty. This allows sources from all (both authenticated and\nunauthenticated) users and workloads, for example:\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n name: httpbin\n namespace: foo\nspec:\n selector:\n   matchLabels:\n     app: httpbin\n     version: v1\n action: ALLOW\n rules:\n - to:\n   - operation:\n       methods: [\"GET\", \"POST\"]\nTo allow only authenticated users, set\nprincipals\nto\n\"*\"\ninstead, for\nexample:\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n name: httpbin\n namespace: foo\nspec:\n selector:\n   matchLabels:\n     app: httpbin\n     version: v1\n action: ALLOW\n rules:\n - from:\n   - source:\n       principals: [\"*\"]\n   to:\n   - operation:\n       methods: [\"GET\", \"POST\"]\nUsing Istio authorization on plain TCP protocols\nIstio authorization supports workloads using any plain TCP protocols, such as\nMongoDB. In this case, you configure the authorization policy in the same way\nyou did for the HTTP workloads. The difference is that certain fields and\nconditions are only applicable to HTTP workloads. These fields include:\nThe\nrequest_principals\nfield in the source section of the authorization policy object\nThe\nhosts\n,\nmethods\nand\npaths\nfields in the operation section of the authorization policy object\nThe supported conditions are listed in the\nconditions page\n.\nIf you use any HTTP only fields for a TCP workload, Istio will ignore HTTP-only\nfields in the authorization policy.\nAssuming you have a MongoDB service on port\n27017\n, the following example\nconfigures an authorization policy to only allows the\nbookinfo-ratings-v2\nservice in the Istio mesh to access the MongoDB workload.\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: mongodb-policy\n  namespace: default\nspec:\n selector:\n   matchLabels:\n     app: mongodb\n action: ALLOW\n rules:\n - from:\n   - source:\n       principals: [\"cluster.local/ns/default/sa/bookinfo-ratings-v2\"]\n   to:\n   - operation:\n       ports: [\"27017\"]\nDependency on mutual TLS\nIstio uses mutual TLS to securely pass some information from the client to the server. Mutual TLS must be enabled before\nusing any of the following fields in the authorization policy:\nthe\nprincipals\nand\nnotPrincipals\nfield under the\nsource\nsection\nthe\nnamespaces\nand\nnotNamespaces\nfield under the\nsource\nsection\nthe\nsource.principal\ncustom condition\nthe\nsource.namespace\ncustom condition\nNote it is strongly recommended to always use these fields with\nstrict\nmutual TLS mode in the\nPeerAuthentication\nto avoid\npotential unexpected requests rejection or policy bypass when plain text traffic is used with the permissive mutual TLS mode.\nCheck the\nsecurity advisory\nfor more details and alternatives if you cannot enable\nstrict mutual TLS mode.\nLearn more\nAfter learning the basic concepts, there are more resources to review:\nTry out the security policy by following the\nauthentication\nand\nauthorization\ntasks.\nLearn some security\npolicy examples\nthat could be\nused to improve security in your mesh.\nRead\ncommon problems\nto better troubleshoot security policy issues\nwhen something goes wrong.\nSee also\nDry Run\nShows how to dry-run an authorization policy without enforcing it.\nExplicit Deny\nShows how to set up access control to deny traffic explicitly.\nExternal Authorization\nShows how to integrate and delegate access control to an external authorization system.\nHTTP Traffic\nShows how to set up access control for HTTP traffic.\nIngress Access Control\nShows how to set up access control on an ingress gateway.\nTCP Traffic\nShows how to set up access control for TCP traffic.\nTraffic Management\nObservability\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nHigh-level architecture\nIstio identity\nIdentity and certificate management\nAuthentication\nMutual TLS authentication\nPermissive mode\nSecure naming\nAuthentication architecture\nAuthentication policies\nPolicy storage\nSelector field\nPeer authentication\nRequest authentication\nPrincipals\nUpdating authentication policies\nAuthorization\nAuthorization architecture\nImplicit enablement\nAuthorization policies\nPolicy Target\nValue matching\nExclusion matching\nallow-nothing\n,\ndeny-all\nand\nallow-all\npolicy\nCustom conditions\nAuthenticated and unauthenticated identity\nUsing Istio authorization on plain TCP protocols\nDependency on mutual TLS\nLearn more\nSee also\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nConcepts\nObservability\nObservability\n5 minute read\nMetrics\nProxy-level metrics\nService-level metrics\nControl plane metrics\nDistributed traces\nAccess logs\nSee also\nIstio generates detailed telemetry for all service communications within a mesh. This telemetry provides\nobservability\nof service behavior,\nempowering operators to troubleshoot, maintain, and optimize their applications – without imposing any additional burdens on service developers. Through\nIstio, operators gain a thorough understanding of how monitored services are interacting, both with other services and with the Istio components themselves.\nIstio generates the following types of telemetry in order to provide overall service mesh observability:\nMetrics\n. Istio generates a set of service metrics based on the four “golden signals” of monitoring (latency, traffic, errors, and\nsaturation). Istio also provides detailed metrics for the\nmesh control plane\n.\nA default set of mesh monitoring dashboards built on top of these metrics is also provided.\nDistributed Traces\n. Istio generates distributed trace spans for each service, providing operators with a detailed understanding\nof call flows and service dependencies within a mesh.\nAccess Logs\n. As traffic flows into a service within a mesh, Istio can generate a full record of each request, including source and\ndestination metadata. This information enables operators to audit service behavior down to the individual\nworkload instance\nlevel.\nMetrics\nMetrics provide a way of monitoring and understanding behavior in aggregate.\nTo monitor service behavior, Istio generates metrics for all service traffic in, out, and within an Istio service mesh. These metrics provide information on\nbehaviors such as the overall volume of traffic, the error rates within the traffic, and the response times for requests.\nIn addition to monitoring the behavior of services within a mesh, it is also important to monitor the behavior of the mesh itself. Istio components export\nmetrics on their own internal behaviors to provide insight on the health and function of the mesh control plane.\nProxy-level metrics\nIstio metrics collection begins with the sidecar proxies (Envoy). Each proxy generates a rich set of metrics about all traffic passing through the proxy (both\ninbound and outbound). The proxies also provide detailed statistics about the administrative functions of the proxy itself, including configuration and health\ninformation.\nEnvoy-generated metrics provide monitoring of the mesh at the granularity of Envoy resources (such as listeners and clusters). As a result, understanding the\nconnection between mesh services and Envoy resources is required for monitoring the Envoy metrics.\nIstio enables operators to select which of the Envoy metrics are generated and collected at each workload instance. By default, Istio enables only a small\nsubset of the Envoy-generated statistics to avoid overwhelming metrics backends and to reduce the CPU overhead associated with metrics collection. However,\noperators can easily expand the set of collected proxy metrics when required. This enables targeted debugging of networking behavior, while reducing the\noverall cost of monitoring across the mesh.\nThe\nEnvoy documentation site\nincludes a detailed overview of\nEnvoy statistics collection\n.\nThe operations guide on\nEnvoy Statistics\nprovides more information on controlling the generation of proxy-level metrics.\nExample proxy-level Metrics:\nenvoy_cluster_internal_upstream_rq{response_code_class=\"2xx\",cluster_name=\"xds-grpc\"} 7163\n\nenvoy_cluster_upstream_rq_completed{cluster_name=\"xds-grpc\"} 7164\n\nenvoy_cluster_ssl_connection_error{cluster_name=\"xds-grpc\"} 0\n\nenvoy_cluster_lb_subsets_removed{cluster_name=\"xds-grpc\"} 0\n\nenvoy_cluster_internal_upstream_rq{response_code=\"503\",cluster_name=\"xds-grpc\"} 1\nService-level metrics\nIn addition to the proxy-level metrics, Istio provides a set of service-oriented metrics for monitoring service communications. These metrics cover the four\nbasic service monitoring needs: latency, traffic, errors, and saturation. Istio ships with a default set of\ndashboards\nfor monitoring service behaviors based on these metrics.\nThe\nstandard Istio metrics\nare\nexported to\nPrometheus\nby default.\nUse of the service-level metrics is entirely optional. Operators may choose to turn off generation and collection of these metrics to meet their individual\nneeds.\nExample service-level metric:\nistio_requests_total{\n  connection_security_policy=\"mutual_tls\",\n  destination_app=\"details\",\n  destination_canonical_service=\"details\",\n  destination_canonical_revision=\"v1\",\n  destination_principal=\"cluster.local/ns/default/sa/default\",\n  destination_service=\"details.default.svc.cluster.local\",\n  destination_service_name=\"details\",\n  destination_service_namespace=\"default\",\n  destination_version=\"v1\",\n  destination_workload=\"details-v1\",\n  destination_workload_namespace=\"default\",\n  reporter=\"destination\",\n  request_protocol=\"http\",\n  response_code=\"200\",\n  response_flags=\"-\",\n  source_app=\"productpage\",\n  source_canonical_service=\"productpage\",\n  source_canonical_revision=\"v1\",\n  source_principal=\"cluster.local/ns/default/sa/default\",\n  source_version=\"v1\",\n  source_workload=\"productpage-v1\",\n  source_workload_namespace=\"default\"\n} 214\nControl plane metrics\nThe Istio control plane also provides a collection of self-monitoring metrics. These metrics allow monitoring of the behavior\nof Istio itself (as distinct from that of the services within the mesh).\nFor more information on which metrics are maintained, please refer to the\nreference documentation\n.\nDistributed traces\nDistributed tracing provides a way to monitor and understand behavior by monitoring individual requests as they flow through a mesh.\nTraces empower mesh operators to understand service dependencies and the sources of latency within their service mesh.\nIstio supports distributed tracing through the Envoy proxies. The proxies automatically generate trace spans on behalf of the applications they proxy,\nrequiring only that the applications forward the appropriate request context.\nIstio supports a number of tracing backends, including\nZipkin\n,\nJaeger\n, and many tools and services that support\nOpenTelemetry\n. Operators control the sampling rate for trace generation (that is, the rate at\nwhich tracing data is generated per request). This allows operators to control the amount and rate of tracing data being produced for their mesh.\nMore information about Distributed Tracing with Istio is found in our\nFAQ on Distributed Tracing\n.\nExample Istio-generated distributed trace for a single request:\nDistributed Trace for a single request\nAccess logs\nAccess logs provide a way to monitor and understand behavior from the perspective of an individual workload instance.\nIstio can generate access logs for service traffic in a configurable set of formats, providing operators with full control of the how, what, when and where of\nlogging. For more information, please refer to\nGetting Envoy’s Access Logs\n.\nExample Istio access log:\n[2019-03-06T09:31:27.360Z] \"GET /status/418 HTTP/1.1\" 418 - \"-\" 0 135 5 2 \"-\" \"curl/7.60.0\" \"d209e46f-9ed5-9b61-bbdd-43e22662702a\" \"httpbin:8000\" \"127.0.0.1:80\" inbound|8000|http|httpbin.default.svc.cluster.local - 172.30.146.73:80 172.30.146.82:38618 outbound_.8000_._.httpbin.default.svc.cluster.local\nSee also\nApache SkyWalking\nLearn how to configure the proxies to send tracing requests to Apache SkyWalking.\nClassifying Metrics Based on Request or Response\nThis task shows you how to improve telemetry by grouping requests and responses by their type.\nCollecting Metrics for TCP Services\nThis task shows you how to configure Istio to collect metrics for TCP services.\nConfigure access logs with Telemetry API\nThis task shows you how to configure Envoy proxies to send access logs with Telemetry API.\nConfigure trace sampling\nLearn the different approaches on how to configure trace sampling on the proxies.\nConfigure tracing using MeshConfig and pod annotations\nHow to configure tracing options using MeshConfig and pod annotations.\nSecurity\nExtensibility\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nMetrics\nProxy-level metrics\nService-level metrics\nControl plane metrics\nDistributed traces\nAccess logs\nSee also\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nConcepts\nExtensibility\nExtensibility\n2 minute read\nHigh-level architecture\nExample\nEcosystem\nSee also\nWebAssembly is a sandboxing technology which can be used to extend the Istio proxy (Envoy). The Proxy-Wasm sandbox API replaces Mixer as the primary extension mechanism in Istio.\nWebAssembly sandbox goals:\nEfficiency\n- An extension adds low latency, CPU, and memory overhead.\nFunction\n- An extension can enforce policy, collect telemetry, and perform payload mutations.\nIsolation\n- A programming error or crash in one plugin doesn’t affect other plugins.\nConfiguration\n- The plugins are configured using an API that is consistent with other Istio APIs. An extension can be configured dynamically.\nOperator\n- An extension can be canaried and deployed as log-only, fail-open or fail-close.\nExtension developer\n- The plugin can be written in several programming languages.\nThis\nvideo talk\nis an introduction about architecture of WebAssembly integration.\nHigh-level architecture\nIstio extensions (Proxy-Wasm plugins) have several components:\nFilter Service Provider Interface (SPI)\nfor building Proxy-Wasm plugins for filters.\nSandbox\nV8 Wasm Runtime embedded in Envoy.\nHost APIs\nfor headers, trailers and metadata.\nCall out APIs\nfor gRPC and HTTP calls.\nStats and Logging APIs\nfor metrics and monitoring.\nExtending Istio/Envoy\nExample\nAn example C++ Proxy-Wasm plugin for a filter can be found\nhere\n.\nYou can follow\nthis guide\nto implement a Wasm extension with C++.\nEcosystem\nIstio Ecosystem Wasm Extensions\nProxy-Wasm ABI specification\nProxy-Wasm C++ SDK\nProxy-Wasm Rust SDK\nProxy-Wasm AssemblyScript SDK\nWebAssembly Hub\nWebAssembly Extensions For Network Proxies (video)\nSee also\nAnnouncing the alpha availability of WebAssembly Plugins\nIntroduction to the new Wasm Plugin API and updates to the Wasm-based plugin support in Envoy and Istio.\nIstio and Envoy WebAssembly Extensibility, One Year On\nAn update on Envoy and Istio's WebAssembly-based extensibility effort.\nExtended and Improved WebAssemblyHub to Bring the Power of WebAssembly to Envoy and Istio\nCommunity partner tooling of Wasm for Istio by Solo.io.\nDeclarative WebAssembly deployment for Istio\nConfiguring Wasm extensions for Envoy and Istio declaratively.\nRedefining extensibility in proxies - introducing WebAssembly to Envoy and Istio\nThe future of Istio extensibility using WASM.\nObservability\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nHigh-level architecture\nExample\nEcosystem\nSee also\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nSidecar Mode\nInformation for setting up and operating Istio in sidecar mode.\nGetting Started\nTry Istio’s features quickly and easily.\nPlatform Setup\nHow to prepare various Kubernetes platforms before installing Istio.\nInstallation Guides\nChoose the guide that best suits your needs and platform.\nUpgrade Istio\nUpgrade, downgrade, and manage Istio across multiple control plane revisions.\nMore Guides\nMore information on additional setup tasks.\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nGetting Started\nGetting Started\n7 minute read\npage test\nDownload Istio\nInstall Istio\nInstall the Kubernetes Gateway API CRDs\nDeploy the sample application\nOpen the application to outside traffic\nAccess the application\nView the dashboard\nNext steps\nJoin the Istio community\nUninstall\nSee also\nWant to explore Istio’s\nambient mode\n? Visit the\nGetting Started with Ambient Mode\nguide!\nThis guide lets you quickly evaluate Istio. If you are already familiar with\nIstio or interested in installing other configuration profiles or\nadvanced\ndeployment models\n, refer to our\nwhich Istio installation method should I use?\nFAQ page.\nYou will need a Kubernetes cluster to proceed. If you don’t have a cluster, you can use\nkind\nor any other\nsupported Kubernetes platform\n.\nFollow these steps to get started with Istio:\nDownload and install Istio\nInstall the Kubernetes Gateway API CRDs\nDeploy the sample application\nOpen the application to outside traffic\nView the dashboard\nDownload Istio\nGo to the\nIstio release\npage to\ndownload the installation file for your OS, or\ndownload and\nextract the latest release automatically\n(Linux or macOS):\n$ curl -L https://istio.io/downloadIstio | sh -\nMove to the Istio package directory. For example, if the package is\nistio-1.26.2\n:\n$ cd istio-1.26.2\nThe installation directory contains:\nSample applications in\nsamples/\nThe\nistioctl\nclient binary in the\nbin/\ndirectory.\nAdd the\nistioctl\nclient to your path (Linux or macOS):\n$ export PATH=$PWD/bin:$PATH\nInstall Istio\nFor this guide, we use the\ndemo\nconfiguration profile\n. It is\nselected to have a good set of defaults for testing, but there are other\nprofiles for production, performance testing or\nOpenShift\n.\nUnlike\nIstio Gateways\n, creating\nKubernetes Gateways\nwill, by default, also\ndeploy gateway proxy servers\n.\nBecause they won’t be used, we disable the deployment of the default Istio gateway services that\nare normally installed as part of the\ndemo\nprofile.\nInstall Istio using the\ndemo\nprofile, without any gateways:\nZip\n$ istioctl install -f @samples/bookinfo/demo-profile-no-gateways.yaml@ -y\n✔ Istio core installed\n✔ Istiod installed\n✔ Installation complete\nMade this installation the default for injection and validation.\nAdd a namespace label to instruct Istio to automatically inject Envoy\nsidecar proxies when you deploy your application later:\n$ kubectl label namespace default istio-injection=enabled\nnamespace/default labeled\nInstall the Kubernetes Gateway API CRDs\nThe Kubernetes Gateway API CRDs do not come installed by default on most Kubernetes clusters, so make sure they are\ninstalled before using the Gateway API.\nInstall the Gateway API CRDs, if they are not already present:\n$ kubectl get crd gateways.gateway.networking.k8s.io &> /dev/null || \\\n{ kubectl kustomize \"github.com/kubernetes-sigs/gateway-api/config/crd?ref=v1.3.0\" | kubectl apply -f -; }\nDeploy the sample application\nYou have configured Istio to inject sidecar containers into any application you deploy in your\ndefault\nnamespace.\nDeploy the\nBookinfo\nsample application\n:\nZip\n$ kubectl apply -f @samples/bookinfo/platform/kube/bookinfo.yaml@\nservice/details created\nserviceaccount/bookinfo-details created\ndeployment.apps/details-v1 created\nservice/ratings created\nserviceaccount/bookinfo-ratings created\ndeployment.apps/ratings-v1 created\nservice/reviews created\nserviceaccount/bookinfo-reviews created\ndeployment.apps/reviews-v1 created\ndeployment.apps/reviews-v2 created\ndeployment.apps/reviews-v3 created\nservice/productpage created\nserviceaccount/bookinfo-productpage created\ndeployment.apps/productpage-v1 created\nThe application will start. As each pod becomes ready, the Istio sidecar will be\ndeployed along with it.\n$ kubectl get services\nNAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\ndetails       ClusterIP   10.0.0.212      <none>        9080/TCP   29s\nkubernetes    ClusterIP   10.0.0.1        <none>        443/TCP    25m\nproductpage   ClusterIP   10.0.0.57       <none>        9080/TCP   28s\nratings       ClusterIP   10.0.0.33       <none>        9080/TCP   29s\nreviews       ClusterIP   10.0.0.28       <none>        9080/TCP   29s\nand\n$ kubectl get pods\nNAME                              READY   STATUS    RESTARTS   AGE\ndetails-v1-558b8b4b76-2llld       2/2     Running   0          2m41s\nproductpage-v1-6987489c74-lpkgl   2/2     Running   0          2m40s\nratings-v1-7dc98c7588-vzftc       2/2     Running   0          2m41s\nreviews-v1-7f99cc4496-gdxfn       2/2     Running   0          2m41s\nreviews-v2-7d79d5bd5d-8zzqd       2/2     Running   0          2m41s\nreviews-v3-7dbcdcbc56-m8dph       2/2     Running   0          2m41s\nNote that the pods show\nREADY 2/2\n, confirming they have their application container and the Istio sidecar container.\nValidate that the app is running inside the cluster by\nchecking for the page title in the response:\n$ kubectl exec \"$(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')\" -c ratings -- curl -sS productpage:9080/productpage | grep -o \"<title>.*</title>\"\n<title>Simple Bookstore App</title>\nOpen the application to outside traffic\nThe Bookinfo application is deployed, but not accessible from the outside. To make it accessible,\nyou need to create an ingress gateway, which maps a path to a\nroute at the edge of your mesh.\nCreate a\nKubernetes Gateway\nfor the Bookinfo application:\nZip\n$ kubectl apply -f @samples/bookinfo/gateway-api/bookinfo-gateway.yaml@\ngateway.gateway.networking.k8s.io/bookinfo-gateway created\nhttproute.gateway.networking.k8s.io/bookinfo created\nBy default, Istio creates a\nLoadBalancer\nservice for a gateway. As we will access this gateway by a tunnel, we don’t need a load balancer. If you want to learn about how load balancers are configured for external IP addresses, read the\ningress gateways\ndocumentation.\nChange the service type to\nClusterIP\nby annotating the gateway:\n$ kubectl annotate gateway bookinfo-gateway networking.istio.io/service-type=ClusterIP --namespace=default\nTo check the status of the gateway, run:\n$ kubectl get gateway\nNAME               CLASS   ADDRESS                                            PROGRAMMED   AGE\nbookinfo-gateway   istio   bookinfo-gateway-istio.default.svc.cluster.local   True         42s\nAccess the application\nYou will connect to the Bookinfo\nproductpage\nservice through the gateway you just provisioned. To access the gateway, you need to use the\nkubectl port-forward\ncommand:\n$ kubectl port-forward svc/bookinfo-gateway-istio 8080:80\nOpen your browser and navigate to\nhttp://localhost:8080/productpage\nto view the Bookinfo application.\nBookinfo Application\nIf you refresh the page, you should see the book reviews and ratings changing as the requests are distributed across the different versions of the\nreviews\nservice.\nView the dashboard\nIstio integrates with\nseveral different telemetry applications\n. These can help you gain\nan understanding of the structure of your service mesh, display the topology of the mesh, and analyze the health of your mesh.\nUse the following instructions to deploy the\nKiali\ndashboard, along with\nPrometheus\n,\nGrafana\n, and\nJaeger\n.\nInstall\nKiali and the other addons\nand wait for them to be deployed.\nZip\n$ kubectl apply -f @samples/addons@\n$ kubectl rollout status deployment/kiali -n istio-system\nWaiting for deployment \"kiali\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"kiali\" successfully rolled out\nAccess the Kiali dashboard.\n$ istioctl dashboard kiali\nIn the left navigation menu, select\nGraph\nand in the\nNamespace\ndrop down, select\ndefault\n.\nTo see trace data, you must send requests to your service. The number of requests depends on Istio’s sampling rate and can be configured using the\nTelemetry API\n. With the default sampling rate of 1%, you need to send at least 100 requests before the first trace is visible.\nTo send 100 requests to the\nproductpage\nservice, use the following command:\n$ for i in $(seq 1 100); do curl -s -o /dev/null \"http://$GATEWAY_URL/productpage\"; done\nThe Kiali dashboard shows an overview of your mesh with the relationships\nbetween the services in the\nBookinfo\nsample application. It also provides\nfilters to visualize the traffic flow.\nKiali Dashboard\nNext steps\nCongratulations on completing the evaluation installation!\nThese tasks are a great place for beginners to further evaluate Istio’s\nfeatures using this\ndemo\ninstallation:\nRequest routing\nFault injection\nTraffic shifting\nQuerying metrics\nVisualizing metrics\nAccessing external services\nVisualizing your mesh\nBefore you customize Istio for production use, see these resources:\nDeployment models\nDeployment best practices\nPod requirements\nGeneral installation instructions\nJoin the Istio community\nWe welcome you to ask questions and give us feedback by joining the\nIstio community\n.\nUninstall\nTo delete the\nBookinfo\nsample application and its configuration, see\nBookinfo\ncleanup\n.\nThe Istio uninstall deletes the RBAC permissions and all resources hierarchically\nunder the\nistio-system\nnamespace. It is safe to ignore errors for non-existent\nresources because they may have been deleted hierarchically.\nZip\n$ kubectl delete -f @samples/addons@\n$ istioctl uninstall -y --purge\nThe\nistio-system\nnamespace is not removed by default.\nIf no longer needed, use the following command to remove it:\n$ kubectl delete namespace istio-system\nThe label to instruct Istio to automatically inject Envoy sidecar proxies is not removed by default.\nIf no longer needed, use the following command to remove it:\n$ kubectl label namespace default istio-injection-\nIf you installed the Kubernetes Gateway API CRDs and would now like to remove them, run one of the following commands:\nIf you ran any tasks that required the\nexperimental version\nof the CRDs:\n$ kubectl kustomize \"github.com/kubernetes-sigs/gateway-api/config/crd/experimental?ref=v1.3.0\" | kubectl delete -f -\nOtherwise:\n$ kubectl kustomize \"github.com/kubernetes-sigs/gateway-api/config/crd?ref=v1.3.0\" | kubectl delete -f -\nSee also\nGetting Started without the Gateway API\nTry Istio’s features with the legacy Istio APIs.\nInstalling Gateways\nInstall and customize Istio Gateways.\nKubernetes Native Sidecars in Istio\nDemoing the new SidecarContainers feature with Istio.\nGetting started with the Kubernetes Gateway API\nUsing the Gateway API to configure ingress traffic for your Kubernetes cluster.\nExtending Gateway API support in Istio\nA standard API for service mesh, in Istio and in the broader community.\nDeploying Istio Control Planes Outside the Mesh\nA new deployment model for Istio.\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nDownload Istio\nInstall Istio\nInstall the Kubernetes Gateway API CRDs\nDeploy the sample application\nOpen the application to outside traffic\nAccess the application\nView the dashboard\nNext steps\nJoin the Istio community\nUninstall\nSee also\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nPlatform Setup\nHow to prepare various Kubernetes platforms before installing Istio.\nAlibaba Cloud\nInstructions to set up an Alibaba Cloud Kubernetes cluster for Istio.\nAmazon EKS\nInstructions to set up Istio on Amazon EKS in AWS cloud.\nAzure\nInstructions to set up an Azure cluster for Istio.\nDocker Desktop\nInstructions to set up Docker Desktop for Istio.\nGoogle Kubernetes Engine\nInstructions to set up a Google Kubernetes Engine cluster for Istio.\nHuawei Cloud\nInstructions to set up an Huawei Cloud kubernetes cluster for Istio.\nIBM Cloud\nInstructions to set up an IBM Cloud cluster for Istio.\nk3d\nInstructions to set up k3d for Istio.\nkind\nInstructions to set up kind for Istio.\nKops\nInstructions to set up Kops for use with Istio.\nKubernetes Gardener\nInstructions to set up a Gardener cluster for Istio.\nKubeSphere Container Platform\nInstructions to set up a KubeSphere Container Platform for Istio.\nMicroK8s\nInstructions to set up MicroK8s for use with Istio.\nMinikube\nInstructions to set up minikube for Istio.\nOpenShift\nInstructions to set up an OpenShift cluster for Istio.\nOracle Cloud Infrastructure\nInstructions to prepare a cluster for Istio using Oracle Container Engine for Kubernetes (OKE).\nTencent Cloud\nInstructions to set up Istio quickly in Tencent Cloud.\nIstio 1.26 is\nsupported on these Kubernetes versions\n: 1.29, 1.30, 1.31, 1.32.\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nAlibaba Cloud\nAlibaba Cloud\n2 minute read\nPrerequisites\nProcedure\nThis vendor-provided document has not been tested on the Istio 1.9 release and may contain bugs.\nFollow these instructions to prepare an\nAlibaba Cloud Kubernetes Container Service\ncluster for Istio.\nYou can deploy a Kubernetes cluster to Alibaba Cloud quickly and easily in the\nContainer Service console\n, which fully supports Istio.\nAlibaba Cloud offers a fully managed service mesh platform named Alibaba Cloud Service Mesh (ASM),\nwhich is fully compatible with Istio. Refer to\nAlibaba Cloud Service Mesh\nfor\ndetails and instructions.\nPrerequisites\nFollow the Alibaba Cloud instructions\nto activate the following services: Container Service, Resource Orchestration\nService (ROS), and RAM.\nProcedure\nLog on to the\nContainer Service console\n, and click\nClusters\nunder\nKubernetes\nin the left-side navigation pane to enter the\nCluster List\npage.\nClick the\nCreate Kubernetes Cluster\nbutton in the upper-right corner.\nEnter the cluster name. The cluster name can be 1–63 characters long and\nit can contain numbers, Chinese characters, English letters, and hyphens (-).\nSelect the\nregion\nand\nzone\nin which the cluster resides.\nSet the cluster network type. Kubernetes clusters only support the VPC\nnetwork type now.\nConfigure the node type, Pay-As-You-Go and Subscription types are supported.\nConfigure the master nodes. Select the generation, family, and type for the\nmaster nodes.\nConfigure the worker nodes. Select whether to create a worker node or add an\nexisting ECS instance as the worker node.\nConfigure the logon mode, and configure the Pod Network CIDR and Service\nCIDR.\nAmazon EKS\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nPrerequisites\nProcedure\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nAmazon EKS\nAmazon EKS\nFollow\nIstio on EKS\ninstructions to provision EKS cluster with Istio setup in AWS cloud.\nAlibaba Cloud\nAzure\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nAzure\nAzure\n2 minute read\npage test\nAKS\nFollow these instructions to prepare an Azure cluster for Istio.\nAzure offers a\nmanaged control plane\nadd-on for the Azure Kubernetes Service (AKS),\nwhich you can use instead of installing Istio manually.\nPlease refer to\nDeploy Istio-based service mesh add-on for Azure Kubernetes Service\nfor details and instructions.\nYou can deploy a Kubernetes cluster to Azure via\nAKS\nor\nCluster API provider for Azure (CAPZ) for self-managed Kubernetes or AKS\nwhich fully supports Istio.\nAKS\nYou can create an AKS cluster via numerous means such as\nthe az cli\n,\nthe Azure portal\n,\naz cli with Bicep\n, or\nTerraform\nFor the\naz\ncli option, complete\naz login\nauthentication OR use cloud shell, then run the following commands below.\nDetermine the desired region name which supports AKS\n$ az provider list --query \"[?namespace=='Microsoft.ContainerService'].resourceTypes[] | [?resourceType=='managedClusters'].locations[]\" -o tsv\nVerify the supported Kubernetes versions for the desired region\nReplace\nmy location\nusing the desired region value from the above step, and then execute:\n$ az aks get-versions --location \"my location\" --query \"orchestrators[].orchestratorVersion\"\nCreate the resource group and deploy the AKS cluster\nReplace\nmyResourceGroup\nand\nmyAKSCluster\nwith desired names,\nmy location\nusing the value from step 1,\n1.28.3\nif not supported in the region, and then execute:\n$ az group create --name myResourceGroup --location \"my location\"\n$ az aks create --resource-group myResourceGroup --name myAKSCluster --node-count 3 --kubernetes-version 1.28.3 --generate-ssh-keys\nGet the AKS\nkubeconfig\ncredentials\nReplace\nmyResourceGroup\nand\nmyAKSCluster\nwith the names from the previous step and execute:\n$ az aks get-credentials --resource-group myResourceGroup --name myAKSCluster\nAmazon EKS\nDocker Desktop\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nAKS\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nDocker Desktop\nDocker Desktop\nTo run Istio with Docker Desktop, install a version which contains a\nsupported Kubernetes version\n(1.29, 1.30, 1.31, 1.32).\nIf you want to run Istio under Docker Desktop’s built-in Kubernetes, you need to increase Docker’s memory limit\nunder the\nResources->Advanced\npane of Docker Desktop’s\nSettings…\n. Set the resources to at least 8.0\nGB\nof memory and 4\nCPUs\n.\nDocker Preferences\nMinimum memory requirements vary. 8\nGB\nis sufficient to run\nIstio and Bookinfo. If you don’t have enough memory allocated in Docker Desktop,\nthe following errors could occur:\nimage pull failures\nhealthcheck timeout failures\nkubectl failures on the host\ngeneral network instability of the hypervisor\nAdditional Docker Desktop resources may be freed up using:\n$ docker system prune\nAzure\nGoogle Kubernetes Engine\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nGoogle Kubernetes Engine\nGoogle Kubernetes Engine\n2 minute read\npage test\nMulti-cluster communication\nFollow these instructions to prepare a GKE cluster for Istio.\nCreate a new cluster.\n$ export PROJECT_ID=`gcloud config get-value project` && \\\n  export M_TYPE=n1-standard-2 && \\\n  export ZONE=us-west2-a && \\\n  export CLUSTER_NAME=${PROJECT_ID}-${RANDOM} && \\\n  gcloud services enable container.googleapis.com && \\\n  gcloud container clusters create $CLUSTER_NAME \\\n  --cluster-version latest \\\n  --machine-type=$M_TYPE \\\n  --num-nodes 4 \\\n  --zone $ZONE \\\n  --project $PROJECT_ID\nThe default installation of Istio requires nodes with >1 vCPU. If you are\ninstalling with the\ndemo configuration profile\n,\nyou can remove the\n--machine-type\nargument to use the smaller\nn1-standard-1\nmachine size instead.\nTo use the Istio CNI feature on GKE Standard, please check the\nCNI installation guide\nfor prerequisite cluster configuration steps. Since the CNI node agent requires the SYS_ADMIN capability, it is not available on GKE Autopilot. Instead, use the istio-init container.\nFor private GKE clusters\nAn automatically created firewall rule does not open port 15017. This is needed by the istiod discovery validation webhook.\nTo review this firewall rule for master access:\n$ gcloud compute firewall-rules list --filter=\"name~gke-${CLUSTER_NAME}-[0-9a-z]*-master\"\nTo replace the existing rule and allow master access:\n$ gcloud compute firewall-rules update <firewall-rule-name> --allow tcp:10250,tcp:443,tcp:15017\nRetrieve your credentials for\nkubectl\n.\n$ gcloud container clusters get-credentials $CLUSTER_NAME \\\n    --zone $ZONE \\\n    --project $PROJECT_ID\nGrant cluster administrator (admin) permissions to the current user. To\ncreate the necessary RBAC rules for Istio, the current user requires admin\npermissions.\n$ kubectl create clusterrolebinding cluster-admin-binding \\\n    --clusterrole=cluster-admin \\\n    --user=$(gcloud config get-value core/account)\nMulti-cluster communication\nIn some cases, a firewall rule must be explicitly created to allow cross-cluster traffic.\nThe following instructions will allow communication between\nall\nclusters in your project. Adjust the commands as needed.\nGather information about your clusters’ network.\n$ function join_by { local IFS=\"$1\"; shift; echo \"$*\"; }\n$ ALL_CLUSTER_CIDRS=$(gcloud --project $PROJECT_ID container clusters list --format='value(clusterIpv4Cidr)' | sort | uniq)\n$ ALL_CLUSTER_CIDRS=$(join_by , $(echo \"${ALL_CLUSTER_CIDRS}\"))\n$ ALL_CLUSTER_NETTAGS=$(gcloud --project $PROJECT_ID compute instances list --format='value(tags.items.[0])' | sort | uniq)\n$ ALL_CLUSTER_NETTAGS=$(join_by , $(echo \"${ALL_CLUSTER_NETTAGS}\"))\nCreate the firewall rule.\n$ gcloud compute firewall-rules create istio-multicluster-pods \\\n    --allow=tcp,udp,icmp,esp,ah,sctp \\\n    --direction=INGRESS \\\n    --priority=900 \\\n    --source-ranges=\"${ALL_CLUSTER_CIDRS}\" \\\n    --target-tags=\"${ALL_CLUSTER_NETTAGS}\" --quiet\nDocker Desktop\nHuawei Cloud\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nMulti-cluster communication\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nHuawei Cloud\nHuawei Cloud\n2 minute read\npage test\nFollow these instructions to prepare a cluster for Istio using the\nHuawei Cloud Container Engine\n.\nYou can deploy a Kubernetes cluster to Huawei Cloud quickly and easily in the\nCloud Container Engine Console\n, which fully supports Istio.\nHuawei offers a\nmanaged control plane\nadd-on for the Huawei Cloud Container Engine,\nwhich you can use instead of installing Istio manually.\nRefer to\nHuawei Application Service Mesh\nfor details and instructions.\nFollowing the\nHuawei Cloud Instructions\nto prepare a cluster before manually installing Istio, proceed as follows:\nLog in to the CCE console. Choose\nDashboard\n>\nBuy Cluster\nto open the\nBuy Hybrid Cluster\npage. An alternative way to open that page is to choose\nResource Management\n>\nClusters\nin the navigation pane and click\nBuy\nnext to\nHybrid Cluster\n.\nOn the\nConfigure Cluster\npage, configure cluster parameters.\nIn this example, a majority of parameters retain default values. After the cluster configuration is complete, click Next:\nCreate Node\nto go to the node creation page.\nIstio release has some requirements for the Kubernetes version,\nselect the version according to Istio’s\nsupport policy\n.\nThe image below shows the GUI where you create and configure the cluster:\nConfigure Cluster\nOn the node creation page, configure the following parameters\nIstio adds some additional resource consumption,\nfrom our experience, reserve at least 4 vCPU and 8 GB memory to begin playing.\nThe image below shows the GUI where you create and configure the node:\nConfigure Node\nConfigure kubectl\nNow you can install Istio on CCE cluster according to\ninstall guide\n.\nConfigure\nELB\nto expose Istio ingress gateway if needed.\nCreate Elastic Load Balancer\nBind the ELB instance to\nistio-ingressgateway\nservice\nSet the ELB instance ID and\nloadBalancerIP\nto\nistio-ingressgateway\n.\n$ kubectl apply -f - <<EOF\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kubernetes.io/elb.class: union\n    kubernetes.io/elb.id: 4ee43d2b-cec5-4100-89eb-2f77837daa63 # ELB ID\n    kubernetes.io/elb.lb-algorithm: ROUND_ROBIN\n  labels:\n    app: istio-ingressgateway\n    install.operator.istio.io/owning-resource: unknown\n    install.operator.istio.io/owning-resource-namespace: istio-system\n    istio: ingressgateway\n    istio.io/rev: default\n    operator.istio.io/component: IngressGateways\n    operator.istio.io/managed: Reconcile\n    operator.istio.io/version: 1.9.0\n    release: istio\n  name: istio-ingressgateway\n  namespace: istio-system\nspec:\n  clusterIP: 10.247.7.192\n  externalTrafficPolicy: Cluster\n  loadBalancerIP: 119.8.36.132     ## ELB EIP\n  ports:\n  - name: status-port\n    nodePort: 32484\n    port: 15021\n    protocol: TCP\n    targetPort: 15021\n  - name: http2\n    nodePort: 30294\n    port: 80\n    protocol: TCP\n    targetPort: 8080\n  - name: https\n    nodePort: 31301\n    port: 443\n    protocol: TCP\n    targetPort: 8443\n  - name: tcp\n    nodePort: 30229\n    port: 31400\n    protocol: TCP\n    targetPort: 31400\n  - name: tls\n    nodePort: 32028\n    port: 15443\n    protocol: TCP\n    targetPort: 15443\n  selector:\n    app: istio-ingressgateway\n    istio: ingressgateway\n  sessionAffinity: None\n  type: LoadBalancer\nEOF\nStart playing with Istio by trying out the various\ntasks\n.\nGoogle Kubernetes Engine\nIBM Cloud\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nIBM Cloud\nIBM Cloud\n2 minute read\npage test\nFollow these instructions to prepare a cluster for Istio using the\nIBM Cloud Kubernetes Service\n.\nIBM offers a\nmanaged control plane\nadd-on for the IBM Cloud Kubernetes Service,\nwhich you can use instead of installing Istio manually.\nRefer to\nIstio on IBM Cloud Kubernetes Service\nfor details and instructions.\nTo prepare a cluster before manually installing Istio, proceed as follows:\nInstall the IBM Cloud CLI, the IBM Cloud Kubernetes Service plug-in, and the Kubernetes CLI\n.\nCreate a standard Kubernetes cluster using the following command.\nReplace\n<cluster-name>\nwith the name you want to use for your cluster and\n<zone-name>\nwith the name of an\navailable zone.\nYou can display your available zones by running\nibmcloud ks zones --provider classic\n.\nThe IBM Cloud Kubernetes Service\nLocations Reference Guide\ndescribes the available zones and how to specify them.\n$ ibmcloud ks cluster create classic --zone <zone-name> --machine-type b3c.4x16 \\\n  --workers 3 --name <cluster-name>\nIf you already have a private or a public VLAN, you must specify them in the above command\nusing the\n--private-vlan\nand\n--public-vlan\noptions. Otherwise, they will be automatically created for you.\nYou can view your available VLANs by running\nibmcloud ks vlans --zone <zone-name>\n.\nRun the following command to download your cluster configuration.\n$ ibmcloud ks cluster config --cluster <cluster-name>\nMake sure to use the\nkubectl\nCLI version that matches the Kubernetes version of your cluster.\nHuawei Cloud\nk3d\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nk3d\nk3d\n3 minute read\npage test\nPrerequisites\nInstallation\nSet up Istio for k3d\nSet up Dashboard UI for k3d\nUninstall\nk3d is a lightweight wrapper to run\nk3s\n(Rancher Lab’s minimal Kubernetes distribution) in docker.\nk3d makes it very easy to create single- and multi-node k3s clusters in docker, e.g. for local development on Kubernetes.\nPrerequisites\nTo use k3d, you will also need to\ninstall docker\n.\nInstall the latest version of\nk3d\n.\nTo interact with the Kubernetes cluster\nkubectl\n(Optional)\nHelm\nis the package manager for Kubernetes\nInstallation\nCreate a cluster and disable\nTraefik\nwith the following command:\n$ k3d cluster create --api-port 6550 -p '9080:80@loadbalancer' -p '9443:443@loadbalancer' --agents 2 --k3s-arg '--disable=traefik@server:*'\nTo see the list of k3d clusters, use the following command:\n$ k3d cluster list\nk3s-default\nTo list the local Kubernetes contexts, use the following command.\n$ kubectl config get-contexts\nCURRENT   NAME                 CLUSTER              AUTHINFO             NAMESPACE\n*         k3d-k3s-default      k3d-k3s-default      k3d-k3s-default\nk3d-\nis prefixed to the context and cluster names, for example:\nk3d-k3s-default\nIf you run multiple clusters, you need to choose which cluster\nkubectl\ntalks to. You can set a default cluster\nfor\nkubectl\nby setting the current context in the\nKubernetes kubeconfig\nfile. Additionally you can run following command\nto set the current context for\nkubectl\n.\n$ kubectl config use-context k3d-k3s-default\nSwitched to context \"k3d-k3s-default\".\nSet up Istio for k3d\nOnce you are done setting up a k3d cluster, you can proceed to\ninstall Istio with Helm 3\non it.\n$ kubectl create namespace istio-system\n$ helm install istio-base istio/base -n istio-system --wait\n$ helm install istiod istio/istiod -n istio-system --wait\n(Optional) Install an ingress gateway:\n$ helm install istio-ingressgateway istio/gateway -n istio-system --wait\nSet up Dashboard UI for k3d\nk3d does not have a built-in Dashboard UI like minikube. But you can still set up Dashboard, a web based Kubernetes UI, to view your cluster.\nFollow these instructions to set up Dashboard for k3d.\nTo deploy Dashboard, run the following command:\n$ helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/\n$ helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard\nVerify that Dashboard is deployed and running.\n$ kubectl get pod -n kubernetes-dashboard\nNAME                                         READY   STATUS    RESTARTS   AGE\ndashboard-metrics-scraper-8c47d4b5d-dd2ks    1/1     Running   0          25s\nkubernetes-dashboard-67bd8fc546-4xfmm        1/1     Running   0          25s\nCreate a\nServiceAccount\nand\nClusterRoleBinding\nto provide admin access to the newly created cluster.\n$ kubectl create serviceaccount -n kubernetes-dashboard admin-user\n$ kubectl create clusterrolebinding -n kubernetes-dashboard admin-user --clusterrole cluster-admin --serviceaccount=kubernetes-dashboard:admin-user\nTo log in to your Dashboard, you need a Bearer Token. Use the following command to store the token in a variable.\n$ token=$(kubectl -n kubernetes-dashboard create token admin-user)\nDisplay the token using the\necho\ncommand and copy it to use for logging in to your Dashboard.\n$ echo $token\nYou can access your Dashboard using the kubectl command-line tool by running the following command:\n$ kubectl proxy\nStarting to serve on 127.0.0.1:8001\nClick\nKubernetes Dashboard\nto\nview your deployments and services.\nYou have to save your token somewhere, otherwise you have to run step number 4 everytime you need a token to log in to your Dashboard.\nUninstall\nWhen you are done experimenting and you want to delete the existing cluster, use the following command:\n$ k3d cluster delete k3s-default\nDeleting cluster \"k3s-default\" ...\nIBM Cloud\nkind\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nPrerequisites\nInstallation\nSet up Istio for k3d\nSet up Dashboard UI for k3d\nUninstall\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nkind\nkind\n3 minute read\npage test\nPrerequisites\nInstallation steps\nSetup LoadBalancer for kind\nSetup Dashboard UI for kind\nkind\nis a tool for running local Kubernetes clusters using Docker container\nnodes\n.\nkind was primarily designed for testing Kubernetes itself, but may be used for local development or CI.\nFollow these instructions to prepare a kind cluster for Istio installation.\nPrerequisites\nPlease use the latest Go version.\nTo use kind, you will also need to\ninstall docker\n.\nInstall the latest version of\nkind\n.\nIncrease Docker’s\nmemory limit\n.\nInstallation steps\nCreate a cluster with the following command:\n$ kind create cluster --name istio-testing\n--name\nis used to assign a specific name to the cluster. By default, the cluster will be given the name “kind”.\nTo see the list of kind clusters, use the following command:\n$ kind get clusters\nistio-testing\nTo list the local Kubernetes contexts, use the following command.\n$ kubectl config get-contexts\nCURRENT   NAME                 CLUSTER              AUTHINFO             NAMESPACE\n*         kind-istio-testing   kind-istio-testing   kind-istio-testing\n          minikube             minikube             minikube\nkind\nis prefixed to the context and cluster names, for example:\nkind-istio-testing\nIf you run multiple clusters, you need to choose which cluster\nkubectl\ntalks to. You can set a default cluster\nfor\nkubectl\nby setting the current context in the\nKubernetes kubeconfig\nfile. Additionally you can run following command\nto set the current context for\nkubectl\n.\n$ kubectl config use-context kind-istio-testing\nSwitched to context \"kind-istio-testing\".\nOnce you are done setting up a kind cluster, you can proceed to\ninstall Istio\non it.\nWhen you are done experimenting and you want to delete the existing cluster, use the following command:\n$ kind delete cluster --name istio-testing\nDeleting cluster \"istio-testing\" ...\nSetup LoadBalancer for kind\nkind does not have any built-in way to provide IP addresses to your\nLoadbalancer\nservice types, to ensure IP address assignments to\nGateway\nServices please consult\nthis guide\nfor more information.\nSetup Dashboard UI for kind\nkind does not have a built-in Dashboard UI like minikube. But you can still setup Dashboard, a web-based Kubernetes UI, to view your cluster.\nFollow these instructions to set up Dashboard for kind.\nTo deploy Dashboard, run the following command:\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\nVerify that Dashboard is deployed and running.\n$ kubectl get pod -n kubernetes-dashboard\nNAME                                         READY   STATUS    RESTARTS   AGE\ndashboard-metrics-scraper-76585494d8-zdb66   1/1     Running   0          39s\nkubernetes-dashboard-b7ffbc8cb-zl8zg         1/1     Running   0          39s\nCreate a\nServiceAccount\nand\nClusterRoleBinding\nto provide admin access to the newly created cluster.\n$ kubectl create serviceaccount -n kubernetes-dashboard admin-user\n$ kubectl create clusterrolebinding -n kubernetes-dashboard admin-user --clusterrole cluster-admin --serviceaccount=kubernetes-dashboard:admin-user\nTo log in to your Dashboard, you need a Bearer Token. Use the following command to store the token in a variable.\n$ token=$(kubectl -n kubernetes-dashboard create token admin-user)\nDisplay the token using the\necho\ncommand and copy it to use for logging in to your Dashboard.\n$ echo $token\nYou can access your Dashboard using the kubectl command-line tool by running the following command:\n$ kubectl proxy\nStarting to serve on 127.0.0.1:8001\nClick\nKubernetes Dashboard\nto\nview your deployments and services.\nYou have to save your token somewhere, otherwise you have to run step number 4 everytime you need a token to log in to your Dashboard.\nk3d\nKops\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nPrerequisites\nInstallation steps\nSetup LoadBalancer for kind\nSetup Dashboard UI for kind\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nKops\nKops\nNo special configuration is required to run Istio on Kubernetes clusters version 1.22 or newer. For prior Kubernetes versions, you will need to continue to perform these steps.\nIf you wish to run Istio\nSecret Discovery Service\n(SDS) for your mesh on Kops managed clusters, you must add\nextra configurations\nto enable service account token projection volumes in the api-server.\nOpen the configuration file:\n$ kops edit cluster $YOURCLUSTER\nAdd the following in the configuration file:\nkubeAPIServer:\n    apiAudiences:\n    - api\n    - istio-ca\n    serviceAccountIssuer: kubernetes.default.svc\nPerform the update:\n$ kops update cluster\n$ kops update cluster --yes\nLaunch the rolling update:\n$ kops rolling-update cluster\n$ kops rolling-update cluster --yes\nkind\nKubernetes Gardener\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nKubernetes Gardener\nKubernetes Gardener\n3 minute read\npage test\nBootstrapping Gardener\nInstall and configure\nkubectl\nAccess Gardener\nCreating a Kubernetes cluster\nConfigure\nkubectl\nfor your cluster\nCleaning up\nBootstrapping Gardener\nTo set up your own\nGardener\nfor your organization’s Kubernetes-as-a-Service needs, follow the\ndocumentation\n.\nFor testing purposes, you can set up\nGardener on your laptop\nby checking out the source code repository and simply running\nmake kind-up gardener-up\n(the easiest developer way of checking out Gardener!).\nAlternatively,\n23 Technologies GmbH\noffers a fully-managed Gardener service that conveniently works with all supported cloud providers and comes with a free trial:\nOkeanos\n. Similarly, cloud providers such as\nSTACKIT\n,\nB'Nerd\n,\nMetalStack\n, and many others run Gardener as their Kubernetes Engine.\nTo learn more about the inception of this open source project, read\nGardener Project Update\nand\nGardener - The Kubernetes Botanist\non\nkubernetes.io\n.\nGardener yourself a Shoot with Istio, custom Domains, and Certificates\nis a detailed tutorial for the end user of Gardener.\nInstall and configure\nkubectl\nIf you already have\nkubectl\nCLI, run\nkubectl version --short\nto check\nthe version. You need a current version that at least matches your Kubernetes\ncluster version you want to order. If your\nkubectl\nis older, follow the\nnext step to install a newer version.\nInstall the\nkubectl\nCLI\n.\nAccess Gardener\nCreate a project in the Gardener dashboard. This will essentially create a\nKubernetes namespace with the name\ngarden-<my-project>\n.\nConfigure access to your Gardener project\nusing a kubeconfig.\nYou can skip this step if you intend to create and interact with your cluster using the Gardener dashboard and the embedded webterminal; this step is only needed for programmatic access.\nIf you are not the Gardener Administrator already, you can create a technical user in the Gardener dashboard:\ngo to the “Members” section and add a service account. You can then download the kubeconfig for your project.\nMake sure you\nexport KUBECONFIG=garden-my-project.yaml\nin your shell.\nCreating a Kubernetes cluster\nYou can create your cluster using the\nkubectl\ncli by providing a cluster\nspecification yaml file. You can find an example for GCP\nhere\n.\nMake sure the namespace matches that of your project. Then apply the\nprepared so-called “shoot” cluster manifest with\nkubectl\n:\n$ kubectl apply --filename my-cluster.yaml\nAn easier alternative is to create the cluster following the cluster creation\nwizard in the Gardener dashboard:\nConfigure\nkubectl\nfor your cluster\nYou can now download the kubeconfig for your freshly created cluster in the\nGardener dashboard or via cli as follows:\n$ kubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode > my-cluster.yaml\nThis kubeconfig file has full administrator access to you cluster.\nFor any activities with the payload cluster be sure you have\nexport KUBECONFIG=my-cluster.yaml\nset.\nCleaning up\nUse the Gardener dashboard to delete your cluster, or execute the following with\nkubectl\npointing to your\ngarden-my-project.yaml\nkubeconfig:\n$ kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.garden.sapcloud.io/deletion=true\n$ kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster\nKops\nKubeSphere Container Platform\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nBootstrapping Gardener\nInstall and configure\nkubectl\nAccess Gardener\nCreating a Kubernetes cluster\nConfigure\nkubectl\nfor your cluster\nCleaning up\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nKubeSphere Container Platform\nKubeSphere Container Platform\n2 minute read\npage test\nPrerequisites\nProvisioning a Kubernetes cluster\nEnable installing Istio on Kubernetes\nThis page was last updated March 9, 2021.\nThis vendor-provided document has not been tested on the Istio 1.9 release and may contain bugs.\nFollow these instructions to prepare the\nKubeSphere Container Platform\nfor Istio. You can download KubeSphere to easily install a Kubernetes cluster on your Linux machines.\nKubeSphere provides\nAll-in-One\nand\nMulti-Node\ninstallations. This enables quick setup and manages Kubernetes and Istio in a unified web console. This tutorial will walk you through the All-in-One installation. Reference\nMulti-node Installation\nfor further information.\nPrerequisites\nA Linux machine that is either a virtual machine or bare metal. This machine requires at a minimum:\nHardware:\nCPU: at least 2 Cores\nMemory: at least 4\nGB\nOperating Systems:\nCentOS 7.4 ~ 7.7 (\n64-bit\n)\nUbuntu 16.04/18.04 LTS (\n64-bit\n)\nRHEL 7.4 (\n64-bit\n)\nDebian Stretch 9.5 (\n64-bit\n)\nEnsure your firewall meets the\nport requirements\n. If this is not immediately feasible, you may evaluate Istio and KubeSphere by disabling the firewall as documented in your distribution.\nProvisioning a Kubernetes cluster\nDownload KubeSphere to your Linux machine, move to the KubeSphere directory. For example, if the created directory is\nkubesphere-all-v2.1.1\n:\n$ curl -L https://kubesphere.io/download/stable/latest > installer.tar.gz\n$ tar -xzf installer.tar.gz\n$ cd kubesphere-all-v2.1.1/scripts\nExecute the installation script, it will create a standard Kubernetes cluster. Select the\n“1) All-in-one”\noption when prompted:\n$ ./install.sh\nInstallation may take 15 ~ 20 minutes. Wait until all pods are running. Access the console using the account information obtained from the installation logs:\n#####################################################\n###              Welcome to KubeSphere!           ###\n#####################################################\nConsole: http://192.168.0.8:30880\nAccount: admin\nPassword: It will be generated by KubeSphere Installer\nAt the same time, Kubernetes has been installed into your environment.\nEnable installing Istio on Kubernetes\nKubeSphere will install Istio within Kubernetes. Now reference\nEnable Service Mesh\nto enable Istio.\nKubernetes Gardener\nMicroK8s\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nPrerequisites\nProvisioning a Kubernetes cluster\nEnable installing Istio on Kubernetes\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nMicroK8s\nMicroK8s\nThis page was last updated August 28, 2019.\nThis vendor-provided document has not been tested on the Istio 1.9 release and may contain bugs.\nFollow these instructions to prepare MicroK8s for using Istio.\nAdministrative privileges are required to run MicroK8s.\nInstall the latest version of\nMicroK8s\nusing the command\n$ sudo snap install microk8s --classic\nEnable Istio with the following command:\n$ microk8s.enable istio\nWhen prompted, choose whether to enforce mutual TLS authentication among sidecars.\nIf you have a mixed deployment with non-Istio and Istio enabled services or you’re unsure, choose No.\nPlease run the following command to check deployment progress:\n$ watch microk8s.kubectl get all --all-namespaces\nKubeSphere Container Platform\nMinikube\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nMinikube\nMinikube\n2 minute read\npage test\nPrerequisites\nInstallation steps\nFollow these instructions to prepare minikube for Istio installation with sufficient\nresources to run Istio and some basic applications.\nPrerequisites\nAdministrative privileges are required to run minikube.\nTo enable the\nSecret Discovery Service\n(SDS) for your mesh, you must add\nextra configurations\nto your Kubernetes deployment.\nRefer to the\napi-server\nreference docs\nfor the most up-to-date flags.\nInstallation steps\nInstall the latest version of\nminikube\nand a\nminikube hypervisor driver\n.\nIf you’re not using the default driver, set your minikube hypervisor driver.\nFor example, if you installed the KVM hypervisor, set the\ndriver\nwithin the minikube configuration using the following command:\n$ minikube config set driver kvm2\nStart minikube with 16384\nMB\nof memory and 4\nCPUs\n. This example uses Kubernetes version\n1.26.1\n.\nYou can change the version to any Kubernetes version supported by Istio by altering the\n--kubernetes-version\nvalue:\n$ minikube start --memory=16384 --cpus=4 --kubernetes-version=v1.26.1\nDepending on the hypervisor you use and the platform on which the hypervisor\nis run, minimum memory requirements vary. 16384\nMB\nis sufficient to run\nIstio and bookinfo.\nIf you don’t have enough RAM allocated to the minikube\nvirtual machine, the following errors could occur:\nimage pull failures\nhealthcheck timeout failures\nkubectl failures on the host\ngeneral network instability of the virtual machine and the host\ncomplete lock-up of the virtual machine\nhost NMI watchdog reboots\nOne effective way to monitor memory usage in minikube is to\nssh\ninto the minikube virtual machine\nand from that prompt run the top command:\n$ minikube ssh\n$ top\nGiB Mem : 12.4/15.7\nThis shows 12.4GiB used of an available 15.7 GiB RAM within the virtual\nmachine. This data was generated with the VMWare Fusion hypervisor on a\nMacbook Pro 13\" with 16GiB RAM running Istio 1.2 with bookinfo installed.\n(Optional, recommended) If you want minikube to provide a load balancer for use\nby Istio, you can use the\nminikube tunnel\nfeature.\nRun this command in a different terminal, because the minikube tunnel feature will block your\nterminal to output diagnostic information about the network:\n$ minikube tunnel\nSometimes minikube does not clean up the tunnel network properly. To force a proper\ncleanup:\n$ minikube tunnel --cleanup\nMicroK8s\nOpenShift\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nPrerequisites\nInstallation steps\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of ISTIO.\nStart by understanding the core concepts from this content.\n\nDOC START:\nOverview\nWhat is Istio?\nWhy choose Istio?\nSidecar or ambient?\nQuickstart\nConcepts\nTraffic Management\nSecurity\nObservability\nExtensibility\nSidecar Mode\nGetting Started\nPlatform Setup\nAlibaba Cloud\nAmazon EKS\nAzure\nDocker Desktop\nGoogle Kubernetes Engine\nHuawei Cloud\nIBM Cloud\nk3d\nkind\nKops\nKubernetes Gardener\nKubeSphere Container Platform\nMicroK8s\nMinikube\nOpenShift\nOracle Cloud Infrastructure\nTencent Cloud\nInstall\nInstall with Istioctl\nInstall with Helm\nInstall Multicluster\nBefore you begin\nInstall Multi-Primary\nInstall Primary-Remote\nInstall Multi-Primary on different networks\nInstall Primary-Remote on different networks\nVerify the installation\nInstall Istio with an External Control Plane\nInstall Multiple Istio Control Planes in a Single Cluster\nVirtual Machine Installation\nUpgrade\nCanary Upgrades\nIn-place Upgrades\nUpgrade with Helm\nMore Guides\nDownload the Istio release\nInstallation Configuration Profiles\nCompatibility Versions\nInstalling Gateways\nInstalling the Sidecar\nCustomizing the installation configuration\nAdvanced Helm Chart Customization\nInstall Istio in Dual-Stack mode\nInstall Istio with Pod Security Admission\nInstall the Istio CNI node agent\nGetting Started without the Gateway API\nAmbient Mode\nOverview\nGetting Started\nDeploy a sample application\nSecure and visualize the application\nEnforce authorization policies\nManage traffic\nClean up\nInstall\nPlatform-Specific Prerequisites\nInstall with Helm\nInstall with istioctl\nUpgrade\nUpgrade with Helm\nUser Guides\nAdd workloads to the mesh\nVerify mutual TLS is enabled\nAmbient and Kubernetes NetworkPolicy\nUse Layer 4 security policy\nConfigure waypoint proxies\nUse Layer 7 features\nExtend waypoints with WebAssembly plugins *\nTroubleshoot connectivity issues with ztunnel\nTroubleshoot issues with waypoints\nArchitecture\nAmbient and the Istio control plane\nAmbient data plane\nHBONE\nZtunnel traffic redirection\nTasks\nTraffic Management\nRequest Routing\nFault Injection\nTraffic Shifting\nTCP Traffic Shifting\nRequest Timeouts\nCircuit Breaking\nMirroring\nLocality Load Balancing\nBefore you begin\nLocality failover\nLocality weighted distribution\nCleanup\nIngress\nIngress Gateways\nSecure Gateways\nIngress Gateway without TLS Termination\nIngress Sidecar TLS Termination\nKubernetes Ingress\nKubernetes Gateway API\nEgress\nAccessing External Services\nEgress TLS Origination\nEgress Gateways\nEgress Gateways with TLS Origination\nEgress using Wildcard Hosts\nKubernetes Services for Egress Traffic\nUsing an External HTTPS Proxy\nSecurity\nCertificate Management\nPlug in CA Certificates\nCustom CA Integration using Kubernetes CSR *\nAuthentication\nAuthentication Policy\nJWT claim based routing *\nCopy JWT Claims to HTTP Headers *\nMutual TLS Migration\nAuthorization\nHTTP Traffic\nTCP Traffic\nJWT Token\nExternal Authorization\nExplicit Deny\nIngress Access Control\nTrust Domain Migration\nDry Run *\nTLS Configuration\nIstio Workload Minimum TLS Version Configuration\nPolicy Enforcement\nEnabling Rate Limits using Envoy\nObservability\nTelemetry API\nMetrics\nCustomizing Istio Metrics with Telemetry API\nCollecting Metrics for TCP Services\nCustomizing Istio Metrics\nClassifying Metrics Based on Request or Response\nQuerying Metrics from Prometheus\nVisualizing Metrics with Grafana\nLogs\nConfigure access logs with Telemetry API\nEnvoy Access Logs\nOpenTelemetry\nDistributed Tracing\nOverview\nConfigure tracing with Telemetry API\nConfigure tracing using MeshConfig and pod annotations\nConfigure trace sampling\nOpenTelemetry\nJaeger\nZipkin\nApache SkyWalking\nVisualizing Your Mesh\nRemotely Accessing Telemetry Addons\nExtensibility\nDistributing WebAssembly Modules *\nExamples\nBookinfo Application\nBookinfo with a Virtual Machine\nLearn Microservices using Kubernetes and Istio\nPrerequisites\nSet up a Kubernetes Cluster\nSet up a Local Computer\nRun a Microservice Locally\nRun ratings in Docker\nRun Bookinfo with Kubernetes\nTest in production\nAdd a new version of reviews\nEnable Istio on productpage\nEnable Istio on all the microservices\nConfigure Istio Ingress Gateway\nMonitoring with Istio\nOperations\nDeployment\nPlatform Requirements\nArchitecture\nSecurity Model\nDeployment Models\nVirtual Machine Architecture\nPerformance and Scalability\nApplication Requirements\nConfiguration\nMesh Configuration\nDynamic Admission Webhooks Overview\nHealth Checking of Istio Services\nConfiguration Scoping\nTraffic Management\nProtocol Selection\nManaging In-Mesh Certificates\nTLS Configuration\nTraffic Routing\nDNS\nConfiguring Gateway Network Topology *\nDNS Proxying\nMulti-cluster Traffic Management\nSecurity\nSecurity policy examples\nHarden Docker Container Images\nObservability\nEnvoy Statistics\nMonitoring Multicluster Istio with Prometheus\nExtensibility\nPull Policy for WebAssembly Modules *\nBest Practices\nDeployment Best Practices\nTraffic Management Best Practices\nSecurity Best Practices\nImage Signing and Validation\nObservability Best Practices\nCommon Problems\nTraffic Management Problems\nSecurity Problems\nObservability Problems\nSidecar Injection Problems\nConfiguration Validation Problems\nUpgrade Problems\nDiagnostic Tools\nUsing the Istioctl Command-line Tool\nDebugging Envoy and Istiod\nUnderstand your Mesh with Istioctl Describe\nDiagnose your Configuration with Istioctl Analyze\nVerifying Istio Sidecar Injection with Istioctl Check-Inject\nIstiod Introspection\nComponent Logging\nDebugging Virtual Machines\nTroubleshooting Multicluster\nTroubleshooting the Istio CNI plugin\nIntegrations\ncert-manager\nGrafana\nJaeger\nKiali\nPrometheus\nSPIRE\nApache SkyWalking\nZipkin\nThird Party Load Balancers\nReleases\nFeature Status\nReporting Bugs\nSecurity Vulnerabilities\nSupported Releases\nContribute Documentation\nWork with GitHub\nAdd New Documentation\nRemove Retired Documentation\nBuild and serve the website locally\nFront matter\nDocumentation Review Process\nAdd Code Blocks\nUse Shortcodes\nFollow Formatting Standards\nStyle Guide\nTerminology Standards\nDiagram Creation Guidelines\nWebsite Content Changes\nReference\nConfiguration\nAnalysis Messages\nGlobal Mesh Options\nIstioOperator Options\nConfiguration Status Field\nProxy Extensions\nWasm Plugin\nTraffic Management\nDestination Rule\nEnvoy Filter\nGateway\nProxyConfig\nService Entry\nSidecar\nVirtual Service\nWorkload Entry\nWorkload Group\nSecurity\nPeerAuthentication\nRequestAuthentication\nAuthorization Policy\nAuthorization Policy Conditions\nAuthorization Policy Normalization\nTelemetry\nCommon Types\nWorkload Selector\nIstio Standard Metrics\nResource Annotations\nResource Labels\nConfiguration Analysis Messages\nAlphaAnnotation\nAnalyzer Message Format\nConflictingMeshGatewayVirtualServiceHosts\nConflictingSidecarWorkloadSelectors\nConflictingTelemetryWorkloadSelectors\nDeploymentAssociatedToMultipleServices\nDeploymentConflictingPorts\nDeprecated\nDeprecatedAnnotation\nEnvoyFilterUsesAddOperationIncorrectly\nEnvoyFilterUsesRelativeOperation\nEnvoyFilterUsesRelativeOperationWithProxyVersion\nEnvoyFilterUsesRemoveOperationIncorrectly\nEnvoyFilterUsesReplaceOperationIncorrectly\nExternalControlPlaneAddressIsNotAHostname\nExternalNameServiceTypeInvalidPortName\nGatewayPortNotDefinedOnService\nIneffectivePolicy\nIneffectiveSelector\nInternalError\nInvalidAnnotation\nInvalidApplicationUID\nInvalidExternalControlPlaneConfig\nInvalidGatewayCredential\nInvalidTelemetryProvider\nLocalhostListener\nMisplacedAnnotation\nMultipleSidecarsWithoutWorkloadSelectors\nMultipleTelemetriesWithoutWorkloadSelectors\nNamespaceMultipleInjectionLabels\nNamespaceNotInjected\nNoMatchingWorkloadsFound\nNoServerCertificateVerificationDestinationLevel\nNoServerCertificateVerificationPortLevel\nPodMissingProxy\nPodsIstioProxyImageMismatchInNamespace\nPortNameIsNotUnderNamingConvention\nReferencedResourceNotFound\nSchemaValidationError\nServiceEntryAddressesRequired\nUnknownAnnotation\nVirtualServiceDestinationPortSelectorRequired\nVirtualServiceHostNotFoundInGateway\nVirtualServiceIneffectiveMatch\nVirtualServiceUnreachableRule\nCommands\ninstall-cni\nistioctl\npilot-agent\npilot-discovery\nGlossary\nContents\nDocumentation\nSidecar Mode\nPlatform Setup\nOpenShift\nOpenShift\nFollow these instructions to prepare an OpenShift cluster for Istio.\nInstall Istio using the OpenShift profile:\n$ istioctl install --set profile=openshift\nAfter installation is complete, expose an OpenShift route for the ingress gateway.\n$ oc -n istio-system expose svc/istio-ingressgateway --port=http2\nMinikube\nOracle Cloud Infrastructure\nWas this information useful?\nYes\nNo\nDo you have any suggestions for improvement?\nThanks for your feedback!\nLinks\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the ISTIO official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "istio", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM_CORE.\nStart by understanding the core concepts from this content.\n\nDOC START:\n<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><title>Overview - Configuration Language | Terraform | HashiCorp Developer</title><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"og:type\" content=\"website\"/><meta property=\"article:publisher\" content=\"https://www.facebook.com/HashiCorp/\"/><meta name=\"twitter:site\" content=\"@HashiCorp\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"theme-color\" content=\"#000\"/><meta name=\"description\" property=\"og:description\" content=\"Use the Terraform configuration language to describe the infrastructure that Terraform manages.\"/><meta name=\"twitter:description\" content=\"Use the Terraform configuration language to describe the infrastructure that Terraform manages.\"/><meta property=\"og:site_name\" content=\"Overview - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:title\" content=\"Overview - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"canonical\" href=\"https://developer.hashicorp.com/terraform/language\"/><link rel=\"icon\" href=\"/favicon.ico\" sizes=\"48x48\"/><link rel=\"icon\" href=\"/favicon.svg\" sizes=\"any\" type=\"image/svg+xml\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/><meta name=\"google-site-verification\" content=\"zRQZqfAsOX-ypXfU0mzAIzb5rUvj5fA4Zw2jWJRN-JI\"/><meta name=\"ahrefs-site-verification\" content=\"9d739b21c466a84dae91df1c79396081a9072152e64f765c9390fed6f249fb84\"/><meta name=\"zd-site-verification\" content=\"xnvnvv1i0wl4tf0pzaymb\" data-product=\"hcp\"/><meta name=\"zd-site-verification\" content=\"36fp2i1pj67d8cjtzf4fi9\" data-product=\"terraform\"/><meta name=\"zd-site-verification\" content=\"nbe87kfyp1sck7b13vj1i\" data-product=\"packer\"/><meta name=\"zd-site-verification\" content=\"wvsmmqa1cfb9am9zlwfj\" data-product=\"consul\"/><meta name=\"zd-site-verification\" content=\"cw22oopfwr68ujj0n5pn9i\" data-product=\"boundary\"/><meta name=\"zd-site-verification\" content=\"lkslvganwfgfqfwpdeicgh\" data-product=\"vault\"/><meta name=\"zd-site-verification\" content=\"jl2wp2hoej9rz6c8rmqdns\" data-product=\"nomad\"/><meta name=\"zd-site-verification\" content=\"2eb0cu6xceo73t0038m0tb\" data-product=\"waypoint\"/><meta name=\"zd-site-verification\" content=\"n52xisxdf3higs2q9kuogd\" data-product=\"vagrant\"/><meta name=\"twitter:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"prefetch\" href=\"https://www.datadoghq-browser-agent.com/datadog-rum-v4.js\"/><script type=\"application/ld+json\">[{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Developer\",\"item\":\"https://developer.hashicorp.com/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Terraform\",\"item\":\"https://developer.hashicorp.com/terraform\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Configuration Language\",\"item\":\"https://developer.hashicorp.com/terraform/language\"}]}]</script><meta name=\"next-head-count\" content=\"32\"/><link data-next-font=\"\" rel=\"preconnect\" href=\"/\" crossorigin=\"anonymous\"/><link rel=\"preload\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-g=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\"></script><script src=\"/_next/static/chunks/webpack-732b58dfe7855356.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" defer=\"\"></script><script src=\"/_next/static/chunks/framework-f2b655730fee1ac7.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" defer=\"\"></script><script src=\"/_next/static/chunks/mai\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM_CORE official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform_core", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM_CORE.\nStart by understanding the core concepts from this content.\n\nDOC START:\n<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><title>Syntax Overview - Configuration Language | Terraform | HashiCorp Developer</title><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"og:type\" content=\"website\"/><meta property=\"article:publisher\" content=\"https://www.facebook.com/HashiCorp/\"/><meta name=\"twitter:site\" content=\"@HashiCorp\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"theme-color\" content=\"#000\"/><meta name=\"description\" property=\"og:description\" content=\"Terraform language syntax for both the native and JSON variants. Also learn formatting conventions that you can enforce with terraform fmt.\"/><meta name=\"twitter:description\" content=\"Terraform language syntax for both the native and JSON variants. Also learn formatting conventions that you can enforce with terraform fmt.\"/><meta property=\"og:site_name\" content=\"Syntax Overview - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:title\" content=\"Syntax Overview - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"canonical\" href=\"https://developer.hashicorp.com/terraform/language/syntax\"/><link rel=\"icon\" href=\"/favicon.ico\" sizes=\"48x48\"/><link rel=\"icon\" href=\"/favicon.svg\" sizes=\"any\" type=\"image/svg+xml\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/><meta name=\"google-site-verification\" content=\"zRQZqfAsOX-ypXfU0mzAIzb5rUvj5fA4Zw2jWJRN-JI\"/><meta name=\"ahrefs-site-verification\" content=\"9d739b21c466a84dae91df1c79396081a9072152e64f765c9390fed6f249fb84\"/><meta name=\"zd-site-verification\" content=\"xnvnvv1i0wl4tf0pzaymb\" data-product=\"hcp\"/><meta name=\"zd-site-verification\" content=\"36fp2i1pj67d8cjtzf4fi9\" data-product=\"terraform\"/><meta name=\"zd-site-verification\" content=\"nbe87kfyp1sck7b13vj1i\" data-product=\"packer\"/><meta name=\"zd-site-verification\" content=\"wvsmmqa1cfb9am9zlwfj\" data-product=\"consul\"/><meta name=\"zd-site-verification\" content=\"cw22oopfwr68ujj0n5pn9i\" data-product=\"boundary\"/><meta name=\"zd-site-verification\" content=\"lkslvganwfgfqfwpdeicgh\" data-product=\"vault\"/><meta name=\"zd-site-verification\" content=\"jl2wp2hoej9rz6c8rmqdns\" data-product=\"nomad\"/><meta name=\"zd-site-verification\" content=\"2eb0cu6xceo73t0038m0tb\" data-product=\"waypoint\"/><meta name=\"zd-site-verification\" content=\"n52xisxdf3higs2q9kuogd\" data-product=\"vagrant\"/><meta name=\"twitter:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"prefetch\" href=\"https://www.datadoghq-browser-agent.com/datadog-rum-v4.js\"/><script type=\"application/ld+json\">[{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Developer\",\"item\":\"https://developer.hashicorp.com/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Terraform\",\"item\":\"https://developer.hashicorp.com/terraform\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Configuration Language\",\"item\":\"https://developer.hashicorp.com/terraform/language\"},{\"@type\":\"ListItem\",\"position\":4,\"name\":\"Syntax\",\"item\":\"https://developer.hashicorp.com/terraform/language/syntax\"}]}]</script><meta name=\"next-head-count\" content=\"32\"/><link data-next-font=\"\" rel=\"preconnect\" href=\"/\" crossorigin=\"anonymous\"/><link rel=\"preload\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-g=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\"></script><script src=\"/_next/static/chunks/webpack-\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM_CORE official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform_core", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM_CORE.\nStart by understanding the core concepts from this content.\n\nDOC START:\n<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><title>Modules Overview - Configuration Language | Terraform | HashiCorp Developer</title><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"og:type\" content=\"website\"/><meta property=\"article:publisher\" content=\"https://www.facebook.com/HashiCorp/\"/><meta name=\"twitter:site\" content=\"@HashiCorp\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"theme-color\" content=\"#000\"/><meta name=\"description\" property=\"og:description\" content=\"Modules are containers for multiple resources that are used together in a configuration. Find resources for using, developing, and publishing modules.\"/><meta name=\"twitter:description\" content=\"Modules are containers for multiple resources that are used together in a configuration. Find resources for using, developing, and publishing modules.\"/><meta property=\"og:site_name\" content=\"Modules Overview - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:title\" content=\"Modules Overview - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"canonical\" href=\"https://developer.hashicorp.com/terraform/language/modules\"/><link rel=\"icon\" href=\"/favicon.ico\" sizes=\"48x48\"/><link rel=\"icon\" href=\"/favicon.svg\" sizes=\"any\" type=\"image/svg+xml\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/><meta name=\"google-site-verification\" content=\"zRQZqfAsOX-ypXfU0mzAIzb5rUvj5fA4Zw2jWJRN-JI\"/><meta name=\"ahrefs-site-verification\" content=\"9d739b21c466a84dae91df1c79396081a9072152e64f765c9390fed6f249fb84\"/><meta name=\"zd-site-verification\" content=\"xnvnvv1i0wl4tf0pzaymb\" data-product=\"hcp\"/><meta name=\"zd-site-verification\" content=\"36fp2i1pj67d8cjtzf4fi9\" data-product=\"terraform\"/><meta name=\"zd-site-verification\" content=\"nbe87kfyp1sck7b13vj1i\" data-product=\"packer\"/><meta name=\"zd-site-verification\" content=\"wvsmmqa1cfb9am9zlwfj\" data-product=\"consul\"/><meta name=\"zd-site-verification\" content=\"cw22oopfwr68ujj0n5pn9i\" data-product=\"boundary\"/><meta name=\"zd-site-verification\" content=\"lkslvganwfgfqfwpdeicgh\" data-product=\"vault\"/><meta name=\"zd-site-verification\" content=\"jl2wp2hoej9rz6c8rmqdns\" data-product=\"nomad\"/><meta name=\"zd-site-verification\" content=\"2eb0cu6xceo73t0038m0tb\" data-product=\"waypoint\"/><meta name=\"zd-site-verification\" content=\"n52xisxdf3higs2q9kuogd\" data-product=\"vagrant\"/><meta name=\"twitter:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"prefetch\" href=\"https://www.datadoghq-browser-agent.com/datadog-rum-v4.js\"/><script type=\"application/ld+json\">[{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Developer\",\"item\":\"https://developer.hashicorp.com/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Terraform\",\"item\":\"https://developer.hashicorp.com/terraform\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Configuration Language\",\"item\":\"https://developer.hashicorp.com/terraform/language\"},{\"@type\":\"ListItem\",\"position\":4,\"name\":\"Modules\",\"item\":\"https://developer.hashicorp.com/terraform/language/modules\"}]}]</script><meta name=\"next-head-count\" content=\"32\"/><link data-next-font=\"\" rel=\"preconnect\" href=\"/\" crossorigin=\"anonymous\"/><link rel=\"preload\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-g=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\"></script><script src=\"/\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM_CORE official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform_core", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM_CORE.\nStart by understanding the core concepts from this content.\n\nDOC START:\n<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><title>Variables and Outputs | Terraform | HashiCorp Developer</title><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"og:type\" content=\"website\"/><meta property=\"article:publisher\" content=\"https://www.facebook.com/HashiCorp/\"/><meta name=\"twitter:site\" content=\"@HashiCorp\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"theme-color\" content=\"#000\"/><meta name=\"description\" property=\"og:description\" content=\"An overview of input variables, output values, and local values in Terraform language.\"/><meta name=\"twitter:description\" content=\"An overview of input variables, output values, and local values in Terraform language.\"/><meta property=\"og:site_name\" content=\"Variables and Outputs | Terraform | HashiCorp Developer\"/><meta property=\"og:title\" content=\"Variables and Outputs | Terraform | HashiCorp Developer\"/><meta property=\"og:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"canonical\" href=\"https://developer.hashicorp.com/terraform/language/values\"/><link rel=\"icon\" href=\"/favicon.ico\" sizes=\"48x48\"/><link rel=\"icon\" href=\"/favicon.svg\" sizes=\"any\" type=\"image/svg+xml\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/><meta name=\"google-site-verification\" content=\"zRQZqfAsOX-ypXfU0mzAIzb5rUvj5fA4Zw2jWJRN-JI\"/><meta name=\"ahrefs-site-verification\" content=\"9d739b21c466a84dae91df1c79396081a9072152e64f765c9390fed6f249fb84\"/><meta name=\"zd-site-verification\" content=\"xnvnvv1i0wl4tf0pzaymb\" data-product=\"hcp\"/><meta name=\"zd-site-verification\" content=\"36fp2i1pj67d8cjtzf4fi9\" data-product=\"terraform\"/><meta name=\"zd-site-verification\" content=\"nbe87kfyp1sck7b13vj1i\" data-product=\"packer\"/><meta name=\"zd-site-verification\" content=\"wvsmmqa1cfb9am9zlwfj\" data-product=\"consul\"/><meta name=\"zd-site-verification\" content=\"cw22oopfwr68ujj0n5pn9i\" data-product=\"boundary\"/><meta name=\"zd-site-verification\" content=\"lkslvganwfgfqfwpdeicgh\" data-product=\"vault\"/><meta name=\"zd-site-verification\" content=\"jl2wp2hoej9rz6c8rmqdns\" data-product=\"nomad\"/><meta name=\"zd-site-verification\" content=\"2eb0cu6xceo73t0038m0tb\" data-product=\"waypoint\"/><meta name=\"zd-site-verification\" content=\"n52xisxdf3higs2q9kuogd\" data-product=\"vagrant\"/><meta name=\"twitter:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"prefetch\" href=\"https://www.datadoghq-browser-agent.com/datadog-rum-v4.js\"/><script type=\"application/ld+json\">[{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Developer\",\"item\":\"https://developer.hashicorp.com/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Terraform\",\"item\":\"https://developer.hashicorp.com/terraform\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Configuration Language\",\"item\":\"https://developer.hashicorp.com/terraform/language\"},{\"@type\":\"ListItem\",\"position\":4,\"name\":\"Variables and Outputs\",\"item\":\"https://developer.hashicorp.com/terraform/language/values\"}]}]</script><meta name=\"next-head-count\" content=\"32\"/><link data-next-font=\"\" rel=\"preconnect\" href=\"/\" crossorigin=\"anonymous\"/><link rel=\"preload\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-g=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\"></script><script src=\"/_next/static/chunks/webpack-732b58dfe7855356.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" defer=\"\"></script><script src=\"/_next/static/chunks/framework-f2b655730fee1ac7.js?dpl=dpl_\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM_CORE official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform_core", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM_CORE.\nStart by understanding the core concepts from this content.\n\nDOC START:\n<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><title>State | Terraform | HashiCorp Developer</title><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"og:type\" content=\"website\"/><meta property=\"article:publisher\" content=\"https://www.facebook.com/HashiCorp/\"/><meta name=\"twitter:site\" content=\"@HashiCorp\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"theme-color\" content=\"#000\"/><meta name=\"description\" property=\"og:description\" content=\"An introduction to state, information that Terraform uses to map resources to a configuration, track metadata, and improve performance.\"/><meta name=\"twitter:description\" content=\"An introduction to state, information that Terraform uses to map resources to a configuration, track metadata, and improve performance.\"/><meta property=\"og:site_name\" content=\"State | Terraform | HashiCorp Developer\"/><meta property=\"og:title\" content=\"State | Terraform | HashiCorp Developer\"/><meta property=\"og:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"canonical\" href=\"https://developer.hashicorp.com/terraform/language/state\"/><link rel=\"icon\" href=\"/favicon.ico\" sizes=\"48x48\"/><link rel=\"icon\" href=\"/favicon.svg\" sizes=\"any\" type=\"image/svg+xml\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/><meta name=\"google-site-verification\" content=\"zRQZqfAsOX-ypXfU0mzAIzb5rUvj5fA4Zw2jWJRN-JI\"/><meta name=\"ahrefs-site-verification\" content=\"9d739b21c466a84dae91df1c79396081a9072152e64f765c9390fed6f249fb84\"/><meta name=\"zd-site-verification\" content=\"xnvnvv1i0wl4tf0pzaymb\" data-product=\"hcp\"/><meta name=\"zd-site-verification\" content=\"36fp2i1pj67d8cjtzf4fi9\" data-product=\"terraform\"/><meta name=\"zd-site-verification\" content=\"nbe87kfyp1sck7b13vj1i\" data-product=\"packer\"/><meta name=\"zd-site-verification\" content=\"wvsmmqa1cfb9am9zlwfj\" data-product=\"consul\"/><meta name=\"zd-site-verification\" content=\"cw22oopfwr68ujj0n5pn9i\" data-product=\"boundary\"/><meta name=\"zd-site-verification\" content=\"lkslvganwfgfqfwpdeicgh\" data-product=\"vault\"/><meta name=\"zd-site-verification\" content=\"jl2wp2hoej9rz6c8rmqdns\" data-product=\"nomad\"/><meta name=\"zd-site-verification\" content=\"2eb0cu6xceo73t0038m0tb\" data-product=\"waypoint\"/><meta name=\"zd-site-verification\" content=\"n52xisxdf3higs2q9kuogd\" data-product=\"vagrant\"/><meta name=\"twitter:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"prefetch\" href=\"https://www.datadoghq-browser-agent.com/datadog-rum-v4.js\"/><script type=\"application/ld+json\">[{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Developer\",\"item\":\"https://developer.hashicorp.com/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Terraform\",\"item\":\"https://developer.hashicorp.com/terraform\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Configuration Language\",\"item\":\"https://developer.hashicorp.com/terraform/language\"},{\"@type\":\"ListItem\",\"position\":4,\"name\":\"State\",\"item\":\"https://developer.hashicorp.com/terraform/language/state\"}]}]</script><meta name=\"next-head-count\" content=\"32\"/><link data-next-font=\"\" rel=\"preconnect\" href=\"/\" crossorigin=\"anonymous\"/><link rel=\"preload\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-g=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\"></script><script src=\"/_next/static/chunks/webpack-732b58dfe7855356.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" defer=\"\"></script><script src=\"/_next/static/chunks/framew\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM_CORE official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform_core", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM_CORE.\nStart by understanding the core concepts from this content.\n\nDOC START:\n<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><title>Resources Overview - Configuration Language | Terraform | HashiCorp Developer</title><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"og:type\" content=\"website\"/><meta property=\"article:publisher\" content=\"https://www.facebook.com/HashiCorp/\"/><meta name=\"twitter:site\" content=\"@HashiCorp\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"theme-color\" content=\"#000\"/><meta name=\"description\" property=\"og:description\" content=\"Resources describe infrastructure objects in Terraform configurations. Find documentation for resource syntax, behavior, and meta-arguments.\"/><meta name=\"twitter:description\" content=\"Resources describe infrastructure objects in Terraform configurations. Find documentation for resource syntax, behavior, and meta-arguments.\"/><meta property=\"og:site_name\" content=\"Resources Overview - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:title\" content=\"Resources Overview - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"canonical\" href=\"https://developer.hashicorp.com/terraform/language/resources\"/><link rel=\"icon\" href=\"/favicon.ico\" sizes=\"48x48\"/><link rel=\"icon\" href=\"/favicon.svg\" sizes=\"any\" type=\"image/svg+xml\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/><meta name=\"google-site-verification\" content=\"zRQZqfAsOX-ypXfU0mzAIzb5rUvj5fA4Zw2jWJRN-JI\"/><meta name=\"ahrefs-site-verification\" content=\"9d739b21c466a84dae91df1c79396081a9072152e64f765c9390fed6f249fb84\"/><meta name=\"zd-site-verification\" content=\"xnvnvv1i0wl4tf0pzaymb\" data-product=\"hcp\"/><meta name=\"zd-site-verification\" content=\"36fp2i1pj67d8cjtzf4fi9\" data-product=\"terraform\"/><meta name=\"zd-site-verification\" content=\"nbe87kfyp1sck7b13vj1i\" data-product=\"packer\"/><meta name=\"zd-site-verification\" content=\"wvsmmqa1cfb9am9zlwfj\" data-product=\"consul\"/><meta name=\"zd-site-verification\" content=\"cw22oopfwr68ujj0n5pn9i\" data-product=\"boundary\"/><meta name=\"zd-site-verification\" content=\"lkslvganwfgfqfwpdeicgh\" data-product=\"vault\"/><meta name=\"zd-site-verification\" content=\"jl2wp2hoej9rz6c8rmqdns\" data-product=\"nomad\"/><meta name=\"zd-site-verification\" content=\"2eb0cu6xceo73t0038m0tb\" data-product=\"waypoint\"/><meta name=\"zd-site-verification\" content=\"n52xisxdf3higs2q9kuogd\" data-product=\"vagrant\"/><meta name=\"twitter:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"prefetch\" href=\"https://www.datadoghq-browser-agent.com/datadog-rum-v4.js\"/><script type=\"application/ld+json\">[{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Developer\",\"item\":\"https://developer.hashicorp.com/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Terraform\",\"item\":\"https://developer.hashicorp.com/terraform\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Configuration Language\",\"item\":\"https://developer.hashicorp.com/terraform/language\"},{\"@type\":\"ListItem\",\"position\":4,\"name\":\"Resources\",\"item\":\"https://developer.hashicorp.com/terraform/language/resources\"}]}]</script><meta name=\"next-head-count\" content=\"32\"/><link data-next-font=\"\" rel=\"preconnect\" href=\"/\" crossorigin=\"anonymous\"/><link rel=\"preload\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-g=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\"></script><script src=\"/_next/st\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM_CORE official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform_core", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM_CORE.\nStart by understanding the core concepts from this content.\n\nDOC START:\n<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><title>Data Sources - Configuration Language | Terraform | HashiCorp Developer</title><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"og:type\" content=\"website\"/><meta property=\"article:publisher\" content=\"https://www.facebook.com/HashiCorp/\"/><meta name=\"twitter:site\" content=\"@HashiCorp\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"theme-color\" content=\"#000\"/><meta name=\"description\" property=\"og:description\" content=\"Data sources allow Terraform to use external data, function output, and data from other configurations. Learn data resource arguments, behavior, and lifecycle.\"/><meta name=\"twitter:description\" content=\"Data sources allow Terraform to use external data, function output, and data from other configurations. Learn data resource arguments, behavior, and lifecycle.\"/><meta property=\"og:site_name\" content=\"Data Sources - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:title\" content=\"Data Sources - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"canonical\" href=\"https://developer.hashicorp.com/terraform/language/data-sources\"/><link rel=\"icon\" href=\"/favicon.ico\" sizes=\"48x48\"/><link rel=\"icon\" href=\"/favicon.svg\" sizes=\"any\" type=\"image/svg+xml\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/><meta name=\"google-site-verification\" content=\"zRQZqfAsOX-ypXfU0mzAIzb5rUvj5fA4Zw2jWJRN-JI\"/><meta name=\"ahrefs-site-verification\" content=\"9d739b21c466a84dae91df1c79396081a9072152e64f765c9390fed6f249fb84\"/><meta name=\"zd-site-verification\" content=\"xnvnvv1i0wl4tf0pzaymb\" data-product=\"hcp\"/><meta name=\"zd-site-verification\" content=\"36fp2i1pj67d8cjtzf4fi9\" data-product=\"terraform\"/><meta name=\"zd-site-verification\" content=\"nbe87kfyp1sck7b13vj1i\" data-product=\"packer\"/><meta name=\"zd-site-verification\" content=\"wvsmmqa1cfb9am9zlwfj\" data-product=\"consul\"/><meta name=\"zd-site-verification\" content=\"cw22oopfwr68ujj0n5pn9i\" data-product=\"boundary\"/><meta name=\"zd-site-verification\" content=\"lkslvganwfgfqfwpdeicgh\" data-product=\"vault\"/><meta name=\"zd-site-verification\" content=\"jl2wp2hoej9rz6c8rmqdns\" data-product=\"nomad\"/><meta name=\"zd-site-verification\" content=\"2eb0cu6xceo73t0038m0tb\" data-product=\"waypoint\"/><meta name=\"zd-site-verification\" content=\"n52xisxdf3higs2q9kuogd\" data-product=\"vagrant\"/><meta name=\"twitter:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"prefetch\" href=\"https://www.datadoghq-browser-agent.com/datadog-rum-v4.js\"/><script type=\"application/ld+json\">[{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Developer\",\"item\":\"https://developer.hashicorp.com/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Terraform\",\"item\":\"https://developer.hashicorp.com/terraform\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Configuration Language\",\"item\":\"https://developer.hashicorp.com/terraform/language\"},{\"@type\":\"ListItem\",\"position\":4,\"name\":\"Data Sources\",\"item\":\"https://developer.hashicorp.com/terraform/language/data-sources\"}]}]</script><meta name=\"next-head-count\" content=\"32\"/><link data-next-font=\"\" rel=\"preconnect\" href=\"/\" crossorigin=\"anonymous\"/><link rel=\"preload\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-g=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\"></\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM_CORE official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform_core", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM_CORE.\nStart by understanding the core concepts from this content.\n\nDOC START:\n<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><title>Functions - Configuration Language | Terraform | HashiCorp Developer</title><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"og:type\" content=\"website\"/><meta property=\"article:publisher\" content=\"https://www.facebook.com/HashiCorp/\"/><meta name=\"twitter:site\" content=\"@HashiCorp\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"theme-color\" content=\"#000\"/><meta name=\"description\" property=\"og:description\" content=\"An introduction to the built-in functions that you can use to transform and combine values in expressions.\"/><meta name=\"twitter:description\" content=\"An introduction to the built-in functions that you can use to transform and combine values in expressions.\"/><meta property=\"og:site_name\" content=\"Functions - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:title\" content=\"Functions - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"canonical\" href=\"https://developer.hashicorp.com/terraform/language/functions\"/><link rel=\"icon\" href=\"/favicon.ico\" sizes=\"48x48\"/><link rel=\"icon\" href=\"/favicon.svg\" sizes=\"any\" type=\"image/svg+xml\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/><meta name=\"google-site-verification\" content=\"zRQZqfAsOX-ypXfU0mzAIzb5rUvj5fA4Zw2jWJRN-JI\"/><meta name=\"ahrefs-site-verification\" content=\"9d739b21c466a84dae91df1c79396081a9072152e64f765c9390fed6f249fb84\"/><meta name=\"zd-site-verification\" content=\"xnvnvv1i0wl4tf0pzaymb\" data-product=\"hcp\"/><meta name=\"zd-site-verification\" content=\"36fp2i1pj67d8cjtzf4fi9\" data-product=\"terraform\"/><meta name=\"zd-site-verification\" content=\"nbe87kfyp1sck7b13vj1i\" data-product=\"packer\"/><meta name=\"zd-site-verification\" content=\"wvsmmqa1cfb9am9zlwfj\" data-product=\"consul\"/><meta name=\"zd-site-verification\" content=\"cw22oopfwr68ujj0n5pn9i\" data-product=\"boundary\"/><meta name=\"zd-site-verification\" content=\"lkslvganwfgfqfwpdeicgh\" data-product=\"vault\"/><meta name=\"zd-site-verification\" content=\"jl2wp2hoej9rz6c8rmqdns\" data-product=\"nomad\"/><meta name=\"zd-site-verification\" content=\"2eb0cu6xceo73t0038m0tb\" data-product=\"waypoint\"/><meta name=\"zd-site-verification\" content=\"n52xisxdf3higs2q9kuogd\" data-product=\"vagrant\"/><meta name=\"twitter:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"prefetch\" href=\"https://www.datadoghq-browser-agent.com/datadog-rum-v4.js\"/><script type=\"application/ld+json\">[{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Developer\",\"item\":\"https://developer.hashicorp.com/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Terraform\",\"item\":\"https://developer.hashicorp.com/terraform\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Configuration Language\",\"item\":\"https://developer.hashicorp.com/terraform/language\"},{\"@type\":\"ListItem\",\"position\":4,\"name\":\"Functions\",\"item\":\"https://developer.hashicorp.com/terraform/language/functions\"}]}]</script><meta name=\"next-head-count\" content=\"32\"/><link data-next-font=\"\" rel=\"preconnect\" href=\"/\" crossorigin=\"anonymous\"/><link rel=\"preload\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-g=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\"></script><script src=\"/_next/static/chunks/webpack-732b58dfe7855356.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" defer=\"\"></script\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM_CORE official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform_core", "type": "documentation_ingestion"}
{"prompt": "\nYou are learning from the official documentation of TERRAFORM_CORE.\nStart by understanding the core concepts from this content.\n\nDOC START:\n<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><title>Expressions - Configuration Language | Terraform | HashiCorp Developer</title><meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"og:type\" content=\"website\"/><meta property=\"article:publisher\" content=\"https://www.facebook.com/HashiCorp/\"/><meta name=\"twitter:site\" content=\"@HashiCorp\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"theme-color\" content=\"#000\"/><meta name=\"description\" property=\"og:description\" content=\"An overview of expressions to reference or compute values in Terraform configurations, including types, operators, and functions.\"/><meta name=\"twitter:description\" content=\"An overview of expressions to reference or compute values in Terraform configurations, including types, operators, and functions.\"/><meta property=\"og:site_name\" content=\"Expressions - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:title\" content=\"Expressions - Configuration Language | Terraform | HashiCorp Developer\"/><meta property=\"og:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"canonical\" href=\"https://developer.hashicorp.com/terraform/language/expressions\"/><link rel=\"icon\" href=\"/favicon.ico\" sizes=\"48x48\"/><link rel=\"icon\" href=\"/favicon.svg\" sizes=\"any\" type=\"image/svg+xml\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/><meta name=\"google-site-verification\" content=\"zRQZqfAsOX-ypXfU0mzAIzb5rUvj5fA4Zw2jWJRN-JI\"/><meta name=\"ahrefs-site-verification\" content=\"9d739b21c466a84dae91df1c79396081a9072152e64f765c9390fed6f249fb84\"/><meta name=\"zd-site-verification\" content=\"xnvnvv1i0wl4tf0pzaymb\" data-product=\"hcp\"/><meta name=\"zd-site-verification\" content=\"36fp2i1pj67d8cjtzf4fi9\" data-product=\"terraform\"/><meta name=\"zd-site-verification\" content=\"nbe87kfyp1sck7b13vj1i\" data-product=\"packer\"/><meta name=\"zd-site-verification\" content=\"wvsmmqa1cfb9am9zlwfj\" data-product=\"consul\"/><meta name=\"zd-site-verification\" content=\"cw22oopfwr68ujj0n5pn9i\" data-product=\"boundary\"/><meta name=\"zd-site-verification\" content=\"lkslvganwfgfqfwpdeicgh\" data-product=\"vault\"/><meta name=\"zd-site-verification\" content=\"jl2wp2hoej9rz6c8rmqdns\" data-product=\"nomad\"/><meta name=\"zd-site-verification\" content=\"2eb0cu6xceo73t0038m0tb\" data-product=\"waypoint\"/><meta name=\"zd-site-verification\" content=\"n52xisxdf3higs2q9kuogd\" data-product=\"vagrant\"/><meta name=\"twitter:image\" content=\"https://developer.hashicorp.com/og-image/terraform.jpg\"/><link rel=\"prefetch\" href=\"https://www.datadoghq-browser-agent.com/datadog-rum-v4.js\"/><script type=\"application/ld+json\">[{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Developer\",\"item\":\"https://developer.hashicorp.com/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Terraform\",\"item\":\"https://developer.hashicorp.com/terraform\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Configuration Language\",\"item\":\"https://developer.hashicorp.com/terraform/language\"},{\"@type\":\"ListItem\",\"position\":4,\"name\":\"Expressions\",\"item\":\"https://developer.hashicorp.com/terraform/language/expressions\"}]}]</script><meta name=\"next-head-count\" content=\"32\"/><link data-next-font=\"\" rel=\"preconnect\" href=\"/\" crossorigin=\"anonymous\"/><link rel=\"preload\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/f49486d3b4c819d0.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-g=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a3fdbf75d77eb323.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/a23a4d5b8f64c49d.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/686380cdcb9bba8b.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/0d0717cf606673e4.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><link rel=\"preload\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/7e4ee183b358eb93.css?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js?dpl=dpl_3kQ4H5TGXYTHfgkrakGSXPDjnFEm\"></script><script src=\"/_next/static/chunks/webpack-732b58dfe7855356.\nDOC END\n\nBased on this official documentation, please:\n\n1. **Summarize Key Points**: Extract the main concepts, features, and capabilities\n2. **List Tools/APIs**: Identify any command-line tools, APIs, or interfaces mentioned\n3. **Highlight Best Practices**: Note any recommended practices, patterns, or guidelines\n4. **Identify Warnings**: Capture any warnings, limitations, or common pitfalls\n5. **Extract Examples**: Note any code examples or configuration samples\n6. **Security Considerations**: Identify any security-related information\n7. **Performance Notes**: Capture any performance considerations or optimizations\n\nFormat your response as structured markdown with clear sections.\n", "response": "\nBased on the TERRAFORM_CORE official documentation:\n\n## Key Points Summary\n[Extracted key concepts and features]\n\n## Tools and APIs\n[List of command-line tools, APIs, and interfaces]\n\n## Best Practices\n[Recommended practices and patterns]\n\n## Warnings and Limitations\n[Common pitfalls and limitations]\n\n## Security Considerations\n[Security-related information and recommendations]\n\n## Performance Notes\n[Performance considerations and optimizations]\n", "technology": "terraform_core", "type": "documentation_ingestion"}
